1
00:00:00,000 --> 00:00:13,440
 I'm quite familiar with these three courses and I'm teaching two of them. On Monday, 6401,

2
00:00:13,440 --> 00:00:22,520
 now the 7402. But this year it happened to be probably due to too many courses and we

3
00:00:22,520 --> 00:00:31,480
 have quite a number of courses on the same day. That's why 6401 and 7401 happened to

4
00:00:31,480 --> 00:00:41,400
 be on the same Monday. But anyway, this course is on a different day. Otherwise, I cannot

5
00:00:41,400 --> 00:00:50,400
 teach both courses at the same time. I never taught the 7401.

6
00:00:50,400 --> 00:00:58,519
 So just a very quick introduction. As you can see, my name is Lin Chih-ping and the

7
00:00:58,519 --> 00:01:10,880
 office telephone and the email is there. So I think now it's still the edge of theory.

8
00:01:10,880 --> 00:01:23,640
 Over the past few days, we are talking about statistical signal processing. We also monitor

9
00:01:23,640 --> 00:01:32,560
 the statistics of the course registration. Now it's more stable because I'm involved

10
00:01:32,560 --> 00:01:42,400
 in the signal processing and machine learning MSc program. I think some of you should know

11
00:01:42,400 --> 00:01:56,120
 me also. Because as I said, this week is still at 12 and then some students are also in the

12
00:01:57,120 --> 00:02:09,800
 They may fear not sure whether they can take. So I usually, we want to try to make it slower

13
00:02:09,800 --> 00:02:21,080
 and also not to go into too much detail. So I hope those of you who maybe already know

14
00:02:21,080 --> 00:02:34,840
 this topic well, perhaps can bear with me on that. Also, I want to try to use this opportunity,

15
00:02:34,840 --> 00:02:43,680
as I say, being a bit slower. I want to get some feedback from you also, say what's your

16
00:02:43,680 --> 00:02:59,680
 expectations and how much you fear about this course and so on. In relation with other courses,

17
00:02:59,680 --> 00:03:12,280
 6401, which I was teaching this Monday, and also 7401, as I say, happened to be also on Monday.

18
00:03:12,280 --> 00:03:33,560
 Okay, so let's start. It's time to start now. Let me just have a quick show on this because from

19
00:03:33,560 --> 00:03:44,200
 student point of view, they always want to know, you see, like what's the exams and the CA here is a

20
00:03:44,200 --> 00:03:54,560
 continuous assessment. So normally, it would be either assignment or quiz or both. So yeah,

21
00:03:55,400 --> 00:04:06,120
 is a very, very standard percentage in terms of percentage, except a few courses, very few in the

22
00:04:06,120 --> 00:04:12,680
 EE. Some other school, actually I noticed the variation is quite large, you know, so from again,

23
00:04:12,680 --> 00:04:21,480
 from statistical point of view, variation is a very important indicator, you see. So if you take

24
00:04:21,480 --> 00:04:29,880
 about percentage is one thing, that's more talking about average, but variation. So let

25
00:04:29,880 --> 00:04:39,920
 me explain a little bit more why. Usually for final exams, EE courses mostly follow the 60%

26
00:04:39,920 --> 00:04:48,480
 and 40%, but I noticed some other school, the MSc course, they have sometimes final exam only 30%,

27
00:04:48,840 --> 00:04:59,160
 40%, or sometimes zero, so purely based on continuous CA. And in EE, we did have a course

28
00:04:59,160 --> 00:05:10,720
 starting from last semester, still ongoing, 6405, the NLP, Natural Language Processing. There is no

29
00:05:10,720 --> 00:05:16,560
 final exam. So I saw that most students maybe want to go for that, but it turned out to be the

30
00:05:16,560 --> 00:05:29,520
 the, currently the class size is not that large, but because that course, every week you have the

31
00:05:29,520 --> 00:05:35,040
 CA, you see, continuous assessment. So it's the real meaning of continuous assessment. You cannot

32
00:05:35,040 --> 00:05:41,400
 miss, you cannot do it online or do it at home. If you miss day one, you will reduce a certain

33
00:05:41,400 --> 00:05:48,320
 percentage counting towards the final exam. Okay, so that's one thing. So final exam, usually,

34
00:05:48,320 --> 00:05:54,799
 you go back in the past year, we had five questions, but in the past two or three years,

35
00:05:54,799 --> 00:06:02,760
 you can go for four questions, four or five. So most of the paper now is four questions.

36
00:06:02,760 --> 00:06:12,719
 And in this course, I happen to have like two parts, which I will go into that later. So we

37
00:06:12,719 --> 00:06:24,560
 will have four questions equal in each part. In terms of CA, because 40%, then according to the

38
00:06:24,560 --> 00:06:35,600
 regulation, each component cannot be more than 20. So you see, minimum number of CA will be two,

39
00:06:35,600 --> 00:06:47,720
 but maximum, like just now, the early one, the every week one thing could be maximum 13, but not

40
00:06:47,720 --> 00:06:56,240
 that likely. But it's quite common to have three or four in other MSc course. Or we say this is

41
00:06:56,240 --> 00:07:06,840
 7,000 series, it's more like research student course, 7,000. Okay, so yeah, but that thing,

42
00:07:06,840 --> 00:07:14,480
 we only have two components, but each is, must be 20%, because it cannot be smaller than 20,

43
00:07:14,480 --> 00:07:22,400
 and you have two components, you don't know. So it's equal. So I already follow the path,

44
00:07:22,400 --> 00:07:33,960
 will be just one homework assignment. So I hope now, maybe not that easy to use the CHED GPT to do.

45
00:07:33,960 --> 00:07:43,760
 So yeah, anyway, let's see how it goes this year in the past, seems okay. Because this course is

46
00:07:43,760 --> 00:07:53,280
 only offer once per year, not every semester. I think it's not likely to repeat next semester,

47
00:07:53,280 --> 00:08:01,400
 but we'll see how. So for those considering, say, or you may take next semester, then may,

48
00:08:01,400 --> 00:08:13,280
 that may not happen. But every year, it will repeat. Okay, so I plan to do it in week six,

49
00:08:13,280 --> 00:08:20,520
 so that after basically, after I finish the part one, I plan to do part one in about five weeks,

50
00:08:20,520 --> 00:08:31,280
 depending on the progress. Because this year, at least so far, we should have over 100 students,

51
00:08:31,280 --> 00:08:38,799
 according to the column. Actually, more than that, maybe 180 so far, but I don't know whether,

52
00:08:38,799 --> 00:08:48,000
 and so on. And yeah, so we have a much larger number of students. So we do this and part two

53
00:08:48,000 --> 00:08:58,199
 will be a quiz for this part two, and this will be week 11. So just make it very clear so that in case

54
00:08:58,360 --> 00:09:07,440
 you decide to take this course and you are maybe planning some overseas trips or so on,

55
00:09:07,440 --> 00:09:20,320
 then you at least avoid this physical, yeah, this is in class. Okay, so it should be clear.

56
00:09:20,800 --> 00:09:34,480
 Any question about this? We will go into more detail later. Okay, so that's at least give you

57
00:09:34,480 --> 00:09:47,920
 some idea. It's quite similar to the 6401, the Vance DSP, except the 6401 is taught by two lecturers.

58
00:09:48,319 --> 00:09:58,120
 For this course, yeah, I should say I am the only one teaching the whole course for this one. Maybe

59
00:09:58,120 --> 00:10:09,240
 to make it a little bit informal, I'll give you a quick quiz from you. How many years have I been

60
00:10:09,240 --> 00:10:23,720
 teaching this course by myself? Can you, just a quick guess? Well, very close, so you are doing

61
00:10:23,720 --> 00:10:32,360
 a good parameter estimation. Yeah, estimate this. So this is one of the numbers where estimate,

62
00:10:32,360 --> 00:10:39,480
 although this is a very simple one, I'll just guess. Yeah, so if you already want to get the

63
00:10:39,480 --> 00:10:48,040
 better estimation, I mean, of course, that's research. You can, for example, do an internet

64
00:10:48,040 --> 00:10:56,480
 search and get the CV, then I might at least when I began to teach, you know, which course. So 15

65
00:10:56,480 --> 00:11:05,240
 years is about the time. And that's also when this course was proposed. We start, I think some of

66
00:11:05,240 --> 00:11:13,920
 you may hear from you, me, during the orientations about MSc program. We started this program many,

67
00:11:13,920 --> 00:11:21,640
 25 years, and that changed a lot. And initially, many years ago, there was no 7,000 series.

68
00:11:21,640 --> 00:11:35,199
 Everything is 6401, 6402, and so on. And about 15 years ago, we arrived, we converted this course,

69
00:11:35,199 --> 00:11:43,000
 initially, the same name, the same title, statistical signal processing from MSc to PhD.

70
00:11:43,000 --> 00:11:53,200
 And for several years, it was mainly, it was only for PhD and MSc students, not open to, at the time,

71
00:11:53,200 --> 00:12:03,040
 the courses are separate. MSc course for MSc students, 7,000 for PhD and MSc students. But

72
00:12:03,040 --> 00:12:14,079
 the class size is always very small, whether it's for MSc student or research student. So,

73
00:12:14,079 --> 00:12:20,959
 because I taught the 6,000 series even before this 7402 for some year, but only for part of it,

74
00:12:20,959 --> 00:12:29,280
 for the parameter estimation. Yeah, for that one, I taught about 20 years. And the detection one is

75
00:12:29,280 --> 00:12:39,000
 the co-lecturer, he resigned from NTU, then took over. So, that's been teaching this one for so many

76
00:12:39,000 --> 00:12:50,560
 years. Okay, so I hope at least my experience maybe will help you. Because also, this is,

77
00:12:50,560 --> 00:12:57,680
 this course, as you can see, is more towards mathematical. So, mathematical, not like AI or those,

78
00:12:57,680 --> 00:13:08,319
 you know, machine learning, it change, change a lot. This one is changing a little bit slower

79
00:13:08,320 --> 00:13:15,320
 Okay, so let's start now. Let's see what we have.

80
00:13:39,320 --> 00:13:51,800
 Okay, so you can see here, we have also mainly part one and part two later, mentioned a little

81
00:13:51,800 --> 00:14:02,040
 bit on part three, which is just for introductory, not for the CA and exams. So, we will have part

82
00:14:02,319 --> 00:14:12,719
 one starting with estimation, estimation theory, followed by the detection theory. So, these two

83
00:14:12,719 --> 00:14:20,719
 part follow the, we should say, actually it's two books, but we call it as the same book,

84
00:14:20,719 --> 00:14:29,560
 all fundamental statistical signal processing, making it volume one is one book, volume two is

85
00:14:29,560 --> 00:14:40,280
 another book, but the same author, it's a single author. And then, this is more recent, like reference,

86
00:14:40,280 --> 00:14:50,760
 but I always use these two books. And our library, I don't know whether you know, I'm not there,

87
00:14:50,960 --> 00:14:59,160
 all those, thank you my course. In library, once you go in the level one, you have the reserved rooms.

88
00:14:59,160 --> 00:15:11,120
 That one, normally it keeps the textbook where students or staff cannot borrow for too long,

89
00:15:11,480 --> 00:15:21,840
 so to make sure everyone is there, you can go there. So, I request, I didn't check whether

90
00:15:21,840 --> 00:15:29,040
 it's still there or not, it's here. Hopefully it's there. So, you can go there to take a look at these two

91
00:15:29,040 --> 00:15:40,280
 textbooks. And talking about, just earlier on, I told this statistical signal processing doesn't

92
00:15:40,280 --> 00:15:53,520
 change too much. And this one is, I get a, I won't say proof, but just evidence from this

93
00:15:53,520 --> 00:16:12,520
 author, K. So, this professor is an author, it's very interesting, you know, I hope you don't mind,

94
00:16:12,520 --> 00:16:22,400
 a little bit of small story. Some authors, they keep updating books every few years,

95
00:16:22,400 --> 00:16:30,840
 you know, like the first version, second version every few years. It could be up to fifth version,

96
00:16:30,840 --> 00:16:40,800
 sixth edition, you know, the famous guy, you know, with the book Simon Haking,

97
00:16:41,760 --> 00:16:50,199
 he's one of the big topic, adaptive signal processing neural network. And he publishes

98
00:16:50,199 --> 00:16:58,000
 every few years, now the book is probably in sixth edition, at least fifth. But this,

99
00:16:58,000 --> 00:17:05,440
 on the other hand, some professor, some author, they say they don't need to update. A few years

100
00:17:05,440 --> 00:17:11,920
 ago, I wrote to him, NTU always want newer one, so they say, oh, this two book too old,

101
00:17:11,920 --> 00:17:19,080
 you cannot use it for textbook. So, that's why I include this one. But then I wrote to professor,

102
00:17:19,080 --> 00:17:28,120
 this K, and he, the first year he's very proud, being a Westerner, gentleman. The first year,

103
00:17:28,119 --> 00:17:40,040
 he replied me, he said, oh, sorry, I haven't got time to edit the new book. I was also patient,

104
00:17:40,040 --> 00:17:47,719
 I think it was a few years back. So, the second, the next year I wrote to him again, he replied

105
00:17:47,719 --> 00:17:54,760
 the same using similar. Then the third year, I asked him again, say, do you have time to get

106
00:17:54,760 --> 00:18:02,520
 a new edition? He got impatient, he replied me, say, don't ask me again, because I don't have

107
00:18:02,520 --> 00:18:07,240
 time to update. And there's no need, because this book is more like a mathematical, you don't,

108
00:18:07,240 --> 00:18:16,040
 calculus and those, you know, very basic theory. You don't need to update, every year is the same.

109
00:18:16,040 --> 00:18:22,440
 So, he said, oh, I will keep this book. And that's true, you know, even by now, after how many years?

110
00:18:22,680 --> 00:18:33,240
 30 years, I don't see a new edition. So, this is an interesting story. This is a personal one,

111
00:18:33,720 --> 00:18:50,120
 also. So, let's now go to the so-called business. Let's see whether, one point,

112
00:18:50,360 --> 00:18:57,959
 okay. Yeah, there is, but at the end, we will talk a little bit about adaptive

113
00:18:57,959 --> 00:19:07,879
 signal processing just for introductions. I won't make a detail comment on that.

114
00:19:09,000 --> 00:19:19,800
 So, let's look at what we will have for this part one. And for part one, I use about

115
00:19:21,080 --> 00:19:33,080
 two thirds from the book, the volume one. So, yeah, the chapter on

116
00:19:34,840 --> 00:19:44,840
 like methods of moments, which I taught in the early days, but it becomes a bit more difficult.

117
00:19:45,560 --> 00:19:54,439
 So, I try to reduce some contents and then make it a little bit slower. That's why we have more time.

118
00:19:54,439 --> 00:20:03,240
 So, it covers about two thirds of the book. And I try to make the lecture know in some detail.

119
00:20:03,240 --> 00:20:10,120
 At least, it's easier to follow. So, even if you don't read the book, I think, mostly, you should be

120
00:20:10,120 --> 00:20:19,959
 able to go through. Some of the proofs I purposely leave out and then in the class, I take some time

121
00:20:22,520 --> 00:20:32,760
 to derive. In the past, this course is always, actually, in my memory, never had more than

122
00:20:32,760 --> 00:20:40,600
 60 students in the class. So, this year is at least about three times. And

123
00:20:43,560 --> 00:20:52,040
 in some years, the number reduced to just about 10. And instead of even empty, we go into tutorial

124
00:20:52,040 --> 00:21:01,080
 room. And then that was very good because I taught the 10 students. So, every week, you take time

125
00:21:01,960 --> 00:21:10,919
 to come up to do some derivations in certain parts. And the course is almost closed because

126
00:21:10,919 --> 00:21:19,320
 if you have too few students, NTU doesn't allow you to open because you're not around this course.

127
00:21:20,199 --> 00:21:30,600
 Anyway, it's good to survive after that. And then it's a big pity if we have time, we can

128
00:21:31,480 --> 00:21:41,720
 talk a little bit about the Taman filter because all this is more like the batch processing. We

129
00:21:41,720 --> 00:21:51,159
 do have a little bit about sequential processing, which I will include. And then roughly,

130
00:21:51,240 --> 00:22:02,760
 among this, about two thirds of this part is more on traditional parameter estimation theory up to

131
00:22:04,360 --> 00:22:11,880
 least square. And then in the end, we also introduce Bayesian estimation, which is

132
00:22:13,080 --> 00:22:19,560
 a little bit more mass, but I think it's quite useful. It's more related to machine learning

133
00:22:19,560 --> 00:22:26,280
 because you all know there's always a compare with so-called naive approach, which is the Bayesian

134
00:22:26,280 --> 00:22:35,000
 approach. But it's not really naive. It's one of the very important fundamental theory in

135
00:22:36,679 --> 00:22:47,800
 statistical and also in signal processing. So, that's what we will cover in this part.

136
00:22:50,280 --> 00:22:58,600
 Okay, so any questions you'd like to ask before we... Yes?

137
00:22:58,600 --> 00:23:05,639
 Yes.

138
00:23:08,760 --> 00:23:20,840
 Yeah, that's a good question. Yes. So, actually, you raise a very good question. In the

139
00:23:21,800 --> 00:23:30,360
 very early days when I told you before I taught this course by myself, the curriculum

140
00:23:30,360 --> 00:23:41,879
 when this course was for... At that time, it was called 6421. Yeah, it's a 6000 series. And it was

141
00:23:41,960 --> 00:23:49,160
 set as detection first and then estimation. And because that's follow what you say is from the

142
00:23:49,880 --> 00:23:54,280
 application point of view, from the natural application point of view, you should do that

143
00:23:54,280 --> 00:24:02,200
 way. You'll detect first and then do estimation. But mathematically, it turned out to be

144
00:24:02,360 --> 00:24:14,840
 what we will be teaching here. The detection, some of the results we use, we derive is used for the

145
00:24:14,840 --> 00:24:21,720
 detection part, but not vice versa. And that was why when I took over, only when I took over,

146
00:24:21,720 --> 00:24:27,640
 because when you follow your two lecturers, the other one used the teaching first. So,

147
00:24:28,200 --> 00:24:34,360
 he said, okay, we keep the sequence. And after I took over, because it's myself, so I have freedom.

148
00:24:34,360 --> 00:24:41,000
 I said, it's better to change to do estimation first, although a little bit against the

149
00:24:41,720 --> 00:24:49,240
 application. But in terms of theory, later you will see what we derive in the estimation

150
00:24:50,040 --> 00:24:57,640
 partially can be used in the detection. And furthermore, I also argue if you look at,

151
00:25:00,200 --> 00:25:06,520
 if you go back to this author, he published a book in this order, you see. You see here,

152
00:25:06,520 --> 00:25:12,920
 estimation theory first. And then after five years, he wrote this detection. So I say that we should

153
00:25:12,920 --> 00:25:20,440
 follow the author, because the author must think this is more important, more fundamental. And

154
00:25:20,440 --> 00:25:28,600
 it turned out to be the case, because if you learn estimation, it's more diverse. There are

155
00:25:28,600 --> 00:25:35,240
 several different methods and so on. And once you have a good understanding of estimation theory,

156
00:25:35,240 --> 00:25:43,960
 it's easier to do the detection. Of course, the detection book, I only use half, because the first

157
00:25:43,960 --> 00:25:52,840
 half is easier. The subsequent half is getting very difficult. So, yeah. So, actually, to assure you,

158
00:25:52,840 --> 00:25:59,240
 because many, particularly MSc students, they always fear it's called 7,000 series, maybe

159
00:26:00,040 --> 00:26:05,400
 very difficult and so on. Yes, and they try to avoid. But I will say,

160
00:26:08,280 --> 00:26:18,840
 most is about the same level with the advanced BSP. So, yeah, that's a 6,000 series. And it's

161
00:26:18,840 --> 00:26:27,480
 even easier, because these two parts, you follow the same book, the principles are quite similar.

162
00:26:28,440 --> 00:26:36,200
 You use almost the same, later you will see, same model, except the objectives are different. Yeah.

163
00:26:40,760 --> 00:26:53,720
 That's a good question. Yes. So, yeah. Later, we will show the so-called connection of these two.

164
00:26:54,600 --> 00:27:02,360
 But anyway, I think you all agree, you know, estimations, not just in signal processing,

165
00:27:02,360 --> 00:27:11,160
 because we, EO signal processing is an example, because this book, the author of the books,

166
00:27:11,160 --> 00:27:20,040
 he happened to be an expert researcher in signal processing. So, he also explained EO's that as,

167
00:27:21,000 --> 00:27:28,040
 you know, it's more like the showcase. But the estimation theory, I will say,

168
00:27:28,040 --> 00:27:33,080
 is more primates, you can think that way.

169
00:27:33,080 --> 00:27:54,679
 So, in terms of application, you can see here, you see, you can almost apply everywhere,

170
00:27:55,640 --> 00:28:02,680
 radar, sonar, speech, and so on. And the estimation

171
00:28:07,240 --> 00:28:16,040
 could be in terms of, you know, software, which you design some algorithm. It could be also

172
00:28:16,040 --> 00:28:24,600
 incorporated into a real hardware system. For example, in radar, sonar, and sometimes even

173
00:28:26,120 --> 00:28:36,040
 speech, you see, you need a system to do. And that's estimation of certain parameter, or even

174
00:28:36,120 --> 00:28:47,159
 waveforms. It's one of the very important part in this system. So, what I do is, you take

175
00:28:47,720 --> 00:28:55,159
 measurements, and then measurement is like signal in noise, later you will see.

176
00:28:56,360 --> 00:29:04,200
 And from there, you try to extract the useful feature. So, this thing is very

177
00:29:04,200 --> 00:29:13,240
 related to nowadays, you know, talk about machine learning. And except here, we are not really

178
00:29:13,240 --> 00:29:20,600
 going to neural networks or deep learning. It's still more the models, you know, based on

179
00:29:22,440 --> 00:29:33,720
 analytic approach. And examples of signal, you see, yeah, then again, let's talk about

180
00:29:35,000 --> 00:29:44,840
 physical walls. How do you measure the signal? Of course, nowadays, you have different sensors, and

181
00:29:45,720 --> 00:29:52,520
 you can do all the kinds of measurements from our daily life, like acoustic,

182
00:29:53,240 --> 00:30:01,960
 electromagnetic, and those. Even financial data, you see, financial data like your

183
00:30:02,680 --> 00:30:11,560
 like the stock market, your monitoring that, and you know, those and so on. And even interestingly,

184
00:30:11,560 --> 00:30:19,160
 is like mechanical vibration. And that's why every year, it seems we always have some student,

185
00:30:19,160 --> 00:30:26,440
 PhD student coming from other school, like MAE and civil engineering. Because they say, oh,

186
00:30:27,240 --> 00:30:36,680
 their research, you know, really, you know, applying this estimation theory or statistical

187
00:30:37,480 --> 00:30:45,000
 processing into the, you know, some of the vibration data or earthquake. So, you see here,

188
00:30:45,000 --> 00:30:52,120
 there's more geophysics. And yeah, and of course, you know, there's poll and survey data.

189
00:30:52,120 --> 00:30:59,400
 Some of the data are not continued time. Also, we are used to continue time data, and we do,

190
00:30:59,400 --> 00:31:05,960
 we say we use sampling theory to get to compare to discrete time signal, but not necessarily.

191
00:31:05,960 --> 00:31:13,240
 If, for example, some of the finance data we receive is already in discrete time format,

192
00:31:13,880 --> 00:31:24,840
 poll and survey, you know, nowadays, the US going to the president election, so they keep conducting

193
00:31:24,840 --> 00:31:33,560
 those things. And yeah, and so even in the more biomedical, you see, you have the genes or,

194
00:31:33,960 --> 00:31:46,600
 it's more in the bioinformatics, gene expression. So there you can apply those parameter estimations.

195
00:31:50,600 --> 00:32:00,919
 And then, if, as you can see from the, or understand from the title of this

196
00:32:01,000 --> 00:32:08,760
 call, you know, we start with statistical signal processing. So it's very natural to

197
00:32:09,640 --> 00:32:18,040
 link into random signal or random process. And that's also the reason in the past, they always

198
00:32:18,760 --> 00:32:27,320
 have a very small class size because it's not easy unless they, at the same time, learn the

199
00:32:27,399 --> 00:32:33,639
 probability and random process. And that's true because if you're

200
00:32:35,720 --> 00:32:43,480
 not considering noise, you see, but your measure is more almost accurate. So if you are doing

201
00:32:44,840 --> 00:32:57,080
 accurate measurement, then to estimate parameter is somewhat easier. And yeah, so that's why

202
00:32:57,080 --> 00:33:07,720
 here our Samsung is, Samsung is always have noise. The question is, how big is the noise? How you

203
00:33:07,720 --> 00:33:13,480
 categorize this noise? And of course, you all know noise, you must model using

204
00:33:15,080 --> 00:33:22,360
 random variable because it doesn't make sense to say, oh, noise is a deterministic signal.

205
00:33:23,080 --> 00:33:30,679
 That was why I was a tutor. Actually, not today, today is Wednesday. Yesterday I had a class

206
00:33:31,800 --> 00:33:37,560
 undergraduate, because here usually we need to teach both postgraduate and undergraduate.

207
00:33:37,560 --> 00:33:44,840
 And the undergraduate course is also called signals and system. You know, those attending my

208
00:33:45,480 --> 00:33:53,639
 advanced DSP, I relate that. And year two student, then there we were talking about different

209
00:33:53,639 --> 00:33:59,879
 kind of signal. We say, oh, you have the random signal. And sometimes in the classification of

210
00:33:59,879 --> 00:34:09,960
 signal to test them about the quiz, a very simple test every week. There is a 15 minutes we call

211
00:34:10,520 --> 00:34:17,800
 IRAA. And then the question is a multiple choice. One question, they give a sideway or give some,

212
00:34:18,840 --> 00:34:25,400
 you know, like a unistep function. They ask students five choice, all function, even function,

213
00:34:25,400 --> 00:34:32,600
 energy or power and so on. And there is a, there is one of the answer is the random. And some

214
00:34:32,600 --> 00:34:39,480
 students they don't know, they select random. Then I get them a hint, I say, oh, random signal

215
00:34:39,480 --> 00:34:44,440
 is not at your year two undergraduate level. Basically, you can almost show there is

216
00:34:46,520 --> 00:34:51,719
 undergraduate level, at least year two, we don't deal with random. So don't choose

217
00:34:52,760 --> 00:34:58,600
 random. I give them a little bit of hint. But here is the opposite, you see? Everything is random.

218
00:34:58,600 --> 00:35:06,040
 Okay. So we may have deterministic signal in random noise. Or sometimes in later part,

219
00:35:06,040 --> 00:35:14,200
 we also consider random signal in random noise. So you need to build in this concept. We are

220
00:35:14,200 --> 00:35:22,440
 dealing with a ringed, noisy measurement. And so our approach here is among those,

221
00:35:23,400 --> 00:35:30,520
 because you have noise, then the measurement are not accurate in the sense of, you know, your

222
00:35:31,480 --> 00:35:40,040
 noise, you have some contamination. So you need to try a way to reduce noise. And at the same time,

223
00:35:40,040 --> 00:35:47,320
 try to extract the useful information. So that's how the meaning of estimation

224
00:35:50,360 --> 00:35:58,120
 is being applied here. So, okay. So for example, again, this is the daily

225
00:35:58,600 --> 00:36:12,600
 life example. You are very anxious or pay attention to very sensitive to your body weight.

226
00:36:12,600 --> 00:36:18,600
 So, for example, every day you take your own weight. Nowadays, very easy at home. You have a

227
00:36:19,080 --> 00:36:30,520
 very simple scale just in the bathroom. So your measure every day, say, it will not be exactly

228
00:36:30,520 --> 00:36:41,960
 the same. I'm not sure. Even, yeah, 69.9, 70.1, and so on. So this is very, you know, very common.

229
00:36:41,960 --> 00:36:52,520
 And then with that, you'll try to see what is your true weight. Because if you are

230
00:36:53,720 --> 00:37:02,120
 reporting to the doctor, you know, very recently, for new student, you have the medical check. But

231
00:37:02,120 --> 00:37:08,760
 there, you only do one measurement. Then you know that one. But that may not be your true weight.

232
00:37:08,840 --> 00:37:18,520
 Okay. So the question here is, at least over a certain time, maybe one month, we'll see

233
00:37:18,520 --> 00:37:25,960
 whether we can estimate our true weight based on the measurement. And the amount there is how much

234
00:37:25,960 --> 00:37:34,200
 committed in your estimate. For example, the very easy one is you just sum all of them together,

235
00:37:34,200 --> 00:37:47,399
 take the average. But so the estimate one, you'll get whether this is the true reflection of your

236
00:37:49,560 --> 00:37:57,960
 weight or there is some false alarm or there are some outliers there and so on. So that's why

237
00:37:58,920 --> 00:38:07,960
 we want to see how much is the variation, you see, to see. Not just the so-called estimate value,

238
00:38:07,960 --> 00:38:14,680
 the average. Typically, very often, we use the mean value. But then after getting the means,

239
00:38:14,680 --> 00:38:22,280
 that's not enough. How much is the variation? And then, furthermore, if we have more measure

240
00:38:22,920 --> 00:38:33,880
 data, whether the variation of the estimate will decrease so that give you the more reliable.

241
00:38:34,680 --> 00:38:40,760
 And that was right from the beginning, you see. Not just the average, but the variation is

242
00:38:41,560 --> 00:38:50,440
 very important. And that's more or less the basic questions or the important question.

243
00:38:50,440 --> 00:38:57,240
 We are going to study this in this course. Now here, there is a more

244
00:39:00,280 --> 00:39:08,360
 real-life example. And there's also some more link with detection also.

245
00:39:09,320 --> 00:39:20,920
 Yeah. And it happened to be my research also, not fully on this, but I did some research,

246
00:39:20,920 --> 00:39:31,560
 at least, in radar and sonar systems. So we have a good understanding of this topic.

247
00:39:32,279 --> 00:39:43,080
 Actually, one of the projects is really down to earth project that we work with the ministry of

248
00:39:44,360 --> 00:39:54,680
 defense in Singapore. And then we bought a very expensive radar, and went all the way to the

249
00:39:55,080 --> 00:40:04,200
 to the western side of Singapore. NTU is already in the west, but not the far west. I don't know

250
00:40:04,200 --> 00:40:15,799
 whether you heard of this. Tuas, Tuas is more like southwest further down. NTU is very close

251
00:40:16,120 --> 00:40:21,080
 to west, but if you go further down to the industry part, there you will see

252
00:40:23,800 --> 00:40:37,720
 some very remote place. Then we carry the radar to measure the UAV. Maybe next week I can show you

253
00:40:38,359 --> 00:40:48,359
 some of the radars and some of the data we take there. So this is what we have is

254
00:40:49,319 --> 00:40:58,040
 in the radar system, we have a transmitter. I guess most of you, if not all, most will be

255
00:40:58,040 --> 00:41:07,480
 in the engineering side. A few, maybe your background, maybe a primate, but the majority

256
00:41:08,600 --> 00:41:17,000
 engineering. So you should know the rear transmitter and the send out signal. It's a

257
00:41:17,640 --> 00:41:27,319
 physical one. And after that, the purpose is of course not to just play around with doing some

258
00:41:27,319 --> 00:41:37,000
 useful stuff. Typically, you try to detect certain targets. And then the target,

259
00:41:38,120 --> 00:41:48,279
 you know, like the RF, typically it's RF signal, you hit the target and that's the one you'll be

260
00:41:49,080 --> 00:41:56,520
 echoed, bouncing back, and that's our so-called de-zai signal. So here, again, this is a

261
00:41:57,640 --> 00:42:03,879
 very good justification as what I say early. If we do not have noise, you see,

262
00:42:04,520 --> 00:42:10,360
 a very clean signal. You see, I send out, hit the target, it comes back. Should have almost

263
00:42:11,160 --> 00:42:19,880
 very similar waveform. It may be reduced, but still, you know, if we send out a continuous wave

264
00:42:19,880 --> 00:42:27,720
 or even an impulse, what you'll get back will be a similar waveform. So that's very easy. We don't

265
00:42:27,720 --> 00:42:36,120
 need much detection or estimation to do. What you'll do is just, probably the signal getting lower,

266
00:42:36,680 --> 00:42:44,919
 then you'll be there. But however, in the real world, you know, we have all this

267
00:42:45,799 --> 00:42:51,240
 so-called interference and noise, which we all can see the noise, echoes from other objects,

268
00:42:51,240 --> 00:43:02,680
 and that's very common. You do your project facing the real world, you know, not to select,

269
00:43:03,560 --> 00:43:09,879
 I'm not talking about laboratory. In the lab, you can say, oh, I placed this target there,

270
00:43:09,879 --> 00:43:16,680
 then everything makes the room very quiet, no other interference. That way, measure that

271
00:43:17,080 --> 00:43:23,160
 that signal. That's okay, but you go like what I did. Yeah, as I said, next week I'll remember to

272
00:43:23,160 --> 00:43:35,160
 bring, actually, in my USB, I will see during the period. So what you'll see is a real signal. You

273
00:43:35,160 --> 00:43:42,200
 cannot avoid, as I said, let other people go away, no. So that's why you have other echoes from other

274
00:43:43,000 --> 00:43:49,640
 objects that are coming. And sometimes these echoes are not the one we want. It may be even

275
00:43:49,640 --> 00:43:57,879
 larger than that, so that causes a big challenge. And there will be other external noise here,

276
00:43:57,879 --> 00:44:06,600
 and then even the receiver itself. Your receiver may not be the ideal one. You will have some noise

277
00:44:06,600 --> 00:44:13,880
 and also. So it turns out to be you can broadly say we have the ideal signal in noise.

278
00:44:15,319 --> 00:44:22,600
 So after that, this is what we are trying to extract. And assuming, of course,

279
00:44:23,160 --> 00:44:30,920
 we will later come back to the detection part. Assuming we are doing the estimation,

280
00:44:30,920 --> 00:44:39,560
 we already have the feeling the signal should be there. So we can do the estimation of range,

281
00:44:40,520 --> 00:44:49,640
 velocities, and bearings, bearings in the direction of arrivals. And some others. So this is

282
00:44:49,720 --> 00:45:01,400
 one of the main purposes of using the radar and sonar. And, of course, you can also do it from

283
00:45:01,400 --> 00:45:08,440
 image processing or video processing point of view. That's another way. But here we are more

284
00:45:09,080 --> 00:45:17,320
 or less assuming the signal is more or less a 1D signal in the time series rather than

285
00:45:18,440 --> 00:45:27,960
 image. That's another topic. Although some of the ideas could be similar. So this gives you a

286
00:45:29,240 --> 00:45:34,760
 picture and also highlights why we need to consider noise because in real world,

287
00:45:35,400 --> 00:45:40,120
 any measured data will be a signal in noise.

288
00:45:45,240 --> 00:45:55,880
 And then come back to our aims of this estimation theory. Here we try to learn some techniques and

289
00:45:55,960 --> 00:46:06,440
 some theoretical frameworks for the estimation of either disturbance and system parameter.

290
00:46:07,080 --> 00:46:16,600
 Those are related to the system. Just now I mentioned in real world, we always have signal

291
00:46:16,600 --> 00:46:22,920
 and system. These two are related but not the same. Why? Because, for example, if those are

292
00:46:23,880 --> 00:46:30,600
 disturbance and system parameter, channel parameter, for example, I'm not sure. Every

293
00:46:32,040 --> 00:46:39,080
 year taking this class, we have students or PhD students or math students from the communication

294
00:46:39,080 --> 00:46:46,920
 engineering area. So there, channel parameter is very important. And channel parameter is

295
00:46:47,160 --> 00:46:56,840
 your link to the system. It's always there. It's not from the signal. It's from the system,

296
00:46:56,840 --> 00:47:02,600
 except all the channels. On the other hand, we also have signal parameter,

297
00:47:03,480 --> 00:47:09,320
 which is the signal may be there, the signal may disappear. So it's on and off. For example,

298
00:47:09,800 --> 00:47:16,280
 amplitude, you know, you've been talking about side wave amplitude, frequency, time delay,

299
00:47:16,280 --> 00:47:27,880
 topa, baling, and so on. All these are related to just signal and waveforms, not from image or

300
00:47:27,880 --> 00:47:35,880
 video point of view. We are not looking there. And then furthermore, there is going forward,

301
00:47:35,880 --> 00:47:45,720
 you can even estimate signal waveforms, which is one step forward. If you look at the book

302
00:47:45,720 --> 00:47:56,200
 in the last chapter, it's called Kalman Filter. Kalman Filter is estimate signal waveforms.

303
00:47:57,000 --> 00:48:05,960
 And that's how this, yeah, this, you need a little bit more advanced theory, because you can

304
00:48:05,960 --> 00:48:16,359
 combine like some system theory. And that's, again, there's another PhD or

305
00:48:17,240 --> 00:48:27,480
 7000 series offered by the control people. You have the 7204. So don't get confused with

306
00:48:27,480 --> 00:48:33,480
 our colleges, 7402. The other one, linear system. Currently, the size is about the same.

307
00:48:34,360 --> 00:48:40,200
 We are against sister courses. Yeah, I gave you a little bit of a

308
00:48:40,759 --> 00:48:48,759
 linear story. And that's, you know, there's one or two years ago, this course, the

309
00:48:50,439 --> 00:48:56,839
 exam hall, because the class size is small, then the exam hall have these two courses to do

310
00:48:57,480 --> 00:49:09,319
 exam together. And then I'm the examiner of 7402, and another professor is 7204.

311
00:49:09,320 --> 00:49:15,400
 And we, at the beginning, we're lucky we went early. And also, I'm a little bit more

312
00:49:16,200 --> 00:49:23,320
 careful. Usually, I check. And suddenly find out the aiming people, those aiming people,

313
00:49:24,600 --> 00:49:33,480
 they mix these two, you know, they only brought the 7402 paper, they forgot the other one. So I

314
00:49:33,480 --> 00:49:40,520
 say, okay, no, these two courses are different. The name looks very similar, 7204. Well, here,

315
00:49:40,520 --> 00:49:50,840
 it's 7402. So luckily, called the other people, then they deliver the exam paper. And this

316
00:49:51,400 --> 00:49:58,680
 finally solved the problem before the exam start. And there, you will need to learn the

317
00:49:59,399 --> 00:50:08,440
 state space model, because common field is, you must learn state space. And from there, you

318
00:50:09,720 --> 00:50:15,640
 build up this signal polarization or control theory. It's combined together. But

319
00:50:16,919 --> 00:50:25,160
 we are not going to go into that part in this course. Okay, just let you know for some of you,

320
00:50:25,879 --> 00:50:31,879
 I think in the end, it depends on the time. If I have time, I will try to give some

321
00:50:32,920 --> 00:50:42,120
 introduction. It should be possible. Okay, now coming to the starting point, a little bit more

322
00:50:42,920 --> 00:50:51,720
 technique, we need to make some assumptions and notation. I'll say mostly throughout the course,

323
00:50:51,799 --> 00:50:58,839
 but there may be certain part, they may use some slightly different notation, but

324
00:50:59,959 --> 00:51:08,680
 not to worry too much. It's mostly we are consistent. So here, our observation is we are

325
00:51:09,560 --> 00:51:17,480
 assuming like discrete time signal. Our measure data is not like the continuous waveform.

326
00:51:18,200 --> 00:51:25,320
 So we are taking finite samples. So we can put it as a vector. And as I say,

327
00:51:26,040 --> 00:51:35,560
 most of the time, we are dealing with one D-signal scalar. So you put all this together into a vector.

328
00:51:35,560 --> 00:51:45,720
 At the end, we may consider special case where one sample itself is a two by one vector.

329
00:51:47,480 --> 00:51:55,560
 That's not common. And then similarly, now we also have a vector of parameter.

330
00:51:58,120 --> 00:52:04,360
 Usually, we always start with a simple case where you have a single parameter.

331
00:52:05,880 --> 00:52:13,240
 Then it's easier. And then after that, we extend to two or usually it's multiple. And that's A.

332
00:52:13,560 --> 00:52:23,479
 For the observation of data, typically we have several N. But in some special case like the

333
00:52:23,479 --> 00:52:31,640
 detection, we can start from just one measure data. So you have to be careful. For example,

334
00:52:35,000 --> 00:52:40,759
 particularly in the exam paper, you have to read carefully whether it's a single data sample

335
00:52:40,840 --> 00:52:47,240
 or multiple. And also how many parameters. So these are important.

336
00:52:48,200 --> 00:52:57,160
 So these are the very basic assumptions. Measure data, vector of parameter, we're going to estimate.

337
00:52:59,160 --> 00:53:06,680
 And then now based on the measure data, you see. So our study here looks like very simple.

338
00:53:07,319 --> 00:53:15,640
 Based on the metadata, we try to estimate this true parameter. Because the data doesn't tell you

339
00:53:16,600 --> 00:53:28,200
 how this parameter looks like. It's hidden there in the data. So we use the head. The theta is the

340
00:53:28,200 --> 00:53:35,560
 true one. You use the head. That means we estimate that. And since we're based on the data,

341
00:53:35,560 --> 00:53:44,919
 you see. So it's a function of X. And yeah, that's how this is formulated. And how to do that,

342
00:53:44,919 --> 00:53:51,319
 then there are several ways. And which one is better or so on. That's another question. So that's

343
00:53:51,960 --> 00:54:02,040
 the technical detail. But in terms of the aim, it looks quite simple. So now the next question is,

344
00:54:02,840 --> 00:54:11,640
 once you have this estimate, how good is this? I would really say you measure your body weight and

345
00:54:11,640 --> 00:54:17,880
 then you simply just guess or take the average and so on. So you want to have some competence,

346
00:54:17,880 --> 00:54:28,360
 some so-called metrics. Nowadays in machine learning, you use one, you can have two. Accuracy

347
00:54:29,160 --> 00:54:41,080
 and then depending on which area you'll be dealing with. So here, the very simple

348
00:54:41,080 --> 00:54:53,400
 performance measure here is the estimation error here. And this is the estimate, this is the true

349
00:54:53,400 --> 00:55:09,000
 one. So we want to see how big it is. Just a simple subtraction. And then remember, our data

350
00:55:09,000 --> 00:55:18,280
 is like the random, at least contain noise. It's partially, well, we can say it's random.

351
00:55:18,280 --> 00:55:25,880
 If you all have a deterministic signal plus random noise, then of course we say this

352
00:55:25,880 --> 00:55:33,560
 combined together, it will be a random signal. Even so, you have the deterministic part because

353
00:55:33,560 --> 00:55:44,520
 always when you describe a signal or function, we always use the more so-called complicated one,

354
00:55:44,520 --> 00:55:52,600
 because deterministic part is easier compared to the random one. So that's why this is a random

355
00:55:53,400 --> 00:56:00,680
 quantity. As I say, in the first several weeks, maybe four weeks, we will consider

356
00:56:00,919 --> 00:56:09,799
 the parameter here as deterministic. They are not random. So even so, this vector is deterministic.

357
00:56:10,520 --> 00:56:20,600
 We have this one is random. Your estimate is based on random vectors, so it will be not

358
00:56:20,600 --> 00:56:28,120
 deterministic. So therefore, the error itself is a random quantity. So you need to keep this in mind.

359
00:56:28,440 --> 00:56:35,640
 You will need to study further. And then another very useful one, so this basically measures the

360
00:56:35,640 --> 00:56:50,040
 variation. So this is just to tell you how big will be the error. So now since we are, later you

361
00:56:50,040 --> 00:56:55,640
 will see we need to take the expectation because this is a random quantity. So this is,

362
00:56:56,200 --> 00:57:05,000
 you also need to be very clear about dimension. And that's why I introduced maybe today or

363
00:57:05,000 --> 00:57:11,319
 some other time. I will talk about linear algebra because that's very important. Vector and matrix

364
00:57:11,319 --> 00:57:19,480
 is unavoidable in this course. So you can see here this is a column vector. This is a row vector.

365
00:57:20,440 --> 00:57:27,080
 So this is an inner product. So inner product except before you take the expectation,

366
00:57:27,080 --> 00:57:37,400
 it will still be a random variable, but it will be a scalar. However, once you take expectation,

367
00:57:38,600 --> 00:57:44,600
 anything random after taking the so-called average or expectation, the mean, it will become

368
00:57:45,400 --> 00:57:54,520
 a no random anymore. It will be a deterministic number, and that's why it becomes a single number.

369
00:57:54,520 --> 00:58:03,560
 So it's very important, this E here, denote the expectation value, or we say this is you are doing

370
00:58:03,560 --> 00:58:14,040
 the expectation operation. So don't overlook that. We'll go through that later. Actually, in this

371
00:58:14,040 --> 00:58:23,560
 course, both part one and part two, you will see everywhere we apply this E. So expectation,

372
00:58:23,560 --> 00:58:31,960
 just simply you are taking the mean value. Okay, then how are we going to characterize?

373
00:58:33,000 --> 00:58:41,320
 As I say, how good is estimate? The first one is bias, and then the second one is consistency.

374
00:58:41,880 --> 00:58:50,760
 Consistency, then the third one is efficiency. So don't get confused. The efficiency is here.

375
00:58:50,760 --> 00:58:58,920
 It's not the computational efficiency. We'll talk about this later. Although it's the same

376
00:58:58,920 --> 00:59:09,720
 in English, but different meaning. Bias is very easy. It looks like some student initially confused

377
00:59:09,720 --> 00:59:18,359
 with this estimation error here. Here we take the estimator and then directly take the sum.

378
00:59:18,919 --> 00:59:27,959
 So it's random. Bias is different. Here, sometimes for convenience, we don't put bracket x. We just

379
00:59:27,959 --> 00:59:39,319
 simply use theta hat. So we do the expectation on this first, then subtract the true parameter

380
00:59:39,320 --> 00:59:45,160
 vector. So this is called bias. So the difference is one is this E, the other you don't have.

381
00:59:45,800 --> 00:59:54,280
 And now in this case, this is no longer a random signal. Remember, a random variable or random

382
00:59:54,280 --> 01:00:02,440
 vector. After you take expectation, become no random. And then for this first few weeks,

383
01:00:02,920 --> 01:00:10,040
 this is no random. Therefore, this is no random. Therefore, if it is possible,

384
01:00:12,360 --> 01:00:22,840
 the result here could be zero. Because you are talking about two deterministic vectors.

385
01:00:23,400 --> 01:00:33,560
 So there's no random. Therefore, if it happens to be zero, it means the expectation of this

386
01:00:34,120 --> 01:00:41,560
 random quality vector is the same as theta. So we call this unbiased. But going back, if you are

387
01:00:41,640 --> 01:00:59,320
 talking about this, then you can never say this is zero. You always get a no zero one. It could be

388
01:00:59,320 --> 01:01:09,320
 occasionally zero, but because a random variable is something that keeps changing. And then that's

389
01:01:09,320 --> 01:01:19,320
 why if you measure this estimator error, you have to take the expectation to reduce this randomness.

390
01:01:19,320 --> 01:01:30,200
 But here, it makes sense. So that's the starting point. Bias, whether it's biased or unbiased,

391
01:01:30,279 --> 01:01:41,240
 you take the expectation of this estimate first. For estimation, you do it one time,

392
01:01:41,240 --> 01:01:48,919
 you get one estimate. You do it again next time, you will get another one. They are not exactly

393
01:01:48,919 --> 01:01:57,720
 the same. They can never be exactly the same. It may look similar. Remember, it's a signal plus noise.

394
01:01:57,720 --> 01:02:04,359
 Signal plus noise. So the noise keeps changing. You don't expect the noise will behave

395
01:02:05,560 --> 01:02:14,680
 sideways. Not possible. So that's one thing. So for this one, it's a measure of how close

396
01:02:15,959 --> 01:02:19,879
 the estimate is to its true value in the sense of the average.

397
01:02:19,880 --> 01:02:31,080
 So the bias estimate usually depends on the number of observations. I'm talking about if

398
01:02:31,080 --> 01:02:43,000
 your estimate is not unbiased, if it's biased, then we have to consider it will be a function of the

399
01:02:43,000 --> 01:02:52,040
 number of observations. So it depends on how much you estimate. And in the case of

400
01:02:53,880 --> 01:03:06,440
 web files, but the files may go to zero if your observation number increases. So you keep

401
01:03:06,920 --> 01:03:18,360
 estimating more and more. That's why here we have another measurement, the performance measure,

402
01:03:18,360 --> 01:03:26,280
 called asymptotically unbiased. That means it's biased if you have a finite number of data. But

403
01:03:26,840 --> 01:03:34,280
 when the number of measured data increases to infinity, then this bias approaches zero.

404
01:03:34,280 --> 01:03:42,440
 That's asymptotically unbiased. Ideally, we hope we have unbiased ones. Just use finite number of

405
01:03:43,160 --> 01:03:49,880
 data samples. But if that cannot be achieved in some cases, it's okay. We can go by asymptotically.

406
01:03:55,400 --> 01:04:03,320
 Average of many unbiased estimates will be close to the actual value.

407
01:04:04,520 --> 01:04:12,200
 But however, the estimate is less error than the bias one. So maybe at this point I will

408
01:04:12,200 --> 01:04:22,200
 try to illustrate a little bit. So let me see how we go. This one is working.

409
01:04:22,279 --> 01:04:26,120
 Let me turn to this one.

410
01:04:38,120 --> 01:04:40,120
 Okay, looks like

411
01:04:44,120 --> 01:04:46,120
 the

412
01:04:47,160 --> 01:04:49,160
 X star

413
01:04:52,200 --> 01:04:54,200
 is working.

414
01:04:57,720 --> 01:05:01,319
 Seems not to work.

415
01:05:11,560 --> 01:05:16,120
 Early on I tested, it seems working. I don't know why.

416
01:05:22,200 --> 01:05:37,560
 I think I can try to explain using, yeah. So the main purpose here is, for example,

417
01:05:38,279 --> 01:05:44,279
 we are just talking about the simple case where the parameter, you have just one parameter,

418
01:05:44,280 --> 01:05:54,840
 which is more like a DC value. The DC value is a constant. Based on our measured data,

419
01:05:56,840 --> 01:06:06,120
 we try to estimate this DC value. But because our measured data is noise, so each time you estimate,

420
01:06:06,520 --> 01:06:16,839
 you may not get exactly this value. You may be, say, the DC value is five. Your first one is, say,

421
01:06:19,080 --> 01:06:25,720
 4.5. The next one is 5.5 and so on. Then the subsequent one is 4.8. The other one,

422
01:06:25,720 --> 01:06:33,960
 go to the other side, 5.2. Then you will expect in this case, you see, after you take many

423
01:06:33,960 --> 01:06:45,160
 measurements, you'll take the average. The average is what we call the E of this, the expectation of

424
01:06:45,160 --> 01:06:53,400
 this estimate. It happens to be, take the average, you will be fine. Then in that case, we get the

425
01:06:53,480 --> 01:07:06,040
 unbiased estimate. You will be good. However, we may have the average value, which is the same as

426
01:07:07,000 --> 01:07:13,080
 unbiased, meaning it will be the same as the true value. But the variation can be quite big in the

427
01:07:13,080 --> 01:07:21,080
 case. For example, it can be minus 10, 10. The next value will go down to, say, minus five and five

428
01:07:21,799 --> 01:07:30,600
 or so on. Sorry. This is what I'm trying to say. If the DC value is zero, we estimate the zero

429
01:07:30,600 --> 01:07:43,799
 means. Or in the early example, five, then you see minus four, for example, and minus four. Then

430
01:07:43,880 --> 01:07:53,400
 you'll be low. How much is below five? You'll need nine. Then you can go to the other one. Five plus

431
01:07:53,400 --> 01:08:03,800
 nine will be 14. So you add together, divide by two, you'll get five. But the fluctuation is very big.

432
01:08:03,800 --> 01:08:11,880
 So you may have the mean value is unbiased, but if the variation is very, very big,

433
01:08:12,840 --> 01:08:23,000
 in this case, maybe I will get the biased one, which is very close to five, say 4.9, for example,

434
01:08:23,560 --> 01:08:31,160
 very consistent, very close. Then at least we have some commitment is very close to the true one,

435
01:08:31,160 --> 01:08:41,800
 although there is a small bias, but the variation is very small. So it depends on how you measure

436
01:08:42,440 --> 01:08:53,240
 that. In any case, what we'll be dealing with is we need both to be, ideally, it will be unbiased

437
01:08:53,240 --> 01:09:00,680
 and then small variation. That's how we will be talking in this course.

438
01:09:02,360 --> 01:09:08,840
 Let's quickly go through other measures before we take a break. Consistent is

439
01:09:09,000 --> 01:09:20,840
 the mean square estimation error, 10 to 0, and when the number of observations becomes large.

440
01:09:21,480 --> 01:09:32,040
 So remember, we define this. Because as I mentioned here, if you don't take the expectation,

441
01:09:33,000 --> 01:09:40,840
 this inner product is always greater or equal to zero, although it's a random. A random signal,

442
01:09:41,880 --> 01:09:52,439
 it can be always on one side. Random, it can be 0.1, 0.5, and 10, and so on. It will never go

443
01:09:52,519 --> 01:10:02,519
 down to negative. However, if the expectation of this, when the number gets bigger and bigger,

444
01:10:02,519 --> 01:10:10,040
 the number of observations, then it is possible it becomes 0. So in that case, we say this is a

445
01:10:10,040 --> 01:10:17,400
 consistent estimator. That means in the same group. What we need to do is just keep the

446
01:10:18,120 --> 01:10:25,240
 data getting bigger and bigger, then we will have more and more competence. That's one very

447
01:10:26,679 --> 01:10:37,320
 important measure. Therefore, it means accuracy increases. As I was saying, the estimate is also

448
01:10:37,960 --> 01:10:45,799
 asymptotically unbiased. On the other hand, if you have an inconsistent estimate, then

449
01:10:48,120 --> 01:11:01,639
 it's not very certain. Because if you have not enough data, then we are not quite sure

450
01:11:02,120 --> 01:11:11,960
 how good is this estimate. In this case, you may also have problems. Even if you have more

451
01:11:11,960 --> 01:11:19,960
 observations, it doesn't help to reduce the value. So that's very important. Finally,

452
01:11:19,960 --> 01:11:28,280
 we come to efficiency. Here, it's not computational efficiency. We are comparing with the benchmark

453
01:11:28,280 --> 01:11:39,480
 quantity, which we call Cramarov's lower bound, or CILB. In some other books or papers,

454
01:11:39,480 --> 01:11:47,960
 we just simply call it CRB. Because the CRLRB is always the lower bound. Some people argue,

455
01:11:47,960 --> 01:11:55,559
 no need to lower. The bound means lower bound. This is one of the very useful benchmarks. We

456
01:11:55,560 --> 01:12:10,520
 will explain one chapter just focusing on CRLRB. Why? If you assume you have an efficient estimate,

457
01:12:12,120 --> 01:12:21,080
 then in that case, the CRLRB is the greatest lower bound. Therefore, that means no other estimate

458
01:12:21,400 --> 01:12:31,800
 has a smaller mean square error. That means it's already the best. It's a benchmark.

459
01:12:31,800 --> 01:12:38,440
 Whether you can estimate, you can achieve that or not. If you can achieve, you're done. Remember,

460
01:12:38,440 --> 01:12:46,440
 I said there may be several different methods to do it. If you already get the best one, then

461
01:12:47,240 --> 01:12:51,639
 you don't need to do it. However, if you cannot achieve that, then you may want to try

462
01:12:52,759 --> 01:13:06,200
 find a better one, or you may stop after the errors are reduced to a certain percentage.

463
01:13:06,599 --> 01:13:14,120
 You are satisfied. We will go into that later because this is a very important one.

464
01:13:15,400 --> 01:13:22,840
 However, for many practical problems, we don't have an efficient estimate.

465
01:13:23,800 --> 01:13:31,000
 Wherever you try, you will never achieve this theoretically optimal one. He thinks the CRLRB

466
01:13:31,000 --> 01:13:40,600
 is the ideal case. In practice, we often don't have the ideal situation, so you need to see

467
01:13:42,360 --> 01:13:50,200
 where you're going to stop. Nevertheless, CRLRB is very useful benchmark. That's why

468
01:13:50,760 --> 01:13:58,200
 research papers typically, if possible, you can get CRLRB. You always use that as a benchmark.

469
01:13:58,840 --> 01:14:09,800
 Okay, so that's summarized what we have discussed so far. First question, how close is the estimate

470
01:14:09,800 --> 01:14:17,639
 to true value? Then, whether there are better estimators than the ones we've chosen.

471
01:14:19,960 --> 01:14:26,440
 Because what you get is if you cannot meet the CRLRB, then that means it's probably not the best,

472
01:14:26,440 --> 01:14:36,519
 but we are not quite sure. We want to see how to get a better one, or if possible.

473
01:14:39,000 --> 01:14:46,280
 Then, we're dealing with random variable. Therefore, the performance here,

474
01:14:47,000 --> 01:14:57,000
 it can at best, you can completely describe it statistically. In that case,

475
01:14:57,559 --> 01:15:04,200
 random variable, the best description is if you know the probability density function,

476
01:15:05,960 --> 01:15:12,759
 that's the best you can describe. You can't describe it as using a deterministic number,

477
01:15:12,760 --> 01:15:23,720
 because it's random. You need to take this in mind. Then, also, even many years ago,

478
01:15:24,440 --> 01:15:31,080
 this is mainly from the books. If you can't get the theoretical result, you may say,

479
01:15:31,080 --> 01:15:37,800
 nowadays it's common, particularly deep learning. You always say, oh, I do simulations. Sometimes,

480
01:15:37,800 --> 01:15:45,240
 it works, but if you cannot prove it, you still don't have at least

481
01:15:48,120 --> 01:15:57,480
 a theoretical proof showing that's always the best, because you cannot rule out that maybe

482
01:15:57,480 --> 01:16:05,480
 someday another method can do better. So, in that sense, computer simulation,

483
01:16:05,959 --> 01:16:16,599
 at least, it can show some good justification, but you cannot 100% use it to prove. That's why

484
01:16:17,160 --> 01:16:26,440
 quite often my students or researchers, they write papers. They say, do experiments and prove this.

485
01:16:26,440 --> 01:16:33,960
 I always ask them, don't use proof, because if some reviewer like me,

486
01:16:36,440 --> 01:16:43,400
 I will say your statement is not correct. It's true. We have been running some simulations,

487
01:16:43,400 --> 01:16:48,839
 even if you run 100 times, 1,000 times, using multi-color simulations, you cannot prove.

488
01:16:49,320 --> 01:16:56,040
 So, what about I run 10 times more compared to you? Then, suddenly, some

489
01:16:56,920 --> 01:17:02,840
 bug or some problem may come up. Also, another very important trade-off is

490
01:17:03,960 --> 01:17:12,040
 between performance and computational complexity. Here, the complexity is what we often use,

491
01:17:12,840 --> 01:17:20,040
 computation efficiency. It's more common, but since, as I mentioned in this course, efficiency means

492
01:17:20,040 --> 01:17:28,120
 another sense, so we use computational complexity. You may have a very good

493
01:17:28,840 --> 01:17:35,640
 method, optimal, but it takes forever, days or months, then you cannot use it in practical

494
01:17:35,640 --> 01:17:45,320
 verification. This is also not very good. Sometimes you need to get suboptimal estimator,

495
01:17:45,320 --> 01:17:57,880
 if that's reasonably well, but easier to implement. As I said, the first week, I get

496
01:17:58,600 --> 01:18:09,560
 very slow, so we only take 12 pages. Then, we take a break here, maybe not a long one, and then

497
01:18:09,560 --> 01:18:16,920
 after that, we will go through some of the, as I say, PDF probability density function, which

498
01:18:17,880 --> 01:18:25,880
 some of you may already know, but some may be new. I won't go into too much detail, and then

499
01:18:25,880 --> 01:18:40,040
 we will continue into maybe chapter two today, but not in depth, just a very simple introduction.

500
01:18:43,240 --> 01:18:50,600
 Anyway, you decide on whether this course is suitable for you, but nowadays for MSc students,

501
01:18:50,600 --> 01:18:56,120
 there are a lot of other courses to choose from, not like the first day or two where

502
01:18:56,120 --> 01:19:05,960
 most students panic, but now if you look at the course registration website, you have quite a

503
01:19:05,960 --> 01:19:18,040
 few courses. Even the waiting list is not that long anymore. Anyway, I hope you are now in a

504
01:19:18,040 --> 01:19:25,720
 better situation. We have a clock at the back, so maybe now it's 7.49, we take 16 minutes.

505
01:19:27,160 --> 01:19:35,320
 Come back at 5 past 8 if you want to continue. I'll see you after the break.

506
01:19:36,360 --> 01:19:40,680
 Our course here, we usually take just one break in the middle.

507
01:36:48,200 --> 01:37:09,080
 Okay, so let's continue here.

508
01:37:09,960 --> 01:37:27,160
 Okay, as I said, today we will go slower and go through, not in detail, but just at least

509
01:37:27,160 --> 01:37:41,080
 I'm preparing you for some of the important PDFs, probability density functions. What we are

510
01:37:42,360 --> 01:37:51,080
 looking at here, there are random variables, there are two types. One is discrete, and the other one

511
01:37:51,080 --> 01:38:00,040
 is continued. For PDF, we will refer to those continuous random variables.

512
01:38:22,040 --> 01:38:36,200
 Okay, good. So this is one of the very important PDFs, or we should say the most important at

513
01:38:36,200 --> 01:38:48,120
 least in textbooks, because Gaussian PDF is maybe the easiest one to handle. And also

514
01:38:48,760 --> 01:38:59,160
 one very important feature is if you extend Gaussian random variable into a vector case,

515
01:38:59,160 --> 01:39:09,800
 you can steer this quite nicely. Most of the properties will be preserved, and even in the

516
01:39:10,760 --> 01:39:19,160
 case you have not independent Gaussian random variable, you can still use the

517
01:39:20,520 --> 01:39:30,920
 analytic format to describe. Otherwise, other random variables, once you have correlations

518
01:39:31,000 --> 01:39:39,240
 or not independent, they will become very complicated. So let's see what's the Gaussian

519
01:39:39,800 --> 01:39:54,280
 PDF. Sometimes we call it normal, so that's another meaning. And for scalar, you will be

520
01:39:54,360 --> 01:40:06,200
 a normal vector, just a scalar random variable. Then the PDF here is, we use a single scalar

521
01:40:06,200 --> 01:40:18,759
 variable, x, to describe. x is a real variable. And PDF here is an exponential function where

522
01:40:19,400 --> 01:40:28,840
 you only need two parameters. One is the mean, you see. Okay, mean is here. And the other,

523
01:40:29,800 --> 01:40:38,040
 so don't get confused, we use the power of two. Sigma power of two will be the variance,

524
01:40:38,600 --> 01:40:45,000
 so you need to write both together, not to separate. Because sometimes if you just write

525
01:40:45,800 --> 01:40:52,520
 sigma, it will be the standard deviation. So variance refers to sigma power of two.

526
01:40:54,600 --> 01:41:06,600
 And then because we call it normal, you can use this paragraph, capital N, to describe also.

527
01:41:06,600 --> 01:41:14,920
 Because the whole PDF, you only have two parameters. One is the mean and variance,

528
01:41:14,920 --> 01:41:22,200
 so it completely describes this PDF. Of course, the other one, pi, or so on,

529
01:41:22,200 --> 01:41:31,160
 the exponential is a function, you all know. So those are not the parameter. What we have is

530
01:41:31,160 --> 01:41:46,120
 only these two. And here we use this symbol, mean is distributed according to this. So try to get

531
01:41:46,120 --> 01:41:55,080
 familiar with some of this. And sometimes in the special case, we quite often here, we use,

532
01:41:56,040 --> 01:42:03,000
 we are talking about zero means. So zero mean mean is mu equal to zero. And that's similar,

533
01:42:03,000 --> 01:42:10,760
 quite often we talk about measure the data in circuit and so on, we say the DC value. So

534
01:42:10,760 --> 01:42:17,720
 mean is in a sense more like a DC value. If you have zero mean, that mean the DC is zero. So

535
01:42:17,720 --> 01:42:25,480
 every other signal, it will go up and down. The average is zero. And if the standard deviation,

536
01:42:26,040 --> 01:42:30,760
 the variance here equal to one, then it's the same. Variance and standard deviation

537
01:42:32,040 --> 01:42:41,080
 only equal when you both equal to one. Then we call this PDF as standard normal PDF.

538
01:42:41,720 --> 01:42:56,680
 Okay, so at this standard. And this is MATLAB. I think NTU still provides this free MATLAB.

539
01:42:58,040 --> 01:43:06,519
 So now it's less, more and more people using Python, particularly for deep learning. But

540
01:43:06,600 --> 01:43:14,440
 traditionally, MATLAB is still very useful, but you need to pay certain fees.

541
01:43:16,600 --> 01:43:26,680
 So on top of that, we have a cumulative distribution function. On Monday, some students

542
01:43:26,680 --> 01:43:33,160
 also asked what's the difference by adding this cumulative. So cumulative means you start

543
01:43:33,639 --> 01:43:43,400
 you start from minor infinity. So all the way up to one particular value. Because this upper limit

544
01:43:43,400 --> 01:43:54,040
 x, it can also change. You can be, you make it as two, then you integrate up to two. If x becomes

545
01:43:54,760 --> 01:44:00,200
 five, they integrate to a longer interval. So somehow just like,

546
01:44:01,400 --> 01:44:08,200
 a big similar for those who are familiar with the circuit, it's just like capacitor. It stores your

547
01:44:08,200 --> 01:44:10,360
 your

548
01:44:13,639 --> 01:44:21,000
 energies in the device. And then this is

549
01:44:22,760 --> 01:44:28,679
 what they call cumulative from minor infinity up to this point. And assuming in this case,

550
01:44:28,679 --> 01:44:36,360
 the miracle of g low and sigma, the value equal to one, you simplify into this.

551
01:44:38,360 --> 01:44:48,040
 And very often particularly in the detection, the second part, we have another more convenient

552
01:44:48,040 --> 01:44:55,480
 description. We call it the right tail probability. Right tail, of course you can get the meaning,

553
01:44:55,480 --> 01:45:06,040
 it's on the right side. And tail is the one towards the other side. So you see here, it's

554
01:45:06,200 --> 01:45:13,080
 opposite. Here is minor infinity up to x. Where this one, starting from x, then we are looking at

555
01:45:13,080 --> 01:45:20,440
 the tail. And of course you will see if you add these two, assuming the same x, it will be equal

556
01:45:20,440 --> 01:45:33,720
 to one. And therefore these two related by this. So that's very important function we need to

557
01:45:33,720 --> 01:45:41,240
 remember. And then assume, now assume we have this right tail

558
01:45:43,160 --> 01:45:57,160
 probability. And if evaluate this gamma value, then if this happens to be equal to

559
01:45:57,240 --> 01:46:05,320
 p, then we can of course determine if p is given, we can determine this by,

560
01:46:06,440 --> 01:46:13,639
 not analytically, because this integration is you can't really do it analytically,

561
01:46:13,639 --> 01:46:20,840
 but at least numerically you can evaluate that. So this part later in the detection

562
01:46:21,480 --> 01:46:29,480
 theory, we will use it very often. So therefore from here you can get the inverse, once p is

563
01:46:29,480 --> 01:46:43,720
 given, get this. And we call this q inverse, it will be an inverse function. But this function

564
01:46:44,360 --> 01:46:50,200
 analytically is not there. So now this is give you a

565
01:46:53,000 --> 01:47:01,960
 visualization of how this part is a CDF and the other part is the right tail. So the further

566
01:47:03,880 --> 01:47:11,400
 the x becomes bigger, because this part becomes smaller. So on the other hand, the cumulative

567
01:47:12,200 --> 01:47:17,639
 CDF, the more you go, the value gets bigger as you are accumulating both.

568
01:47:21,000 --> 01:47:25,879
 And then there is very useful property for Gaussian, you know,

569
01:47:26,360 --> 01:47:34,679
 co-economic. In the case of mu equal to zero, then it's moment, I guess, do you know how to define

570
01:47:35,320 --> 01:47:47,320
 moment? A random variable, you raise to certain integer order, say power of two,

571
01:47:47,320 --> 01:47:53,320
 power of one is just except, then it will be the expectation of mean. If you go to two, then

572
01:47:54,040 --> 01:48:03,799
 it's second order moment, or you can just simply call it second moment. And if n, then it will be

573
01:48:03,799 --> 01:48:16,599
 n's moment. And if you have the mean value case, then this becomes very easy. For even order,

574
01:48:17,400 --> 01:48:24,840
 it will be one multiplied by three multiplied by five and so on. Because n equal to one, sorry,

575
01:48:24,840 --> 01:48:31,880
 n equal to two, it will give you the variance. And we are talking about

576
01:48:34,680 --> 01:48:39,560
 yeah, the variance here. So you just one multiplied by this

577
01:48:39,560 --> 01:48:48,920
 variance here, n equal to two, and so on. Four, you have multiple of this. And n equal to all,

578
01:48:48,920 --> 01:49:01,480
 then it's zero. Why? Can you guess why this is zero? Any quick answer? We are talking about

579
01:49:01,480 --> 01:49:13,639
 Gaussian and zero mean. Yes, very good. Even symmetry. If you're even, sorry, even

580
01:49:15,879 --> 01:49:24,919
 symmetry and then zero mean, you see. So you are evaluated by the first order,

581
01:49:24,920 --> 01:49:32,440
 it will be just equal to mean. And then order three, you can do also the integration.

582
01:49:34,760 --> 01:49:42,360
 The variable itself is like all function. So you take the mean value, it will be zero.

583
01:49:42,360 --> 01:49:49,000
 Third order, second order, then second order can never be zero. It's always positive. You get some

584
01:49:49,080 --> 01:49:56,600
 deviation, variance, and so on. And then if in the case of not zero mean,

585
01:49:57,160 --> 01:50:05,000
 it's more complicated, you need to explain this, then you also get this analytic formula.

586
01:50:05,000 --> 01:50:13,240
 But this is a little bit more difficult to remember. This is easy. It becomes a function of both

587
01:50:13,719 --> 01:50:20,120
 mu and then you can make good use of this. And the moment,

588
01:50:21,000 --> 01:50:29,080
 yeah, so that means this is, the center is not zero. So moment, we refer to

589
01:50:30,599 --> 01:50:33,480
 x raise to the power of n, where here is more general.

590
01:50:33,480 --> 01:50:43,719
 And then the very useful one here is multivariate Gaussian. So this is why Gaussian is used so

591
01:50:45,799 --> 01:50:53,400
 frequently, particularly if you are looking at some papers and so on. Because nowadays for

592
01:50:54,919 --> 01:51:02,200
 dealing with a practical problem, you always have multiple parameters or multiple variables to

593
01:51:02,200 --> 01:51:13,639
 handle. So you cannot avoid like vector analysis. But however, remember here, our noise, you also

594
01:51:13,639 --> 01:51:21,320
 assume they will come with every entry, every element. So in double ways, you are dealing with

595
01:51:21,320 --> 01:51:33,960
 the noise matrix. And for Gaussian, there is no problem. You can deal with a multivariate

596
01:51:33,960 --> 01:51:51,160
 Gaussian PDF of n by 1 random vector by putting this nicely, the PDF of this data in this form.

597
01:51:51,160 --> 01:52:00,440
 But remember here, this PDF itself, you see, it's still a scalar, you know. Don't get confused,

598
01:52:00,440 --> 01:52:09,400
 you see. We are measuring, we are dealing with a random vector. But the PDF itself,

599
01:52:10,280 --> 01:52:20,200
 where our PDF is referring to try to deal with the joy, the joy behavior of this random vector.

600
01:52:21,160 --> 01:52:28,200
 So we consider there are so-called interactions and so on. So it still ends up with

601
01:52:30,200 --> 01:52:38,679
 scalar functions, you see. It's not a vector PDF, okay. P of x equal to this. Because you can see here,

602
01:52:38,679 --> 01:52:47,080
 determinant of matrix, you see, then becomes a scalar. And this is scalar. The exponential of this

603
01:52:48,040 --> 01:52:56,840
 also becomes a scalar. We have a matrix in the middle. This is column matrix, this is a row

604
01:52:56,840 --> 01:53:06,200
 matrix, row vector. Then it ends up with this exponent itself. It will be scalar. I think

605
01:53:07,800 --> 01:53:12,280
 it's a little bit problem. Next, we will begin to draw some

606
01:53:12,360 --> 01:53:20,759
 illustration and so on. When we come to vector matrix. Therefore, this

607
01:53:22,200 --> 01:53:31,000
 meal here is a mean vector. You have a random vector, you can take the mean vector by taking each of

608
01:53:31,000 --> 01:53:40,599
 the element. Each element is one random variable and take this one. So it can be zero, can be no

609
01:53:40,600 --> 01:53:48,840
 zero here. Assuming no zero in general. And then the covariant matrix is the one, you see, where

610
01:53:51,400 --> 01:53:59,080
 removing the means. So the mean in something, as I say, you can interpret that as a DC value.

611
01:53:59,080 --> 01:54:09,320
 When you measure the voltage, you can always subtract, remove the means. So in a sense,

612
01:54:09,320 --> 01:54:17,400
 we typically try to do that. That's why this covariant matrix is your, yeah, so this is an

613
01:54:17,400 --> 01:54:23,960
 outer product. This is an inner product. This is more like an outer product. Your style is

614
01:54:23,960 --> 01:54:30,360
 column vector first, now derived by a row vector, becomes a matrix. On the other thing, you become

615
01:54:31,320 --> 01:54:40,759
 a scalar. So this is always assuming to be positive definite. And that's another very

616
01:54:40,759 --> 01:54:48,280
 important concept. We will discuss that later. And then we'll have the inverse of this

617
01:54:49,160 --> 01:54:55,719
 matrix. So you can take the inverse. And in this case, we say it is a random vector for all this

618
01:54:56,440 --> 01:55:06,280
 distribution, this normal distribution. So very nicely, we only need the mean vector and the

619
01:55:07,320 --> 01:55:17,000
 covariant matrix. Remember for the scalar case, we need a value, just one value right here. It's a

620
01:55:17,000 --> 01:55:24,680
 matrix. And we take expectation. So this covariant matrix except it's not random. So in this course,

621
01:55:24,680 --> 01:55:32,280
 you always remember which variable or which quantity is random, which one is not random.

622
01:55:33,400 --> 01:55:38,440
 Not random means deterministic. So don't get confused. And that's also why in

623
01:55:39,560 --> 01:55:47,240
 assignment one, I will try to test students this very basic concept.

624
01:55:48,120 --> 01:55:57,160
 Okay, so let's move on. This chi-square one, chi-square or we call it centros

625
01:55:59,240 --> 01:56:08,920
 PDF, is the one we, again, the detection theory, we use it very often. So this is the

626
01:56:09,080 --> 01:56:18,600
 formula. And for chi-square PDF, we need to define the degree of freedom. So that means

627
01:56:19,800 --> 01:56:30,520
 how high is the degree of freedom? This is a parameter. So again, this PDF here,

628
01:56:31,160 --> 01:56:37,880
 depending on, it's only on one side. The positive x, you have a proper description. Looks very

629
01:56:37,880 --> 01:56:43,960
 complicated. You don't need to memorize it. It's for nature. But we also need this gamma

630
01:56:44,680 --> 01:56:52,760
 function here. But for negative x, it's zero. So you only, it only means a node zero for this

631
01:56:53,160 --> 01:57:00,120
 x take positive value. And we use the symbol, you know, this, so indicate the degree of freedom,

632
01:57:00,120 --> 01:57:16,440
 or call DOF here. Okay, so this is how you're going to do. Then we, here we're assuming this is an integer

633
01:57:16,440 --> 01:57:25,719
 with greater or equal to one, starting from one. And this function is a gamma function here.

634
01:57:26,919 --> 01:57:35,799
 And this gamma function in the case of integer, integer value here, it certifies this

635
01:57:35,800 --> 01:57:45,720
 formula. Starting from one over, if one over two, it will be equal to this value. And then you can

636
01:57:45,720 --> 01:57:55,880
 get it recursively. You see, starting from previous value, then you go to the next one.

637
01:57:56,120 --> 01:58:04,120
 And then for integer, if n is integer, then it becomes this formula, which is very easy to

638
01:58:05,000 --> 01:58:11,480
 remember. And if you do a plot, as I said, you can get the same result. So you can get

639
01:58:11,480 --> 01:58:16,920
 the same result. So you can get the same result. So you can get the same result.

640
01:58:16,920 --> 01:58:26,120
 And it becomes this formula, which is very easy to remember. And if you do a plot, as I say,

641
01:58:26,840 --> 01:58:32,920
 the central one, you see, so chi-square is the central one, which refers to zero means,

642
01:58:32,920 --> 01:58:39,160
 and the non-central one. So you only need to plot on the positive x on one side. But this side is

643
01:58:39,160 --> 01:58:47,639
 zero. Again, you use a mallet, you will see this. Then the degree of freedom is small,

644
01:58:47,639 --> 01:58:53,559
 it's very sharp. Starting from high, then all the way go down, zero. When it gets bigger and bigger,

645
01:58:53,559 --> 01:59:05,000
 then it becomes rather small, rather flat. And the wide way of this chi-square PDF,

646
01:59:05,160 --> 01:59:12,360
 this one is also very often related to the communications. It will be,

647
01:59:12,360 --> 01:59:17,720
 depending on the degree of freedom, as you remember, we have these parameters here. So

648
01:59:19,480 --> 01:59:27,720
 simply it's a sum of the independent and identical distribution, or call it IID.

649
01:59:28,120 --> 01:59:38,120
 So each of these variables is following this normal standard, normal Gaussian distribution.

650
01:59:38,760 --> 01:59:49,320
 And you are square it first, and then sum them together to generate another new random variable.

651
01:59:50,280 --> 01:59:56,920
 And you can verify, or just try to derive,

652
01:59:58,280 --> 02:00:08,120
 E of this happens to be the degree of freedom. And also nicely, the value of this

653
02:00:08,120 --> 02:00:15,320
 equals to twice of this. So this is a very useful property you can have.

654
02:00:16,280 --> 02:00:22,599
 And then, in the special case of this degree of freedom equal to two, then we have

655
02:00:23,480 --> 02:00:33,160
 rather simplified exponential PDF. That will be a special case where you can see here,

656
02:00:33,799 --> 02:00:43,639
 it simplifies a lot by, say for example, if this equal to two, then we're looking at gamma one.

657
02:00:43,720 --> 02:00:48,920
 So gamma one here is equal to one, if you look at this. It's all in digital.

658
02:00:49,960 --> 02:00:57,080
 And then if this equal to two, then one minus one becomes zero. And then any number

659
02:00:57,080 --> 02:01:04,760
 raise to the power of zero becomes one. So more or less this part becomes like a constant.

660
02:01:04,760 --> 02:01:12,680
 And you only need to look at this. So that's a very useful special case. We call that as

661
02:01:13,320 --> 02:01:16,040
 exponential PDF, which is equal to this.

662
02:01:19,000 --> 02:01:29,640
 And then, furthermore, we can also work on this right-tail PDF for this chi-square,

663
02:01:29,640 --> 02:01:37,000
 central one. And you follow this formula, but don't forget to indicate this.

664
02:01:37,000 --> 02:01:42,280
 Then right-tail means you integrate from x up to this infinity large. You will get this.

665
02:01:43,000 --> 02:01:49,560
 And however this becomes very complicated, if you are trying to write down,

666
02:01:50,520 --> 02:02:00,920
 you need to separate into this equal to one, then all, all number, or integer, or even integer. So

667
02:02:00,920 --> 02:02:09,800
 each looks very messy. We will not go into detail for this discussion. And next we look at the

668
02:02:09,800 --> 02:02:18,600
 null-sensure chi-square one. So this becomes much more complicated. Null-sensure means we still

669
02:02:18,600 --> 02:02:26,440
 define as sum of each of the random variables, raise to the power of two, then sum them together.

670
02:02:26,440 --> 02:02:33,000
 But however, each of these random variables, they're independent, but

671
02:02:33,960 --> 02:02:40,600
 the mean is no longer zero. So that means null-sensure means null zero.

672
02:02:42,840 --> 02:02:57,800
 So in this case, we have another so-called null-sensure parameter to measure. Summing all these

673
02:02:57,800 --> 02:03:08,760
 square means, sum together, we call it lambda here. And then in this case, the PDF itself even also

674
02:03:08,760 --> 02:03:17,240
 becomes very complicated. Also for if x greater than zero, you have this. And now we introduce

675
02:03:17,800 --> 02:03:26,120
 another function, which is modified basal function of the first chi-end, or the R.

676
02:03:27,160 --> 02:03:35,400
 So you see here, we involve this here. So you can see the PDF becomes very complicated. And you

677
02:03:36,280 --> 02:03:41,480
 are not expected to memorize this. So don't worry about that.

678
02:03:45,559 --> 02:03:52,839
 So only in the later detection part we will go back to it. And then the Rayleigh distribution,

679
02:03:52,839 --> 02:04:00,200
 this is quite useful in the communication area, quite often. And how do we get this?

680
02:04:00,200 --> 02:04:10,519
 So again, we start with this zero mean IID random variable. But only have two.

681
02:04:11,320 --> 02:04:23,800
 Square, sum together, then take the square root. So in this case, we have a PDF, which is again,

682
02:04:24,200 --> 02:04:34,840
 just measure the positive x here. And this is not too complicated. It's more or less like the

683
02:04:34,840 --> 02:04:42,440
 Gaussian exponential signal, but exponential function. But you need to multiply this x here

684
02:04:42,440 --> 02:04:50,600
 also. So it's a product, but still manageable. For negative x, it becomes zero. So you have this.

685
02:04:51,400 --> 02:05:02,200
 And finally, we look at the uniform PDF. So uniform means within that interval, the PDF

686
02:05:02,200 --> 02:05:09,400
 will take a constant, just like one, like the DC value, horizontal line, but only within that

687
02:05:09,400 --> 02:05:18,920
 segment. Outside this interval becomes zero. So far, all of these available, we are assuming is

688
02:05:18,920 --> 02:05:26,520
 continuous, that means x. Even so, it may take a segment, but it's thanks. It's not countable.

689
02:05:27,240 --> 02:05:36,520
 You cannot say number, like one, two, three, four, five. There are always thanks and continuous.

690
02:05:37,560 --> 02:05:46,440
 Then in this case, you can also very easily get the expectation, which is just take the middle

691
02:05:46,440 --> 02:05:57,320
 point between A and B. And variance is, this is the interval, the size of the interval,

692
02:05:57,320 --> 02:06:07,320
 squared divided by 12. You can always get this, at least for the simple one, such as Gaussian or

693
02:06:07,320 --> 02:06:14,920
 uniform distribution, use the formula. Of course, you also remember how to get the means and the

694
02:06:14,920 --> 02:06:23,160
 expectation. I hope you know this very basic definition, and you can derive this also by

695
02:06:23,160 --> 02:06:33,719
 yourself. Okay, so any question before we look into this chapter two? Chapter two is a basic

696
02:06:33,880 --> 02:06:37,240
 part of this whole estimation theory.

697
02:06:46,040 --> 02:06:56,440
 So far, it's all easy. Let's look at now how we are going to deal with minimum variance and unbiased

698
02:06:56,679 --> 02:07:14,919
 estimation. That's how this, in the part one, and this in the classical or traditional

699
02:07:16,919 --> 02:07:21,960
 parameter estimation theory, we're always trying to achieve this.

700
02:07:22,040 --> 02:07:28,120
 Doing it, making the estimation unbiased first, and now with that, we minimize the

701
02:07:28,120 --> 02:07:38,920
 variance of the estimate. So it looks like simple, but this is what we want to achieve. It depends on

702
02:07:39,880 --> 02:07:49,080
 how the data behaves. You can become very complicated or can be very simple. So this is

703
02:07:49,800 --> 02:07:57,000
 the way we want to try to get a good estimate. It may not be very optimal, but at least it looks

704
02:07:59,800 --> 02:08:08,840
 like it meets our objective from the beginning. So how good is it? On average, it will yield

705
02:08:08,840 --> 02:08:16,440
 the true parameter value. That's the definition of unbiased on average. Not necessarily require

706
02:08:17,160 --> 02:08:24,200
 infinity of very large data. We are hoping unbiased means you have a finite measure data.

707
02:08:26,200 --> 02:08:31,560
 And also at the same time, we are trying to achieve the same thing. So we are trying to

708
02:08:31,560 --> 02:08:36,680
 achieve the same thing. So we are trying to achieve the same thing.

709
02:08:36,680 --> 02:08:45,080
 And also at the same time, it gives the least variability. If possible, as I say,

710
02:08:45,080 --> 02:08:51,480
 the oscillation will be as small as possible. It cannot be zero, because if zero, totally zero,

711
02:08:51,480 --> 02:09:02,599
 then that's no noise. That's not our objective in this course. So it's small. Make it very small.

712
02:09:06,599 --> 02:09:17,559
 That's why we use this short form, MBUE, minimum variance, unbiased estimator.

713
02:09:18,200 --> 02:09:24,440
 That's how we are trying to do. Before that, we look at a simple example to start with.

714
02:09:26,760 --> 02:09:34,600
 I should say this is probably the easiest model we have in this one. Later, we will

715
02:09:34,600 --> 02:09:45,080
 come back to this again and again for a simple case. Let's look at what we have here. We have

716
02:09:45,080 --> 02:09:52,920
 the measured data. And you have the deterministic part. This is the parameter. We are going to

717
02:09:53,480 --> 02:10:03,480
 estimate the sum of A in noise. So what is the noise? Zero means Y Gaussian noise.

718
02:10:05,080 --> 02:10:14,440
 So that's how this stuff is. So don't get confused. Our measured data is a discrete sample,

719
02:10:14,440 --> 02:10:20,759
 because in total, typically we hear we start from zero. So if you have a capital N sample,

720
02:10:20,759 --> 02:10:28,839
 up to n minus 1, zero, one, two, three, and so on. But the noise except

721
02:10:31,000 --> 02:10:38,360
 follows Gaussian distribution. It will be a continuous random variable. So that means

722
02:10:38,440 --> 02:10:47,480
 zero mean. That means this Gaussian noise except the value is take any values from minus infinity

723
02:10:47,480 --> 02:10:57,320
 to infinity. But how high the chance of getting this random variable, the noise, to take a very

724
02:10:57,320 --> 02:11:07,080
 large value is very small. Because remember our Gaussian curve, you always remember it's like this.

725
02:11:08,040 --> 02:11:16,680
 Like a bell curve. That's right. You know, like the exams, we always ask whether the exam result,

726
02:11:16,680 --> 02:11:25,080
 the mark will follow Gaussian curve. Yeah, typically, yes, if the student's number is large enough,

727
02:11:25,080 --> 02:11:31,320
 we will see most of the students are near the mean value, where some very good one, very few,

728
02:11:31,400 --> 02:11:37,880
 to the other side. The other very poor one, the other side. But sometimes it doesn't appear in that

729
02:11:37,880 --> 02:11:47,080
 way. It could be mixed Gaussian. I mean, you have two peaks. Those are not very good. But they are

730
02:11:47,080 --> 02:11:55,160
 not towards the end. You're dividing to say, for example, if 60 is the mean value, from zero to 60,

731
02:11:55,320 --> 02:12:06,760
 maybe the average is 40. Then the other side, 60 to 100, the average may be 80. So it all depends.

732
02:12:06,760 --> 02:12:14,599
 But that's how Gaussian, when we talk about single Gaussian, it will be just one mean value and

733
02:12:15,480 --> 02:12:25,320
 follow this bell curve. So here, assuming zero means, and Gaussian noise.

734
02:12:28,280 --> 02:12:38,840
 So therefore, the simple guess is you have the metadata. Our A here is more like the DC value,

735
02:12:39,320 --> 02:12:46,600
 the constant value. Then since noise is zero mean, then the simple way to do it is just

736
02:12:46,600 --> 02:12:52,040
 simply take the sample mean. Sample mean mean, you see, we have capital N sample,

737
02:12:53,640 --> 02:12:59,080
 measure data, we just add them together, divide by n. That's very, that's the easiest

738
02:13:01,160 --> 02:13:07,160
 estimator you can ever think of, you know, with the measure data. I sampled that.

739
02:13:07,639 --> 02:13:15,720
 And that's also very, sometimes it turns out to be this is the best estimator you can have.

740
02:13:16,920 --> 02:13:23,240
 Look at example. In this case, in our setting here, this is a simple setting,

741
02:13:23,240 --> 02:13:30,280
 and satisfy this condition, you see. It turns out to be this is the one, the best you can achieve.

742
02:13:30,440 --> 02:13:36,679
 You know, there's no use. In this case, you'll derive more complicated estimator,

743
02:13:36,679 --> 02:13:45,080
 you will get the things, at most the same or worse. So sometimes, not necessarily you'll go to

744
02:13:45,080 --> 02:13:50,840
 very deep, say for example, you know, simple one may be already the best, then no point to go

745
02:13:51,400 --> 02:14:02,600
 any complicated or deeper. So let's see why. First, we verify it's unbiased. So this is,

746
02:14:04,280 --> 02:14:10,440
 typically this is what we will do in this estimation, and later you'll do

747
02:14:12,360 --> 02:14:18,440
 for the detection is always. Also, we are trying to get the means and

748
02:14:19,080 --> 02:14:27,320
 expectation. We set them really derived from the principle, like how to use a PDF and then do the

749
02:14:27,320 --> 02:14:32,519
 integration. It will be very complicated. What you'll do is you'll operate on the data, you see.

750
02:14:33,240 --> 02:14:39,639
 And for example, for expectation, expectation is a linear operator, you see.

751
02:14:40,440 --> 02:14:49,880
 Ah, linear operator, then you operate into the sums of these, you know, samples. You can

752
02:14:49,880 --> 02:14:57,720
 simply bring it in directly, and whatever constant you can pull out. Because expectation,

753
02:14:57,720 --> 02:15:05,640
 when you apply to a constant, I mean, only the constant has no effect on this. You can just

754
02:15:05,720 --> 02:15:14,600
 simply take it out, okay, because it's a linear operator. Then therefore, you take this out and

755
02:15:14,600 --> 02:15:23,160
 bring this E inside. So that means you are now summing each of the expectations, yeah. And then

756
02:15:23,160 --> 02:15:31,960
 each of these, you go back to here to see E of Xn equal to what? Again, this is linear, E of A,

757
02:15:31,960 --> 02:15:37,800
 A is a constant. So constant, the expectation of course equal to except, you see. You see,

758
02:15:37,800 --> 02:15:45,560
 like the DC value, you know, it's not random. And then, how about this? E of Wn, meaning you are

759
02:15:46,120 --> 02:15:53,080
 taking the means of this. And of course, this is zero mean. So you will be just equal to

760
02:15:54,040 --> 02:16:02,840
 A. And then how many of A sum together? In total you have capital N, then divide by N,

761
02:16:02,840 --> 02:16:09,960
 because equal to A. So that's easy, but it's the fundamental, you see. So you need to

762
02:16:11,240 --> 02:16:21,000
 operate using the same way. So that's unbiased. How about efficient? At the moment we don't know

763
02:16:21,000 --> 02:16:31,160
 CIOB yet. We're doing next chapter, next week. But then we can try to see whether we can apply

764
02:16:31,160 --> 02:16:36,360
 this variance. We need to get the variance, you see. Variance is equal to this. However,

765
02:16:36,360 --> 02:16:45,240
 variance is a second order operation, not a linear one. So you cannot take it ground by

766
02:16:45,959 --> 02:16:56,199
 breaking down this variance as a sum of each of the variance. Except, only in the case of this,

767
02:16:56,199 --> 02:17:03,959
 if this random variable, they are independent. Independent means there is no constant.

768
02:17:04,680 --> 02:17:12,119
 Okay? So if not independent, you will become complicated. You will have to create the constant,

769
02:17:12,120 --> 02:17:20,600
 because this is a square, variation is a second order operation. So we will go into that later.

770
02:17:22,920 --> 02:17:26,920
 Because the white Gaussian noise, they are independent, then you use this property,

771
02:17:26,920 --> 02:17:34,520
 bring in. But the constant here, you have to square it, because second order operator,

772
02:17:34,520 --> 02:17:41,880
 mean the constant, you will become square before you pull it out. Okay? Therefore,

773
02:17:41,880 --> 02:17:47,640
 you pull it out, then apply each. And then we know variance of each is sigma power of two.

774
02:17:48,360 --> 02:17:56,840
 Therefore, you sum this capital N of this together, but you divide by N square. So in the N,

775
02:17:56,840 --> 02:18:04,680
 the variance of this, related to the variance of the noise, you see, this is, okay, I forgot to

776
02:18:05,960 --> 02:18:15,960
 write this zero mean with the variance. If we don't write down, it will be always use sigma power of

777
02:18:15,960 --> 02:18:22,040
 two. That's the same way. That's how it's doing here, sigma power of two, divide by N.

778
02:18:23,000 --> 02:18:31,880
 Yeah. And then later, next week, we will show the CIOB is having to be equal to this. So

779
02:18:32,840 --> 02:18:41,000
 now assume we have it, we say this is efficient, since this variance equal to the CIOB. Therefore,

780
02:18:41,000 --> 02:18:51,640
 it's done. So that's look like very simple. Follow certain step, then you will achieve

781
02:18:51,639 --> 02:19:04,439
 what you, yeah. And then also we can get the PDF because let's go back again. What's our

782
02:19:05,719 --> 02:19:15,320
 estimator looks like? Will be summing a finite number of Gaussian random variable, right? X of

783
02:19:15,320 --> 02:19:25,799
 N. So each of X N, you can consider is no zero mean, Gaussian variable, because this is Gaussian,

784
02:19:25,799 --> 02:19:31,559
 you add a constant, you are shifting the mean. Assuming, of course, A not equal to zero. So you

785
02:19:32,440 --> 02:19:40,360
 make the means no longer zero, shift mean. But it's still Gaussian, you are adding a means,

786
02:19:40,360 --> 02:19:51,960
 adding like shift in terms of the PDF, you will be shifting it to, depending on positive, negative.

787
02:19:52,520 --> 02:20:01,320
 In terms of the sample accept the variable, you'll be shifting from zero mean, either up or down.

788
02:20:01,720 --> 02:20:09,960
 Okay. So therefore, you add finite number of Gaussian random variable together, it's still

789
02:20:09,960 --> 02:20:17,560
 Gaussian. That's a very important property. And that's also one of the very special case for

790
02:20:17,560 --> 02:20:26,440
 Gaussian. If you, for example, if you are adding two uniform distribution random variable, you

791
02:20:27,400 --> 02:20:35,400
 no longer get uniform distribution random variable in general. Similarly, you add any other,

792
02:20:37,000 --> 02:20:43,880
 exponential, you create another new random variable. It's not the same time anymore.

793
02:20:43,880 --> 02:20:52,280
 Only Gaussian preserve this. And that's why Gaussian random variable is so important,

794
02:20:52,280 --> 02:20:59,560
 and so useful in this thing. So therefore, since we know it's a Gaussian, we know the

795
02:21:00,440 --> 02:21:07,320
 PDF must be following the same pattern of each sample. It will be,

796
02:21:09,640 --> 02:21:16,200
 this is each sample equal to that. And estimate of this A will be

797
02:21:16,280 --> 02:21:26,200
 follow a similar distribution, except the mean equal to A, and the value is smaller than this one.

798
02:21:26,200 --> 02:21:41,320
 So you only need to change here into this. So that's it. Okay. So in this case, we're assuming

799
02:21:41,320 --> 02:21:49,640
 D is A is DC value. It can be positive, it can be negative. It can be any real value. Here we are

800
02:21:49,640 --> 02:21:58,039
 dealing only with real data. But sometimes the value of certain unknown parameter, it may be

801
02:21:58,039 --> 02:22:06,119
 restricted in terms of the physical consideration. So it could be only positive. For example, you are

802
02:22:06,120 --> 02:22:13,160
 estimating the variance of the PDF. It cannot be negative. The variance is always at least greater

803
02:22:13,160 --> 02:22:25,160
 or equal to zero. So yeah, so that's the case. You can look at the simulation, or just numerical

804
02:22:25,160 --> 02:22:34,120
 example. Assume A equal to five, and sigma equal to two, and n equal to five. So that means A is

805
02:22:34,360 --> 02:22:40,200
 n equal to five. So that means we have five samples. So that's what I say is, if you measure each,

806
02:22:40,760 --> 02:22:52,040
 it's not necessarily all equal to five. It could be around five. And then now, depending on, you see,

807
02:22:52,040 --> 02:23:02,120
 how this is. So don't get confused. This n is the number of samples you are taking the

808
02:23:03,080 --> 02:23:10,040
 average, your estimate each time. And this histogram is the one you are repeating, you see,

809
02:23:10,040 --> 02:23:20,120
 an ensemble of this one thousand realizations. So you take five, generate five random measured data,

810
02:23:21,880 --> 02:23:28,520
 using our formula, sample mean, to get. And then you can get the measured

811
02:23:28,520 --> 02:23:36,680
 values, the measured means and values. And then up there you regenerate again, another five

812
02:23:36,680 --> 02:23:45,400
 samples, and do it again. So therefore, statistically you can see the estimate of A sometimes is a very,

813
02:23:46,040 --> 02:23:52,040
 have a very big arrow. It gives you a very small value. Sometimes it gives a very big one. But

814
02:23:52,840 --> 02:23:59,960
 most of the time it's still reasonably near to five. So this is the histogram. Okay, you can see.

815
02:24:03,480 --> 02:24:10,360
 Furthermore, if we now increase the n equal to ten, so what happen? You'll see this become narrow,

816
02:24:10,360 --> 02:24:17,080
 you see, like better. Of course you'll see. Very, you don't, among this one thousand, you don't see,

817
02:24:17,080 --> 02:24:24,520
 at the smallest you'll get is three. Highest is seven. You go to the previous one, you can go down

818
02:24:24,520 --> 02:24:33,400
 to two and eight. So you'll expect the more sample you go, say n equal to one hundred, you'll get

819
02:24:33,400 --> 02:24:41,480
 much better. Yeah, so yeah. But even there you'll never get the exact, you know, numerically you

820
02:24:41,480 --> 02:24:47,080
 don't get exactly equal to five, you get something very close but not five. The very standard

821
02:24:47,080 --> 02:24:55,640
 deviation of variant reduced, but not equal to zero. You'll never get into zero. You may be

822
02:24:55,640 --> 02:25:05,080
 approaching zero. Okay, so yeah. Let's look at the case where you have the bios

823
02:25:05,800 --> 02:25:13,400
 estimate for the case of DC level. We won't go into too much detail here, but just highlight

824
02:25:14,280 --> 02:25:21,800
 why we always want to achieve the unbiased one. Because if you have bias, you cause some problem.

825
02:25:21,800 --> 02:25:30,680
 So let's follow the same example, except we purposely use the estimated by take the

826
02:25:31,400 --> 02:25:38,520
 summation but multiply by another parameter a not equal to one. Okay, so this is the one

827
02:25:39,640 --> 02:25:48,520
 we purposely do it. Then from here you can see the expect value of this, you already got the small

828
02:25:48,520 --> 02:25:57,640
 a times a. And since a not equal to one, we know this not equal to capital A. Okay, so therefore

829
02:25:57,640 --> 02:26:08,599
 it will give you a bias, unless in the case of capital A got zero, which is not very meaningful

830
02:26:08,599 --> 02:26:18,599
 here. So this value typically we're trying to see some no zero value. Then in this case we'll see why

831
02:26:18,599 --> 02:26:24,840
 at least the bias estimator is not good. So we want to measure the mean square error here.

832
02:26:25,640 --> 02:26:36,360
 So mean square mean, you see, our estimate subject the true one. This is different from the expectation,

833
02:26:36,360 --> 02:26:43,320
 so don't get confused. Mean square error mean we are defining estimate one subject to the true one.

834
02:26:44,680 --> 02:26:53,960
 And then after that, we remember there is a square here. So we purposely make this a little

835
02:26:53,960 --> 02:27:01,720
 bit more complicated, but by subtracting this expectation of a, since they are not the same,

836
02:27:01,720 --> 02:27:08,839
 you see. Yeah, so we subtract, but you need to add this back. Okay, so we create two terms here.

837
02:27:10,279 --> 02:27:18,519
 And why? Because if you explain this, it will give you the variant of this a,

838
02:27:18,519 --> 02:27:23,560
 that's by definition. This part give you the variant. And then the other part, it will be

839
02:27:24,199 --> 02:27:33,400
 bias term. Bias term raised power of two. Okay, so it's very easy to calculate this equal to that,

840
02:27:33,400 --> 02:27:42,679
 and this one equal to this. So we, yeah, you can try to derive by yourself. Now if you are

841
02:27:43,880 --> 02:27:51,880
 trying to see which a will give you the best to minimize this, then of course our tools here,

842
02:27:52,599 --> 02:27:59,880
 later you will always try to use. It always take derivation, you see. Derivation with respect to

843
02:27:59,880 --> 02:28:09,400
 small a, then making equal to zero, you can easily solve by the optimal small a, called this.

844
02:28:10,359 --> 02:28:19,880
 So, but this is value, optimal value of a depends on the unknown parameter. So that means it's not

845
02:28:19,880 --> 02:28:28,599
 not very good. We want to estimate this capital A, but we want to see how good to get this small a.

846
02:28:28,599 --> 02:28:39,400
 Then we rely on this unknown a, so this is not realizable. I mean you cannot achieve

847
02:28:39,400 --> 02:28:47,880
 this in just based on the data. So, yeah, if you look at the simulation, you also see

848
02:28:48,039 --> 02:28:56,839
 if your a equal to half, then our estimate value is near the 2.5, even when you have more data.

849
02:28:56,839 --> 02:29:06,039
 Okay, so that's, yeah, that's not a good estimate. So therefore, our motivation is always try to

850
02:29:06,439 --> 02:29:16,199
 motivate to get the MBRE, not the, yeah. And then here we give some illustration about

851
02:29:16,920 --> 02:29:27,400
 when we can have MBRE and when there is no MBRE. In the more general case, our parameter here, it could be,

852
02:29:29,880 --> 02:29:37,240
 you know, it could be not always a constant, like what we, we just now the simple example,

853
02:29:37,240 --> 02:29:45,480
 it could be, you know, certain curve, say this is one, the value of this,

854
02:29:46,520 --> 02:29:55,800
 so one follow this, when theta change, say assume real value, then you get this, theta 2 estimate

855
02:29:55,800 --> 02:30:03,640
 this, theta 3. So in this case, theta 3, assume we only have 3 estimate for all, then this is the

856
02:30:03,640 --> 02:30:11,880
 best because this is always lower than other, but not this case because you cannot say which one is

857
02:30:11,880 --> 02:30:20,680
 better, theta 2 hat or theta 3 hat, but in this part, this is better, sorry, in this segment,

858
02:30:20,680 --> 02:30:30,199
 this one is better, but beyond this point, then theta 3 hat is better. So, yeah, so therefore,

859
02:30:31,160 --> 02:30:36,200
 we show this as a uniformly MBRE, this one, you don't have.

860
02:30:38,040 --> 02:30:47,480
 And we can even use one example to show here, and here, you see, as I mentioned, every time you have

861
02:30:47,480 --> 02:30:54,280
 to be careful how many data samples or observations you are dealing with. So now we are looking at two,

862
02:30:55,240 --> 02:31:02,120
 one sample follow this, the other sample follow this, this is for all the theta,

863
02:31:04,600 --> 02:31:11,400
 M is one sample, so it's always the same mean, whereas this one, it depends on

864
02:31:12,120 --> 02:31:21,560
 theta is positive or negative, and you can do different way. So therefore, in this case,

865
02:31:21,560 --> 02:31:34,600
 let's see how we are going to get this. So we now assume we are dealing with, we are using two

866
02:31:35,400 --> 02:31:45,320
 estimators, one is simply sum this two sample divided by two, the other one is we give more

867
02:31:45,320 --> 02:31:53,640
 weight to this one, or less to this one, because this sphere, you may have larger variance here.

868
02:31:55,080 --> 02:32:03,240
 Now we want to see which one is better, so we need to first easy to verify, both are unbiased,

869
02:32:03,960 --> 02:32:11,960
 you can do it easily, but however, to go to the variance, the good thing is those are independent,

870
02:32:11,960 --> 02:32:20,119
 so you simply add the variance of both, but don't forget to square this, any constant you need to

871
02:32:20,119 --> 02:32:30,119
 square, and then the other one you also square the coefficient, the other one square, and now we can

872
02:32:31,400 --> 02:32:38,599
 get the variance of each, but since it involves both data, this data and that data,

873
02:32:38,600 --> 02:32:45,160
 we have to divide the theta into two segments, theta greater than zero,

874
02:32:46,040 --> 02:32:52,520
 for this one, you'll get one variance, theta smaller than zero, you have to

875
02:32:53,240 --> 02:32:59,480
 follow this, then you get a large one, so this is one case. Similarly, for theta two-head,

876
02:33:00,280 --> 02:33:07,560
 you also do the same, theta greater than zero, you get something, theta smaller than zero, you get

877
02:33:07,560 --> 02:33:18,119
 this value. Now we want to see which one, is it here, theta one is for negative theta value, you have

878
02:33:18,119 --> 02:33:27,640
 a larger variance compared to theta two-head, but however, for the positive theta, it gets a smaller

879
02:33:27,640 --> 02:33:38,199
 variance compared to theta, so this is the good example, auto simulation, it verified this case,

880
02:33:39,080 --> 02:33:44,199
 yeah, something like this case, but we only have two, you can ignore this, so that means,

881
02:33:45,800 --> 02:33:49,160
 yeah, you can go back and look at the value here again, so you see,

882
02:33:51,000 --> 02:33:55,640
 this one is smaller than that, but this one is greater than that, so we can't say which one is

883
02:33:55,640 --> 02:34:04,680
 better, so that means there is no minimum value, unbi-estimated in this case, okay, so therefore,

884
02:34:04,680 --> 02:34:14,519
 how to find the MBOE, that's our next topic we are going to do, but different from

885
02:34:16,039 --> 02:34:24,519
 detection theory, that's why I say the second part is somewhat easier, I will say, in a not so accurate

886
02:34:24,520 --> 02:34:37,080
 comparison, you know all of you, learn calculus, you know the derivation, take the differentiation

887
02:34:37,080 --> 02:34:47,480
 and the integration, to take the derivative, however the formula complicated, you can follow

888
02:34:47,480 --> 02:34:55,880
 the chain rules to do it one by one, in the end it may become very long, but you can, it's workable,

889
02:34:55,880 --> 02:35:04,439
 as long as you know how to take the derivative, however, so that's a big similar to the detection

890
02:35:04,439 --> 02:35:10,840
 theory, or at least the easier part of the detection theory, so you think about, you learn some rules,

891
02:35:10,840 --> 02:35:18,360
 then you just apply, and quite often you need to simplify the result, while on the other hand,

892
02:35:18,360 --> 02:35:25,640
 parameter estimation is a little bit like integration, you need to find some tricks to

893
02:35:27,400 --> 02:35:32,440
 make sense to do the integration, although it may be there, but if you cannot think about the

894
02:35:33,160 --> 02:35:39,560
 correct way to do it, you may be stuck, so that's why we say, no turn the crane, turn the crane,

895
02:35:40,199 --> 02:35:47,960
 operate on the crane, you can do it step by step, so yeah, even though the MVOE exists,

896
02:35:48,760 --> 02:35:54,840
 and then how to do that, there are several ways, one is you can, in the next chapter,

897
02:35:54,840 --> 02:36:04,279
 you can use CIOB, and then we check some estimator, whether it satisfies this, then if you meet the

898
02:36:04,280 --> 02:36:14,680
 CIOB, certainly you already get the MVOE, but quite often this may not be achieved, then another

899
02:36:14,680 --> 02:36:22,440
 way is you assume the unbiased estimator to have a linear form, so it's more like a linear function,

900
02:36:22,440 --> 02:36:31,400
 and from there it's easier, you can get the best linear estimator, but this is best only

901
02:36:32,119 --> 02:36:40,279
 for the linear form, not in general, but yeah, that's how we will go next, and there are some other

902
02:36:40,840 --> 02:36:52,359
 methods such as maximum likelihood and so on, we will discuss later, so as I say, we take an easier

903
02:36:52,920 --> 02:37:02,040
 approach to just give you a very basic idea, introduction to this parameter estimation

904
02:37:02,600 --> 02:37:12,920
 theory, and then also some important PDA, and then also start with this very simple instruction to

905
02:37:13,240 --> 02:37:25,160
 MVOE, where subsequently we will go in more detail about how to get MVOE using different

906
02:37:26,200 --> 02:37:36,520
 methods, so I think that's about time to finish, we will end the lecture here,

907
02:37:37,480 --> 02:37:49,640
 we'll see you next week, assuming you decide to take, any questions you can stay back or

908
02:37:49,640 --> 02:38:00,120
 write an email to me, I'll see you next week.

909
02:38:06,520 --> 02:38:14,120
 Thank you.

910
02:44:36,520 --> 02:44:44,120
 Thank you.

911
02:45:36,520 --> 02:45:40,120
 Thank you.

912
02:46:06,520 --> 02:46:10,120
 Thank you.

913
02:46:36,520 --> 02:46:40,120
 Thank you.

914
02:47:06,520 --> 02:47:10,120
 Thank you.

915
02:47:36,520 --> 02:47:38,600
 Thank you.

916
02:48:06,520 --> 02:48:08,600
 Thank you.

917
02:48:36,520 --> 02:48:38,600
 Thank you.

918
02:49:06,520 --> 02:49:08,600
 Thank you.

919
02:49:36,520 --> 02:49:38,600
 Thank you.

920
02:50:06,520 --> 02:50:08,600
 Thank you.

921
02:50:36,520 --> 02:50:38,600
 Thank you.

922
02:51:06,520 --> 02:51:08,600
 Thank you.

923
02:51:36,520 --> 02:51:38,600
 Thank you.

924
02:52:06,520 --> 02:52:08,600
 Thank you.

925
02:52:36,520 --> 02:52:38,600
 Thank you.

926
02:53:06,520 --> 02:53:08,600
 Thank you.

927
02:53:36,520 --> 02:53:38,600
 Thank you.

928
02:54:06,520 --> 02:54:08,600
 Thank you.

929
02:54:36,520 --> 02:54:38,600
 Thank you.

930
02:55:06,520 --> 02:55:08,600
 Thank you.

931
02:55:36,520 --> 02:55:38,600
 Thank you.

932
02:56:06,520 --> 02:56:08,600
 Thank you.

933
02:56:36,520 --> 02:56:38,600
 Thank you.

934
02:57:06,520 --> 02:57:08,600
 Thank you.

935
02:57:36,520 --> 02:57:38,600
 Thank you.

936
02:58:06,520 --> 02:58:08,600
 Thank you.

937
02:58:36,520 --> 02:58:38,600
 Thank you.

938
02:59:06,520 --> 02:59:08,600
 Thank you.

939
02:59:36,520 --> 02:59:38,600
 Thank you.

