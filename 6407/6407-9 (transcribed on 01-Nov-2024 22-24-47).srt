1
00:05:00,000 --> 00:05:26,560
 Okay.

2
00:05:26,560 --> 00:05:35,080
 So in the last two weeks, actually, we started to actually a very important panel classified,

3
00:05:35,080 --> 00:05:41,040
 including the Fischer's linear digital analysis and then the linear spot-widen machines.

4
00:05:41,040 --> 00:05:49,560
 And of course, actually, the two linear classified actually adopt different ideas.

5
00:05:49,560 --> 00:05:55,440
 And in the linear spot-widen machine, we try to find the decision boundary so that the

6
00:05:55,440 --> 00:05:59,280
 marginal separation is maximized.

7
00:05:59,280 --> 00:06:04,560
 So the maximization of the marginal separation.

8
00:06:04,560 --> 00:06:08,960
 So actually, the marginal separation here, actually, we use a distant concept.

9
00:06:08,960 --> 00:06:13,440
 So the distance from the nearest sample to the decision boundary.

10
00:06:13,440 --> 00:06:17,120
 So that is the marginal separation.

11
00:06:17,120 --> 00:06:26,560
 And in the Fischer's linear digital analysis, and we try to project the data into one dimension

12
00:06:26,560 --> 00:06:33,960
 so that actually the difference between the classes, right, that is, you know, between

13
00:06:33,960 --> 00:06:37,640
 class scatter and is maximized.

14
00:06:37,640 --> 00:06:41,000
 And also, the within class scatter is minimized.

15
00:06:41,000 --> 00:06:46,080
 And this scatter actually is based on the concept of the distance, right, the distance

16
00:06:46,080 --> 00:06:52,320
 from each data point to the center to the mean, to the mean, or within the same class,

17
00:06:52,320 --> 00:06:53,320
 right?

18
00:06:53,320 --> 00:06:57,960
 And between class scatter is based on the distance between the two mean values after

19
00:06:57,960 --> 00:07:01,960
 projection of the data onto the dimension of the value.

20
00:07:01,960 --> 00:07:10,479
 So we, in both matters, we use the concept of distance and in particular, the Euclidean

21
00:07:10,479 --> 00:07:11,479
 distance.

22
00:07:11,960 --> 00:07:18,080
 And actually, this Euclidean distance metric actually is valid, is meaningful, only for

23
00:07:18,080 --> 00:07:22,840
 the real value, actually, in Fischer vectors.

24
00:07:22,840 --> 00:07:28,880
 But we know actually, in the first, second week, actually, when we study the different

25
00:07:28,880 --> 00:07:35,120
 types of features, right, we know there are, can you know the features, discrete features?

26
00:07:35,120 --> 00:07:38,480
 For discrete, and we have two types of categoricals.

27
00:07:38,480 --> 00:07:42,720
 One is the ordinal, another is nominal.

28
00:07:42,720 --> 00:07:50,920
 Nominal quite often is the label, the name of the attribute, right?

29
00:07:50,920 --> 00:07:56,120
 For example, for a man, for a gender, right, or sex, you know, we have male or we have

30
00:07:56,120 --> 00:07:58,560
 female, okay?

31
00:07:58,560 --> 00:08:07,920
 And in such a scenario, actually, and the metric, like a distance, is not actually very meaningful.

32
00:08:09,200 --> 00:08:14,200
 And actually, so actually, we know how to calculate the distance, right?

33
00:08:14,200 --> 00:08:19,480
 For any arbitrary two vectors, actually, now we use the difference, right?

34
00:08:19,480 --> 00:08:21,200
 You use the difference between the two.

35
00:08:21,200 --> 00:08:26,680
 Actually, so for the two samples, X, Y, X, O, the feature values, feature number one,

36
00:08:26,680 --> 00:08:27,800
 number two, right?

37
00:08:27,800 --> 00:08:30,720
 And then for the two samples, we have different values, okay?

38
00:08:30,720 --> 00:08:34,560
 The different values, of course, should be different, okay?

39
00:08:34,640 --> 00:08:40,280
 And also, the size of the difference, the size of difference, the difference, okay?

40
00:08:40,280 --> 00:08:45,680
 Or the difference between the two values also show how different they are, okay?

41
00:08:45,680 --> 00:08:50,280
 But for some, actually, like nominal features, although we give them different values.

42
00:08:50,280 --> 00:08:59,520
 For example, and in the such data, like a transportation mode, and we can have,

43
00:08:59,640 --> 00:09:06,480
 like a train, we can have a car, we can have a bus, we can have actually the air,

44
00:09:06,480 --> 00:09:10,480
 and actually, we can have a sea, we can have different modes, right?

45
00:09:10,480 --> 00:09:16,240
 And normally, we need to give a numerical value to each of these, actually, values, okay?

46
00:09:16,240 --> 00:09:23,160
 For example, one to train, and two to car, and three to air, and four to sea, okay?

47
00:09:23,160 --> 00:09:26,720
 Of course, actually, the different values, we use different values, right?

48
00:09:26,720 --> 00:09:28,480
 We assign them different values.

49
00:09:28,480 --> 00:09:32,280
 The difference only show they are different, but the size of the difference

50
00:09:32,280 --> 00:09:35,840
 don't show how different they are.

51
00:09:35,840 --> 00:09:38,840
 For example, actually, we see the train car, right?

52
00:09:38,840 --> 00:09:42,680
 Just one, actually, one is one, one is one, another value is two, okay?

53
00:09:42,680 --> 00:09:45,240
 So the values are different, okay?

54
00:09:45,240 --> 00:09:47,400
 We show they are different, okay?

55
00:09:47,400 --> 00:09:51,360
 And for value train one, and the sea is four, okay?

56
00:09:51,360 --> 00:09:54,240
 So of course, the two values are different.

57
00:09:54,240 --> 00:09:58,120
 But these differences don't show how different they are.

58
00:09:58,160 --> 00:10:03,520
 They don't mean, okay, the distance between train and car is less than the distance

59
00:10:03,520 --> 00:10:05,440
 from train to sea.

60
00:10:05,440 --> 00:10:11,800
 And then we see all the train and car are more similar than the train to, actually, the sea.

61
00:10:11,800 --> 00:10:14,920
 We cannot see that, okay?

62
00:10:14,920 --> 00:10:20,000
 Only, actually, the difference show, you know, really give an evaluation for

63
00:10:20,000 --> 00:10:24,320
 the difference, right, of the two, actually, you know, values.

64
00:10:24,320 --> 00:10:29,000
 And then, actually, the distance metric is meaningful, okay?

65
00:10:29,000 --> 00:10:34,120
 And like, yeah, I think this is quite, I think that's natural, right?

66
00:10:34,120 --> 00:10:36,480
 Like if you have a body weight, okay?

67
00:10:36,480 --> 00:10:42,080
 Why is 60 kg, another 70 kg, another, you know, like a 65 kg, right?

68
00:10:42,080 --> 00:10:43,680
 So the values are different.

69
00:10:43,680 --> 00:10:49,800
 And also this, this, this is actually, the value also show how different they are, okay?

70
00:10:49,800 --> 00:10:56,400
 So in such a scenario, these are real value that we, you know, make us, I think,

71
00:10:56,400 --> 00:11:02,319
 the distance metric is a good evaluation, is a good measure, and to evaluate the similarity

72
00:11:02,319 --> 00:11:05,719
 between the data samples.

73
00:11:05,719 --> 00:11:12,199
 And, but if the data is a mix, a mix, that means the data contains real value,

74
00:11:12,199 --> 00:11:17,920
 no features, contain, actually, the, like, a nominal features.

75
00:11:17,920 --> 00:11:24,719
 And then we should not use, actually, the distance metric based on the classifier.

76
00:11:24,719 --> 00:11:28,439
 Of course, we can still use that, but then the result may not be, actually,

77
00:11:28,439 --> 00:11:30,760
 the optimal result, okay?

78
00:11:30,760 --> 00:11:33,719
 I think you can still use that, okay?

79
00:11:33,719 --> 00:11:36,160
 And this is because the reason here, right?

80
00:11:36,160 --> 00:11:43,599
 And actually, the similarity, actually, can only be measured based on the difference

81
00:11:43,599 --> 00:11:47,520
 between the two values, okay?

82
00:11:47,520 --> 00:11:51,240
 So this is for the nominal features, and another example for the color, right?

83
00:11:51,240 --> 00:11:52,240
 So it's also nominal.

84
00:11:54,800 --> 00:12:03,640
 And actually, for such a mixed data, and certainly, we should use, actually,

85
00:12:03,640 --> 00:12:07,640
 the non-metric, non-metric based method, non-metric.

86
00:12:07,640 --> 00:12:13,439
 That means that we don't try to, actually, design a classifier based on the distance

87
00:12:13,440 --> 00:12:17,880
 between the, actually, the feature vectors, between the samples.

88
00:12:19,240 --> 00:12:22,440
 Okay, we can use a non-metric method, okay?

89
00:12:22,440 --> 00:12:27,240
 Of course, you can still use a metric based method, like linear

90
00:12:27,240 --> 00:12:30,640
 spoil animation, or use a linear, linear,

91
00:12:30,640 --> 00:12:34,360
 these monies, you can still use them, okay?

92
00:12:34,360 --> 00:12:37,720
 But actually, the performance may not be optimal, okay?

93
00:12:38,720 --> 00:12:43,920
 And today, actually, we are going to introduce, actually, one method that is,

94
00:12:43,920 --> 00:12:48,680
 actually, is a kind of non-metric method, okay?

95
00:12:48,680 --> 00:12:53,600
 We don't, based on the specific distance metric, okay?

96
00:12:53,600 --> 00:12:59,160
 And this method is, actually, just the classification tree, classification tree, okay?

97
00:12:59,160 --> 00:13:04,760
 And so, so this diagram shows a classification tree.

98
00:13:04,760 --> 00:13:09,600
 So, certainly, classification tree is a graph model, right?

99
00:13:09,600 --> 00:13:10,640
 It's a graph.

100
00:13:10,640 --> 00:13:12,319
 You can see these are graph, right?

101
00:13:12,319 --> 00:13:18,480
 So, this is different from other models, like, in the linear spoil

102
00:13:18,480 --> 00:13:22,800
 animation, the classifier tree, just a linear, actually, no function, right?

103
00:13:22,800 --> 00:13:24,560
 A linear equation, okay?

104
00:13:24,560 --> 00:13:27,840
 So here, actually, we use a graph, okay?

105
00:13:27,840 --> 00:13:34,040
 And actually, this graph can be considered as a implementation.

106
00:13:34,040 --> 00:13:37,640
 It's a certain way of implementation of the rules, okay?

107
00:13:37,640 --> 00:13:42,560
 So, later, actually, through one example, we can see, actually, correspond to rules, okay?

108
00:13:42,560 --> 00:13:47,719
 So, this is actually the graph representation of the rules, okay?

109
00:13:47,719 --> 00:13:52,800
 So, it's based on, and actually, so, from this graph, actually, or

110
00:13:52,800 --> 00:13:59,160
 this classification tree, and we can see many, like, circles, right?

111
00:13:59,160 --> 00:14:02,040
 This could be quite often, this could, no, not a circle, okay?

112
00:14:02,040 --> 00:14:05,280
 So, here, this is actually, this is kind of a node.

113
00:14:05,280 --> 00:14:07,000
 This is kind of a node, okay?

114
00:14:07,000 --> 00:14:08,920
 And there are many nodes here, right?

115
00:14:08,920 --> 00:14:17,400
 And each node, actually, we ask a question based on one particular attribute or feature, okay?

116
00:14:17,400 --> 00:14:22,000
 For example, here, we are going to classify a fruit, okay?

117
00:14:22,000 --> 00:14:25,560
 And each fruit, actually, represented by a few features.

118
00:14:25,560 --> 00:14:30,800
 For example, the size of the feature, the size of the fruit.

119
00:14:30,800 --> 00:14:33,760
 Another, this is the color of the fruit.

120
00:14:33,760 --> 00:14:35,359
 And then, another is the taste of the fruit.

121
00:14:36,859 --> 00:14:41,240
 The color, the size, the shape, and also the taste, okay?

122
00:14:41,240 --> 00:14:45,959
 So, each fruit is represented by these four features, okay?

123
00:14:45,959 --> 00:14:51,400
 We can see, actually, at a different node, we ask different questions, okay?

124
00:14:51,400 --> 00:14:57,439
 And regarding a different attribute, okay?

125
00:14:57,439 --> 00:14:59,920
 So, this is just a classification tree.

126
00:14:59,920 --> 00:15:07,280
 And actually, on the top, the top level here, the node here is called rule node, rule node.

127
00:15:07,280 --> 00:15:14,160
 And also, we have many nodes, actually, which are the terminal nodes, also called leaf node.

128
00:15:15,719 --> 00:15:17,280
 So, you see, these are leaf nodes.

129
00:15:17,280 --> 00:15:21,479
 And actually, the leaf node, I gave a class label, right?

130
00:15:21,479 --> 00:15:23,839
 Each leaf node has a label.

131
00:15:23,839 --> 00:15:27,079
 For this node, it is a watermelon, okay?

132
00:15:27,080 --> 00:15:35,600
 And this node is apple, and grape, and cherry, or different tree, class labels, okay?

133
00:15:35,600 --> 00:15:39,360
 So, these are the leaf nodes, which are also called terminal nodes.

134
00:15:39,360 --> 00:15:43,640
 So, we have the rule node, and we have the leaf node, okay?

135
00:15:43,640 --> 00:15:46,520
 And also, we have many nodes in between, okay?

136
00:15:46,520 --> 00:15:51,680
 And for one node, a testing sample, you have one sample, right?

137
00:15:51,680 --> 00:16:00,359
 And we have certain features regarding the size, the shape, the taste, or the color, okay?

138
00:16:00,359 --> 00:16:03,880
 And then, we can just go through this graph, okay?

139
00:16:03,880 --> 00:16:06,560
 Based on the question asked, okay?

140
00:16:06,560 --> 00:16:08,160
 Color is all green?

141
00:16:08,160 --> 00:16:11,520
 Yes, we go to the left branch, no, we go to the right branch.

142
00:16:11,520 --> 00:16:15,680
 And then, we ask another question, like, based on the color.

143
00:16:15,680 --> 00:16:19,479
 And then, later, we based on the shape, or based on the taste.

144
00:16:19,480 --> 00:16:29,280
 And we ask a series of questions, or a test sample, answer a series of questions.

145
00:16:29,280 --> 00:16:35,000
 Finally, it will come to one of the terminal nodes, okay?

146
00:16:35,000 --> 00:16:38,800
 Once you come to a terminal node, and then the label is determined, right?

147
00:16:38,800 --> 00:16:43,720
 If you come to this node, then it is apple, okay?

148
00:16:43,720 --> 00:16:46,280
 So, there are many terminal nodes.

149
00:16:46,280 --> 00:16:48,880
 So, this is a class label.

150
00:16:48,880 --> 00:16:56,040
 So, for data, with the nominal feature, and also, other continuous features, right?

151
00:16:56,040 --> 00:17:05,400
 And we can recommend to use this kind of model classification tree to perform the classifications.

152
00:17:05,400 --> 00:17:09,240
 Okay. And some of you, actually, may not be very familiar with classification tree,

153
00:17:09,240 --> 00:17:13,720
 but some of you probably know the random forest, right?

154
00:17:13,720 --> 00:17:16,839
 So, later, we will also talk about the random forest.

155
00:17:16,839 --> 00:17:20,040
 So, random forest consists of many trees.

156
00:17:20,040 --> 00:17:26,639
 Of course, the trees actually are trained on different, actually, data set.

157
00:17:26,639 --> 00:17:30,120
 Random is selected from the training data set.

158
00:17:30,120 --> 00:17:32,280
 Okay. So, it's called random forest.

159
00:17:32,280 --> 00:17:35,480
 Okay. So, later, today, class, we will talk about that.

160
00:17:35,480 --> 00:17:37,600
 Okay. So, you are familiar with that.

161
00:17:37,600 --> 00:17:42,840
 Actually, the foundation, actually, is a classification tree.

162
00:17:42,840 --> 00:17:45,520
 Okay.

163
00:17:45,520 --> 00:17:49,760
 So, actually, from the node, these trees are called branch, okay?

164
00:17:49,760 --> 00:17:55,600
 Branch, actually. So, after I ask this question, the data is the full data.

165
00:17:55,600 --> 00:17:59,240
 At the beginning, actually, the data is a full training data set, right?

166
00:17:59,240 --> 00:18:01,120
 Full training data set. Okay.

167
00:18:01,120 --> 00:18:05,480
 And then, the data will be divided into two paths.

168
00:18:05,480 --> 00:18:10,160
 Okay. Or we will split into two data subset.

169
00:18:10,160 --> 00:18:14,080
 Okay. One subset to the left side, another to the left side.

170
00:18:14,159 --> 00:18:16,240
 Okay. So, this is called branch, okay?

171
00:18:16,240 --> 00:18:18,840
 Or split, or division of the data.

172
00:18:20,600 --> 00:18:24,600
 Actually, each node, after I answer this question,

173
00:18:24,600 --> 00:18:28,120
 the data will be divided into two subsets.

174
00:18:28,120 --> 00:18:31,960
 Okay. So, this is a classification tree.

175
00:18:31,960 --> 00:18:34,840
 Okay. So, it's a flowchart.

176
00:18:34,840 --> 00:18:37,120
 Okay. So, with a tree structure.

177
00:18:37,120 --> 00:18:43,879
 And, okay. So, the question asked at each node is regarding a particular

178
00:18:43,880 --> 00:18:48,040
 property, right? Property just, you know, the feature or attribute.

179
00:18:48,040 --> 00:18:51,200
 Okay. And one of the reasons we use this classification tree,

180
00:18:51,200 --> 00:18:53,200
 I already answered this question, right?

181
00:18:53,200 --> 00:18:59,080
 And because the tree for mixed data, which contains the nominal features.

182
00:18:59,080 --> 00:19:02,640
 And then, the distance metric is not very meaningful.

183
00:19:02,640 --> 00:19:04,480
 Not very meaningful. Okay.

184
00:19:04,480 --> 00:19:08,040
 Of course, this metric is meaningful for those part of the feature,

185
00:19:08,040 --> 00:19:11,320
 continuous features, real value features.

186
00:19:11,439 --> 00:19:14,240
 For part of the feature who are nominal,

187
00:19:14,240 --> 00:19:18,040
 then the distance metric is not very meaningful.

188
00:19:18,040 --> 00:19:23,120
 Okay. Then there's a reason why we use this class feature tree,

189
00:19:23,120 --> 00:19:27,919
 which is a non-metric method, not based on the distance metric.

190
00:19:27,919 --> 00:19:30,240
 It's a non-metric method. Okay.

191
00:19:30,240 --> 00:19:33,960
 Of course, besides this, actually, we have other tree, you know,

192
00:19:35,399 --> 00:19:38,200
 advantage. So, that is interpretability.

193
00:19:39,200 --> 00:19:41,800
 In AI, in machine learning, right?

194
00:19:41,800 --> 00:19:45,480
 Actually, we know sometimes, actually, we just need to,

195
00:19:47,280 --> 00:19:49,880
 we just need a prediction based on the model.

196
00:19:49,880 --> 00:19:55,720
 Okay. We don't need to understand, actually, the inside of the model.

197
00:19:55,720 --> 00:19:57,920
 We don't care, you know. Okay.

198
00:19:57,920 --> 00:20:01,640
 But in some scenarios, actually, not only we want to know, actually,

199
00:20:01,640 --> 00:20:07,360
 the prediction, right, the prediction result, but we also know,

200
00:20:07,360 --> 00:20:09,120
 actually, why we get these results.

201
00:20:37,360 --> 00:20:46,320
 Okay. So, we want to explain, right?

202
00:20:46,320 --> 00:20:48,439
 So, that is the explainable AI.

203
00:20:48,439 --> 00:20:51,479
 We're not going to explain the model.

204
00:20:51,479 --> 00:20:56,959
 We want to know the result, but we also know how these results are obtained.

205
00:20:56,959 --> 00:21:00,320
 So, that is the interpretability of the model. Okay.

206
00:21:00,320 --> 00:21:05,040
 And actually, the class feature tree is a kind of interpretable model.

207
00:21:05,040 --> 00:21:08,040
 Because, actually, at each, you know, we get these results.

208
00:21:08,040 --> 00:21:09,320
 Actually, we get a reason, right?

209
00:21:09,320 --> 00:21:13,840
 When we go through the graph, actually, when we go through this class tree,

210
00:21:13,840 --> 00:21:15,240
 actually, we answer the question.

211
00:21:15,240 --> 00:21:16,399
 So, why we get these results?

212
00:21:16,399 --> 00:21:18,040
 Why is the watermelon okay?

213
00:21:18,040 --> 00:21:21,720
 Or because the size is big and because, you know, green color.

214
00:21:21,720 --> 00:21:25,879
 And because, actually, you know, the taste is sweet, right?

215
00:21:25,879 --> 00:21:27,800
 Based on all these features.

216
00:21:27,800 --> 00:21:30,120
 And then we finally get these results.

217
00:21:30,120 --> 00:21:34,040
 So, I think, through answer this question, and at each question,

218
00:21:34,040 --> 00:21:39,840
 you know, we, at each node, we answer questions about this specific feature, right?

219
00:21:39,840 --> 00:21:43,639
 So, this process, it just, it's just this plan.

220
00:21:43,639 --> 00:21:44,360
 Okay.

221
00:21:44,360 --> 00:21:48,360
 The explanation of why we get these results.

222
00:21:48,360 --> 00:21:51,760
 Okay. So, class tree is interpretable.

223
00:21:51,760 --> 00:21:58,840
 So, this is a very, actually, an important feature.

224
00:21:58,840 --> 00:22:01,120
 And of course, these depend on applications, right?

225
00:22:01,120 --> 00:22:06,800
 So, in my past research, you know, many of my work related to very important areas, right?

226
00:22:06,800 --> 00:22:09,520
 So, like, security or defense.

227
00:22:09,520 --> 00:22:13,840
 So, actually, the explainability is very important.

228
00:22:13,840 --> 00:22:18,760
 Okay. And because we need to make, you know, critical decision-making.

229
00:22:18,760 --> 00:22:23,320
 Okay. We need to, actually, the model could actually, you know, make decision, right?

230
00:22:23,320 --> 00:22:25,520
 But the decision is very critical.

231
00:22:25,520 --> 00:22:28,919
 Okay. And not like, say, you know, it's a web page, right?

232
00:22:28,920 --> 00:22:33,560
 Whether you receive an advertisement, this is a tree, maybe you are not interested in that.

233
00:22:33,560 --> 00:22:35,760
 You will receive such a message.

234
00:22:35,760 --> 00:22:37,440
 But that don't matter, right?

235
00:22:37,440 --> 00:22:41,000
 That don't create a very, you know, severe consequences.

236
00:22:41,000 --> 00:22:47,800
 Okay. But some scenarios, okay, like if you use AI or machine learning to perform diagnosis

237
00:22:47,800 --> 00:22:49,800
 of a certain disease, right?

238
00:22:49,800 --> 00:22:53,640
 If the model tells you, oh, the patient has a cancer, for example, right?

239
00:22:53,640 --> 00:22:59,320
 These are very critical and decision-making problems, right?

240
00:22:59,320 --> 00:23:05,520
 So, we must know why, actually, this patient is diagnosed with, actually, the cancer.

241
00:23:05,520 --> 00:23:07,160
 We want to know the reasons.

242
00:23:07,160 --> 00:23:10,240
 So, the explainability of the model is very important.

243
00:23:10,240 --> 00:23:14,280
 So, this class of clinical tree is just this kind of model.

244
00:23:14,280 --> 00:23:16,840
 Okay. And, okay.

245
00:23:17,040 --> 00:23:28,480
 Another tree, very important feature is that, actually, we can actually put the prior knowledge

246
00:23:28,480 --> 00:23:31,480
 into the class of the clinical tree.

247
00:23:31,480 --> 00:23:33,399
 And the machine learning, of course, is data-driven, right?

248
00:23:33,399 --> 00:23:34,399
 Data-driven.

249
00:23:34,399 --> 00:23:39,399
 And we try to build the models based on the data.

250
00:23:39,399 --> 00:23:45,240
 But some, in some scenarios, actually, we need to incorporate the knowledge.

251
00:23:46,000 --> 00:23:53,000
 Okay. And in particular, if the data, actually, available is in small size.

252
00:23:53,000 --> 00:24:00,000
 Okay. And, you know, nowadays, the model becomes, actually, more and more complex.

253
00:24:00,000 --> 00:24:03,000
 That means we have more and more parameters in the model.

254
00:24:03,000 --> 00:24:09,000
 And then we need to have more and more, actually, training samples in order to estimate, actually,

255
00:24:09,000 --> 00:24:13,000
 the value for the parameters in the model.

256
00:24:13,000 --> 00:24:14,000
 Okay.

257
00:24:14,760 --> 00:24:21,760
 But if we use, actually, the knowledge, the, I mean, the explain knowledge, actually,

258
00:24:21,760 --> 00:24:26,760
 now we can reduce the reliance on the data size.

259
00:24:26,760 --> 00:24:27,760
 Okay.

260
00:24:27,760 --> 00:24:33,760
 And based on the knowledge, okay, even with a small size of training data, we can still,

261
00:24:33,760 --> 00:24:38,760
 you know, build a model with, actually, reasonable performance.

262
00:24:38,760 --> 00:24:39,760
 Okay.

263
00:24:39,760 --> 00:24:42,760
 So, this is very reasonable, right?

264
00:24:42,760 --> 00:24:45,760
 And we already know, actually, oh, water management is big, right?

265
00:24:45,760 --> 00:24:48,760
 So, naturally, I think you can design a rule, right?

266
00:24:48,760 --> 00:24:50,760
 You can design a rule.

267
00:24:50,760 --> 00:24:54,520
 Actually, we recall the development of the artificial intelligence.

268
00:24:54,520 --> 00:24:58,520
 The first generation is just, you know, the rule-based, the rule-based model, right?

269
00:24:58,520 --> 00:25:03,520
 Based on the, I mean, as per knowledge, and we write many rules.

270
00:25:03,520 --> 00:25:04,520
 Okay.

271
00:25:04,520 --> 00:25:09,760
 Actually, in the class-fusion tree, actually, each, actually, node, actually, is equivalent

272
00:25:09,760 --> 00:25:10,760
 to a rule.

273
00:25:10,760 --> 00:25:11,760
 Okay.

274
00:25:11,760 --> 00:25:19,760
 So, in rule, naturally, I think we can actually incorporate, I mean, as per knowledge, right?

275
00:25:19,760 --> 00:25:26,760
 So, we can easily incorporate the expert knowledge into this, this, no, classification tree.

276
00:25:26,760 --> 00:25:27,760
 Okay.

277
00:25:27,760 --> 00:25:35,760
 So, I think this is the advantage of the pen and classification, the classification tree.

278
00:25:35,760 --> 00:25:36,760
 Okay.

279
00:25:36,760 --> 00:25:40,760
 If you think about a different network, they're very deep, right?

280
00:25:40,760 --> 00:25:47,760
 And you probably don't know how to incorporate, actually, the knowledge, the human knowledge

281
00:25:47,760 --> 00:25:52,760
 into the model, but this kind of model can.

282
00:25:52,760 --> 00:25:53,760
 Okay.

283
00:25:53,760 --> 00:25:56,760
 Of course, now we come to the key issue, right?

284
00:25:56,760 --> 00:25:58,760
 The most key issue, how to build a tree?

285
00:25:58,760 --> 00:26:00,760
 How to build a tree?

286
00:26:00,760 --> 00:26:03,760
 We see we have these advantages, right?

287
00:26:03,760 --> 00:26:08,760
 And they can handle the mixed data with nominal features, and the model is interpretable,

288
00:26:08,760 --> 00:26:13,760
 and we can easily incorporate the human knowledge into this classification tree.

289
00:26:13,760 --> 00:26:15,760
 So, these are the advantages, right?

290
00:26:15,760 --> 00:26:21,760
 Then the key is how to build, actually, a classification tree.

291
00:26:21,760 --> 00:26:22,760
 Okay.

292
00:26:22,760 --> 00:26:29,760
 And so, given a training dataset, of course, this dataset needs to be labeled, right?

293
00:26:29,760 --> 00:26:36,760
 So, given a label training dataset, how we could actually build such a model.

294
00:26:36,760 --> 00:26:37,760
 Okay.

295
00:26:37,760 --> 00:26:42,760
 And actually, if you go back to this model, and you can see, actually, at the top, we

296
00:26:42,760 --> 00:26:44,760
 ask questions about the color.

297
00:26:44,760 --> 00:26:50,760
 So, why you ask the question about color in the root node?

298
00:26:50,760 --> 00:26:52,760
 Why you ask this?

299
00:26:52,760 --> 00:26:54,760
 Why you don't use the tree the size?

300
00:26:54,760 --> 00:27:01,760
 Why you don't use the shape or taste to add the question in the root node?

301
00:27:01,760 --> 00:27:02,760
 Okay.

302
00:27:02,760 --> 00:27:07,760
 And then the data branch to two, then followed, why you ask the question based on the size?

303
00:27:07,760 --> 00:27:14,760
 In the left-hand side here, why you ask the question based on the color in the right branch?

304
00:27:14,760 --> 00:27:15,760
 Okay.

305
00:27:15,760 --> 00:27:21,760
 And then why actually you stop further branching, and you stop further branching, and you declare

306
00:27:21,760 --> 00:27:23,760
 this node as a leaf node?

307
00:27:23,760 --> 00:27:26,760
 How do we know that?

308
00:27:26,760 --> 00:27:27,760
 Okay.

309
00:27:27,760 --> 00:27:30,760
 And actually, this is the question that we need to answer in order to build, actually,

310
00:27:30,760 --> 00:27:33,760
 a classification tree from the training data.

311
00:27:33,760 --> 00:27:34,760
 Okay.

312
00:27:34,760 --> 00:27:36,760
 So, then, okay.

313
00:27:36,760 --> 00:27:43,760
 Then to build a separate tree, we need to answer this question.

314
00:27:43,760 --> 00:27:44,760
 Okay.

315
00:27:44,760 --> 00:27:46,760
 Here, first, how many sableit?

316
00:27:46,760 --> 00:27:48,760
 This is not a show you, right?

317
00:27:48,760 --> 00:27:54,760
 Actually, at each node, we ask a question about a particular feature or attribute.

318
00:27:54,760 --> 00:27:55,760
 Okay.

319
00:27:55,760 --> 00:27:57,760
 And then the data is divided into two branches.

320
00:27:57,760 --> 00:28:00,760
 One goes to the left and another to the right.

321
00:28:00,760 --> 00:28:01,760
 Okay.

322
00:28:01,760 --> 00:28:10,760
 And in other words, actually, the data is split in the two subsets at each node.

323
00:28:10,760 --> 00:28:11,760
 Okay.

324
00:28:11,760 --> 00:28:13,760
 So, the split is two.

325
00:28:13,760 --> 00:28:14,760
 Okay.

326
00:28:14,760 --> 00:28:21,760
 And actually, the classification tree, I just show you, is called a binary classification

327
00:28:21,760 --> 00:28:26,760
 tree because at each node, we divide the data into two.

328
00:28:26,760 --> 00:28:28,760
 The split is two.

329
00:28:28,760 --> 00:28:29,760
 Okay.

330
00:28:29,760 --> 00:28:35,760
 Of course, actually, the number of sableit could be two for some features.

331
00:28:35,760 --> 00:28:43,760
 For example, if you ask the question based on the gender, right, is a male or female.

332
00:28:43,760 --> 00:28:45,760
 If you ask the question, is the person male?

333
00:28:45,760 --> 00:28:50,760
 And then, of course, actually, it will come to the two branch, right?

334
00:28:50,760 --> 00:28:51,760
 Male or female.

335
00:28:51,760 --> 00:28:52,760
 Okay.

336
00:28:52,760 --> 00:28:54,760
 So, I think there will be no issue, right?

337
00:28:54,760 --> 00:28:57,760
 So, certainly, the sableit should be two.

338
00:28:57,760 --> 00:28:58,760
 Okay.

339
00:28:58,760 --> 00:29:04,760
 But if for some features, for example, just in the previous, right, for the size of the

340
00:29:04,760 --> 00:29:08,760
 fruits, the example, the size could be big, could be small, could be medium.

341
00:29:08,760 --> 00:29:09,760
 Okay.

342
00:29:09,760 --> 00:29:10,760
 Three values.

343
00:29:10,760 --> 00:29:13,760
 And then, how many sableit?

344
00:29:13,760 --> 00:29:25,760
 If we ask a question about the, based on the size of the fruits, okay, and then how many

345
00:29:25,760 --> 00:29:28,760
 subset, which should be obtained?

346
00:29:28,760 --> 00:29:29,760
 Okay.

347
00:29:29,760 --> 00:29:30,760
 After sableit.

348
00:29:30,760 --> 00:29:38,760
 Or in other words, what should be the number of sableit we should use for this particular

349
00:29:38,760 --> 00:29:39,760
 feature?

350
00:29:39,760 --> 00:29:40,760
 This is a question, right?

351
00:29:40,760 --> 00:29:42,760
 We need to answer this question.

352
00:29:42,760 --> 00:29:43,760
 Okay.

353
00:29:43,760 --> 00:29:45,760
 Otherwise, actually, you don't know, right?

354
00:29:45,760 --> 00:29:46,760
 You don't know.

355
00:29:46,760 --> 00:29:47,760
 Okay.

356
00:29:47,760 --> 00:29:51,760
 So, you want to build a class-quenching tree, actually, we need to answer this question.

357
00:29:51,760 --> 00:29:52,760
 How many questions, right?

358
00:29:52,760 --> 00:29:54,760
 How many questions to ask?

359
00:29:54,760 --> 00:29:56,760
 That means, actually, sableit.

360
00:29:56,760 --> 00:30:00,760
 If I just ask one question, the data will sableit into two.

361
00:30:00,760 --> 00:30:01,760
 Right?

362
00:30:01,760 --> 00:30:06,760
 If I have two questions, the data will sableit into three.

363
00:30:06,760 --> 00:30:07,760
 Okay.

364
00:30:07,760 --> 00:30:11,760
 So, this is the question to ask.

365
00:30:11,760 --> 00:30:16,760
 And then, which probably should be used as a node?

366
00:30:16,760 --> 00:30:21,760
 We have each node that we ask a question about the particular feature.

367
00:30:21,760 --> 00:30:23,760
 And what feature we should use?

368
00:30:23,760 --> 00:30:25,760
 This feature is just a property, right?

369
00:30:25,760 --> 00:30:28,760
 Or what attribute we should use to ask a question?

370
00:30:28,760 --> 00:30:34,760
 In particular, at the top level, and for the rule node.

371
00:30:34,760 --> 00:30:42,760
 So, why you ask the question based on the color, rather than based on the size, or based

372
00:30:42,760 --> 00:30:45,760
 on the shape, or based on the taste?

373
00:30:45,760 --> 00:30:46,760
 Okay.

374
00:30:46,760 --> 00:30:57,760
 So, we must actually develop some metrics based on which, and we can actually decide

375
00:30:57,760 --> 00:31:00,760
 which feature to use.

376
00:31:01,760 --> 00:31:07,760
 And then, after we select the particular feature, for that node, we also need to determine the

377
00:31:07,760 --> 00:31:12,760
 first question, how many questions to ask based on this particular feature, based on

378
00:31:12,760 --> 00:31:19,760
 the size, how many questions to ask, based on ask one question, right?

379
00:31:19,760 --> 00:31:23,760
 The size is big.

380
00:31:23,760 --> 00:31:25,760
 Then, just one question, right?

381
00:31:25,760 --> 00:31:32,760
 The data becomes big, and then small and medium, first we'll be put into another group.

382
00:31:32,760 --> 00:31:33,760
 Okay.

383
00:31:33,760 --> 00:31:35,760
 The big we'll put in one group.

384
00:31:35,760 --> 00:31:37,760
 All we ask three questions.

385
00:31:37,760 --> 00:31:40,760
 The size is big, this is one question.

386
00:31:40,760 --> 00:31:45,760
 The size is the medium, the size is small.

387
00:31:45,760 --> 00:31:46,760
 Okay.

388
00:31:46,760 --> 00:31:50,760
 Then, we ask three questions, then we have three subsets.

389
00:31:50,760 --> 00:31:51,760
 Okay.

390
00:31:51,760 --> 00:31:58,760
 So, we see the, I think, after the feature selection, after the determination of the feature,

391
00:31:58,760 --> 00:32:04,760
 particular feature to use a certain node, and then we need to actually determine the number

392
00:32:04,760 --> 00:32:05,760
 of questions.

393
00:32:05,760 --> 00:32:06,760
 Okay.

394
00:32:06,760 --> 00:32:10,760
 And later, we will say, you know, even after the feature, we determine the feature, and

395
00:32:10,760 --> 00:32:17,760
 in particular, for those continuous features, we need to decide a split point, split point

396
00:32:17,760 --> 00:32:20,760
 for continuous features, there are so many values.

397
00:32:20,760 --> 00:32:21,760
 Okay.

398
00:32:21,760 --> 00:32:25,760
 So, based on which value, we should split the data into two parts or more parts.

399
00:32:25,760 --> 00:32:28,760
 There's also a question, right?

400
00:32:28,760 --> 00:32:33,760
 They need to answer in order to construct a class feature tree.

401
00:32:33,760 --> 00:32:34,760
 Okay.

402
00:32:34,760 --> 00:32:40,760
 When should we be a node, be declared as a leaf node?

403
00:32:40,760 --> 00:32:43,760
 We have so many leaf nodes, right?

404
00:32:43,760 --> 00:32:47,760
 And all these leaf nodes actually are at different levels.

405
00:32:47,760 --> 00:32:52,760
 So, for example, this is a level, no, top level, level zero, level one, the level two.

406
00:32:52,760 --> 00:32:57,760
 And these nodes in the level two, it is declared as a leaf node.

407
00:32:57,760 --> 00:33:02,760
 But for this one, a level three, and then the level four.

408
00:33:02,760 --> 00:33:04,760
 So, they are at different levels.

409
00:33:04,760 --> 00:33:05,760
 Okay.

410
00:33:05,760 --> 00:33:13,760
 And when we should actually declare a node as a leaf node, and then further branching,

411
00:33:13,760 --> 00:33:14,760
 we're not conducted.

412
00:33:14,760 --> 00:33:21,760
 The data no longer need to be split into more parts.

413
00:33:21,760 --> 00:33:25,760
 We can just stop the split of the data.

414
00:33:25,760 --> 00:33:26,760
 Okay.

415
00:33:26,760 --> 00:33:35,760
 So, this is the, I think the question.

416
00:33:35,760 --> 00:33:42,760
 The fourth question is that if the tree becomes too large, how can there be a smaller and

417
00:33:42,760 --> 00:33:43,760
 simpler?

418
00:33:43,760 --> 00:33:44,760
 So, that is called a prong.

419
00:33:44,760 --> 00:33:45,760
 Okay.

420
00:33:45,760 --> 00:33:52,760
 And the tree, the construction of a tree, a tree, a classification tree, normally involves

421
00:33:52,760 --> 00:33:53,760
 two phases.

422
00:33:53,760 --> 00:33:58,760
 The first phase is the phase of gruel.

423
00:33:58,760 --> 00:34:01,760
 We start from nothing, right, from empty.

424
00:34:01,760 --> 00:34:02,760
 Okay.

425
00:34:02,760 --> 00:34:04,760
 And then we grew the tree.

426
00:34:04,760 --> 00:34:06,760
 So, this is the first phase.

427
00:34:06,760 --> 00:34:07,760
 Okay.

428
00:34:07,760 --> 00:34:09,760
 And the tree could be very big.

429
00:34:09,760 --> 00:34:10,760
 Very big.

430
00:34:10,760 --> 00:34:12,760
 That means we have many nodes, right, many nodes.

431
00:34:12,760 --> 00:34:18,760
 And then one is extremely case that actually each node in the terminal node or leaf node

432
00:34:18,760 --> 00:34:21,760
 just cover one training sample.

433
00:34:21,760 --> 00:34:29,760
 So, in such a scenario, actually the model actually extremely overfit the training data.

434
00:34:29,760 --> 00:34:35,760
 Of course, we know when a model overfit the training data, then the performance on the

435
00:34:35,760 --> 00:34:40,760
 testing data, that is the generalization capability, could be very bad.

436
00:34:40,760 --> 00:34:41,760
 Okay.

437
00:34:41,760 --> 00:34:45,760
 So, we prefer a small and simpler model.

438
00:34:45,760 --> 00:34:46,760
 Okay.

439
00:34:46,760 --> 00:34:49,760
 So, you already grew a very big model.

440
00:34:49,760 --> 00:34:52,760
 Then how the model can become small.

441
00:34:52,760 --> 00:34:53,760
 Okay.

442
00:34:53,760 --> 00:34:58,760
 So, that is the second stage, the pronging of the tree.

443
00:34:58,760 --> 00:35:01,760
 The pronging is just the cut.

444
00:35:01,760 --> 00:35:08,760
 Some of the branches merge or join some of the terminal nodes of the leaf node so that

445
00:35:08,760 --> 00:35:12,760
 the tree could become smaller.

446
00:35:12,760 --> 00:35:18,760
 A smaller model is less likely to overfit the training data.

447
00:35:18,760 --> 00:35:19,760
 Okay.

448
00:35:19,760 --> 00:35:25,760
 So, given a classification problem, so all ways that we, if we can use a small or simpler

449
00:35:25,760 --> 00:35:32,760
 model to solve the problem, then we should use the small and simple model to solve.

450
00:35:32,760 --> 00:35:33,760
 Okay.

451
00:35:33,760 --> 00:35:35,760
 Rather than using a more complex model.

452
00:35:35,760 --> 00:35:36,760
 Okay.

453
00:35:36,760 --> 00:35:39,760
 So, now actually machine learning, deep learning become very popular.

454
00:35:39,760 --> 00:35:44,760
 So, everyone want to know, like you know, use a deep learning to solve every problem.

455
00:35:44,760 --> 00:35:45,760
 They're not right.

456
00:35:45,760 --> 00:35:46,760
 Okay.

457
00:35:46,760 --> 00:35:49,760
 You don't need to use a GPU to calculate 1 plus 1 equals 2.

458
00:35:49,760 --> 00:35:50,760
 Right.

459
00:35:50,760 --> 00:35:53,760
 Actually, you can just calculate it right easily.

460
00:35:53,760 --> 00:35:57,760
 You don't need to use a calculator to calculate these values.

461
00:35:57,760 --> 00:35:58,760
 Right.

462
00:35:58,760 --> 00:35:59,760
 Okay.

463
00:35:59,760 --> 00:36:03,760
 If you can solve a problem using a simpler solution, right, then we prefer the simpler

464
00:36:03,760 --> 00:36:04,760
 solution.

465
00:36:04,760 --> 00:36:07,760
 We prefer the smaller or simpler models.

466
00:36:07,760 --> 00:36:08,760
 Okay.

467
00:36:08,760 --> 00:36:10,760
 So, that is the second stage, right.

468
00:36:10,760 --> 00:36:16,760
 The pronging of the tree, the cut, some of the branches in particular, starting from the

469
00:36:16,760 --> 00:36:18,760
 merging of the leaf node.

470
00:36:18,760 --> 00:36:20,760
 So, later we will talk about this issue again.

471
00:36:20,760 --> 00:36:21,760
 Okay.

472
00:36:21,760 --> 00:36:26,760
 So, this is the fourth question, actually, to large pronging.

473
00:36:26,760 --> 00:36:27,760
 Okay.

474
00:36:27,760 --> 00:36:34,760
 And so, if a node is in pure, actually now, actually the leaf node declare, right, declare

475
00:36:34,760 --> 00:36:42,760
 and then this node, any sample finally come to this node and then this cut label will

476
00:36:42,760 --> 00:36:44,760
 be assigned to that sample, right.

477
00:36:44,760 --> 00:36:47,760
 So, this is a leaf node.

478
00:36:47,760 --> 00:36:54,760
 Of course, actually, you know, if we, you know, each, you know, if a model overfit the

479
00:36:54,760 --> 00:37:00,760
 tree data and then, actually, the data is very pure in each, actually, you know, the

480
00:37:00,760 --> 00:37:06,760
 leaf node because the leaf node maybe can recover one or two, actually, you know, samples,

481
00:37:06,760 --> 00:37:07,760
 right.

482
00:37:07,760 --> 00:37:08,760
 That's an extreme case, of course.

483
00:37:08,760 --> 00:37:14,760
 And then, actually, the leaf node also contains a sample from class two, for example.

484
00:37:14,760 --> 00:37:19,760
 Then, of course, actually, we can assign class two to this leaf node.

485
00:37:19,760 --> 00:37:24,760
 Any sample come to this node will be class two sample.

486
00:37:24,760 --> 00:37:25,760
 Okay.

487
00:37:25,760 --> 00:37:29,760
 But in some scenarios, actually, in particular, if we prong some of the branches, right, some

488
00:37:29,760 --> 00:37:33,760
 of the nodes will merge, some of the leaf nodes will merge, right.

489
00:37:33,760 --> 00:37:41,760
 And then, in this, actually, node, we could have some, you know, a sample from a project

490
00:37:41,760 --> 00:37:42,760
 class.

491
00:37:42,760 --> 00:37:47,760
 Or like, you know, we can have some sample from another class, like a negative class.

492
00:37:47,760 --> 00:37:50,760
 Or we can have some sample from a fruit.

493
00:37:50,760 --> 00:37:55,760
 We can have some sample that we from, like, orange.

494
00:37:55,760 --> 00:37:56,760
 Okay.

495
00:37:56,760 --> 00:37:58,760
 They all come to the same node.

496
00:37:58,760 --> 00:37:59,760
 Okay.

497
00:37:59,760 --> 00:38:01,760
 So the node, actually, is a mist.

498
00:38:01,760 --> 00:38:02,760
 Mist.

499
00:38:02,760 --> 00:38:05,760
 Mist, here, actually, that means contain the sample from different classes.

500
00:38:05,760 --> 00:38:10,760
 And then, how should the category label be assigned to this leaf node?

501
00:38:10,760 --> 00:38:17,760
 If the node is not pure, can see the samples from different classes.

502
00:38:17,760 --> 00:38:18,760
 Okay.

503
00:38:18,760 --> 00:38:20,760
 So these are all the questions that we need to answer.

504
00:38:20,760 --> 00:38:24,760
 You know, to construct a classification tree.

505
00:38:24,760 --> 00:38:25,760
 Right.

506
00:38:25,760 --> 00:38:30,760
 So let's look at the first one, the first question, right.

507
00:38:30,760 --> 00:38:42,760
 The number at split, number of split at each node.

508
00:38:42,760 --> 00:38:45,760
 And actually, let's first look at this.

509
00:38:45,760 --> 00:38:46,760
 Okay.

510
00:38:46,760 --> 00:38:47,760
 And we have one example.

511
00:38:47,760 --> 00:38:50,760
 And then, this is the example, it's just attribute.

512
00:38:50,760 --> 00:38:52,760
 The name is CP.

513
00:38:52,760 --> 00:38:56,760
 And for this attribute, we have four values.

514
00:38:56,760 --> 00:38:58,760
 One, two, three, four.

515
00:38:58,760 --> 00:38:59,760
 Okay.

516
00:38:59,760 --> 00:39:04,760
 And actually, if we, the feature has just two values, right, it's a binary, and then

517
00:39:04,760 --> 00:39:06,760
 there's not a problem.

518
00:39:06,760 --> 00:39:07,760
 Okay.

519
00:39:07,760 --> 00:39:15,760
 We can just actually use the node, use this feature to divide the data, right, into two

520
00:39:15,760 --> 00:39:16,760
 branches.

521
00:39:16,760 --> 00:39:20,760
 One, each branch corresponds to one of the, no, two values.

522
00:39:20,760 --> 00:39:21,760
 Okay.

523
00:39:21,760 --> 00:39:29,760
 But if a feature actually has more than two values, like this, have four values.

524
00:39:29,760 --> 00:39:30,760
 Okay.

525
00:39:30,760 --> 00:39:34,760
 And so how many actually split we should use?

526
00:39:34,760 --> 00:39:37,760
 Of course, we can still use actually binary split.

527
00:39:37,760 --> 00:39:45,760
 Actually, the classification tree, I show you, it is a binary classification tree.

528
00:39:45,760 --> 00:39:46,760
 Okay.

529
00:39:46,760 --> 00:39:50,760
 And in other words, the number of splits, as you should know, is two.

530
00:39:50,760 --> 00:39:51,760
 Okay.

531
00:39:51,760 --> 00:39:58,760
 So for all nodes, the number of splits is two, no matter how many values for that particular

532
00:39:58,760 --> 00:39:59,760
 feature.

533
00:39:59,760 --> 00:40:01,760
 We just have two.

534
00:40:01,760 --> 00:40:02,760
 Okay.

535
00:40:02,760 --> 00:40:06,760
 So for this feature, although we have no four values, we can still use actually the binary

536
00:40:06,760 --> 00:40:07,760
 split.

537
00:40:07,760 --> 00:40:08,760
 Okay.

538
00:40:08,760 --> 00:40:13,760
 Of course, the tree, based on these four values, and then the question is, so how to split

539
00:40:13,760 --> 00:40:14,760
 the data?

540
00:40:14,760 --> 00:40:19,760
 How to split the data into two parts?

541
00:40:19,760 --> 00:40:20,760
 Okay.

542
00:40:20,760 --> 00:40:29,760
 So what is the split, the splitting, you know, point we should use to split the data?

543
00:40:29,760 --> 00:40:30,760
 Okay.

544
00:40:30,760 --> 00:40:35,760
 And one example here is that a CP is greater than two.

545
00:40:35,760 --> 00:40:39,760
 Because then two, that means the value is three or four, right?

546
00:40:39,760 --> 00:40:42,760
 Then go to the left-hand side.

547
00:40:42,760 --> 00:40:48,760
 And then if no, that means the CP value is one or two, then we come to this right-hand

548
00:40:48,760 --> 00:40:49,760
 side.

549
00:40:49,760 --> 00:40:54,760
 So this could be a question, right, to divide the data into two parts.

550
00:40:54,760 --> 00:40:55,760
 Okay.

551
00:40:55,760 --> 00:41:05,760
 And although the next case is still actually the two splits, the binary split, but the

552
00:41:05,760 --> 00:41:07,760
 question is very different, right?

553
00:41:07,760 --> 00:41:11,760
 So if we divide the CP equals one, if you, yes, then left, then no.

554
00:41:11,760 --> 00:41:15,760
 No, that means the tree, the CP equals two or three or four.

555
00:41:15,760 --> 00:41:18,760
 And then we go to the right branch.

556
00:41:18,760 --> 00:41:21,760
 So this is actually the question, right?

557
00:41:21,760 --> 00:41:23,760
 I think this is also a challenge.

558
00:41:23,760 --> 00:41:30,760
 Given a feature, if I'm more than two values, even if we adopt a binary split, then what

559
00:41:30,760 --> 00:41:32,760
 is the splitting at the point?

560
00:41:32,760 --> 00:41:43,760
 So what question we should ask to split the data into two branches, into two subset?

561
00:41:43,760 --> 00:41:51,760
 And later we will see we need to follow some criteria in order to split the data.

562
00:41:51,760 --> 00:41:59,760
 And the principle is that we should actually build a small classification tree.

563
00:41:59,760 --> 00:42:02,760
 Based on this principle, we split the data.

564
00:42:02,760 --> 00:42:03,760
 Okay.

565
00:42:03,760 --> 00:42:08,760
 But how to do that, actually, this is the key issue we need to address today.

566
00:42:08,760 --> 00:42:09,760
 Okay.

567
00:42:09,760 --> 00:42:10,760
 Okay.

568
00:42:10,760 --> 00:42:12,760
 And how to split the data.

569
00:42:12,760 --> 00:42:13,760
 Okay.

570
00:42:13,760 --> 00:42:14,760
 The binary split.

571
00:42:14,760 --> 00:42:16,760
 So there's a look at the multiple split.

572
00:42:16,760 --> 00:42:22,760
 So for this feature, we can also adopt the multiple split.

573
00:42:22,760 --> 00:42:29,760
 And in the left-hand side, actually, the data is split or divided into three parts, right?

574
00:42:29,760 --> 00:42:37,760
 If CP equals one, if CP equals two, if CP is greater than two.

575
00:42:37,760 --> 00:42:38,760
 Okay.

576
00:42:38,760 --> 00:42:42,760
 So we have three parts.

577
00:42:42,760 --> 00:42:48,760
 And then in the right-hand side, actually, the data is split into four parts.

578
00:42:48,760 --> 00:42:50,760
 We ask four questions, actually.

579
00:42:50,760 --> 00:42:53,760
 CP equals one, yes, then here, right?

580
00:42:53,760 --> 00:42:56,760
 CP equals two, yes, yes.

581
00:42:56,760 --> 00:42:57,760
 Okay.

582
00:42:57,760 --> 00:43:00,760
 So the data divided into four subset.

583
00:43:00,760 --> 00:43:01,760
 Okay.

584
00:43:01,760 --> 00:43:08,760
 So, and actually, in the sum of the famous classification tree, like a cart, so later we will talk about

585
00:43:08,760 --> 00:43:10,760
 that classification and regression tree cart.

586
00:43:10,760 --> 00:43:14,760
 And it adopt, actually, the binary split.

587
00:43:14,760 --> 00:43:18,760
 In all nodes, actually, we split the data into two parts.

588
00:43:18,760 --> 00:43:20,760
 So the binary split.

589
00:43:20,760 --> 00:43:21,760
 Okay.

590
00:43:21,760 --> 00:43:30,760
 But in another very famous classification tree, which is called CE4.5, and it adopts, actually,

591
00:43:30,760 --> 00:43:32,760
 the multiple split.

592
00:43:32,760 --> 00:43:33,760
 Okay.

593
00:43:33,760 --> 00:43:38,760
 So different models, actually, actually, different, you know, classification trees adopt different,

594
00:43:38,760 --> 00:43:42,760
 you know, splits, number of splits.

595
00:43:42,760 --> 00:43:47,760
 Could be binary, that could be multiple splits.

596
00:43:47,760 --> 00:43:48,760
 Okay.

597
00:43:48,760 --> 00:43:55,760
 So this is a typical, when the binary split, binary tree.

598
00:43:55,760 --> 00:43:58,760
 Binary tree, that means it splits into two.

599
00:43:58,760 --> 00:43:59,760
 One, two, right?

600
00:43:59,760 --> 00:44:03,760
 Left-hand side, right, left branch, right, right, right, right, right, right.

601
00:44:03,760 --> 00:44:07,760
 For every node, we divide the data into two subset.

602
00:44:07,760 --> 00:44:08,760
 Every node.

603
00:44:08,760 --> 00:44:09,760
 Okay.

604
00:44:09,760 --> 00:44:11,760
 You can see that, two, right?

605
00:44:11,760 --> 00:44:14,760
 Then for this node, further divided into two.

606
00:44:14,760 --> 00:44:15,760
 Okay.

607
00:44:15,760 --> 00:44:22,760
 So this is a binary tree, a cut, classification tree, cut, and then belongs to this kind of

608
00:44:22,760 --> 00:44:25,760
 binary tree.

609
00:44:25,760 --> 00:44:27,760
 Okay.

610
00:44:27,760 --> 00:44:30,760
 And the next tree is a query selection.

611
00:44:30,760 --> 00:44:33,760
 Query selection is just a question to ask.

612
00:44:33,760 --> 00:44:34,760
 Okay.

613
00:44:34,760 --> 00:44:36,760
 And the node impurity.

614
00:44:36,760 --> 00:44:38,760
 We talk about this concept.

615
00:44:38,760 --> 00:44:39,760
 Okay.

616
00:44:39,760 --> 00:44:46,760
 And so, given a tree of data set, when to start, where to start, right?

617
00:44:46,760 --> 00:44:50,760
 So which node should be used as a rule node?

618
00:44:50,760 --> 00:44:51,760
 Okay.

619
00:44:51,760 --> 00:44:57,760
 And even if we have determined which value, which feature, or which attribute to use,

620
00:44:57,760 --> 00:45:01,760
 then we also need to determine what the question to ask, right?

621
00:45:01,760 --> 00:45:05,760
 So, different questions can have different splits, right?

622
00:45:05,760 --> 00:45:08,760
 It can have different numbers of splits.

623
00:45:08,760 --> 00:45:09,760
 Okay.

624
00:45:09,760 --> 00:45:19,760
 And so by the principle here is that the simplicity, simplicity is the principle.

625
00:45:19,760 --> 00:45:20,760
 Okay.

626
00:45:20,760 --> 00:45:28,760
 And we should try our best to select the questions, to select the attribute or features, and

627
00:45:28,760 --> 00:45:33,760
 so that the tree we construct is a simple tree.

628
00:45:33,760 --> 00:45:34,760
 Okay.

629
00:45:34,760 --> 00:45:39,760
 So if you ask one question, for example, you have two class question problems, right?

630
00:45:39,760 --> 00:45:41,760
 You have two class question problems.

631
00:45:41,760 --> 00:45:48,760
 If you ask a question, then this question can actually classify the sample correctly into

632
00:45:48,760 --> 00:45:49,760
 two branch.

633
00:45:49,760 --> 00:45:50,760
 Okay.

634
00:45:50,760 --> 00:45:52,760
 And one branch corresponds to class one.

635
00:45:52,760 --> 00:45:56,760
 And then another branch corresponds to class two.

636
00:45:56,760 --> 00:45:57,760
 Okay.

637
00:45:57,760 --> 00:46:05,760
 In other words, actually by actually asking one question based on one feature, one attribute,

638
00:46:05,760 --> 00:46:11,760
 and then the data actually, you know, is divided into two branch, right?

639
00:46:11,760 --> 00:46:14,760
 And each branch, the data is very pure.

640
00:46:14,760 --> 00:46:19,760
 So pure, that means that all the sample belong to the same class.

641
00:46:19,760 --> 00:46:22,760
 These are very good questions, right?

642
00:46:22,760 --> 00:46:24,760
 You just use one question.

643
00:46:24,760 --> 00:46:27,760
 And just one step, you are completely constructed on the tree.

644
00:46:27,760 --> 00:46:32,760
 Actually, then, to the class question tree, just as you know, the two levels, right?

645
00:46:32,760 --> 00:46:35,760
 The level zero, the rule node.

646
00:46:35,760 --> 00:46:36,760
 Okay.

647
00:46:36,760 --> 00:46:37,760
 Then into the leaf node.

648
00:46:37,760 --> 00:46:40,760
 We just have three nodes.

649
00:46:40,760 --> 00:46:42,760
 These are very simple tree, right?

650
00:46:42,760 --> 00:46:43,760
 Very simple tree.

651
00:46:43,760 --> 00:46:44,760
 Okay.

652
00:46:44,760 --> 00:46:47,760
 But then we look at the characteristic of the simple tree.

653
00:46:47,760 --> 00:46:57,760
 And then the tree, or the node, or the tree of data in the left node, the hand side branch,

654
00:46:57,760 --> 00:47:00,760
 or the right hand side branch are very pure.

655
00:47:00,760 --> 00:47:01,760
 Very pure.

656
00:47:01,760 --> 00:47:02,760
 Okay.

657
00:47:02,760 --> 00:47:11,760
 So the purity will be used as a metric to decide which feature to select to ask questions.

658
00:47:11,760 --> 00:47:19,760
 And also, we need to decide which question to ask for this particular feature.

659
00:47:19,760 --> 00:47:20,760
 Okay.

660
00:47:20,760 --> 00:47:22,760
 So based on the purity measure.

661
00:47:22,760 --> 00:47:24,760
 All purity, all the impurity.

662
00:47:24,760 --> 00:47:25,760
 Okay.

663
00:47:25,760 --> 00:47:27,760
 So we based on the purity measure.

664
00:47:27,760 --> 00:47:28,760
 Okay.

665
00:47:28,760 --> 00:47:30,760
 So we seek a property test.

666
00:47:30,760 --> 00:47:37,760
 Property test, I mean, we seek a test on a particular feature.

667
00:47:37,760 --> 00:47:40,760
 And also a particular question.

668
00:47:40,760 --> 00:47:41,760
 Okay.

669
00:47:41,760 --> 00:47:48,760
 And at each node, and that makes the data reaching the immediate descendant nodes as pure as

670
00:47:48,760 --> 00:47:49,760
 possible.

671
00:47:49,760 --> 00:47:50,760
 Okay.

672
00:47:50,760 --> 00:47:55,760
 So for this, just now we talk about the rule node, right?

673
00:47:55,760 --> 00:47:58,760
 Even for other nodes, we ask the same question.

674
00:47:58,760 --> 00:47:59,760
 Okay.

675
00:47:59,760 --> 00:48:00,760
 These are rule nodes.

676
00:48:00,760 --> 00:48:04,760
 And then the root below this level is called descendant nodes.

677
00:48:04,760 --> 00:48:05,760
 Okay.

678
00:48:05,760 --> 00:48:07,760
 So like parent node and children node, right?

679
00:48:07,760 --> 00:48:11,760
 Actually, this is like a descendant node.

680
00:48:11,760 --> 00:48:12,760
 Okay.

681
00:48:12,760 --> 00:48:13,760
 So this is the rule node descend node.

682
00:48:13,760 --> 00:48:19,760
 Of course, actually, for each descendant node in the next level, we can further divide

683
00:48:19,760 --> 00:48:21,760
 the data into two branches.

684
00:48:21,760 --> 00:48:24,760
 Then we also get the descendant node, right?

685
00:48:24,760 --> 00:48:25,760
 Okay.

686
00:48:25,760 --> 00:48:35,760
 So for every node, after split of the data, actually, the data should be as pure as possible.

687
00:48:36,760 --> 00:48:37,760
 Okay.

688
00:48:37,760 --> 00:48:45,760
 At every level, at the first level, the rule node, okay, and after division, after split

689
00:48:45,760 --> 00:48:52,760
 by a selected feature or attribute, and also as a particular question about this feature,

690
00:48:52,760 --> 00:48:57,760
 okay, after split, the data should be as pure as possible.

691
00:48:57,760 --> 00:49:01,760
 As pure as possible, that doesn't mean it's really 100% pure.

692
00:49:01,760 --> 00:49:02,760
 Just as pure as possible.

693
00:49:02,760 --> 00:49:08,760
 And actually, for a practical problem, we need to have many levels, right?

694
00:49:08,760 --> 00:49:09,760
 Many levels.

695
00:49:09,760 --> 00:49:14,760
 And of course, actually, we see as pure as possible, but no 100% could be very low even

696
00:49:14,760 --> 00:49:15,760
 at the beginning.

697
00:49:15,760 --> 00:49:16,760
 Okay.

698
00:49:16,760 --> 00:49:23,760
 You cannot be on just one feature, right, to classify the samples correctly, right?

699
00:49:23,760 --> 00:49:27,760
 Just one feature, you can separate some of the samples.

700
00:49:27,760 --> 00:49:29,760
 You can classify some of the samples correctly.

701
00:49:29,760 --> 00:49:34,760
 But most of them will be misclassified, just based on one feature, right?

702
00:49:34,760 --> 00:49:35,760
 Yeah.

703
00:49:35,760 --> 00:49:39,760
 So that, actually, you know, the classification tree, actually, ask questions just based on

704
00:49:39,760 --> 00:49:41,760
 one feature at each node.

705
00:49:41,760 --> 00:49:45,760
 So certainly, you cannot actually classify the sample correctly based on one, just one

706
00:49:45,760 --> 00:49:46,760
 feature.

707
00:49:46,760 --> 00:49:57,760
 So after feature, the data split, okay, they are still very impure, very impure, not very

708
00:49:57,760 --> 00:49:58,760
 pure.

709
00:49:58,760 --> 00:50:02,760
 And it contains the data from different classes, okay?

710
00:50:02,760 --> 00:50:08,760
 But we should actually make this as pure as possible, okay?

711
00:50:08,760 --> 00:50:11,760
 So this is the principle we follow, okay?

712
00:50:11,760 --> 00:50:17,760
 And to select the features and those two to ask the question regarding this feature,

713
00:50:17,760 --> 00:50:19,760
 based on this problem.

714
00:50:19,760 --> 00:50:24,760
 After split of the data, the data should be as pure as possible.

715
00:50:25,760 --> 00:50:30,760
 Okay, of course, actually, a very natural question is that, so how to measure the purity?

716
00:50:30,760 --> 00:50:32,760
 How to measure impurity, right?

717
00:50:32,760 --> 00:50:33,760
 Okay.

718
00:50:35,760 --> 00:50:40,760
 And, okay, actually, you know, here, actually, we are interested in the concept of impurity.

719
00:50:40,760 --> 00:50:41,760
 Purity, right?

720
00:50:41,760 --> 00:50:44,760
 Impurity, just the inverse of that, right?

721
00:50:44,760 --> 00:50:52,760
 If purity is 100%, then the impurity is zero, right?

722
00:50:52,760 --> 00:50:55,760
 If the purity is zero, then the impurity is one, okay?

723
00:50:55,760 --> 00:51:04,760
 So this is actually, we use impurity as a measure, okay, to select features and also decide which

724
00:51:04,760 --> 00:51:10,760
 question to ask regarding this selected feature, okay?

725
00:51:10,760 --> 00:51:12,760
 So now, actually, we can see that node.

726
00:51:12,760 --> 00:51:15,760
 I mean, actually, the impurity.

727
00:51:15,760 --> 00:51:17,760
 I, see, I mean the impurity.

728
00:51:17,760 --> 00:51:19,760
 N, see, I mean node.

729
00:51:19,760 --> 00:51:24,760
 And for a particular node, N, actually, the impurity, okay?

730
00:51:24,760 --> 00:51:27,760
 The impurity, you know, the impurity, okay?

731
00:51:27,760 --> 00:51:31,760
 And the impurity, we want the impurity.

732
00:51:31,760 --> 00:51:33,760
 This is a measure, right?

733
00:51:33,760 --> 00:51:39,760
 The impurity measure could, we want this to be zero.

734
00:51:39,760 --> 00:51:45,760
 If all the samples that reach to the node be at the same category level, okay?

735
00:51:45,760 --> 00:51:48,760
 All the sample categories of the nodes are from the same class.

736
00:51:48,760 --> 00:51:51,760
 Because the purity is very high, right?

737
00:51:51,760 --> 00:51:55,760
 And then the impurity is zero, okay?

738
00:51:55,760 --> 00:51:57,760
 So we want to have such a metric.

739
00:51:57,760 --> 00:52:03,760
 This metric, actually, should be, this impurity measure should be zero.

740
00:52:03,760 --> 00:52:10,760
 If all the samples in this node are from the same class, okay?

741
00:52:10,760 --> 00:52:12,760
 So this impurity measure should certify this condition, right?

742
00:52:12,760 --> 00:52:14,760
 This is one condition, okay?

743
00:52:14,760 --> 00:52:20,760
 And then, actually, another condition, that, and this impurity measure should be very big.

744
00:52:20,760 --> 00:52:28,760
 If the data is mixed, and that the, the reach or the proportion is the same for all classes.

745
00:52:28,760 --> 00:52:35,760
 Okay, so we see, and, actually, the, if the, you know, categories are equally represented.

746
00:52:35,760 --> 00:52:38,760
 50-50, yeah, two class confirm problem.

747
00:52:38,760 --> 00:52:41,760
 Okay, so in this node, we have some samples.

748
00:52:41,760 --> 00:52:45,760
 50% from class one, 50% from class two, okay?

749
00:52:45,760 --> 00:52:47,760
 So this is a very impure, right?

750
00:52:47,760 --> 00:52:50,760
 So the impurity measure should be big.

751
00:52:50,760 --> 00:52:52,760
 We need to have such a metric, right?

752
00:52:52,760 --> 00:52:58,760
 When the data is all from the same category, the impurity measure should be zero.

753
00:52:58,760 --> 00:53:03,760
 So this metric should have a zero value if the data from the same class, okay?

754
00:53:03,760 --> 00:53:07,760
 And also this impurity metric should be big.

755
00:53:07,760 --> 00:53:09,760
 Should be big, okay?

756
00:53:09,760 --> 00:53:16,760
 If the data, actually, you know, actually have equal proportion, right?

757
00:53:16,760 --> 00:53:21,760
 From different classes, okay?

758
00:53:21,760 --> 00:53:23,760
 So this value should be big.

759
00:53:23,760 --> 00:53:27,760
 So we hope to have such a metric, okay?

760
00:53:27,760 --> 00:53:31,760
 Then what metric set for this, you know, conditions?

761
00:53:31,760 --> 00:53:34,760
 Here, actually, we will introduce a few.

762
00:53:34,760 --> 00:53:38,760
 And the first one is called entropy impurity, entropy.

763
00:53:38,760 --> 00:53:41,760
 It's a very important concept in machine learning, right?

764
00:53:41,760 --> 00:53:45,760
 And also in information theory, entropy, okay?

765
00:53:45,760 --> 00:53:47,760
 So what is entropy?

766
00:53:47,760 --> 00:53:57,760
 Actually, entropy is defined as the product of the probability under the logarithm, right?

767
00:53:57,760 --> 00:54:00,760
 Of the probability, okay?

768
00:54:00,760 --> 00:54:03,760
 So here, actually, we assume the data in the mix, right?

769
00:54:03,760 --> 00:54:04,760
 In the mix.

770
00:54:04,760 --> 00:54:07,760
 Then we have samples from different classes, okay?

771
00:54:07,760 --> 00:54:12,760
 And of course, actually, you know, a special scenario that the data is pure, right?

772
00:54:12,760 --> 00:54:18,760
 And then, actually, you know, of course, actually, then the probability for that particular class is zero, okay?

773
00:54:18,760 --> 00:54:26,760
 So p omega j is a fraction of probability of samples belonging to omega j.

774
00:54:26,760 --> 00:54:28,760
 This is something like a prior probability, right?

775
00:54:28,760 --> 00:54:36,760
 Given a data set, and we count how many samples are from, actually, you know, class omega one or class one.

776
00:54:36,760 --> 00:54:41,760
 Then we calculate the proportion, the fraction, the percentage.

777
00:54:41,760 --> 00:54:44,760
 So that is just the, you know, prior probability, right?

778
00:54:44,760 --> 00:54:46,760
 The prior probability, okay?

779
00:54:46,760 --> 00:54:51,760
 So that is here, the pure omega j is a fraction of samples, okay?

780
00:54:51,760 --> 00:54:53,760
 So based on the cost, right?

781
00:54:53,760 --> 00:54:56,760
 How many samples belong to class one, class two?

782
00:54:56,760 --> 00:54:58,760
 Then we can calculate this fraction.

783
00:54:58,760 --> 00:55:00,760
 Just a probability, right?

784
00:55:00,760 --> 00:55:02,760
 Percentage, okay?

785
00:55:02,760 --> 00:55:06,760
 And then, based on this, we can calculate the entropy.

786
00:55:06,760 --> 00:55:09,760
 And because, actually, the probability is less than one, right?

787
00:55:09,760 --> 00:55:10,760
 Normally, less than one.

788
00:55:10,760 --> 00:55:14,760
 And then, the log will have a negative value, right?

789
00:55:14,760 --> 00:55:19,760
 So, like, then we put a negative sign here, negative sign here, okay?

790
00:55:19,760 --> 00:55:22,760
 So this is actually the impurity measure.

791
00:55:22,760 --> 00:55:27,760
 The entropy, actually, impurity measure, actually, is a positive number, positive.

792
00:55:27,760 --> 00:55:30,760
 It could be zero, could be zero, okay?

793
00:55:30,760 --> 00:55:33,760
 Actually, it's non-negative, non-negative.

794
00:55:33,760 --> 00:55:37,760
 So this is the entropy impurity.

795
00:55:37,760 --> 00:55:43,760
 And actually, the entropy impurity, you can see, even though one, there are two classes.

796
00:55:43,760 --> 00:55:48,760
 One, from class one, the opium omega j is one.

797
00:55:48,760 --> 00:55:52,760
 That means that this, this, this, you know, they are very pure, right?

798
00:55:52,760 --> 00:55:53,760
 Very pure.

799
00:55:53,760 --> 00:55:55,760
 And that omega j equals one, right?

800
00:55:55,760 --> 00:55:59,760
 And then, omega p omega j omega 2 is zero, okay?

801
00:55:59,760 --> 00:56:03,760
 So we just use, actually, you know, omega one to calculate the entropy, right?

802
00:56:03,760 --> 00:56:08,760
 Then, one times log one, this is value zero.

803
00:56:08,760 --> 00:56:10,760
 So this is a set of other conditions here, right?

804
00:56:10,760 --> 00:56:17,760
 If the data is pure, all the samples are from the same class in this data set.

805
00:56:17,760 --> 00:56:19,760
 And then, the impurity measure is zero.

806
00:56:19,760 --> 00:56:24,760
 So, indeed, actually, this is the entropy impurity measure could set this condition.

807
00:56:25,760 --> 00:56:26,760
 Okay.

808
00:56:26,760 --> 00:56:31,760
 So this is the first measure, actually, we can use the entropy impurity measure.

809
00:56:31,760 --> 00:56:32,760
 Okay.

810
00:56:34,760 --> 00:56:37,760
 And then, we'll have a look of one example.

811
00:56:37,760 --> 00:56:42,760
 And in this example, and we have, totally, we have 100 samples.

812
00:56:42,760 --> 00:56:49,760
 And one from, 10 from class one, and then 90 from class two.

813
00:56:49,760 --> 00:56:50,760
 Class two.

814
00:56:50,760 --> 00:56:54,760
 And actually, 90% of the samples belong to class two, right?

815
00:56:54,760 --> 00:56:58,760
 So this is actually, I think the data set is relatively pure, right?

816
00:56:58,760 --> 00:57:01,760
 90% from one class, okay?

817
00:57:01,760 --> 00:57:04,760
 Of course, no 100%.

818
00:57:04,760 --> 00:57:14,760
 So now, we can use entropy impurity measure to evaluate the impurity.

819
00:57:14,760 --> 00:57:15,760
 Okay.

820
00:57:15,760 --> 00:57:19,760
 So based on the distribution of the data in the two classes, we can kind of pure omega

821
00:57:19,760 --> 00:57:21,760
 to 10 over 100, right?

822
00:57:21,760 --> 00:57:22,760
 So it's 0.1.

823
00:57:22,760 --> 00:57:29,760
 And people may have to, the perfection of the sample from class two is 90% 0.9.

824
00:57:29,760 --> 00:57:33,760
 Then, we can substitute formula to calculate, right?

825
00:57:33,760 --> 00:57:40,760
 So this is the cross entropy impurity measure, the entropy impurity measure.

826
00:57:40,760 --> 00:57:41,760
 Okay.

827
00:57:42,760 --> 00:57:49,760
 Probability times the logarithm of the probability, right?

828
00:57:49,760 --> 00:57:52,760
 So the base of the logarithm here is two.

829
00:57:52,760 --> 00:57:53,760
 Okay.

830
00:57:53,760 --> 00:57:57,760
 Then minus 0.9 pure omega two, right?

831
00:57:57,760 --> 00:58:02,760
 Times logarithm of pure omega two.

832
00:58:02,760 --> 00:58:03,760
 Okay.

833
00:58:03,760 --> 00:58:07,760
 So this value is 0.6, 4.6 null.

834
00:58:07,760 --> 00:58:08,760
 Okay.

835
00:58:08,760 --> 00:58:11,760
 So this is the entropy impurity.

836
00:58:11,760 --> 00:58:12,760
 Okay.

837
00:58:12,760 --> 00:58:19,760
 And the full data set with 50-50% distribution, 50-50, right?

838
00:58:19,760 --> 00:58:24,760
 Just now, we see all the classes are equally represented.

839
00:58:24,760 --> 00:58:30,760
 That means the fraction for each class is the same, the same.

840
00:58:30,760 --> 00:58:31,760
 Okay.

841
00:58:31,760 --> 00:58:34,760
 For two class, currently we're probably on 50-50.

842
00:58:34,760 --> 00:58:35,760
 Okay.

843
00:58:35,760 --> 00:58:37,760
 So these are 50-50, actually.

844
00:58:37,760 --> 00:58:41,760
 Then the entropy impurity should be very big.

845
00:58:41,760 --> 00:58:42,760
 Okay.

846
00:58:42,760 --> 00:58:44,760
 Then we can evaluate this.

847
00:58:44,760 --> 00:58:45,760
 Okay.

848
00:58:45,760 --> 00:58:50,760
 So it's minus 0.5, the probability times log 0.5, right?

849
00:58:50,760 --> 00:58:56,760
 Then minus another 0.5, omega two, pure omega two, and the logarithm pure omega two.

850
00:58:56,760 --> 00:58:58,760
 Actually, this value equals one.

851
00:58:58,760 --> 00:58:59,760
 So the impurity measure.

852
00:58:59,760 --> 00:59:00,760
 Okay.

853
00:59:00,760 --> 00:59:04,760
 Actually, now we know, actually, if the data is pure, you know, that all the samples from

854
00:59:04,760 --> 00:59:09,760
 the same class, then the impurity measure is 0.

855
00:59:09,760 --> 00:59:14,760
 Even all the classes are equally represented in this data set.

856
00:59:14,760 --> 00:59:19,760
 That means the fraction of the sample from each of the classes are the same.

857
00:59:19,760 --> 00:59:24,760
 And then the entropy impurity measure is one.

858
00:59:24,760 --> 00:59:26,760
 This is a big value.

859
00:59:26,760 --> 00:59:28,760
 Impurity measure is big, right?

860
00:59:28,760 --> 00:59:30,760
 So it means one.

861
00:59:30,760 --> 00:59:33,760
 That means the impurity.

862
00:59:33,760 --> 00:59:34,760
 Okay.

863
00:59:34,760 --> 00:59:39,760
 And so this one is pure because 90% from the same class, right?

864
00:59:39,760 --> 00:59:41,760
 So this value is relatively small.

865
00:59:41,760 --> 00:59:42,760
 0.469.

866
00:59:42,760 --> 00:59:43,760
 Okay.

867
00:59:43,760 --> 00:59:46,760
 Even 100% from the same class.

868
00:59:46,760 --> 00:59:47,760
 Okay.

869
00:59:47,760 --> 00:59:48,760
 Then you can imagine, right?

870
00:59:48,760 --> 00:59:50,760
 Pure omega one, zero.

871
00:59:50,760 --> 00:59:52,760
 Pure omega two, one.

872
00:59:52,760 --> 00:59:53,760
 Okay.

873
00:59:53,760 --> 00:59:54,760
 Then you can substitute.

874
00:59:54,760 --> 00:59:56,760
 Of course, actually, we don't need to care about class one, right?

875
00:59:56,760 --> 00:59:58,760
 Because we cannot have a log of zero.

876
00:59:58,760 --> 00:59:59,760
 We cannot have that.

877
00:59:59,760 --> 01:00:02,760
 But then we have one times the logarithm one.

878
01:00:02,760 --> 01:00:03,760
 So zero.

879
01:00:03,760 --> 01:00:04,760
 Okay.

880
01:00:04,760 --> 01:00:12,760
 So this is the entropy impurity measure.

881
01:00:12,760 --> 01:00:13,760
 Okay.

882
01:00:13,760 --> 01:00:14,760
 Okay.

883
01:00:14,760 --> 01:00:19,760
 So this is the...

884
01:00:19,760 --> 01:00:21,760
 So that's actually...

885
01:00:21,760 --> 01:00:28,760
 We look at another measure, which is called a few virus.

886
01:00:28,760 --> 01:00:29,760
 Okay.

887
01:00:30,760 --> 01:00:31,760
 Okay.

888
01:00:31,760 --> 01:00:32,760
 Okay.

889
01:00:32,760 --> 01:00:33,760
 Yeah.

890
01:00:33,760 --> 01:00:35,760
 I think we should have a break, right?

891
01:00:35,760 --> 01:00:36,760
 Okay.

892
01:00:36,760 --> 01:00:37,760
 Maybe ten in a break.

893
01:00:37,760 --> 01:00:52,760
 So after a break, we look at another metric for the evaluation of the impurity.

894
01:00:52,760 --> 01:00:53,760
 Okay.

895
01:00:53,760 --> 01:00:54,760
 Okay.

896
01:00:54,760 --> 01:00:55,760
 Okay.

897
01:00:55,760 --> 01:00:56,760
 Okay.

898
01:00:56,760 --> 01:00:57,760
 Okay.

899
01:00:57,760 --> 01:00:58,760
 Okay.

900
01:00:58,760 --> 01:00:59,760
 Okay.

901
01:00:59,760 --> 01:01:00,760
 Okay.

902
01:01:00,760 --> 01:01:01,760
 Okay.

903
01:01:01,760 --> 01:01:02,760
 Okay.

904
01:01:02,760 --> 01:01:03,760
 Okay.

905
01:01:03,760 --> 01:01:04,760
 Okay.

906
01:01:04,760 --> 01:01:05,760
 Okay.

907
01:01:05,760 --> 01:01:06,760
 Okay.

908
01:01:06,760 --> 01:01:07,760
 Okay.

909
01:01:07,760 --> 01:01:08,760
 Okay.

910
01:01:08,760 --> 01:01:09,760
 Okay.

911
01:01:09,760 --> 01:01:10,760
 Okay.

912
01:01:10,760 --> 01:01:11,760
 Okay.

913
01:01:11,760 --> 01:01:12,760
 Okay.

914
01:01:12,760 --> 01:01:13,760
 Okay.

915
01:01:13,760 --> 01:01:14,760
 Okay.

916
01:01:14,760 --> 01:01:15,760
 Okay.

917
01:01:15,760 --> 01:01:16,760
 Okay.

918
01:01:16,760 --> 01:01:17,760
 Okay.

919
01:01:17,760 --> 01:01:18,760
 Okay.

920
01:01:18,760 --> 01:01:19,760
 Okay.

921
01:01:19,760 --> 01:01:20,760
 Okay.

922
01:01:20,760 --> 01:01:21,760
 Okay.

923
01:01:21,760 --> 01:01:22,760
 Okay.

924
01:01:22,760 --> 01:01:23,760
 Okay.

925
01:01:23,760 --> 01:01:24,760
 Okay.

926
01:01:24,760 --> 01:01:25,760
 Okay.

927
01:01:25,760 --> 01:01:26,760
 Okay.

928
01:01:26,760 --> 01:01:27,760
 Okay.

929
01:01:27,760 --> 01:01:28,760
 Okay.

930
01:01:28,760 --> 01:01:29,760
 Okay.

931
01:01:29,760 --> 01:01:30,760
 Okay.

932
01:01:30,760 --> 01:01:31,760
 Okay.

933
01:01:31,760 --> 01:01:32,760
 Okay.

934
01:01:32,760 --> 01:01:33,760
 Okay.

935
01:01:33,760 --> 01:01:34,760
 Okay.

936
01:01:34,760 --> 01:01:35,760
 Okay.

937
01:01:35,760 --> 01:01:36,760
 Okay.

938
01:01:36,760 --> 01:01:37,760
 Okay.

939
01:01:37,760 --> 01:01:38,760
 Okay.

940
01:01:38,760 --> 01:01:39,760
 Okay.

941
01:01:39,760 --> 01:01:40,760
 Okay.

942
01:01:40,760 --> 01:01:41,760
 Okay.

943
01:01:41,760 --> 01:01:42,760
 Okay.

944
01:01:42,760 --> 01:01:43,760
 Okay.

945
01:01:43,760 --> 01:01:44,760
 Okay.

946
01:01:44,760 --> 01:01:45,760
 Okay.

947
01:01:45,760 --> 01:01:46,760
 Okay.

948
01:01:46,760 --> 01:01:47,760
 Okay.

949
01:01:47,760 --> 01:01:48,760
 Okay.

950
01:01:48,760 --> 01:01:49,760
 Okay.

951
01:01:49,760 --> 01:01:50,760
 Okay.

952
01:01:50,760 --> 01:01:51,760
 Okay.

953
01:02:20,760 --> 01:02:21,760
 Okay.

954
01:02:50,760 --> 01:02:51,760
 Okay.

955
01:03:20,760 --> 01:03:21,760
 Okay.

956
01:03:50,760 --> 01:03:51,760
 Okay.

957
01:04:20,760 --> 01:04:21,760
 Okay.

958
01:04:50,760 --> 01:04:51,760
 Okay.

959
01:05:20,760 --> 01:05:21,760
 Okay.

960
01:05:50,760 --> 01:05:51,760
 Okay.

961
01:06:20,760 --> 01:06:21,760
 Okay.

962
01:06:50,760 --> 01:06:51,760
 Okay.

963
01:07:20,760 --> 01:07:21,760
 Okay.

964
01:07:50,760 --> 01:07:51,760
 Okay.

965
01:08:20,760 --> 01:08:21,760
 Okay.

966
01:08:21,760 --> 01:08:22,760
 Okay.

967
01:08:22,760 --> 01:08:23,760
 Okay.

968
01:08:23,760 --> 01:08:24,760
 So,

969
01:08:24,760 --> 01:08:27,760
 so,

970
01:08:27,760 --> 01:08:28,760
 look,

971
01:08:29,760 --> 01:08:30,760
 look.

972
01:08:30,760 --> 01:08:31,760
 Okay.

973
01:08:31,760 --> 01:08:32,760
 Yes.

974
01:08:32,760 --> 01:08:33,760
 And the

975
01:08:33,760 --> 01:08:34,760
 What do you think?

976
01:08:34,760 --> 01:08:35,760
 Okay.

977
01:08:35,760 --> 01:08:36,760
 Okay.

978
01:08:36,760 --> 01:08:37,760
 Okay.

979
01:08:37,760 --> 01:08:38,760
 Okay.

980
01:08:38,760 --> 01:08:39,760
 Okay.

981
01:08:39,760 --> 01:08:40,760
 Okay.

982
01:08:40,760 --> 01:08:42,260
 Okay.

983
01:08:42,260 --> 01:08:43,260
 Okay.

984
01:08:43,260 --> 01:08:44,260
 Okay.

985
01:08:44,260 --> 01:08:45,260
 Okay.

986
01:08:45,260 --> 01:08:46,760
 Okay.

987
01:08:46,760 --> 01:08:47,760
 Okay.

988
01:08:47,760 --> 01:08:48,760
 Okay.

989
01:08:48,760 --> 01:08:51,300
 And the gene impurity.

990
01:08:59,680 --> 01:09:02,720
 So what is the wiring impurity?

991
01:09:02,720 --> 01:09:06,040
 And this wiring impurity definition here is very simple.

992
01:09:06,040 --> 01:09:08,360
 Actually, the wiring impurity is just

993
01:09:08,360 --> 01:09:14,760
 the product of the two, omega 1 times p omega 2.

994
01:09:14,760 --> 01:09:16,440
 And actually, from this here, we can also

995
01:09:16,439 --> 01:09:20,639
 see if the data is pure, it's 100%.

996
01:09:20,639 --> 01:09:23,279
 And all the samples from the same class,

997
01:09:23,279 --> 01:09:26,080
 then one of the p omega 1 or omega 2,

998
01:09:26,080 --> 01:09:27,559
 p omega 2 will be 0.

999
01:09:27,559 --> 01:09:29,439
 Then the product is also 0.

1000
01:09:29,439 --> 01:09:32,960
 So again, if we use this wiring impurity,

1001
01:09:32,960 --> 01:09:35,040
 and it is satisfied the condition,

1002
01:09:35,040 --> 01:09:39,279
 when one of the data from the same class,

1003
01:09:39,279 --> 01:09:42,639
 and then the impurity matter should be 0.

1004
01:09:42,639 --> 01:09:45,919
 It is 0 if the data is pure.

1005
01:09:45,920 --> 01:09:52,120
 Because one of the probability or fraction, p omega 1 or p omega

1006
01:09:52,120 --> 01:09:53,920
 2, actually is 0.

1007
01:09:53,920 --> 01:09:56,560
 Then the product is 0.

1008
01:09:56,560 --> 01:10:03,000
 So this is a matter called actually a wiring impurity.

1009
01:10:03,000 --> 01:10:05,920
 Actually, a more commonly used impurity

1010
01:10:05,920 --> 01:10:09,600
 is the so-called gene impurity.

1011
01:10:09,600 --> 01:10:14,320
 And the gene impurity actually is

1012
01:10:14,320 --> 01:10:20,679
 the tension of the wiring impurity to more than two classes.

1013
01:10:20,679 --> 01:10:23,920
 So the gene impurity is different.

1014
01:10:23,920 --> 01:10:29,200
 It's the summation of a pair-wise product.

1015
01:10:29,200 --> 01:10:32,440
 For example, we have three classes.

1016
01:10:32,440 --> 01:10:33,679
 We have p omega 1.

1017
01:10:33,679 --> 01:10:34,719
 We have p omega 2.

1018
01:10:34,719 --> 01:10:36,040
 We have p omega 3, right?

1019
01:10:36,040 --> 01:10:37,240
 Three classes.

1020
01:10:37,240 --> 01:10:40,320
 And then actually, the gene impurity,

1021
01:10:40,320 --> 01:10:45,200
 which is the summation of p omega 1 times p omega 2.

1022
01:10:45,200 --> 01:10:48,840
 And p omega 1 times p omega 3.

1023
01:10:48,840 --> 01:10:52,920
 And then p omega 2 times p omega 3.

1024
01:10:52,920 --> 01:10:57,480
 So this is actually the gene impurity.

1025
01:10:57,480 --> 01:10:59,320
 We need to calculate this actually,

1026
01:10:59,320 --> 01:11:02,679
 pair-wise actually, the product of the probabilities.

1027
01:11:02,679 --> 01:11:04,759
 Then get the summation.

1028
01:11:04,759 --> 01:11:09,400
 Of course, if the number of the classes is big,

1029
01:11:09,400 --> 01:11:13,000
 and then there's no pairs, the number of pairs

1030
01:11:13,000 --> 01:11:16,960
 could be a very big value, a very big number.

1031
01:11:16,960 --> 01:11:19,280
 And then that means you need to do a lot of calculations.

1032
01:11:22,080 --> 01:11:26,519
 So actually, this actually is equivalent to this measurement.

1033
01:11:26,519 --> 01:11:32,759
 So one subtract and p omega j squared.

1034
01:11:32,759 --> 01:11:35,080
 p omega j squared.

1035
01:11:35,080 --> 01:11:39,200
 And the j from 1 until the total number of classes.

1036
01:11:39,200 --> 01:11:42,920
 So for each of the classes, and we have a probability,

1037
01:11:42,920 --> 01:11:48,440
 or we have the fraction, then we just get the square.

1038
01:11:48,440 --> 01:11:54,880
 Then one subtract, p omega 1 squared, p omega 2 squared,

1039
01:11:54,880 --> 01:11:57,480
 then your p omega c squared.

1040
01:11:57,480 --> 01:11:59,320
 Then divide it by 2.

1041
01:11:59,320 --> 01:12:02,559
 So this is a method called gene impurity.

1042
01:12:02,559 --> 01:12:06,559
 And this gene impurity is frequently used.

1043
01:12:06,560 --> 01:12:12,600
 So later in the demonstration, I will use this gene impurity

1044
01:12:12,600 --> 01:12:16,440
 to calculate the impurity measure after the data

1045
01:12:16,440 --> 01:12:24,160
 split by a particular feature or a particular question.

1046
01:12:24,160 --> 01:12:25,400
 OK.

1047
01:12:25,400 --> 01:12:28,120
 So this is a gene impurity.

1048
01:12:28,120 --> 01:12:30,800
 And in that sense, we look at this calculation.

1049
01:12:30,800 --> 01:12:33,480
 I think this is very straightforward calculation.

1050
01:12:33,480 --> 01:12:35,680
 And based on the distribution of the sample

1051
01:12:35,680 --> 01:12:38,200
 in the two classes, we can easily calculate

1052
01:12:38,200 --> 01:12:41,240
 omega 1 and the fraction for class 1, class 2.

1053
01:12:41,240 --> 01:12:43,920
 This fraction is just a probability law.

1054
01:12:43,920 --> 01:12:45,320
 Probability is just a fraction.

1055
01:12:45,320 --> 01:12:48,040
 This is one of the methods to estimate the probability.

1056
01:12:48,040 --> 01:12:49,880
 It's the fraction.

1057
01:12:49,880 --> 01:12:56,680
 So class 1, 10%, 7.1, class 2, 90%, 7.9.

1058
01:12:56,680 --> 01:12:59,080
 Then actually, the varying impurity

1059
01:12:59,080 --> 01:13:01,160
 is just the product of the two.

1060
01:13:01,160 --> 01:13:06,639
 The gene impurity is the same.

1061
01:13:06,639 --> 01:13:11,200
 1 minus omega 1 squared minus actually

1062
01:13:11,200 --> 01:13:15,240
 pi omega 2 squared divided by 2.

1063
01:13:15,240 --> 01:13:16,480
 So this is gene impurity.

1064
01:13:21,400 --> 01:13:25,440
 And another impurity metric that we can use

1065
01:13:25,440 --> 01:13:28,440
 is called miscarcification impurity.

1066
01:13:28,440 --> 01:13:31,040
 Miscarcification.

1067
01:13:31,040 --> 01:13:33,440
 And actually, this miscarcification,

1068
01:13:33,440 --> 01:13:38,800
 and we know actually in the leaf node,

1069
01:13:38,800 --> 01:13:41,960
 we need to declare the class label of that leaf node.

1070
01:13:41,960 --> 01:13:46,400
 And normally, even the leaf node is not 100% pure.

1071
01:13:46,400 --> 01:13:49,440
 If we have 100% pure, most likely,

1072
01:13:49,440 --> 01:13:53,480
 the classification tree overfills the training data.

1073
01:13:53,480 --> 01:13:59,600
 So in the leaf node, the data could be mixed,

1074
01:13:59,600 --> 01:14:03,320
 continue, sample from different classes.

1075
01:14:03,320 --> 01:14:08,440
 And then when we actually declare the class label

1076
01:14:08,440 --> 01:14:16,240
 for that particular leaf node, and then we identify the class

1077
01:14:16,240 --> 01:14:19,400
 with the largest probabilities.

1078
01:14:19,400 --> 01:14:23,480
 For example, we have two classes, just another sample.

1079
01:14:23,480 --> 01:14:26,800
 And in this node, if this is a leaf node,

1080
01:14:26,840 --> 01:14:34,200
 10% of the data from class 1, 90% of the data from class 2.

1081
01:14:34,200 --> 01:14:38,040
 And if this is a leaf node, then the label of this leaf node

1082
01:14:38,040 --> 01:14:41,960
 actually is class 2, class 90%, class 2,

1083
01:14:41,960 --> 01:14:45,800
 because 90% is greater than 10%.

1084
01:14:45,800 --> 01:14:55,160
 So we declare the class label of this leaf node class 2.

1085
01:14:55,160 --> 01:14:56,800
 But actually, all the samples here

1086
01:14:56,800 --> 01:14:59,320
 will be classified to class 2.

1087
01:14:59,320 --> 01:15:04,480
 So certainly, 10% of the data will be misclassified.

1088
01:15:04,480 --> 01:15:06,840
 And actually, we can, based on this,

1089
01:15:06,840 --> 01:15:10,440
 this is actually the classification area

1090
01:15:10,440 --> 01:15:14,480
 as an impurity measure.

1091
01:15:14,480 --> 01:15:17,320
 So here, the misclassification emc,

1092
01:15:17,320 --> 01:15:20,200
 and misclassification impurity measure

1093
01:15:20,200 --> 01:15:25,400
 is 1 subtract the maximum p omega j.

1094
01:15:25,400 --> 01:15:28,080
 Just now, we see p omega j, the maximum,

1095
01:15:28,080 --> 01:15:30,240
 actually is 90% 0.9.

1096
01:15:30,240 --> 01:15:35,080
 1 minus 0.9, just 1, 10%, 0.1.

1097
01:15:35,080 --> 01:15:37,720
 So this, of course, is for a two-class classroom problem.

1098
01:15:37,720 --> 01:15:39,920
 For a three-class classroom problem, for example,

1099
01:15:39,920 --> 01:15:41,120
 we have an example.

1100
01:15:41,120 --> 01:15:46,599
 And p omega 1 is 60%.

1101
01:15:46,680 --> 01:15:53,720
 p omega 2 is 25%.

1102
01:15:53,720 --> 01:15:56,880
 And then p omega 3 will be 15%, right?

1103
01:15:56,880 --> 01:15:58,520
 15%.

1104
01:15:58,520 --> 01:16:03,000
 So if this is a leaf node, then, of course,

1105
01:16:03,000 --> 01:16:06,400
 the class label of this leaf node will be class 1.

1106
01:16:06,400 --> 01:16:12,080
 This is because the p omega 1 is the largest, is the 60%.

1107
01:16:12,080 --> 01:16:19,120
 Others are actually 15% or 25%.

1108
01:16:19,120 --> 01:16:23,280
 And then, actually, if we declare this leaf node as the label

1109
01:16:23,280 --> 01:16:27,720
 of leaf node is class 1, then 40% of the data

1110
01:16:27,720 --> 01:16:29,400
 will be misclassified, right?

1111
01:16:29,400 --> 01:16:30,280
 40%.

1112
01:16:30,280 --> 01:16:34,440
 All the data will be classified, actually, as the class 1.

1113
01:16:34,440 --> 01:16:38,840
 Then the area is also the 1 subtract the maximum value.

1114
01:16:38,840 --> 01:16:42,360
 The maximum here is 60%, 0.6.

1115
01:16:42,360 --> 01:16:45,360
 Then the 1 minus 0.6 is 0.4.

1116
01:16:45,360 --> 01:16:48,640
 So that is 40% of the data will be misclassified.

1117
01:16:48,640 --> 01:16:52,320
 So this could be used as an impurity measure,

1118
01:16:52,320 --> 01:16:55,640
 classification area.

1119
01:16:55,640 --> 01:16:59,760
 So this is the impurity.

1120
01:16:59,760 --> 01:17:03,440
 And for the example, right, for example, it's 10%.

1121
01:17:03,440 --> 01:17:08,520
 This is a misclassification impurity measure.

1122
01:17:08,520 --> 01:17:10,400
 So we have introduced three, right?

1123
01:17:10,400 --> 01:17:12,760
 First, actually, the entropy, right?

1124
01:17:12,760 --> 01:17:15,680
 The entropy impurity measure.

1125
01:17:15,680 --> 01:17:18,160
 Second, actually, we have the variance.

1126
01:17:18,160 --> 01:17:20,080
 And then we send to a more class,

1127
01:17:20,080 --> 01:17:22,240
 that is the gene impurity, right?

1128
01:17:22,240 --> 01:17:24,160
 Vibrant gene impurity.

1129
01:17:24,160 --> 01:17:30,800
 And then we have the misclassification impurity,

1130
01:17:30,800 --> 01:17:32,880
 so the three impurities.

1131
01:17:32,880 --> 01:17:37,600
 And actually, the entropy impurity

1132
01:17:37,680 --> 01:17:39,080
 is frequently used.

1133
01:17:39,080 --> 01:17:42,000
 And because, actually, it is a very important concept

1134
01:17:42,000 --> 01:17:43,280
 in the information theory.

1135
01:17:43,280 --> 01:17:45,520
 Information theory, entropy, right?

1136
01:17:45,520 --> 01:17:49,200
 Entropy is a very important concept.

1137
01:17:49,200 --> 01:17:52,480
 Now in deep learning, we use the loss function

1138
01:17:52,480 --> 01:17:55,640
 as a cross entropy.

1139
01:17:55,640 --> 01:17:59,800
 So this is an important, actually, measure.

1140
01:17:59,800 --> 01:18:01,520
 So entropy.

1141
01:18:01,520 --> 01:18:03,920
 But actually, the gene impurity receive more and more

1142
01:18:03,920 --> 01:18:06,360
 considerations.

1143
01:18:06,400 --> 01:18:09,320
 So that's the reason why in the later demonstrations,

1144
01:18:09,320 --> 01:18:13,280
 I use the gene impurity, right?

1145
01:18:13,280 --> 01:18:16,960
 We don't need to calculate a logarithm.

1146
01:18:16,960 --> 01:18:23,240
 Just one subtract, a square of the probabilities.

1147
01:18:23,240 --> 01:18:25,599
 Then we can get the divide by two.

1148
01:18:25,599 --> 01:18:28,320
 We can get the gene impurities.

1149
01:18:28,320 --> 01:18:34,160
 OK, so this is the impurity measures.

1150
01:18:34,160 --> 01:18:37,400
 So naturally, we will use these impurity measures.

1151
01:18:37,400 --> 01:18:43,760
 In particular, we will use the gene impurity measure

1152
01:18:43,760 --> 01:18:49,559
 to select attributes at each node,

1153
01:18:49,559 --> 01:18:54,120
 including the level zero, the root node,

1154
01:18:54,120 --> 01:18:55,440
 and then the descendant node.

1155
01:18:55,440 --> 01:18:57,280
 Actually, each descendant node, you

1156
01:18:57,280 --> 01:19:00,080
 need to perform a split of the data.

1157
01:19:00,080 --> 01:19:04,120
 And then we need to select one feature, one attribute.

1158
01:19:04,920 --> 01:19:06,760
 After we have selected attributes,

1159
01:19:06,760 --> 01:19:10,200
 we also need to decide actually a question.

1160
01:19:10,200 --> 01:19:16,320
 And we need to decide actually the splitting point.

1161
01:19:16,320 --> 01:19:17,040
 OK.

1162
01:19:17,040 --> 01:19:19,080
 But actually, both.

1163
01:19:19,080 --> 01:19:22,920
 That means that the selection of the attribute

1164
01:19:22,920 --> 01:19:28,280
 and also the determination of the splitting point,

1165
01:19:28,280 --> 01:19:32,280
 the two are all selected, are determined

1166
01:19:32,280 --> 01:19:36,880
 based on the impurity measure.

1167
01:19:36,880 --> 01:19:37,480
 OK.

1168
01:19:37,480 --> 01:19:41,440
 So naturally, we apply this impurity measure

1169
01:19:41,440 --> 01:19:44,160
 through one example to see how we can construct

1170
01:19:44,160 --> 01:19:46,040
 actually a classification tree.

1171
01:19:50,559 --> 01:19:51,200
 OK.

1172
01:19:51,200 --> 01:19:56,160
 So naturally, we will look at that actually a measure.

1173
01:19:56,160 --> 01:19:57,759
 We have a node.

1174
01:19:57,759 --> 01:20:01,880
 After data split, we all call it after data split.

1175
01:20:01,880 --> 01:20:05,200
 Actually, we can have an entropy measure.

1176
01:20:05,200 --> 01:20:08,480
 Before data split, we can have an entropy measure.

1177
01:20:08,480 --> 01:20:09,280
 It's very easy.

1178
01:20:09,280 --> 01:20:12,960
 We just based on the proportion of the data in each class.

1179
01:20:12,960 --> 01:20:13,460
 OK.

1180
01:20:13,460 --> 01:20:16,760
 We can see the entropy.

1181
01:20:16,760 --> 01:20:18,440
 We can measure, impurity measure.

1182
01:20:18,440 --> 01:20:20,640
 We can have the gene impurity measure.

1183
01:20:20,640 --> 01:20:23,400
 We can have the misclassification actually

1184
01:20:23,400 --> 01:20:28,520
 in entropy, misclassification in impurity measure.

1185
01:20:28,520 --> 01:20:30,760
 And maybe we can have an impurity measure.

1186
01:20:30,760 --> 01:20:33,920
 Before split, we can have an impurity measure

1187
01:20:33,920 --> 01:20:35,600
 for this data set.

1188
01:20:35,600 --> 01:20:40,000
 After split, we can also have an impurity measure.

1189
01:20:40,000 --> 01:20:40,720
 OK.

1190
01:20:40,720 --> 01:20:43,160
 And actually, as I already explained,

1191
01:20:43,160 --> 01:20:47,520
 we hope actually we can have a maximum reduction

1192
01:20:47,520 --> 01:20:50,120
 after split.

1193
01:20:50,120 --> 01:20:51,800
 We can have a maximum reduction.

1194
01:20:51,800 --> 01:20:54,600
 The ideal scenario is that actually after split,

1195
01:20:54,600 --> 01:20:58,480
 actually the impurity measure is 0.

1196
01:20:58,480 --> 01:21:00,280
 The data is a pure.

1197
01:21:00,280 --> 01:21:03,759
 All the data that need a same node,

1198
01:21:03,759 --> 01:21:06,480
 left node or right node after split.

1199
01:21:06,480 --> 01:21:11,440
 And the impurity measure is 0.

1200
01:21:11,440 --> 01:21:11,759
 OK.

1201
01:21:11,759 --> 01:21:16,000
 So we can actually, we hope we can have a maximum reduction

1202
01:21:16,000 --> 01:21:20,840
 of the impurity after split by a feature.

1203
01:21:20,840 --> 01:21:25,120
 And also a question based on that feature.

1204
01:21:25,120 --> 01:21:27,440
 We can have a maximum reduction.

1205
01:21:27,440 --> 01:21:31,919
 So let's say we look at the impurity reduction or drop.

1206
01:21:31,919 --> 01:21:39,320
 So before split in the node n, and we have an entropy measure,

1207
01:21:39,320 --> 01:21:41,080
 i, i, n.

1208
01:21:41,080 --> 01:21:43,480
 So this is the before split of the data.

1209
01:21:43,480 --> 01:21:48,839
 So naturally, after split by a paid feature,

1210
01:21:48,839 --> 01:21:51,360
 and also one question based on the feature.

1211
01:21:51,360 --> 01:21:54,519
 And then for the data in the left branch,

1212
01:21:54,519 --> 01:21:56,960
 because the data divide in the two parts,

1213
01:21:57,640 --> 01:22:01,200
 two subset, one subset in the left-hand side branch,

1214
01:22:01,200 --> 01:22:06,360
 then in the left side branch or in the left side node.

1215
01:22:06,360 --> 01:22:10,080
 And then another data set in the right-hand side branch

1216
01:22:10,080 --> 01:22:12,400
 or in the right-hand side node.

1217
01:22:12,400 --> 01:22:14,480
 For each of these data subset, we

1218
01:22:14,480 --> 01:22:18,560
 can have a measure of the impurity.

1219
01:22:18,560 --> 01:22:20,560
 We can have a impurity measure impurity.

1220
01:22:20,560 --> 01:22:30,760
 Assumably, in the left branch, we have a node l.

1221
01:22:30,760 --> 01:22:32,760
 l here means the left.

1222
01:22:32,760 --> 01:22:35,520
 In the left branch, the left node,

1223
01:22:35,520 --> 01:22:37,760
 descendant node after split.

1224
01:22:37,760 --> 01:22:39,680
 This is called descendant node.

1225
01:22:39,680 --> 01:22:45,440
 And we have the impurity of measure of i.

1226
01:22:45,440 --> 01:22:48,520
 And then for the another node, the node here also

1227
01:22:48,560 --> 01:22:51,440
 corresponds to another subset of the data.

1228
01:22:51,440 --> 01:22:54,960
 We can have an impurity measure.

1229
01:22:54,960 --> 01:23:01,880
 And actually, the total impurity after split

1230
01:23:01,880 --> 01:23:06,160
 is a weighted summation of the two impurity measures.

1231
01:23:06,160 --> 01:23:07,560
 For the data in the left-hand side,

1232
01:23:07,560 --> 01:23:08,840
 we have impurity measure.

1233
01:23:08,840 --> 01:23:10,160
 For the data in the right-hand side,

1234
01:23:10,160 --> 01:23:11,480
 we have the impurity measure.

1235
01:23:11,480 --> 01:23:15,080
 So i, l, i, and r.

1236
01:23:15,080 --> 01:23:17,520
 We need to combine these two impurity measures

1237
01:23:17,520 --> 01:23:21,160
 to get a total impurity after data split.

1238
01:23:21,160 --> 01:23:24,760
 So this total impurity actually is a linear combination.

1239
01:23:24,760 --> 01:23:29,240
 Weight is a summation of these two impurity measures.

1240
01:23:29,240 --> 01:23:30,640
 What is the weight?

1241
01:23:30,640 --> 01:23:32,280
 The weight here is pl.

1242
01:23:32,280 --> 01:23:32,920
 pl.

1243
01:23:32,920 --> 01:23:33,960
 What is pl?

1244
01:23:33,960 --> 01:23:35,160
 What is pl?

1245
01:23:35,160 --> 01:23:37,800
 I read pl.

1246
01:23:37,800 --> 01:23:41,360
 pl here actually is just the proportion of the data

1247
01:23:41,360 --> 01:23:43,360
 going to the left.

1248
01:23:43,360 --> 01:23:45,679
 pl is the proportion of the data going to the right.

1249
01:23:48,719 --> 01:23:52,519
 So for example, you, based on one feature and one question,

1250
01:23:52,519 --> 01:23:54,440
 the data is divided into two parts.

1251
01:23:54,440 --> 01:23:56,679
 Initially, we have 1,000.

1252
01:23:56,679 --> 01:23:59,240
 700 go to the left.

1253
01:23:59,240 --> 01:24:01,679
 300 go to the right.

1254
01:24:01,679 --> 01:24:05,200
 Then this pl, pl, the proportion of the data

1255
01:24:05,200 --> 01:24:07,200
 going to the left.

1256
01:24:07,200 --> 01:24:08,519
 The left node, right?

1257
01:24:08,519 --> 01:24:13,120
 It's a 700 divided by 1,000.

1258
01:24:13,880 --> 01:24:15,760
 So it's 0.7.

1259
01:24:15,760 --> 01:24:19,559
 Then the data to the right, the pl, 300,

1260
01:24:19,559 --> 01:24:21,599
 is divided by 1,000.

1261
01:24:21,599 --> 01:24:25,720
 It's 30% or 0.3.

1262
01:24:25,720 --> 01:24:27,800
 Then the two impurity measures need

1263
01:24:27,800 --> 01:24:31,040
 to be combined, one multiplied by 0.7,

1264
01:24:31,040 --> 01:24:33,320
 another multiplied by 0.3.

1265
01:24:33,320 --> 01:24:38,080
 So the total impurity after data split

1266
01:24:38,080 --> 01:24:41,480
 is a weighted summation of the two impurities.

1267
01:24:41,480 --> 01:24:43,959
 The left-hand side impurity, right-hand side impurity,

1268
01:24:43,959 --> 01:24:44,959
 weighted summation.

1269
01:24:44,959 --> 01:24:48,040
 The weight is just the proportion of the data

1270
01:24:48,040 --> 01:24:52,080
 in the left node and in the right node.

1271
01:24:52,080 --> 01:24:55,400
 So we hope actually we can have a maximum reduction.

1272
01:24:55,400 --> 01:24:57,240
 Maximum reduction.

1273
01:24:57,240 --> 01:25:00,679
 And actually from this formula, data, data, data,

1274
01:25:00,679 --> 01:25:05,200
 ion, that ion is a drop or reduction of the impurity measure.

1275
01:25:05,200 --> 01:25:07,480
 And actually you could ion.

1276
01:25:07,480 --> 01:25:08,400
 So this drop.

1277
01:25:08,400 --> 01:25:10,280
 Actually before split, of course,

1278
01:25:10,280 --> 01:25:14,840
 this ion is already the data that is fixed value.

1279
01:25:14,840 --> 01:25:18,559
 How we can have a maximum data ion, the maximum drop.

1280
01:25:18,559 --> 01:25:22,200
 That means this part is the minimum.

1281
01:25:22,200 --> 01:25:27,000
 If this part is the smallest, after data split,

1282
01:25:27,000 --> 01:25:31,719
 the entropy measure, the impurity measure, is a minimum.

1283
01:25:31,719 --> 01:25:36,040
 Then this is the best selection, the best split of the data.

1284
01:25:36,040 --> 01:25:38,480
 Because this will achieve the maximum drop

1285
01:25:38,480 --> 01:25:42,440
 or maximum reduction in the impurities.

1286
01:25:42,440 --> 01:25:45,400
 So if the after data split, all the in the left-hand side,

1287
01:25:45,400 --> 01:25:47,120
 the sample belongs to the same class.

1288
01:25:47,120 --> 01:25:48,679
 All the sample in the right-hand side,

1289
01:25:48,679 --> 01:25:50,519
 node belongs to the same class.

1290
01:25:50,519 --> 01:25:52,559
 The impurity is zero.

1291
01:25:52,559 --> 01:25:56,320
 Then we have a very maximum reduction of the impurity.

1292
01:25:56,320 --> 01:25:59,679
 So that means the E. So later we

1293
01:25:59,679 --> 01:26:01,959
 don't look at the reduction.

1294
01:26:01,959 --> 01:26:07,679
 We just look at the impurity measure after data split.

1295
01:26:07,680 --> 01:26:15,040
 PL, R and L, PR, R and R. We just look at the two parts.

1296
01:26:15,040 --> 01:26:20,160
 We should select the one that has the minimum value of this.

1297
01:26:20,160 --> 01:26:21,920
 OK.

1298
01:26:21,920 --> 01:26:26,400
 So this is the principle we should follow.

1299
01:26:26,400 --> 01:26:34,520
 You have to determine at this node which feature we should use

1300
01:26:34,520 --> 01:26:36,680
 and what question we should ask.

1301
01:26:36,680 --> 01:26:39,440
 Actually, what question we should ask basically

1302
01:26:39,440 --> 01:26:43,200
 is to determine the splitting point, which value we

1303
01:26:43,200 --> 01:26:45,320
 should use to split the data.

1304
01:26:54,200 --> 01:26:57,440
 So with the maximum drop, actually, so later we look at,

1305
01:26:57,440 --> 01:26:59,760
 we try to minimize this.

1306
01:26:59,760 --> 01:27:06,560
 We find a split so that the impurity measure

1307
01:27:06,600 --> 01:27:07,440
 is minimized.

1308
01:27:11,800 --> 01:27:14,760
 So then the tree, there is another tree.

1309
01:27:20,960 --> 01:27:24,040
 So now if we are actually assuming

1310
01:27:24,040 --> 01:27:27,320
 there will be multiple split.

1311
01:27:27,320 --> 01:27:32,480
 And then we just talk about the two.

1312
01:27:32,480 --> 01:27:34,280
 Then the natural split is this.

1313
01:27:34,280 --> 01:27:39,200
 If we have more than two, we have multiple split.

1314
01:27:39,200 --> 01:27:42,719
 And then, of course, I think that then not just left or right.

1315
01:27:42,719 --> 01:27:44,240
 Now we have a branch one.

1316
01:27:44,240 --> 01:27:45,120
 We have branch two.

1317
01:27:45,120 --> 01:27:46,440
 We have branch three.

1318
01:27:46,440 --> 01:27:49,080
 And if we totally, we have B branched.

1319
01:27:49,080 --> 01:27:51,120
 We have the data split.

1320
01:27:51,120 --> 01:27:54,799
 Data is split into B part.

1321
01:27:54,799 --> 01:27:59,759
 Then what is total impurity measure after data split?

1322
01:27:59,759 --> 01:28:01,200
 So that is the same.

1323
01:28:01,200 --> 01:28:04,599
 For each of the parts, for each of the parts,

1324
01:28:04,599 --> 01:28:07,599
 not just two parts, left, right, right, part, right.

1325
01:28:07,599 --> 01:28:08,880
 That's not just two nodes.

1326
01:28:08,880 --> 01:28:10,880
 Left node, right node.

1327
01:28:10,880 --> 01:28:13,559
 We have multiple nodes after data split.

1328
01:28:13,559 --> 01:28:17,240
 If we adopt multiple split scheme,

1329
01:28:17,240 --> 01:28:19,840
 then we have a multiple subset.

1330
01:28:19,840 --> 01:28:25,639
 For each of the subset, we can have a measure of the impurity.

1331
01:28:25,639 --> 01:28:28,800
 And then we combine all the impurity measures

1332
01:28:28,800 --> 01:28:34,600
 based on the proportion of the data in that sub-data subset.

1333
01:28:34,600 --> 01:28:40,520
 So these are very natural extension of this case.

1334
01:28:40,520 --> 01:28:41,120
 They are very natural.

1335
01:28:41,120 --> 01:28:43,920
 Now there are two split, binary split.

1336
01:28:43,920 --> 01:28:45,320
 So now we have multiple split.

1337
01:28:45,320 --> 01:28:46,200
 Then we have this data.

1338
01:28:49,800 --> 01:28:51,960
 So here, actually, we need to decide

1339
01:28:51,960 --> 01:28:54,640
 whether we should use a binary split

1340
01:28:54,640 --> 01:28:58,000
 or we should use a multiple split.

1341
01:28:58,000 --> 01:29:00,640
 Of course, actually, I think it's very natural, right?

1342
01:29:00,640 --> 01:29:04,680
 For a data with multiple values, like the one CP,

1343
01:29:04,680 --> 01:29:05,960
 we have used the example.

1344
01:29:05,960 --> 01:29:07,320
 We have 1234.

1345
01:29:07,320 --> 01:29:11,920
 Or transportation mode, we have 1234, even more values.

1346
01:29:11,920 --> 01:29:15,720
 So actually, we can divide them into many split.

1347
01:29:15,720 --> 01:29:19,200
 We can divide them into many subset.

1348
01:29:19,200 --> 01:29:23,520
 Or we can also just divide them into two subset.

1349
01:29:23,520 --> 01:29:26,160
 Whether we should use a binary split

1350
01:29:26,160 --> 01:29:28,200
 or we should use a multiple split.

1351
01:29:28,200 --> 01:29:37,800
 I think a very natural thinking is that we should evaluate.

1352
01:29:37,800 --> 01:29:41,160
 We should evaluate if the B is 2.

1353
01:29:41,160 --> 01:29:45,240
 Then we look at the impurity measure.

1354
01:29:45,240 --> 01:29:49,200
 And then we look at the B equal to 3 or 4.

1355
01:29:49,200 --> 01:29:52,599
 We look at the impurity measure.

1356
01:29:52,600 --> 01:29:58,080
 And then we see which one will achieve the maximum drop,

1357
01:29:58,080 --> 01:30:03,400
 maximum drop, or which impurity is the minimum after split,

1358
01:30:03,400 --> 01:30:07,120
 by binary or by multiple split.

1359
01:30:07,120 --> 01:30:11,280
 Then we can decide which split a number we should use,

1360
01:30:11,280 --> 01:30:13,640
 binary or multiple split.

1361
01:30:13,640 --> 01:30:16,760
 But here, there is one question, one issue with that.

1362
01:30:16,760 --> 01:30:24,280
 This measure actually will fewer this actually.

1363
01:30:24,280 --> 01:30:26,760
 This measure fewer as a larger B. That

1364
01:30:26,760 --> 01:30:32,960
 means actually, if you the data derive more split,

1365
01:30:32,960 --> 01:30:37,640
 and then or more subset, and then actually this

1366
01:30:37,640 --> 01:30:42,640
 actually drop will be more.

1367
01:30:42,640 --> 01:30:45,400
 OK, so this measure will actually fewer

1368
01:30:45,400 --> 01:30:47,400
 the multiple split.

1369
01:30:47,400 --> 01:30:52,320
 The more split we use, then we have a more significant reduction

1370
01:30:52,320 --> 01:30:55,799
 in the impurity measure.

1371
01:30:55,799 --> 01:30:58,920
 OK, actually, this is not a good thing.

1372
01:30:58,920 --> 01:31:01,519
 This is because actually, then based on this question,

1373
01:31:01,519 --> 01:31:04,559
 there are every time that you will use a multiple split.

1374
01:31:04,559 --> 01:31:08,920
 Multiple split normally makes the tree more complex.

1375
01:31:08,920 --> 01:31:11,960
 OK, because the fewer multiple split, right?

1376
01:31:11,960 --> 01:31:14,280
 So every node for every feature, as long

1377
01:31:14,280 --> 01:31:17,400
 as the tree has more than two values,

1378
01:31:17,400 --> 01:31:19,400
 it will use multiple split.

1379
01:31:19,400 --> 01:31:22,440
 But this may not be necessary.

1380
01:31:22,440 --> 01:31:31,480
 OK, so then we need to change this, modify this actually,

1381
01:31:31,480 --> 01:31:32,080
 drop, right?

1382
01:31:32,080 --> 01:31:36,639
 We need to modify this actually, drop of the impurity measure.

1383
01:31:36,639 --> 01:31:39,280
 So this is a modified one.

1384
01:31:39,280 --> 01:31:42,800
 So it's kind of scaled actually impurity measure.

1385
01:31:42,800 --> 01:31:47,120
 So data, I, and I, and then divided by this.

1386
01:31:47,120 --> 01:31:55,480
 So this is actually is a modified, or scaled impurity drop.

1387
01:31:55,480 --> 01:31:57,600
 Scaled impurity drop, OK?

1388
01:31:57,600 --> 01:32:00,400
 And then this is actually, even the measure,

1389
01:32:00,400 --> 01:32:06,240
 will not actually fewer the multiple split.

1390
01:32:06,240 --> 01:32:09,600
 And actually in the C4.5, right?

1391
01:32:09,600 --> 01:32:11,200
 So it adopt the multiple split.

1392
01:32:11,200 --> 01:32:13,480
 But not necessarily, at every node,

1393
01:32:13,480 --> 01:32:18,120
 it adopts actually multiple split.

1394
01:32:18,120 --> 01:32:22,639
 So it will adopt multiple split if necessary.

1395
01:32:22,639 --> 01:32:28,360
 And then what a measure to use to determine whether we

1396
01:32:28,360 --> 01:32:30,840
 should use or not, they based on this.

1397
01:32:30,840 --> 01:32:35,200
 They based on this actually scaled actually impurity

1398
01:32:35,200 --> 01:32:41,160
 measure in the nowhere frame by the C4.5 classification tree.

1399
01:32:42,160 --> 01:32:42,660
 OK.

1400
01:32:45,320 --> 01:32:48,920
 So next tree, I show you step by step,

1401
01:32:48,920 --> 01:32:50,720
 OK, and how to construct tree.

1402
01:32:50,720 --> 01:32:53,120
 So here, we have one example.

1403
01:32:53,120 --> 01:32:55,559
 This is, I think the data set is also,

1404
01:32:55,559 --> 01:32:58,519
 until you learn the core site, you can also

1405
01:32:58,519 --> 01:33:00,920
 download this data to play, right?

1406
01:33:00,920 --> 01:33:03,320
 To try to practice.

1407
01:33:03,320 --> 01:33:06,960
 So here, in this data set, the tree, the data set

1408
01:33:06,960 --> 01:33:10,880
 is about the heart attack, heart disease data set.

1409
01:33:11,680 --> 01:33:14,240
 The tree in the left column is just

1410
01:33:14,240 --> 01:33:16,040
 index of the samples.

1411
01:33:16,040 --> 01:33:18,320
 Each sample is one rule.

1412
01:33:18,320 --> 01:33:21,400
 It's a one rule, right?

1413
01:33:21,400 --> 01:33:27,760
 And the first feature, H, the second feature is sex.

1414
01:33:27,760 --> 01:33:29,680
 Then that feature, CP.

1415
01:33:29,680 --> 01:33:35,880
 And then actually we have the rest of BPS.

1416
01:33:35,880 --> 01:33:39,680
 So in the written state, what is the pulse?

1417
01:33:39,680 --> 01:33:41,120
 I think all the values.

1418
01:33:41,120 --> 01:33:43,760
 So totally, we have 13 features.

1419
01:33:43,760 --> 01:33:45,360
 So these are 13 features.

1420
01:33:45,360 --> 01:33:50,200
 Are the properties of the characteristics of the patient?

1421
01:33:50,200 --> 01:33:52,880
 OK, so for each patient, for each subject,

1422
01:33:52,880 --> 01:34:00,000
 we have one measurement for all the 13 attributes or properties.

1423
01:34:00,000 --> 01:34:00,920
 OK?

1424
01:34:00,920 --> 01:34:04,120
 And some of the features are actually binary.

1425
01:34:04,120 --> 01:34:08,120
 You can see, even from the example, right, for sex,

1426
01:34:08,120 --> 01:34:11,680
 of course, we know we only have two possible values, right?

1427
01:34:11,680 --> 01:34:13,120
 And male or female.

1428
01:34:13,120 --> 01:34:15,800
 And then, of course, here, we use the numerical value

1429
01:34:15,800 --> 01:34:18,040
 to denote male or female.

1430
01:34:18,040 --> 01:34:21,599
 So one, maybe denote male and zero different female.

1431
01:34:21,599 --> 01:34:25,920
 OK, so we can see this sex count is a binary feature, right?

1432
01:34:25,920 --> 01:34:27,000
 Only two values.

1433
01:34:27,000 --> 01:34:32,640
 So for this feature, sex, I think it's easy to determine,

1434
01:34:32,640 --> 01:34:33,140
 right?

1435
01:34:33,140 --> 01:34:36,200
 We should use the two split, right?

1436
01:34:36,200 --> 01:34:38,200
 So binary split, OK?

1437
01:34:38,200 --> 01:34:42,160
 And then, also, for the CP, right?

1438
01:34:42,160 --> 01:34:44,840
 For the CP, actually, here, we have

1439
01:34:44,840 --> 01:34:47,360
 seen that we have multiple values, right?

1440
01:34:47,360 --> 01:34:48,400
 Not just two.

1441
01:34:48,400 --> 01:34:51,559
 One, two, three, four.

1442
01:34:51,559 --> 01:34:53,480
 All the four values.

1443
01:34:53,480 --> 01:34:54,960
 If you can have four possible values.

1444
01:34:57,559 --> 01:35:00,320
 And therefore, for the age, age, actually,

1445
01:35:00,320 --> 01:35:01,679
 is a continuous variable.

1446
01:35:01,679 --> 01:35:03,639
 It has multiple values.

1447
01:35:03,639 --> 01:35:05,080
 So this data is a myth, right?

1448
01:35:05,760 --> 01:35:07,400
 It has many other features, right?

1449
01:35:07,400 --> 01:35:09,000
 Like, it's a myth data.

1450
01:35:09,000 --> 01:35:10,240
 We have a binary feature.

1451
01:35:10,240 --> 01:35:14,240
 We have discrete features, like one, two, three, four, right?

1452
01:35:14,240 --> 01:35:16,080
 And also, we have continuous features.

1453
01:35:18,519 --> 01:35:20,720
 And then, we look at other features,

1454
01:35:20,720 --> 01:35:23,280
 like FBS is a one-zero, right?

1455
01:35:23,280 --> 01:35:24,680
 It's a binary feature.

1456
01:35:24,680 --> 01:35:28,440
 And then, it's changed.

1457
01:35:28,440 --> 01:35:31,640
 It is a zero-one, all the binary features.

1458
01:35:31,640 --> 01:35:34,960
 For this, actually, it's continuous.

1459
01:35:35,080 --> 01:35:37,200
 For this, actually, it's a multiply, right?

1460
01:35:37,200 --> 01:35:38,080
 OK.

1461
01:35:38,080 --> 01:35:39,480
 Now, it's a zero.

1462
01:35:39,480 --> 01:35:41,160
 Two can be three.

1463
01:35:41,160 --> 01:35:42,360
 It can be one.

1464
01:35:42,360 --> 01:35:44,320
 It's four values.

1465
01:35:44,320 --> 01:35:47,960
 So, actually, and then, as I said, we look at it.

1466
01:35:47,960 --> 01:35:51,720
 For all these features, which feature

1467
01:35:51,720 --> 01:35:56,160
 we should use as the root node, as the first node in the zero

1468
01:35:56,160 --> 01:35:57,800
 level, level zero.

1469
01:35:57,800 --> 01:35:59,200
 Which node we should use?

1470
01:35:59,200 --> 01:36:01,120
 Which feature we should use?

1471
01:36:01,120 --> 01:36:01,920
 OK.

1472
01:36:01,920 --> 01:36:05,600
 So, actually, we don't have ID, right?

1473
01:36:05,600 --> 01:36:09,480
 But we have the method to determine which feature we

1474
01:36:09,480 --> 01:36:13,160
 should use as the root node.

1475
01:36:13,160 --> 01:36:18,040
 In other words, actually, we need to evaluate the impurity

1476
01:36:18,040 --> 01:36:19,080
 measure.

1477
01:36:19,080 --> 01:36:28,080
 And when each of them attributes is used in the root node,

1478
01:36:28,080 --> 01:36:29,480
 then we evaluate each of them.

1479
01:36:29,519 --> 01:36:30,000
 OK.

1480
01:36:30,000 --> 01:36:35,519
 Then we identify the one that has the smallest impurity

1481
01:36:35,519 --> 01:36:40,559
 measure, so after split by the feature and also the question

1482
01:36:40,559 --> 01:36:41,639
 based on the feature.

1483
01:36:43,639 --> 01:36:44,679
 And OK.

1484
01:36:44,679 --> 01:36:46,799
 So, in that tree, we look at that.

1485
01:36:46,799 --> 01:36:50,040
 So now, we just have a summarization of the data set.

1486
01:36:50,040 --> 01:36:52,559
 And this data set, actually, contains samples

1487
01:36:52,559 --> 01:36:53,839
 from two classes.

1488
01:36:53,839 --> 01:36:57,759
 One class is the patient with a heart attack,

1489
01:36:57,759 --> 01:36:59,320
 and a heart problem, right?

1490
01:36:59,320 --> 01:37:07,480
 And then the class two contains samples without heart attack.

1491
01:37:07,480 --> 01:37:10,320
 And the number of samples in class one is 139.

1492
01:37:10,320 --> 01:37:12,799
 The number of samples in class two is 164.

1493
01:37:12,799 --> 01:37:15,519
 So these are the distribution in the two classes, right?

1494
01:37:15,519 --> 01:37:18,559
 And also, we know each of the samples

1495
01:37:18,559 --> 01:37:23,840
 is represented by 13 features, 13 attributes, 13 properties.

1496
01:37:23,840 --> 01:37:26,200
 So age, sex, we have seen that, right?

1497
01:37:26,200 --> 01:37:33,040
 So for each of the attributes, we have these plots on them,

1498
01:37:33,040 --> 01:37:33,540
 right?

1499
01:37:33,540 --> 01:37:36,360
 We know they are binary, or they take multiple values,

1500
01:37:36,360 --> 01:37:38,000
 or they are continuous.

1501
01:37:38,000 --> 01:37:39,080
 OK.

1502
01:37:39,080 --> 01:37:43,920
 And so now, actually, based on this understanding

1503
01:37:43,920 --> 01:37:46,320
 of the data set and also the features,

1504
01:37:46,320 --> 01:37:50,080
 so, naturally, we first decide the first node, the root node.

1505
01:37:50,080 --> 01:37:52,840
 So which feature we should use?

1506
01:37:52,840 --> 01:37:55,679
 We should use age, or we should use sex,

1507
01:37:55,680 --> 01:37:57,840
 or we should use a CPU or others.

1508
01:37:57,840 --> 01:38:01,800
 It's the first feature in the root node.

1509
01:38:04,440 --> 01:38:08,560
 So, OK, so, naturally, we assume sex is used.

1510
01:38:08,560 --> 01:38:10,160
 Sex is used, right?

1511
01:38:10,160 --> 01:38:12,920
 And if sex is used, for this feature,

1512
01:38:12,920 --> 01:38:16,520
 we just have, actually, two values.

1513
01:38:16,520 --> 01:38:19,600
 So, naturally, we can just divide the data into two branches,

1514
01:38:19,600 --> 01:38:20,100
 right?

1515
01:38:20,100 --> 01:38:21,520
 Two branches, OK?

1516
01:38:21,520 --> 01:38:24,960
 And so here, in this course, actually, I just follow two.

1517
01:38:24,960 --> 01:38:26,160
 We just assume two.

1518
01:38:26,160 --> 01:38:28,040
 Even the feature, actually, adopts,

1519
01:38:28,040 --> 01:38:30,440
 actually, the multiple split.

1520
01:38:30,440 --> 01:38:35,600
 Have multiple values, we can still use the binary split.

1521
01:38:35,600 --> 01:38:38,600
 So here, I just use the binary split.

1522
01:38:38,600 --> 01:38:40,560
 And so here, in the sex, of course,

1523
01:38:40,560 --> 01:38:43,160
 this question about sex, we can really

1524
01:38:43,160 --> 01:38:46,880
 ask whether sex equals 0 or sex equals 1, right?

1525
01:38:46,880 --> 01:38:48,640
 Then, because it's just two values,

1526
01:38:48,640 --> 01:38:52,280
 so naturally, we can divide the data into two parts.

1527
01:38:52,280 --> 01:38:53,480
 OK.

1528
01:38:53,519 --> 01:38:57,400
 So then, actually, the data come to the left node.

1529
01:38:57,400 --> 01:38:58,839
 Data come to the right node.

1530
01:38:58,839 --> 01:39:02,320
 In the left node, left branch.

1531
01:39:02,320 --> 01:39:06,040
 And, totally, we have 97 samples.

1532
01:39:06,040 --> 01:39:10,040
 And 25 from class 1, 72 from class 2.

1533
01:39:10,040 --> 01:39:14,200
 So these are the samples in the left side, in the left branch.

1534
01:39:14,200 --> 01:39:17,400
 And then, in the right branch, we have most data set,

1535
01:39:17,400 --> 01:39:18,480
 and the data.

1536
01:39:18,480 --> 01:39:22,599
 We have 206 data samples.

1537
01:39:22,599 --> 01:39:23,280
 OK.

1538
01:39:23,280 --> 01:39:26,360
 And 119 from class 1.

1539
01:39:26,360 --> 01:39:29,160
 Actually, from class 2, we have 92 samples.

1540
01:39:29,160 --> 01:39:29,679
 OK.

1541
01:39:29,679 --> 01:39:33,559
 So certainly, actually, the data is very impure, right?

1542
01:39:33,559 --> 01:39:36,960
 Because, actually, here, we see 25, 30.

1543
01:39:36,960 --> 01:39:41,480
 This is like 25% or 70% from one class.

1544
01:39:41,480 --> 01:39:44,040
 25 or 30% from another class, right?

1545
01:39:44,040 --> 01:39:46,000
 So impure.

1546
01:39:46,000 --> 01:39:48,200
 Not like 100 versus 0, right?

1547
01:39:48,200 --> 01:39:49,120
 Not like that.

1548
01:39:49,120 --> 01:39:51,080
 Or 95 versus 5%.

1549
01:39:51,120 --> 01:39:53,760
 So here is like 70, 30%.

1550
01:39:53,760 --> 01:39:54,400
 OK.

1551
01:39:54,400 --> 01:39:55,280
 Very impure.

1552
01:39:55,280 --> 01:39:56,080
 OK.

1553
01:39:56,080 --> 01:39:56,920
 And then, we call the tree.

1554
01:39:56,920 --> 01:39:59,000
 We can measure the impurity measure.

1555
01:39:59,000 --> 01:40:05,680
 If sex is used as a rule node, and then, based on this question,

1556
01:40:05,680 --> 01:40:09,920
 sex is 0 and the data divided into two parts,

1557
01:40:09,920 --> 01:40:12,320
 then we will look the impurity measure

1558
01:40:12,320 --> 01:40:17,600
 for each of the two parts, for each of the two data subset.

1559
01:40:17,600 --> 01:40:18,519
 OK.

1560
01:40:18,520 --> 01:40:23,160
 And for the data in the left branch, 25, 79, right?

1561
01:40:23,160 --> 01:40:26,560
 So based on this, we can calculate the probability, right?

1562
01:40:26,560 --> 01:40:29,600
 The probability belonging to class 1, p omega 1,

1563
01:40:29,600 --> 01:40:32,800
 is just a p omega class 1, right?

1564
01:40:32,800 --> 01:40:35,320
 25 divided by 97.

1565
01:40:35,320 --> 01:40:39,640
 Then, p omega 2 is 72 divided by 97.

1566
01:40:39,640 --> 01:40:40,840
 So we know p omega 1.

1567
01:40:40,840 --> 01:40:42,360
 We know p omega 2, right?

1568
01:40:42,360 --> 01:40:48,480
 So naturally, we can easily calculate the impurity measure.

1569
01:40:48,480 --> 01:40:51,280
 Any of the three measures we can use,

1570
01:40:51,280 --> 01:40:54,759
 assume the gene impurity measure is used,

1571
01:40:54,759 --> 01:40:57,759
 and then we can calculate the samples of the data set

1572
01:40:57,759 --> 01:40:59,839
 in the left side gene impurity.

1573
01:41:03,360 --> 01:41:06,080
 1 subtract p omega 1, right?

1574
01:41:06,080 --> 01:41:08,879
 Square minus p omega 2 square, then divided by 2.

1575
01:41:08,879 --> 01:41:09,599
 OK.

1576
01:41:09,599 --> 01:41:13,839
 So omega 1, 25 divided by 97.

1577
01:41:13,839 --> 01:41:17,759
 p omega 2, 72 divided by 97.

1578
01:41:17,760 --> 01:41:22,680
 Then, divided by 2, we have this value, 0.19, 1, 3.

1579
01:41:22,680 --> 01:41:27,600
 So this is the impurity measure for the data subset,

1580
01:41:27,600 --> 01:41:30,800
 actually, in the left node.

1581
01:41:30,800 --> 01:41:33,160
 For the data in the right node, similarly,

1582
01:41:33,160 --> 01:41:36,360
 then we can evaluate the impurity measure, right?

1583
01:41:36,360 --> 01:41:40,280
 The impurity measure in the right side.

1584
01:41:40,280 --> 01:41:40,960
 OK.

1585
01:41:40,960 --> 01:41:43,440
 So here, omega 1 is 1, 1, 4, right?

1586
01:41:43,440 --> 01:41:46,800
 Divided by the total number of samples in the left node.

1587
01:41:46,840 --> 01:41:48,280
 In the right node, 2, 6.

1588
01:41:51,160 --> 01:41:51,960
 OK.

1589
01:41:51,960 --> 01:41:56,240
 Then, for class 2, it's 92 divided by 2, 6.

1590
01:41:56,240 --> 01:41:59,200
 Then we can measure, 0.2471.

1591
01:41:59,200 --> 01:42:02,760
 So these are the impurity measures for the left node,

1592
01:42:02,760 --> 01:42:07,040
 left descendant node, right descendant node.

1593
01:42:07,040 --> 01:42:10,760
 So next, we need to combine the two nodes, right?

1594
01:42:10,760 --> 01:42:15,440
 The two nodes impurity measure into one single impurity

1595
01:42:15,440 --> 01:42:18,280
 measure, combined the two.

1596
01:42:18,280 --> 01:42:20,759
 The combination of the overall impurity measure,

1597
01:42:20,759 --> 01:42:24,719
 just weight the summation of these two, weight the summation.

1598
01:42:24,719 --> 01:42:27,160
 That means each of these impurity measures

1599
01:42:27,160 --> 01:42:29,480
 should have a weight, right?

1600
01:42:29,480 --> 01:42:30,719
 What is the weight?

1601
01:42:30,719 --> 01:42:32,480
 The weight is just the proportion of the data

1602
01:42:32,480 --> 01:42:35,280
 going to the left, going to the right.

1603
01:42:35,280 --> 01:42:36,759
 OK.

1604
01:42:36,759 --> 01:42:41,400
 So it's the proportion, 97 going to the left, right?

1605
01:42:41,400 --> 01:42:44,320
 Total sample is the 303.

1606
01:42:44,320 --> 01:42:45,320
 OK.

1607
01:42:45,360 --> 01:42:47,320
 So the weight goes to the right, 2, 6,

1608
01:42:47,320 --> 01:42:49,000
 goes to the left, right?

1609
01:42:49,000 --> 01:42:51,799
 And then, 303 is the total number of samples.

1610
01:42:51,799 --> 01:42:54,599
 So this is the PL, this is the PR.

1611
01:42:54,599 --> 01:42:57,120
 So linearly combine them, or weight the summation

1612
01:42:57,120 --> 01:42:59,440
 of the two impurity measures.

1613
01:42:59,440 --> 01:43:04,679
 Then we get the overall impurity measure, 0.2276.

1614
01:43:04,679 --> 01:43:10,679
 So this is the impurity measure if the impurity measure,

1615
01:43:10,679 --> 01:43:13,639
 if the sex is used.

1616
01:43:14,440 --> 01:43:15,600
 OK.

1617
01:43:15,600 --> 01:43:20,080
 So whether this sex is the best selection,

1618
01:43:20,080 --> 01:43:22,920
 we're not very sure, because we have no try others, right?

1619
01:43:22,920 --> 01:43:27,280
 So the necessary we should try others, one by one.

1620
01:43:27,280 --> 01:43:30,200
 So necessary we try FBS, for example.

1621
01:43:30,200 --> 01:43:32,880
 FBS, again, if you look at the values,

1622
01:43:32,880 --> 01:43:34,840
 is a binary feature.

1623
01:43:34,840 --> 01:43:36,600
 And we have two values, 0 or 1.

1624
01:43:37,600 --> 01:43:43,800
 So then, again, we can ask questions like, FBS equals 0.

1625
01:43:43,800 --> 01:43:48,080
 You can also ask, FBS equals 1.

1626
01:43:48,080 --> 01:43:51,920
 Then you can divide the data into two subsets.

1627
01:43:51,920 --> 01:43:56,520
 So for the FBS, then we have this split of the data.

1628
01:43:56,520 --> 01:44:01,920
 In the left-hand side, then we have two 5-8 samples.

1629
01:44:01,920 --> 01:44:06,520
 And in the right-hand side, we have 45 samples.

1630
01:44:07,320 --> 01:44:08,920
 And then based on the distribution of the data

1631
01:44:08,920 --> 01:44:11,680
 in the two classes, then we can find the proportion.

1632
01:44:11,680 --> 01:44:15,840
 Pumir 1, Pumir 2, then we can find the gene impurity measure.

1633
01:44:15,840 --> 01:44:17,520
 For the data in the left-hand side,

1634
01:44:17,520 --> 01:44:21,160
 for the data in the right-hand side.

1635
01:44:21,160 --> 01:44:24,520
 So I think this measure is quite straightforward, right?

1636
01:44:24,520 --> 01:44:29,600
 Pumir 1, 1 by 7 divided by the total number in the left.

1637
01:44:29,600 --> 01:44:35,840
 Minus Pumir 2, 1 for 1, divided by all of these.

1638
01:44:35,840 --> 01:44:38,960
 So this is the left-side impurity measure.

1639
01:44:38,960 --> 01:44:43,320
 And then this is the right-side impurity measure.

1640
01:44:43,320 --> 01:44:46,440
 And then that's a treatment to combine these two measures,

1641
01:44:46,440 --> 01:44:50,680
 two values to produce one overall impurity measure.

1642
01:44:50,680 --> 01:44:54,240
 Based on the proportion of data going to the left,

1643
01:44:54,240 --> 01:44:56,040
 going to the right.

1644
01:44:56,040 --> 01:45:00,480
 So the overall impurity measure, if FBS is used in the rule

1645
01:45:00,480 --> 01:45:02,360
 of the descendant node, right?

1646
01:45:02,360 --> 01:45:06,919
 And then this is 0.2481.

1647
01:45:06,919 --> 01:45:11,599
 So the first one, if sex is 0.2276, right?

1648
01:45:11,599 --> 01:45:15,240
 So here is 0.222481.

1649
01:45:15,240 --> 01:45:19,000
 So certainly the sex is better than this FBS, right?

1650
01:45:19,000 --> 01:45:22,040
 Because the sex is used, the impurity measure,

1651
01:45:22,040 --> 01:45:27,639
 in the descendant node is less than the descendant node

1652
01:45:27,640 --> 01:45:30,640
 because FBS is used, right?

1653
01:45:30,640 --> 01:45:35,280
 Because 1 is 0.22 something, and not 0.24 something, OK?

1654
01:45:35,280 --> 01:45:37,800
 So the sex is better than FBS.

1655
01:45:37,800 --> 01:45:43,000
 But whether sex is the best among all the 13, we don't know.

1656
01:45:43,000 --> 01:45:45,080
 We have no try others, right?

1657
01:45:45,080 --> 01:45:47,840
 So necessarily we should try others one by one, OK?

1658
01:45:47,840 --> 01:45:51,880
 So necessarily we try the age.

1659
01:45:51,880 --> 01:45:52,840
 Why age is selected?

1660
01:45:52,840 --> 01:45:54,440
 Because age is different.

1661
01:45:54,440 --> 01:45:56,840
 Age is a continuous variable.

1662
01:45:56,840 --> 01:46:00,840
 Just now we have FBS, we have sex,

1663
01:46:00,840 --> 01:46:03,040
 which are binary features, right?

1664
01:46:03,040 --> 01:46:04,800
 Binary features are easy to determine.

1665
01:46:04,800 --> 01:46:11,360
 Just the splitting point is easy to determine.

1666
01:46:11,360 --> 01:46:13,080
 Because you just have two values, right?

1667
01:46:13,080 --> 01:46:15,240
 You use other value to ask questions.

1668
01:46:15,240 --> 01:46:20,320
 If you have 0 or equal 1, you can ask the question easy, right?

1669
01:46:20,320 --> 01:46:25,960
 And before age is continuous, we have so many values, right?

1670
01:46:26,000 --> 01:46:29,320
 So which value should be used to divide the data into two

1671
01:46:29,320 --> 01:46:31,320
 subsets?

1672
01:46:31,320 --> 01:46:33,080
 So that's the issue, OK?

1673
01:46:33,080 --> 01:46:37,480
 Because it's continuous, OK?

1674
01:46:37,480 --> 01:46:42,600
 And actually for other discrete features like CP,

1675
01:46:42,600 --> 01:46:44,320
 we have 1, 2, 3, 4, right?

1676
01:46:44,320 --> 01:46:49,080
 Actually, we can follow a similar method like age.

1677
01:46:52,400 --> 01:46:55,160
 So because you have multiple values.

1678
01:46:55,360 --> 01:46:56,840
 For age, OK?

1679
01:46:56,840 --> 01:46:58,280
 So it's continuous.

1680
01:46:58,280 --> 01:46:59,960
 So how to find the splitting point?

1681
01:47:02,559 --> 01:47:05,480
 Which age we should use to divide the data?

1682
01:47:05,480 --> 01:47:08,000
 Age should be greater than 65 years old,

1683
01:47:08,000 --> 01:47:11,320
 based on this question, to divide the data into two subsets,

1684
01:47:11,320 --> 01:47:13,800
 from very young to 66, right?

1685
01:47:13,800 --> 01:47:18,360
 65, 66, and then from over 66, right?

1686
01:47:18,360 --> 01:47:23,120
 Until we're at old age, two data subsets, OK?

1687
01:47:23,120 --> 01:47:27,200
 So we need to decide the data points.

1688
01:47:27,200 --> 01:47:29,599
 So how to decide?

1689
01:47:29,599 --> 01:47:34,280
 First, we need to sort the data in ascending order,

1690
01:47:34,280 --> 01:47:38,400
 increasing order, from a smallest to biggest.

1691
01:47:38,400 --> 01:47:42,920
 We need to arrange the data, look at all the data, the values.

1692
01:47:42,920 --> 01:47:46,760
 Actually, the 29 is smallest, then 23, OK?

1693
01:47:46,760 --> 01:47:51,760
 So from this example, this is 29, OK?

1694
01:47:51,760 --> 01:47:56,200
 34, 35, 37, and all, very big values, right?

1695
01:47:56,200 --> 01:47:59,120
 And then for each of the two adjacent values,

1696
01:47:59,120 --> 01:48:02,240
 neighboring values, we find the middle point.

1697
01:48:02,240 --> 01:48:04,280
 Middle point, actually, just the mean value,

1698
01:48:04,280 --> 01:48:05,880
 every value of the two, right?

1699
01:48:05,880 --> 01:48:11,880
 For each adjacent point, for example, 29, 34, we get average.

1700
01:48:11,880 --> 01:48:12,760
 OK?

1701
01:48:12,760 --> 01:48:15,440
 Then 34, 35, we get average.

1702
01:48:15,440 --> 01:48:17,440
 35, 37, we get average.

1703
01:48:17,440 --> 01:48:21,000
 So we get the middle point of the adjacent values.

1704
01:48:21,000 --> 01:48:22,960
 Then we can have this.

1705
01:48:22,960 --> 01:48:27,000
 31.5 is the value between 29, 34, right?

1706
01:48:27,000 --> 01:48:28,080
 The middle point.

1707
01:48:28,080 --> 01:48:33,960
 And then 34.5 is the between 34, 35.

1708
01:48:33,960 --> 01:48:36,320
 36 is between 35, 37, right?

1709
01:48:36,320 --> 01:48:40,200
 So you can actually all get all the middle point values.

1710
01:48:40,200 --> 01:48:43,760
 Then we need to use each of these points

1711
01:48:43,760 --> 01:48:47,880
 as a possible splitting point to divide the data

1712
01:48:47,880 --> 01:48:51,480
 into two types, into two subsets.

1713
01:48:51,480 --> 01:48:52,440
 OK?

1714
01:48:52,440 --> 01:48:58,320
 And for example, if we use 31.5 to divide the data

1715
01:48:58,320 --> 01:48:59,120
 into two subsets.

1716
01:49:03,360 --> 01:49:06,640
 So each of the middle points is one candidate splitting

1717
01:49:06,640 --> 01:49:08,640
 some point.

1718
01:49:08,640 --> 01:49:11,200
 And then we divide the data into two subsets.

1719
01:49:11,200 --> 01:49:16,480
 Then we evaluate the impurity measures of the two parts.

1720
01:49:16,480 --> 01:49:18,599
 And then we combine the impurity measure of the two parts

1721
01:49:18,599 --> 01:49:20,519
 into one overall impurity, right?

1722
01:49:20,519 --> 01:49:23,200
 Based on the proportion of the data going to the right,

1723
01:49:23,200 --> 01:49:25,360
 going to the left, going to the right, combined.

1724
01:49:25,360 --> 01:49:25,639
 OK?

1725
01:49:25,639 --> 01:49:27,200
 So this is an impurity measure.

1726
01:49:27,200 --> 01:49:31,040
 If age is selected as the feature to ask a question.

1727
01:49:31,040 --> 01:49:38,040
 And if 31.5 is the splitting point to divide the data,

1728
01:49:38,040 --> 01:49:39,799
 then we can get this value.

1729
01:49:39,799 --> 01:49:40,299
 OK?

1730
01:49:40,299 --> 01:49:45,799
 Then we try 34.5 to get the corresponding impurity

1731
01:49:45,800 --> 01:49:46,680
 measure.

1732
01:49:46,680 --> 01:49:47,400
 OK?

1733
01:49:47,400 --> 01:49:54,360
 So for example, as we use 31.5 to split the data.

1734
01:49:54,360 --> 01:49:58,280
 And then we can find that age greater than 31.5.

1735
01:49:58,280 --> 01:50:03,160
 You can also ask age smaller, no less than 31.5.

1736
01:50:03,160 --> 01:50:04,280
 The same, no problem.

1737
01:50:04,280 --> 01:50:07,080
 Just the tree, the left, come to the right,

1738
01:50:07,080 --> 01:50:08,640
 right come to the left.

1739
01:50:08,640 --> 01:50:10,240
 It's not no matter.

1740
01:50:10,240 --> 01:50:13,800
 So if we use age greater than 31.5,

1741
01:50:13,800 --> 01:50:16,400
 this question to split the data, right?

1742
01:50:16,400 --> 01:50:21,320
 And then actually we have a majority of the data

1743
01:50:21,320 --> 01:50:24,240
 coming to the left node.

1744
01:50:24,240 --> 01:50:28,120
 And only one sample going to the right.

1745
01:50:28,120 --> 01:50:28,880
 OK?

1746
01:50:28,880 --> 01:50:30,880
 And then that's actually we can calculate the impurity

1747
01:50:30,880 --> 01:50:33,280
 measure for all the data in the left node.

1748
01:50:37,000 --> 01:50:37,840
 OK?

1749
01:50:37,840 --> 01:50:41,160
 So for class 1, POML 1139, right?

1750
01:50:41,160 --> 01:50:42,680
 Divided by two to the number of samples.

1751
01:50:42,720 --> 01:50:45,320
 That tree is 302.

1752
01:50:45,320 --> 01:50:49,680
 The minus 163 divided by the total number of samples

1753
01:50:49,680 --> 01:50:50,840
 in the left.

1754
01:50:50,840 --> 01:50:53,320
 So this is POML2, that's square.

1755
01:50:53,320 --> 01:50:54,120
 OK?

1756
01:50:54,120 --> 01:50:55,600
 I divided by these two.

1757
01:50:55,600 --> 01:50:57,120
 We get this value.

1758
01:50:57,120 --> 01:51:01,760
 This is the impurity measure for the subset in the left.

1759
01:51:01,760 --> 01:51:03,720
 And then the impurity in the right.

1760
01:51:03,720 --> 01:51:07,000
 I treat the data in the right-hand side only one, right?

1761
01:51:07,000 --> 01:51:08,360
 It's a pu.

1762
01:51:08,360 --> 01:51:09,120
 It's pu, right?

1763
01:51:09,120 --> 01:51:10,680
 Only sample from one class.

1764
01:51:10,680 --> 01:51:12,960
 I treat the impurity measure should be zero, right?

1765
01:51:12,960 --> 01:51:13,600
 Should be zero.

1766
01:51:13,600 --> 01:51:14,960
 Indeed, it is zero.

1767
01:51:17,640 --> 01:51:20,400
 For class 1, only zero sample, right?

1768
01:51:20,400 --> 01:51:22,480
 So it's probably zero.

1769
01:51:22,480 --> 01:51:27,960
 For class 1, for class 2, one sample, one.

1770
01:51:27,960 --> 01:51:29,000
 This is class 2.

1771
01:51:29,000 --> 01:51:32,160
 So it's POML2 equals 1.

1772
01:51:32,160 --> 01:51:35,640
 Then the total impurity measure for the data in the right

1773
01:51:35,640 --> 01:51:37,120
 side is zero.

1774
01:51:37,120 --> 01:51:39,960
 Then we need to combine this i left, i right,

1775
01:51:39,960 --> 01:51:41,560
 based on the proportion.

1776
01:51:41,560 --> 01:51:44,680
 Going to the left, going to the right, right?

1777
01:51:44,680 --> 01:51:47,400
 So this is the overall impurity measure.

1778
01:51:47,400 --> 01:51:55,600
 And if 31.5 is used as a splitting point for the edge,

1779
01:51:55,600 --> 01:51:57,120
 we can get this.

1780
01:51:57,120 --> 01:52:01,760
 3.2476.

1781
01:52:01,760 --> 01:52:03,600
 And this is 31.5.

1782
01:52:03,600 --> 01:52:07,960
 Whether this 31.5 is a good one, we have no idea.

1783
01:52:07,960 --> 01:52:09,720
 We should try another one, right?

1784
01:52:09,720 --> 01:52:13,240
 So that is 34.5.

1785
01:52:13,240 --> 01:52:17,480
 And actually, for each of the possible splitting points,

1786
01:52:17,480 --> 01:52:21,640
 and we can find the corresponding impurity

1787
01:52:21,640 --> 01:52:26,480
 measure after data splitting by this splitting point.

1788
01:52:29,480 --> 01:52:31,880
 Then we can have this.

1789
01:52:31,880 --> 01:52:33,480
 We can have this.

1790
01:52:33,480 --> 01:52:34,960
 And this is actually the horizontal here.

1791
01:52:34,960 --> 01:52:36,800
 Just shows a splitting point.

1792
01:52:36,800 --> 01:52:41,000
 The vertical here shows the corresponding impurity measure.

1793
01:52:41,000 --> 01:52:44,920
 Certainly, actually, when this value, this is actually

1794
01:52:44,920 --> 01:52:46,440
 34.5.

1795
01:52:46,440 --> 01:52:53,480
 When this value is used as a splitting point,

1796
01:52:53,480 --> 01:52:58,080
 then we have the minimum impurity, impurity measure.

1797
01:52:58,080 --> 01:53:01,320
 So that is around the impurity measure,

1798
01:53:01,320 --> 01:53:05,560
 it is 0.228.

1799
01:53:05,600 --> 01:53:12,040
 So for each, for this feature, the best splitting point

1800
01:53:12,040 --> 01:53:17,240
 is actually 54.5.

1801
01:53:17,240 --> 01:53:20,120
 Of course, actually, here, for illustration,

1802
01:53:20,120 --> 01:53:23,600
 actually, you can write a program to do just a loop,

1803
01:53:23,600 --> 01:53:25,480
 to evaluate one by one.

1804
01:53:25,480 --> 01:53:28,040
 You can write very easily.

1805
01:53:28,040 --> 01:53:33,000
 So although it seems tedious, actually, it's quite easy

1806
01:53:33,000 --> 01:53:33,920
 in practice.

1807
01:53:33,920 --> 01:53:39,200
 You just write a loop to execute this process.

1808
01:53:39,200 --> 01:53:43,240
 Then you can get the best splitting point for H.

1809
01:53:43,240 --> 01:53:46,640
 And then we can also get the best impurity measure,

1810
01:53:46,640 --> 01:53:48,960
 so that is just 0.228.

1811
01:53:48,960 --> 01:53:53,480
 If H is used, and the splitting, we use other question,

1812
01:53:53,480 --> 01:53:58,120
 and if H greater than 54.5, so this feature

1813
01:53:58,120 --> 01:54:02,400
 and this particular question, then

1814
01:54:03,080 --> 01:54:10,799
 impurity measure in the descending node is 0.228.

1815
01:54:10,799 --> 01:54:12,759
 So this is H. And then similarly, we

1816
01:54:12,759 --> 01:54:20,160
 can measure all of them, assume actually that sex is selected.

1817
01:54:20,160 --> 01:54:21,960
 The sex is the minimum.

1818
01:54:21,960 --> 01:54:23,320
 Assume.

1819
01:54:23,320 --> 01:54:26,839
 So until so far, sex is the minimum value.

1820
01:54:26,839 --> 01:54:28,759
 So sex is selected.

1821
01:54:28,759 --> 01:54:30,679
 Then this is the first node.

1822
01:54:30,679 --> 01:54:32,320
 The rule node is determined.

1823
01:54:32,320 --> 01:54:33,920
 And then the question here is easy,

1824
01:54:33,920 --> 01:54:37,120
 because sex is a binary feature.

1825
01:54:37,120 --> 01:54:38,759
 So yes or no.

1826
01:54:38,759 --> 01:54:40,320
 As a splitting point, it's just 0, 1.

1827
01:54:40,320 --> 01:54:41,120
 It doesn't matter.

1828
01:54:41,120 --> 01:54:43,160
 It's just too value.

1829
01:54:43,160 --> 01:54:46,360
 So this is the first node, the rule node.

1830
01:54:46,360 --> 01:54:49,639
 And then we have the descending node, left,

1831
01:54:49,639 --> 01:54:51,960
 descending node, right, descending node.

1832
01:54:51,960 --> 01:54:54,280
 And then from each of the data now,

1833
01:54:54,280 --> 01:54:56,200
 in the left side, in the right side,

1834
01:54:56,200 --> 01:54:58,599
 we should repeat this process.

1835
01:54:58,599 --> 01:55:01,280
 So for this data set in the left,

1836
01:55:01,280 --> 01:55:03,519
 then we should try the features.

1837
01:55:03,519 --> 01:55:09,559
 Which feature we should use to divide the data in the left?

1838
01:55:09,559 --> 01:55:11,440
 Based on the same procedure.

1839
01:55:11,440 --> 01:55:13,480
 We check if a trigger.

1840
01:55:13,480 --> 01:55:16,400
 Maybe sex cannot be used, because sex is just

1841
01:55:16,400 --> 01:55:20,480
 in the parent node, in the rule node.

1842
01:55:20,480 --> 01:55:22,960
 Then maybe here we should not use sex.

1843
01:55:22,960 --> 01:55:24,360
 We should use others.

1844
01:55:24,360 --> 01:55:26,160
 We should try others one by one.

1845
01:55:26,160 --> 01:55:29,280
 You can also try sex.

1846
01:55:29,280 --> 01:55:31,080
 You still try sex.

1847
01:55:31,080 --> 01:55:35,360
 And because sex comes to here, all have the same value, right?

1848
01:55:35,360 --> 01:55:36,840
 Same value to the left.

1849
01:55:36,840 --> 01:55:38,080
 They all have one.

1850
01:55:38,080 --> 01:55:39,559
 You cannot use this, actually.

1851
01:55:39,559 --> 01:55:40,800
 One single value.

1852
01:55:40,800 --> 01:55:42,720
 You cannot use it to divide the data, right?

1853
01:55:42,720 --> 01:55:44,040
 So you cannot use sex.

1854
01:55:44,040 --> 01:55:46,800
 But if a feature, for example, the feature, actually,

1855
01:55:46,800 --> 01:55:50,160
 already known like a CP.

1856
01:55:50,160 --> 01:55:52,920
 Assume CP is the user first.

1857
01:55:52,920 --> 01:55:55,480
 And then you divide the data into two split.

1858
01:55:55,480 --> 01:55:57,640
 For example, in the left-hand side,

1859
01:55:57,640 --> 01:56:02,000
 actually, the data, the CP value equals 1 or 2.

1860
01:56:02,000 --> 01:56:05,560
 In the right-hand side, the CP value equals 2 or 3.

1861
01:56:05,560 --> 01:56:13,040
 And then in the next node, we can still ask questions about CP.

1862
01:56:13,040 --> 01:56:15,200
 Because the CP still have two values, right?

1863
01:56:15,200 --> 01:56:16,640
 1, 2 in the left side.

1864
01:56:16,640 --> 01:56:19,080
 And in the right-hand side, 3 or 4.

1865
01:56:19,080 --> 01:56:21,040
 We can still split.

1866
01:56:21,040 --> 01:56:23,720
 But not for sex, because there are only two values.

1867
01:56:23,720 --> 01:56:25,800
 All the data in the left side, only one value.

1868
01:56:25,800 --> 01:56:27,680
 That is sex equal to zero.

1869
01:56:27,680 --> 01:56:30,720
 You cannot use sex to divide the data.

1870
01:56:30,720 --> 01:56:32,400
 But we can use other features.

1871
01:56:32,400 --> 01:56:37,080
 We can try remaining 12 features one by one,

1872
01:56:37,080 --> 01:56:39,200
 following the same procedure.

1873
01:56:39,200 --> 01:56:41,640
 But just based on this data, not the full data,

1874
01:56:41,640 --> 01:56:46,320
 based on the data in the left, we ask the same question.

1875
01:56:46,320 --> 01:56:49,280
 We follow the same procedures.

1876
01:56:49,280 --> 01:56:52,000
 Then finally, we decide the best one.

1877
01:56:56,480 --> 01:56:58,880
 Because that's actually, we have a break.

1878
01:56:58,880 --> 01:56:59,840
 So timing is broke.

1879
02:04:55,800 --> 02:04:56,300
 OK.

1880
02:05:25,800 --> 02:05:35,300
 OK.

1881
02:05:35,300 --> 02:05:43,880
 So just now, we have learned how to select the rule node.

1882
02:05:43,880 --> 02:05:47,360
 And actually, the selection of the descending node

1883
02:05:47,360 --> 02:05:51,800
 in the next level follows exactly the same procedure.

1884
02:05:51,800 --> 02:06:00,800
 We need to test or we need to evaluate each of the remaining

1885
02:06:00,800 --> 02:06:02,080
 features, right?

1886
02:06:02,080 --> 02:06:08,000
 And all the 12 features, one by one.

1887
02:06:08,000 --> 02:06:12,280
 And for some of the features with multiple values,

1888
02:06:12,280 --> 02:06:15,480
 like continuous features or features with multiple values,

1889
02:06:15,480 --> 02:06:17,680
 like even discrete.

1890
02:06:17,680 --> 02:06:24,200
 And we also decide the optimal splitting point.

1891
02:06:24,200 --> 02:06:33,800
 And then we get the optimal, actually, the smallest

1892
02:06:33,800 --> 02:06:37,880
 impurity measure, if that feature is used.

1893
02:06:37,880 --> 02:06:42,640
 So then finally, we need to compare the impurity measures.

1894
02:06:42,640 --> 02:06:47,160
 Resulted from each of the 12 features, right?

1895
02:06:47,160 --> 02:06:50,080
 Including the best splitting point.

1896
02:06:50,080 --> 02:06:52,639
 Then we decide which one to use.

1897
02:06:52,639 --> 02:06:57,400
 And for example, here, for this, right?

1898
02:06:57,400 --> 02:07:00,040
 After data, we can test the ABS.

1899
02:07:00,040 --> 02:07:02,760
 If the ABS is used, so previously,

1900
02:07:02,760 --> 02:07:07,480
 we evaluated the ABS based on the full data set.

1901
02:07:07,480 --> 02:07:11,720
 But now, actually, the ABS is used based on the data set

1902
02:07:11,720 --> 02:07:14,040
 going to the left only.

1903
02:07:14,040 --> 02:07:18,400
 I think the data going to the left is only 97 samples.

1904
02:07:18,400 --> 02:07:21,640
 So based on these 97 samples, we decide, actually,

1905
02:07:21,640 --> 02:07:25,960
 the impurity measure in the descending nodes

1906
02:07:25,960 --> 02:07:27,960
 if the ABS is used.

1907
02:07:30,480 --> 02:07:34,240
 And similarly, so this is actually very straightforward,

1908
02:07:34,240 --> 02:07:34,740
 right?

1909
02:07:34,740 --> 02:07:35,880
 Your BS is used.

1910
02:07:35,880 --> 02:07:37,960
 And then we look at the sample here, right?

1911
02:07:37,960 --> 02:07:43,120
 And if yes, you have the use.

1912
02:07:43,120 --> 02:07:45,240
 And this is very easy, right?

1913
02:07:45,240 --> 02:07:47,720
 Because the ABS is a binary sample, right?

1914
02:07:47,720 --> 02:07:49,360
 We can only ask this question.

1915
02:07:49,360 --> 02:07:50,760
 Oh, the ABS equals 1, right?

1916
02:07:50,760 --> 02:07:51,260
 That's true.

1917
02:07:51,260 --> 02:07:52,960
 I try identical.

1918
02:07:52,960 --> 02:07:56,560
 And then going to the left, 97, right?

1919
02:07:56,560 --> 02:08:00,040
 We look at 97.

1920
02:08:00,040 --> 02:08:03,040
 Sorry, it's 85, right?

1921
02:08:03,040 --> 02:08:04,680
 85 sample going to the left.

1922
02:08:04,680 --> 02:08:06,840
 Because totally we have 97.

1923
02:08:06,840 --> 02:08:09,400
 I go to the right 12 samples.

1924
02:08:09,400 --> 02:08:16,360
 So for these 85 samples, then we look at the impurity measure.

1925
02:08:16,360 --> 02:08:20,040
 For these 12 samples, we look at the impurity measure.

1926
02:08:20,040 --> 02:08:21,600
 That is the right, right?

1927
02:08:21,600 --> 02:08:23,720
 So then finally, we combine the two.

1928
02:08:23,720 --> 02:08:26,280
 We follow the exact same procedure.

1929
02:08:26,280 --> 02:08:29,040
 Based on the percentage of the sample going to the left

1930
02:08:29,040 --> 02:08:30,800
 or going to the right.

1931
02:08:30,800 --> 02:08:32,920
 So that is the impurity measure.

1932
02:08:32,920 --> 02:08:38,840
 If ABS is used, if ABS is used, right?

1933
02:08:38,840 --> 02:08:44,240
 But whether this is the best, we cannot see at this step.

1934
02:08:44,240 --> 02:08:49,000
 We need to try remaining 11 attributes.

1935
02:08:49,000 --> 02:08:50,200
 We will try one by one.

1936
02:08:53,360 --> 02:08:56,160
 Necessarily, if we use this one, we try this one.

1937
02:08:56,160 --> 02:08:57,480
 It's changed.

1938
02:08:57,480 --> 02:08:59,480
 It's changed to use.

1939
02:08:59,480 --> 02:09:05,360
 Then the 97 sample will be split into two parts.

1940
02:09:05,360 --> 02:09:08,639
 Then the derivative here is different from the previous case.

1941
02:09:08,639 --> 02:09:11,959
 Previous, if you add this, so one set have 12 samples.

1942
02:09:11,959 --> 02:09:15,120
 Another set have 85 samples.

1943
02:09:15,120 --> 02:09:20,519
 But if we use actually the exchange as a feature,

1944
02:09:20,519 --> 02:09:21,839
 it's a binary.

1945
02:09:21,839 --> 02:09:23,879
 And then we have this kind of split.

1946
02:09:23,879 --> 02:09:27,320
 And then we can again calculate the impurity

1947
02:09:27,320 --> 02:09:30,000
 measure in the left node, in the right node.

1948
02:09:30,000 --> 02:09:32,440
 Left, I mean this immediate descendant.

1949
02:09:32,440 --> 02:09:35,080
 Immediate descendant.

1950
02:09:35,080 --> 02:09:38,559
 Then we can have the two impurity measures.

1951
02:09:38,559 --> 02:09:39,599
 Then we can combine them.

1952
02:09:43,360 --> 02:09:45,320
 So this is 0.1493.

1953
02:09:45,320 --> 02:09:48,960
 Previously, if actually the ABS is used,

1954
02:09:48,960 --> 02:09:51,200
 then it's 0.18 something.

1955
02:09:51,200 --> 02:09:56,440
 So here, obviously, this 0.14 is smaller.

1956
02:09:56,440 --> 02:09:58,719
 In other words, the exchange actually

1957
02:09:58,719 --> 02:10:04,799
 is a better feature to be selected as a node.

1958
02:10:04,799 --> 02:10:06,559
 But actually, we are not aware of other.

1959
02:10:06,559 --> 02:10:09,160
 We already know exchange is better than ABS.

1960
02:10:09,160 --> 02:10:11,400
 We also need to check others.

1961
02:10:11,400 --> 02:10:19,360
 Finally, we calculate each of the impurity measure.

1962
02:10:19,360 --> 02:10:25,280
 When each of the features is used, then finally,

1963
02:10:25,280 --> 02:10:26,960
 we decide which one to use.

1964
02:10:31,400 --> 02:10:33,880
 We assume this impurity measure exchange,

1965
02:10:33,880 --> 02:10:35,639
 the exchange is the best.

1966
02:10:35,639 --> 02:10:37,280
 Then we can have this model.

1967
02:10:37,280 --> 02:10:38,559
 This is the one.

1968
02:10:38,559 --> 02:10:43,200
 So after we try 12 times, repeat 12 times.

1969
02:10:43,200 --> 02:10:47,240
 And then we identify exchange is the best, assume.

1970
02:10:47,240 --> 02:10:49,120
 Then we can have this tree.

1971
02:10:49,120 --> 02:10:51,800
 So now we grew the tree from one node.

1972
02:10:51,800 --> 02:10:54,480
 So now we have a tree, a tree more node.

1973
02:10:54,480 --> 02:10:56,639
 We have the next level, right?

1974
02:10:56,639 --> 02:10:57,200
 It's changed.

1975
02:10:57,200 --> 02:11:00,200
 Then we have one, we have zero, right?

1976
02:11:00,200 --> 02:11:03,519
 Then we have a root node, and then two descendant nodes.

1977
02:11:03,519 --> 02:11:05,280
 Then now for the left descendant node,

1978
02:11:05,280 --> 02:11:09,639
 we further have a tree, and this is a tree, the two nodes.

1979
02:11:09,639 --> 02:11:14,440
 So now we have one, two, three, four, five nodes.

1980
02:11:14,440 --> 02:11:15,400
 We have this node.

1981
02:11:15,400 --> 02:11:19,959
 So we grew the tree.

1982
02:11:19,959 --> 02:11:21,799
 And then we grew the tree.

1983
02:11:21,799 --> 02:11:23,480
 So when we should stop?

1984
02:11:23,480 --> 02:11:24,320
 When we should stop?

1985
02:11:24,320 --> 02:11:25,920
 So that is the issue, right?

1986
02:11:25,920 --> 02:11:29,679
 So here, I've got a dream for those data

1987
02:11:29,679 --> 02:11:30,960
 set in the right branch.

1988
02:11:30,960 --> 02:11:32,880
 You can follow the same procedure, right?

1989
02:11:32,880 --> 02:11:39,360
 Evaluate each of the top features.

1990
02:11:39,360 --> 02:11:41,080
 And then you decide which feature

1991
02:11:41,080 --> 02:11:45,120
 to use in this level for the data in the right hand side.

1992
02:11:49,440 --> 02:11:50,719
 When to stop splitting?

1993
02:11:50,719 --> 02:11:52,639
 So this is the problem issue.

1994
02:11:52,640 --> 02:11:54,640
 I want to detail.

1995
02:11:54,640 --> 02:11:58,880
 And in the extremely case, normally,

1996
02:11:58,880 --> 02:12:00,800
 this will not happen.

1997
02:12:00,800 --> 02:12:04,880
 And each leaf node just corresponds to one training

1998
02:12:04,880 --> 02:12:05,880
 on data.

1999
02:12:05,880 --> 02:12:13,200
 So if there is a tree, the data is extremely overfitted.

2000
02:12:13,200 --> 02:12:18,920
 So one leaf node corresponds to one training data.

2001
02:12:18,920 --> 02:12:21,320
 Of course, the purity is very high, right?

2002
02:12:21,320 --> 02:12:23,920
 And also, from the number, you just have one sample.

2003
02:12:23,920 --> 02:12:27,400
 You cannot perform further splitting of the data.

2004
02:12:27,400 --> 02:12:28,719
 You certainly should stop.

2005
02:12:28,719 --> 02:12:34,280
 So that is the terminal or the leaf node.

2006
02:12:34,280 --> 02:12:38,559
 But whether we should grow the tree until this step,

2007
02:12:38,559 --> 02:12:41,440
 certainly we should not.

2008
02:12:41,440 --> 02:12:44,960
 Because this is a tree, we are overfitting the training data.

2009
02:12:44,960 --> 02:12:48,360
 And also, the tree will be very big.

2010
02:12:48,360 --> 02:12:53,480
 You want to identify the classifier one testing sample,

2011
02:12:53,480 --> 02:12:56,120
 then it will go to many branches, right?

2012
02:12:56,120 --> 02:12:58,599
 Many and the path is very long.

2013
02:12:58,599 --> 02:13:02,040
 You want to determine actually the class label

2014
02:13:02,040 --> 02:13:04,200
 of that testing sample.

2015
02:13:04,200 --> 02:13:09,599
 Because the tree grows to a very big tree with many layers,

2016
02:13:09,599 --> 02:13:10,360
 many levels.

2017
02:13:11,280 --> 02:13:11,780
 OK.

2018
02:13:16,160 --> 02:13:18,559
 Conversely, if splitting will stop too early,

2019
02:13:18,559 --> 02:13:20,360
 so these are stopped too late, right?

2020
02:13:20,360 --> 02:13:22,280
 Too late, then we have overfitting.

2021
02:13:22,280 --> 02:13:22,839
 OK.

2022
02:13:22,839 --> 02:13:26,639
 Conversely, if splitting will stop too early,

2023
02:13:26,639 --> 02:13:30,440
 an extremely case is that we just have one root node.

2024
02:13:30,440 --> 02:13:34,480
 And then the next level terminal node or leaf node,

2025
02:13:34,480 --> 02:13:36,000
 these are extremely case, right?

2026
02:13:36,000 --> 02:13:37,679
 Early stopping.

2027
02:13:37,680 --> 02:13:40,680
 And then a tree, such a scenario tree,

2028
02:13:40,680 --> 02:13:44,760
 we see the tree will underfit the data, underfitting.

2029
02:13:44,760 --> 02:13:47,200
 We have overfitting, we have underfitting.

2030
02:13:47,200 --> 02:13:48,720
 And underfitting we can imagine, right?

2031
02:13:48,720 --> 02:13:50,080
 Just based on the sex.

2032
02:13:50,080 --> 02:13:52,360
 For example, just not sex, the best, right?

2033
02:13:52,360 --> 02:13:55,840
 After this, we can see the data is very impure, right?

2034
02:13:55,840 --> 02:14:01,960
 If we just have a leaf node, just like this, just one level.

2035
02:14:01,960 --> 02:14:06,120
 You can see, that is the 25, 32 right here.

2036
02:14:06,120 --> 02:14:09,400
 This means that left side, if you see the leaf node,

2037
02:14:09,400 --> 02:14:13,000
 then the class label of this leaf node will be class 2.

2038
02:14:13,000 --> 02:14:18,800
 Then almost 25 divided by 97% of the data

2039
02:14:18,800 --> 02:14:20,840
 will be misclassified.

2040
02:14:20,840 --> 02:14:22,519
 So the error rate is too high.

2041
02:14:22,519 --> 02:14:26,160
 For the right-hand side, 92 samples

2042
02:14:26,160 --> 02:14:30,760
 among the 206 sample will be misclassified.

2043
02:14:30,760 --> 02:14:31,260
 OK.

2044
02:14:31,260 --> 02:14:36,660
 This is the tree growing is stopped too early.

2045
02:14:36,660 --> 02:14:37,980
 This is an extremely case.

2046
02:14:37,980 --> 02:14:41,860
 Of course, not just this level, we will stop later.

2047
02:14:41,860 --> 02:14:44,380
 But it's early stopping.

2048
02:14:44,380 --> 02:14:48,940
 So the data will underfit it.

2049
02:14:48,940 --> 02:14:50,740
 Underfitted.

2050
02:14:50,740 --> 02:14:52,860
 Underfitted, of course, the performance on the tree data

2051
02:14:52,860 --> 02:14:53,980
 will be bad.

2052
02:14:53,980 --> 02:14:58,220
 And on the text data also will be bad.

2053
02:14:58,220 --> 02:15:01,020
 So we should try to, of course, actually

2054
02:15:01,020 --> 02:15:03,180
 award these two scenarios, right?

2055
02:15:03,180 --> 02:15:05,700
 To extreme cases.

2056
02:15:05,700 --> 02:15:11,140
 One is actually overfitting, and the other is underfitting.

2057
02:15:11,140 --> 02:15:16,460
 And how we could overcome or could award these two scenarios?

2058
02:15:16,460 --> 02:15:19,660
 And actually, so how we decide when to stop?

2059
02:15:22,380 --> 02:15:25,220
 And quite often, we can use the so-called validation,

2060
02:15:25,220 --> 02:15:27,220
 like cross-validation, right?

2061
02:15:27,220 --> 02:15:29,620
 And actually, we are talking about the cross-validation

2062
02:15:29,620 --> 02:15:34,500
 in the part of classification evaluation.

2063
02:15:34,500 --> 02:15:37,980
 And we were talking about different evaluation procedures

2064
02:15:37,980 --> 02:15:40,620
 and also different evaluation metrics.

2065
02:15:40,620 --> 02:15:44,700
 So here, actually, one of them is called cross-validation.

2066
02:15:44,700 --> 02:15:46,180
 And actually, normally, the data is

2067
02:15:46,180 --> 02:15:50,260
 divided into two parts, like training data, testing data.

2068
02:15:50,260 --> 02:15:54,820
 And also, quite often, we divide the data into three parts.

2069
02:15:54,820 --> 02:15:57,380
 Or we divide the training data into two parts.

2070
02:15:57,380 --> 02:15:59,780
 One part is used to fit the model.

2071
02:15:59,780 --> 02:16:02,620
 Another part is used to determine the hyperparameters.

2072
02:16:02,620 --> 02:16:06,740
 So the highest parameter here is just the size of the model.

2073
02:16:06,740 --> 02:16:08,259
 When we should stop?

2074
02:16:08,259 --> 02:16:10,179
 So this is a hyperparameter.

2075
02:16:10,179 --> 02:16:12,940
 How to determine the hyperparameter?

2076
02:16:12,940 --> 02:16:16,259
 Actually, we should be some validation data.

2077
02:16:16,259 --> 02:16:19,420
 OK, so that means that for the training data,

2078
02:16:19,420 --> 02:16:21,179
 we can divide them into two parts.

2079
02:16:21,179 --> 02:16:24,580
 One part is used to group the tree.

2080
02:16:24,580 --> 02:16:28,100
 And one part of the data will be used as a validation data

2081
02:16:28,100 --> 02:16:32,420
 to determine whether we should stop.

2082
02:16:32,420 --> 02:16:39,459
 OK, if the addition of the more node

2083
02:16:39,459 --> 02:16:42,020
 will result in a degradation in the performance

2084
02:16:42,020 --> 02:16:45,219
 on the validation data, then we should stop.

2085
02:16:45,219 --> 02:16:46,260
 We should stop.

2086
02:16:46,260 --> 02:16:49,340
 So this is, often, the criterion we should follow.

2087
02:16:49,340 --> 02:16:52,660
 For almost all the models, we use the cross-validation.

2088
02:16:52,660 --> 02:16:54,539
 In the neural networks, they train

2089
02:16:55,020 --> 02:16:57,620
 actually we train many iterations.

2090
02:16:57,620 --> 02:16:59,860
 The deep neural networks are also.

2091
02:16:59,860 --> 02:17:03,580
 And then the data will be overfitted.

2092
02:17:03,580 --> 02:17:07,300
 So when we should stop, we're based on the validation data

2093
02:17:07,300 --> 02:17:09,380
 performance.

2094
02:17:09,380 --> 02:17:12,220
 So these are frequent use criterion

2095
02:17:12,220 --> 02:17:14,980
 to determine when to stop, to determine

2096
02:17:14,980 --> 02:17:18,900
 actually the size of the suitable model size.

2097
02:17:18,900 --> 02:17:21,940
 Suitable model size determined based on the cross-validation.

2098
02:17:21,940 --> 02:17:24,500
 So this propagation or the validation data

2099
02:17:24,500 --> 02:17:27,820
 is to determine the high-performance.

2100
02:17:27,820 --> 02:17:30,820
 OK, the size of the model here.

2101
02:17:30,820 --> 02:17:36,780
 And so actually another method is that we can set a threshold

2102
02:17:36,780 --> 02:17:38,300
 value.

2103
02:17:38,300 --> 02:17:40,340
 And actually, every time we see we

2104
02:17:40,340 --> 02:17:44,460
 have a reduction of the impurity measure

2105
02:17:44,460 --> 02:17:48,860
 by adding one extra actually level

2106
02:17:48,860 --> 02:17:54,620
 or by performing a further splitting of the data.

2107
02:17:54,620 --> 02:17:59,060
 We can reduce the impurity measure.

2108
02:17:59,060 --> 02:18:00,820
 We can have a reduction.

2109
02:18:00,820 --> 02:18:04,420
 If this reduction, of course, at the beginning of the introduction

2110
02:18:04,420 --> 02:18:06,500
 of the reduction of the impurity measure,

2111
02:18:06,500 --> 02:18:08,300
 is very significant.

2112
02:18:08,300 --> 02:18:11,220
 But with the growing of the tree,

2113
02:18:11,220 --> 02:18:14,060
 and then the further splitting of the data

2114
02:18:14,060 --> 02:18:17,740
 may result in a very small reduction in the impurity

2115
02:18:17,740 --> 02:18:19,219
 measure.

2116
02:18:19,219 --> 02:18:21,940
 Actually, you can set a threshold value.

2117
02:18:21,940 --> 02:18:24,500
 If the reduction of the impurity measure

2118
02:18:24,500 --> 02:18:27,980
 is less than a certain value, then we stop.

2119
02:18:27,980 --> 02:18:34,020
 This is also a frequent use criteria in the model training.

2120
02:18:34,020 --> 02:18:39,260
 And particularly for models, the training procedure

2121
02:18:39,260 --> 02:18:42,660
 is based on the black propagation.

2122
02:18:42,660 --> 02:18:45,780
 And if the model is the right size,

2123
02:18:45,780 --> 02:18:48,500
 we normally, based on this threshold value,

2124
02:18:48,500 --> 02:18:52,980
 if the error is less than something, we stop.

2125
02:18:52,980 --> 02:18:57,740
 Here is if the reduction of the impurity measure

2126
02:18:57,740 --> 02:19:01,340
 is less than something, we stop.

2127
02:19:01,340 --> 02:19:03,460
 So these are criteria we can use.

2128
02:19:03,460 --> 02:19:05,660
 There are some other errors.

2129
02:19:05,660 --> 02:19:07,140
 So I like a cross-validation.

2130
02:19:07,140 --> 02:19:10,020
 I treat the train directly using all the train data.

2131
02:19:10,020 --> 02:19:13,900
 So here we don't divide the train data into two parts.

2132
02:19:13,900 --> 02:19:17,500
 Sometimes the train data is small.

2133
02:19:17,500 --> 02:19:21,820
 It's very precious to get the train data, very costly.

2134
02:19:21,820 --> 02:19:24,139
 A very small train data set it has.

2135
02:19:24,139 --> 02:19:27,580
 But you further divide them into two parts, validation

2136
02:19:27,580 --> 02:19:29,180
 and training.

2137
02:19:29,180 --> 02:19:32,180
 So you are always part of the data.

2138
02:19:32,180 --> 02:19:34,139
 So if we set the threshold value,

2139
02:19:34,139 --> 02:19:39,660
 then we can still use all the train data to grow the tree.

2140
02:19:39,660 --> 02:19:43,619
 So this is the advantage of you setting a threshold value

2141
02:19:43,620 --> 02:19:46,860
 for the reduction of the impurity measure.

2142
02:19:51,900 --> 02:19:54,620
 And also, if you use the threshold value,

2143
02:19:54,620 --> 02:19:57,980
 and then the leaf node can lie in different levels

2144
02:19:57,980 --> 02:19:59,780
 of the tree.

2145
02:19:59,780 --> 02:20:05,260
 If you go back to our model, to this example,

2146
02:20:05,260 --> 02:20:10,820
 you can see this is a tree that the leaf node

2147
02:20:10,820 --> 02:20:12,820
 at different levels.

2148
02:20:12,820 --> 02:20:16,860
 We don't grow the tree until we're low level.

2149
02:20:16,860 --> 02:20:20,900
 We're not this low, zero level, many levels.

2150
02:20:20,900 --> 02:20:23,980
 You go to many levels, then we have a big tree.

2151
02:20:23,980 --> 02:20:28,779
 So we can have a tree like the terminal node

2152
02:20:28,779 --> 02:20:31,420
 at different levels.

2153
02:20:31,420 --> 02:20:37,020
 Different level to make sure we can have a smaller model,

2154
02:20:37,020 --> 02:20:39,820
 smaller tree.

2155
02:20:39,820 --> 02:20:41,500
 So this is the meaning.

2156
02:20:41,500 --> 02:20:44,140
 So this is the advantage of using the threshold

2157
02:20:44,140 --> 02:20:45,260
 as a solving criteria.

2158
02:20:51,700 --> 02:20:53,700
 So this is the advantage.

2159
02:20:53,700 --> 02:20:54,660
 And oh, sorry.

2160
02:20:54,660 --> 02:20:55,660
 We have a figure here.

2161
02:20:55,660 --> 02:20:56,160
 OK.

2162
02:20:56,160 --> 02:20:59,260
 You can look at this.

2163
02:20:59,260 --> 02:21:02,340
 And actually, another method that we can stop,

2164
02:21:02,340 --> 02:21:05,660
 if actually the number of samples in a node

2165
02:21:05,660 --> 02:21:08,260
 is less than a certain value.

2166
02:21:08,260 --> 02:21:11,180
 For example, we can see for the train data,

2167
02:21:11,180 --> 02:21:15,260
 if the data points after splitting,

2168
02:21:15,260 --> 02:21:18,060
 come to the node, the left node or right node,

2169
02:21:18,060 --> 02:21:20,700
 come to the descendant node, if the number of samples

2170
02:21:20,700 --> 02:21:24,060
 is less than 10 or less than 5, this

2171
02:21:24,060 --> 02:21:26,020
 can be a value, a threshold value,

2172
02:21:26,020 --> 02:21:30,420
 then we should not perform further splitting of the data.

2173
02:21:30,420 --> 02:21:32,300
 We just stop.

2174
02:21:32,300 --> 02:21:35,660
 So this can also be used as criteria.

2175
02:21:35,660 --> 02:21:38,620
 So certainly, actually, we should

2176
02:21:38,620 --> 02:21:46,860
 use the solving criteria to stop the growing of the tree.

2177
02:21:46,860 --> 02:21:53,940
 And here, we can have a few methods, validation, results,

2178
02:21:53,940 --> 02:21:56,020
 the result of the value in data.

2179
02:21:56,020 --> 02:21:59,020
 And then we can look at the reduction of the impurity

2180
02:21:59,020 --> 02:21:59,900
 measure.

2181
02:21:59,900 --> 02:22:02,060
 And also, we can look at the number of samples

2182
02:22:02,060 --> 02:22:04,220
 in the descendant node.

2183
02:22:04,220 --> 02:22:05,980
 If it's small, there will be no need

2184
02:22:05,980 --> 02:22:08,859
 to perform further splitting of the data.

2185
02:22:08,859 --> 02:22:11,820
 We can stop for that node.

2186
02:22:11,820 --> 02:22:14,939
 For other nodes, if they still have more than 10,

2187
02:22:14,939 --> 02:22:21,580
 we should continue until the number of samples in that node

2188
02:22:21,580 --> 02:22:25,420
 is less than the threshold value, like 10 or 5,

2189
02:22:25,420 --> 02:22:25,939
 we said that.

2190
02:22:30,859 --> 02:22:33,619
 So here, normally, we can use a trade-off.

2191
02:22:33,620 --> 02:22:35,100
 One trade-off is the size.

2192
02:22:35,100 --> 02:22:39,100
 Another is the leaf node entropy.

2193
02:22:39,100 --> 02:22:40,100
 Even though the entropy.

2194
02:22:44,580 --> 02:22:46,420
 So we should have a balance, right?

2195
02:22:46,420 --> 02:22:47,140
 Balance.

2196
02:22:47,140 --> 02:22:50,580
 Normally, the impurity in the leaf node is small.

2197
02:22:50,580 --> 02:22:51,700
 It's small, right?

2198
02:22:51,700 --> 02:22:52,220
 There is no.

2199
02:22:52,220 --> 02:22:56,860
 So even the size of the model is big.

2200
02:22:56,860 --> 02:22:58,220
 So one is small and another big.

2201
02:22:58,220 --> 02:22:59,540
 So we should have a true, actually,

2202
02:22:59,540 --> 02:23:01,860
 now balance is the tool.

2203
02:23:02,820 --> 02:23:05,020
 The size of the model is not too big.

2204
02:23:05,020 --> 02:23:09,860
 And also, the leaf node, the impurity in the matter,

2205
02:23:09,860 --> 02:23:10,820
 is not too low.

2206
02:23:10,820 --> 02:23:13,020
 No need to be too low.

2207
02:23:13,020 --> 02:23:16,300
 So we have a trade-off between the two.

2208
02:23:19,980 --> 02:23:21,780
 So this is actually the grouillon, right?

2209
02:23:21,780 --> 02:23:23,860
 Grouillon, the tree, then actually

2210
02:23:23,860 --> 02:23:27,500
 we determine when to stop the grouillon process.

2211
02:23:27,500 --> 02:23:31,780
 This is the first phase in the construction of the tree.

2212
02:23:31,780 --> 02:23:34,700
 And in the second phase is the pruning.

2213
02:23:34,700 --> 02:23:37,460
 Pruning to cut some of the branches.

2214
02:23:37,460 --> 02:23:38,380
 How to cut?

2215
02:23:38,380 --> 02:23:45,340
 Actually, we start from merging of the leaf node.

2216
02:23:45,340 --> 02:23:49,060
 So cutting by the trees, actually, called pruning.

2217
02:23:49,060 --> 02:23:52,700
 And this actually tries to reduce the size of the model

2218
02:23:52,700 --> 02:23:56,180
 to avoid or to elevate the overfitting problem.

2219
02:23:57,060 --> 02:23:59,380
 OK.

2220
02:23:59,380 --> 02:24:05,060
 So here, actually, how to perform this pruning?

2221
02:24:05,060 --> 02:24:08,300
 Actually, normally, we look at the leaf node.

2222
02:24:08,300 --> 02:24:09,380
 We start from the leaf node.

2223
02:24:09,380 --> 02:24:10,900
 Of course, from the leaf node.

2224
02:24:10,900 --> 02:24:13,620
 We start from the neighboring leaf node.

2225
02:24:13,620 --> 02:24:16,580
 The leaf node should come to the same, actually, no parent

2226
02:24:16,580 --> 02:24:18,620
 node.

2227
02:24:18,620 --> 02:24:23,380
 And then if we combine these two, then we look at the performance.

2228
02:24:23,380 --> 02:24:24,660
 Look at the performance for mobile.

2229
02:24:24,660 --> 02:24:26,300
 Even on the train data, right?

2230
02:24:26,300 --> 02:24:36,740
 If we combine them, right?

2231
02:24:36,740 --> 02:24:42,140
 And then what should be the degradation of the performance?

2232
02:24:42,140 --> 02:24:44,300
 What should be the degradation of the performance?

2233
02:24:44,300 --> 02:24:50,180
 And if the degradation of performance is tolerable,

2234
02:24:50,180 --> 02:24:52,820
 and then we can combine the two.

2235
02:24:52,820 --> 02:24:56,539
 If we combine the two, we are resulting in a very big drop

2236
02:24:56,539 --> 02:24:58,140
 in the performance.

2237
02:24:58,140 --> 02:25:00,820
 And then we should not combine these two.

2238
02:25:00,820 --> 02:25:01,500
 OK.

2239
02:25:01,500 --> 02:25:06,660
 So actually, now we start from the neighboring leaf node.

2240
02:25:06,660 --> 02:25:08,180
 Then we combine them, right?

2241
02:25:08,180 --> 02:25:11,820
 For example, for this diagram.

2242
02:25:11,820 --> 02:25:12,500
 OK.

2243
02:25:12,500 --> 02:25:14,820
 And then we can look at this, for example, right?

2244
02:25:14,820 --> 02:25:17,940
 And then the cherry and the grape.

2245
02:25:17,940 --> 02:25:19,820
 If the two combine them, that means, actually,

2246
02:25:19,820 --> 02:25:21,740
 we don't do this splitting, right?

2247
02:25:21,740 --> 02:25:25,180
 Then we just, based on this tree, in this node,

2248
02:25:25,180 --> 02:25:26,940
 based on this part of the sample,

2249
02:25:26,940 --> 02:25:28,900
 how many belong to class one?

2250
02:25:28,900 --> 02:25:30,660
 How many belong to class two, right?

2251
02:25:30,660 --> 02:25:33,180
 And then we just decide to determine the class label

2252
02:25:33,180 --> 02:25:37,180
 of that particular branch or node.

2253
02:25:37,180 --> 02:25:41,220
 And then we look at what is the degradation of the performance.

2254
02:25:41,220 --> 02:25:42,940
 If the degradation of the performance is not much,

2255
02:25:42,940 --> 02:25:44,260
 then we can combine.

2256
02:25:44,260 --> 02:25:44,900
 OK.

2257
02:25:44,900 --> 02:25:49,820
 So this tree, cherry and the grape, will be cut.

2258
02:25:49,820 --> 02:25:50,660
 It will be cut.

2259
02:25:50,660 --> 02:25:51,420
 We will cut.

2260
02:25:51,420 --> 02:25:53,340
 Then we have this node.

2261
02:25:53,340 --> 02:25:58,340
 Then, actually, this node will become a leaf node again.

2262
02:25:58,340 --> 02:26:00,900
 And then this leaf node is a leaf node.

2263
02:26:00,900 --> 02:26:03,500
 Then we are still looking at whether this new leaf node,

2264
02:26:03,500 --> 02:26:06,740
 this leaf node, and the apple should be combined.

2265
02:26:06,740 --> 02:26:10,980
 Because leaf node, the apple and this combined leaf node,

2266
02:26:10,980 --> 02:26:15,740
 come from the same node, size small.

2267
02:26:15,740 --> 02:26:19,580
 So we also need to look at others, right?

2268
02:26:19,580 --> 02:26:25,020
 But we combine the leaf node from the same parent node.

2269
02:26:25,020 --> 02:26:26,100
 Come back.

2270
02:26:26,100 --> 02:26:28,460
 Then what is the degradation of performance?

2271
02:26:28,460 --> 02:26:30,420
 If it's small, then we come back.

2272
02:26:30,420 --> 02:26:32,580
 Then this node become a leaf node, right?

2273
02:26:32,580 --> 02:26:34,539
 Then at another level, we also look

2274
02:26:34,539 --> 02:26:37,460
 at whether this leaf node, new leaf node,

2275
02:26:37,460 --> 02:26:40,220
 could be combined with other leaf nodes.

2276
02:26:40,220 --> 02:26:40,820
 OK.

2277
02:26:40,820 --> 02:26:44,220
 So we should continue until the degradation will

2278
02:26:44,220 --> 02:26:49,060
 be significant after the merging of these two leaf nodes.

2279
02:26:49,060 --> 02:26:51,779
 If there is significant degradation of the performance,

2280
02:26:51,779 --> 02:26:55,779
 that means we should not merge these leaf nodes.

2281
02:26:58,980 --> 02:26:59,779
 OK.

2282
02:26:59,779 --> 02:27:06,779
 So this is the pruning process.

2283
02:27:06,779 --> 02:27:10,580
 So we have the grueling to make the tree big.

2284
02:27:10,580 --> 02:27:13,580
 Then we cut some branches, right?

2285
02:27:13,580 --> 02:27:16,100
 We merge the leaf node to make this small.

2286
02:27:16,100 --> 02:27:17,340
 So this is called pruning.

2287
02:27:17,340 --> 02:27:20,420
 Grueling and pruning are the two processes,

2288
02:27:20,420 --> 02:27:24,820
 two phases of the construction of the class-fission tree.

2289
02:27:27,220 --> 02:27:33,060
 So this is the pruning, right?

2290
02:27:33,060 --> 02:27:36,460
 The assignment of the leaf node labels.

2291
02:27:36,460 --> 02:27:38,380
 I already talked about this point.

2292
02:27:38,380 --> 02:27:42,340
 I think for the leaf node, normally we

2293
02:27:42,340 --> 02:27:45,580
 don't expect 80% percent pu.

2294
02:27:45,580 --> 02:27:50,260
 And it contains samples from multiple classes.

2295
02:27:50,260 --> 02:27:54,780
 Then how we could determine the label of this leaf node?

2296
02:27:54,780 --> 02:27:57,660
 We look at the majority.

2297
02:27:57,660 --> 02:28:04,780
 We identify which class has the largest probability.

2298
02:28:04,780 --> 02:28:12,180
 Then we just assign that class label for that leaf node.

2299
02:28:12,180 --> 02:28:13,860
 We look at the majority, right?

2300
02:28:13,860 --> 02:28:15,980
 We look at the majority.

2301
02:28:15,980 --> 02:28:18,220
 So this is the assignment of the leaf node.

2302
02:28:18,220 --> 02:28:20,660
 We look at the majority.

2303
02:28:20,660 --> 02:28:22,820
 So each tree leaf node corresponds

2304
02:28:22,820 --> 02:28:25,340
 to the pattern in a single category.

2305
02:28:25,340 --> 02:28:28,140
 And all that has the largest number of samples.

2306
02:28:38,860 --> 02:28:41,580
 So each leaf node should be labeled

2307
02:28:41,580 --> 02:28:46,100
 by the category that has the most samples.

2308
02:28:50,700 --> 02:28:54,100
 Of course, actually, this will result in some misclassifications.

2309
02:28:54,100 --> 02:28:56,460
 We will result in some class-final areas.

2310
02:28:56,460 --> 02:29:00,060
 But as long as the area is in the tolerant range,

2311
02:29:00,060 --> 02:29:02,140
 then actually, this is OK.

2312
02:29:02,140 --> 02:29:05,539
 If not tolerant range, then you should continue to grow, right?

2313
02:29:05,539 --> 02:29:10,700
 You should divide the data into split data into,

2314
02:29:11,660 --> 02:29:14,020
 two nodes.

2315
02:29:14,020 --> 02:29:17,140
 And then the data will become pure.

2316
02:29:17,140 --> 02:29:25,420
 And then we can have less misclassifications.

2317
02:29:25,420 --> 02:29:29,300
 So this is the assignment of the leaf node.

2318
02:29:29,300 --> 02:29:33,620
 And that's actually we look at one example.

2319
02:29:33,620 --> 02:29:38,980
 So this example has been used before for linear class-file

2320
02:29:38,980 --> 02:29:41,220
 design in the linear sub-blog-1 machine

2321
02:29:41,220 --> 02:29:45,699
 or in the Fisher-Linux human analysis.

2322
02:29:45,699 --> 02:29:47,420
 So you can both.

2323
02:29:47,420 --> 02:29:51,300
 And then, actually, we have a design

2324
02:29:51,300 --> 02:29:55,420
 on the decision boundary linear line, right?

2325
02:29:55,420 --> 02:29:57,380
 To separate the sample in the two classes.

2326
02:29:57,380 --> 02:29:59,779
 So now, use the same data set.

2327
02:29:59,779 --> 02:30:01,539
 We design another class-file.

2328
02:30:01,539 --> 02:30:05,380
 That is the class-filling tree.

2329
02:30:05,380 --> 02:30:08,460
 And so here in the data set, we have two features.

2330
02:30:08,460 --> 02:30:12,660
 X1, X2, horizontal X1, vertical X2.

2331
02:30:12,660 --> 02:30:17,220
 So we need to determine which feature we should use

2332
02:30:17,220 --> 02:30:21,619
 in the root node, X1 or X2.

2333
02:30:21,619 --> 02:30:23,140
 When we determine, then, of course, we

2334
02:30:23,140 --> 02:30:28,060
 should evaluate actually if X1 is used as a root node,

2335
02:30:28,060 --> 02:30:31,660
 so what is actually the impurity measure

2336
02:30:31,660 --> 02:30:34,179
 after data splitting?

2337
02:30:34,179 --> 02:30:37,779
 If X1, 2 is used, so what is the impurity measure

2338
02:30:37,780 --> 02:30:39,820
 after data splitting?

2339
02:30:39,820 --> 02:30:43,860
 Then we compare to determine which one is the best one.

2340
02:30:43,860 --> 02:30:45,820
 But actually, for each of the features,

2341
02:30:45,820 --> 02:30:56,380
 when we evaluate the impurity measure after data splitting,

2342
02:30:56,380 --> 02:30:58,980
 we need to determine another factor, right?

2343
02:30:58,980 --> 02:30:59,540
 Another thing.

2344
02:30:59,540 --> 02:31:02,580
 That is the splitting point.

2345
02:31:02,580 --> 02:31:03,940
 Because actually, the two features

2346
02:31:03,940 --> 02:31:07,900
 are both continuous features.

2347
02:31:07,900 --> 02:31:11,020
 They have more than two values, much more than two values,

2348
02:31:11,020 --> 02:31:13,660
 much more values than two.

2349
02:31:13,660 --> 02:31:16,940
 So we need to design a suitable value of a splitting point.

2350
02:31:16,940 --> 02:31:20,300
 Just like H in the example, H is continuous.

2351
02:31:20,300 --> 02:31:22,780
 So we should sort in the data.

2352
02:31:22,780 --> 02:31:26,820
 We sort the data from a small to a large for X1,

2353
02:31:26,820 --> 02:31:31,900
 then, as a decision value, we determine a middle point.

2354
02:31:31,900 --> 02:31:33,220
 Then each of the middle point will

2355
02:31:33,220 --> 02:31:36,939
 be used as a candidate to divide the data into two parts.

2356
02:31:36,939 --> 02:31:39,820
 Then we evaluate the corresponding impurity measures.

2357
02:31:39,820 --> 02:31:44,619
 Then we identify the best splitting point for this feature,

2358
02:31:44,619 --> 02:31:47,380
 X1, and then similarly for X2.

2359
02:31:47,380 --> 02:31:50,179
 Then we compare another best splitting point.

2360
02:31:50,179 --> 02:31:57,900
 What is the best feature to be used for the root node?

2361
02:31:57,900 --> 02:32:02,179
 So actually, this is X1, the same process.

2362
02:32:02,180 --> 02:32:03,700
 This is for X1.

2363
02:32:03,700 --> 02:32:06,900
 Different nodes, actually, the splitting point

2364
02:32:06,900 --> 02:32:11,740
 have different impurity measures.

2365
02:32:11,740 --> 02:32:12,740
 So this is the one.

2366
02:32:12,740 --> 02:32:16,780
 Around 1.0 something, then the impurity measure is 0.14.

2367
02:32:16,780 --> 02:32:19,340
 This is the best one for X1.

2368
02:32:19,340 --> 02:32:27,540
 Similarly for X2, this is the best one, 0.08, very small.

2369
02:32:27,540 --> 02:32:30,740
 Even the best is 0.14 something.

2370
02:32:30,740 --> 02:32:34,660
 This one, the best is 0.02 for X2.

2371
02:32:34,660 --> 02:32:37,660
 Certainly, X2 is a better feature

2372
02:32:37,660 --> 02:32:42,500
 to be used as a root node.

2373
02:32:42,500 --> 02:32:46,860
 Then you can just actually, or X2 is the best splitting point

2374
02:32:46,860 --> 02:32:48,380
 divided data.

2375
02:32:48,380 --> 02:32:49,860
 Going to the lab, 100 data.

2376
02:32:49,860 --> 02:32:52,420
 Going to write 100 data.

2377
02:32:52,420 --> 02:32:55,180
 Then you can do, for the 100 data in the lab,

2378
02:32:55,180 --> 02:32:57,100
 you can look at X1.

2379
02:32:57,100 --> 02:32:58,860
 Because we can only use X1.

2380
02:32:58,860 --> 02:32:59,940
 You can also use X2.

2381
02:32:59,980 --> 02:33:02,260
 It's possible to use X2.

2382
02:33:02,260 --> 02:33:05,100
 And then you can divide the data.

2383
02:33:05,100 --> 02:33:09,140
 You value the best splitting point for X1.

2384
02:33:09,140 --> 02:33:11,020
 Then you can group.

2385
02:33:11,020 --> 02:33:13,380
 Similarly for this part, you can also

2386
02:33:13,380 --> 02:33:15,700
 determine the best splitting point.

2387
02:33:15,700 --> 02:33:18,180
 So you can group the tree.

2388
02:33:18,180 --> 02:33:21,900
 So actually, this is a procedure.

2389
02:33:21,900 --> 02:33:23,620
 You can group the tree.

2390
02:33:23,620 --> 02:33:26,700
 And actually, of course, in many tour bosses,

2391
02:33:26,740 --> 02:33:29,420
 like in my lab or others, actually,

2392
02:33:29,420 --> 02:33:33,500
 we can have some function to use for classification

2393
02:33:33,500 --> 02:33:34,780
 tree construction.

2394
02:33:34,780 --> 02:33:37,900
 In the my lab, for example, they have a fit classification

2395
02:33:37,900 --> 02:33:41,740
 tree, fit C tree, C mean classification.

2396
02:33:41,740 --> 02:33:47,260
 Then actually, if we use this, we can fit C tree.

2397
02:33:47,260 --> 02:33:48,180
 We can get these out.

2398
02:33:57,660 --> 02:33:58,660
 So this is the tree.

2399
02:33:58,660 --> 02:34:00,140
 And a very big tree, right?

2400
02:34:00,140 --> 02:34:01,020
 Very big trees.

2401
02:34:01,020 --> 02:34:02,540
 Although two features, the tree, we

2402
02:34:02,540 --> 02:34:04,380
 can see there are many splits, right?

2403
02:34:04,380 --> 02:34:07,380
 There are many nodes.

2404
02:34:07,380 --> 02:34:10,620
 And actually, this is a node.

2405
02:34:10,620 --> 02:34:13,300
 Actually, in my lab, a tree, they also output a root.

2406
02:34:13,300 --> 02:34:14,980
 Actually, you can see the tree, right?

2407
02:34:14,980 --> 02:34:19,820
 And each tree, for a tree, each node of a tree is a root.

2408
02:34:19,820 --> 02:34:22,700
 X1 greater than something, or X2 greater than something,

2409
02:34:22,700 --> 02:34:23,820
 or less than something, right?

2410
02:34:23,820 --> 02:34:24,780
 This is a root.

2411
02:34:24,780 --> 02:34:27,580
 And actually, these are the roots.

2412
02:34:27,580 --> 02:34:30,420
 So in the center tree, they also create these roots.

2413
02:34:30,420 --> 02:34:34,660
 So tree is just a graph representation of these roots.

2414
02:34:34,660 --> 02:34:38,700
 So tree is a root base classification, a root base.

2415
02:34:38,700 --> 02:34:41,380
 This root, of course, is the length from the data,

2416
02:34:41,380 --> 02:34:45,700
 rather than from the demysperia knowledge.

2417
02:34:45,700 --> 02:34:51,340
 And so this is the root base classification, right?

2418
02:34:52,300 --> 02:34:56,940
 So this is the descent boundary.

2419
02:34:56,940 --> 02:34:59,220
 And actually, we can see this descent boundary

2420
02:34:59,220 --> 02:35:01,340
 is no longer a straight line.

2421
02:35:01,340 --> 02:35:05,420
 So class of tree is no longer a linear classifier.

2422
02:35:05,420 --> 02:35:07,820
 This is a, the boundary is a nonlinear.

2423
02:35:07,820 --> 02:35:12,980
 Actually, this is a nonlinear, consists of three straight lines,

2424
02:35:12,980 --> 02:35:15,300
 linear boundary, right?

2425
02:35:15,300 --> 02:35:19,780
 Actually, we have a linear boundary here.

2426
02:35:19,860 --> 02:35:20,940
 We have a linear boundary here.

2427
02:35:20,940 --> 02:35:21,940
 We have a linear boundary.

2428
02:35:21,940 --> 02:35:23,700
 We have three lines.

2429
02:35:23,700 --> 02:35:30,740
 So this is a nonlinear boundary, consists of three lines.

2430
02:35:30,740 --> 02:35:34,860
 So this classifier is a nonlinear classifier.

2431
02:35:34,860 --> 02:35:36,460
 Classifier tree is a nonlinear.

2432
02:35:36,460 --> 02:35:40,740
 This boundary is not a hyperplane or a straight line.

2433
02:35:40,740 --> 02:35:44,340
 It is nonlinear.

2434
02:35:44,340 --> 02:35:47,340
 So this is the descent tree.

2435
02:35:47,340 --> 02:35:51,500
 And so from here, we can see six training samples.

2436
02:35:51,500 --> 02:35:54,540
 Actually, I'm misclassified.

2437
02:35:54,540 --> 02:35:58,180
 And so here, actually, if we set, so the tree,

2438
02:35:58,180 --> 02:36:00,660
 you can see the tree is very big, right?

2439
02:36:00,660 --> 02:36:01,820
 We just have two features.

2440
02:36:01,820 --> 02:36:04,500
 Then we have a few layers, right?

2441
02:36:04,500 --> 02:36:07,100
 We have so many nodes.

2442
02:36:07,100 --> 02:36:11,860
 And so if we set some threshold value

2443
02:36:11,860 --> 02:36:15,060
 for the number of samples in each node,

2444
02:36:15,100 --> 02:36:17,619
 we use these criteria to stop, criteria

2445
02:36:17,619 --> 02:36:20,019
 to stop the growing of the tree, right?

2446
02:36:20,019 --> 02:36:22,340
 OK, then this is the result.

2447
02:36:22,340 --> 02:36:29,380
 If we request at least 10 samples at leaf node,

2448
02:36:29,380 --> 02:36:30,539
 then we have this result.

2449
02:36:35,380 --> 02:36:39,060
 Just a tree, a small tree, a much smaller tree, right?

2450
02:36:39,060 --> 02:36:40,019
 Much smaller tree.

2451
02:36:40,020 --> 02:36:45,740
 If we set, actually, the threshold value

2452
02:36:45,740 --> 02:36:50,900
 for the number of samples in a node, then it says 10.

2453
02:36:50,900 --> 02:36:53,820
 But if you set to five, then this is not the tree

2454
02:36:53,820 --> 02:36:55,060
 will be bigger, right?

2455
02:36:55,060 --> 02:36:59,060
 So if you set to five, then these are not the tree.

2456
02:36:59,060 --> 02:37:02,020
 This tree certainly is a bigger tree, right?

2457
02:37:02,020 --> 02:37:02,500
 Bigger tree.

2458
02:37:03,500 --> 02:37:04,500
 OK.

2459
02:37:06,100 --> 02:37:13,660
 And so if you set the number of samples in a leaf node,

2460
02:37:13,660 --> 02:37:18,180
 a threshold value to 10, then we can have this tree,

2461
02:37:18,180 --> 02:37:21,780
 and then this is the decision boundary.

2462
02:37:21,780 --> 02:37:23,860
 This is the decision boundary.

2463
02:37:23,860 --> 02:37:27,580
 And this decision boundary, we have 10 training areas.

2464
02:37:27,580 --> 02:37:30,140
 Previously, we have six training areas.

2465
02:37:30,140 --> 02:37:32,300
 But now a tree is much smaller, right?

2466
02:37:32,300 --> 02:37:35,420
 Then it has 10 areas.

2467
02:37:35,420 --> 02:37:41,140
 From here, you can see the smaller the model,

2468
02:37:41,140 --> 02:37:43,580
 the more areas in the training data.

2469
02:37:43,580 --> 02:37:47,779
 The bigger the tree, the smaller the training areas.

2470
02:37:47,779 --> 02:37:51,060
 So we should have a trade-off.

2471
02:37:51,060 --> 02:37:55,699
 The model size, the area.

2472
02:37:55,699 --> 02:38:01,340
 So as long as the area is in the telearrange, it's acceptable.

2473
02:38:01,340 --> 02:38:05,860
 And then, for example, 10 training areas acceptable,

2474
02:38:05,860 --> 02:38:07,660
 we have 200 samples, right?

2475
02:38:07,660 --> 02:38:12,660
 10%, 5% area is acceptable.

2476
02:38:12,660 --> 02:38:17,220
 And then we can use just this tree.

2477
02:38:17,220 --> 02:38:18,660
 Tree is smaller, right?

2478
02:38:18,660 --> 02:38:22,820
 Then the reason for this is inference process is faster.

2479
02:38:22,820 --> 02:38:25,580
 Inference process, I mean, for testing data,

2480
02:38:25,580 --> 02:38:27,940
 the process will determine the class label.

2481
02:38:27,940 --> 02:38:30,380
 Actually, this process is faster.

2482
02:38:30,380 --> 02:38:32,019
 The area is acceptable.

2483
02:38:32,019 --> 02:38:33,460
 But what area is acceptable?

2484
02:38:33,460 --> 02:38:34,859
 It depends on application, right?

2485
02:38:34,859 --> 02:38:37,220
 For example, I think in classification,

2486
02:38:37,220 --> 02:38:43,500
 if you have some web page, you are looking for something.

2487
02:38:43,500 --> 02:38:47,500
 And then, the system will recommend some web page to you.

2488
02:38:47,500 --> 02:38:50,300
 Even if there is an area, they recommend you 10.

2489
02:38:50,300 --> 02:38:54,660
 Maybe only 9 is relevant, 10% is not relevant.

2490
02:38:54,660 --> 02:38:56,179
 And not a big issue, right?

2491
02:38:56,179 --> 02:38:57,380
 It's acceptable.

2492
02:38:57,380 --> 02:39:00,060
 But we require a fast result.

2493
02:39:00,060 --> 02:39:00,699
 We search.

2494
02:39:00,699 --> 02:39:06,180
 Immediately, we can get the list of the relevant website.

2495
02:39:06,180 --> 02:39:09,180
 We want this process to be fast.

2496
02:39:09,180 --> 02:39:10,580
 Even there is a 10% error.

2497
02:39:10,580 --> 02:39:12,580
 No problem.

2498
02:39:12,580 --> 02:39:15,420
 So we should use this kind of model.

2499
02:39:15,420 --> 02:39:18,140
 But in other scenarios, for example,

2500
02:39:18,140 --> 02:39:19,859
 if we use a tree, a decision tree

2501
02:39:19,859 --> 02:39:26,060
 to decide whether you can access your bank account, right?

2502
02:39:26,060 --> 02:39:28,500
 If the error rate is 10%, that means

2503
02:39:28,500 --> 02:39:30,180
 you have a high chance, right?

2504
02:39:30,180 --> 02:39:33,580
 Actually, the money will transfer to others, right?

2505
02:39:33,580 --> 02:39:35,700
 From your account.

2506
02:39:35,700 --> 02:39:38,260
 Because actually, that's an error rate of 10%,

2507
02:39:38,260 --> 02:39:40,220
 or 5%, 10%, right?

2508
02:39:40,220 --> 02:39:40,900
 That's a no rate.

2509
02:39:40,900 --> 02:39:42,180
 It's too high.

2510
02:39:42,180 --> 02:39:44,300
 Then you need to grow a very big tree

2511
02:39:44,300 --> 02:39:48,300
 to ensure the accuracy is as high as possible.

2512
02:39:48,300 --> 02:39:50,500
 So this all depends on application.

2513
02:39:50,500 --> 02:39:54,940
 You decide the size and also the acceptable error rate.

2514
02:39:55,820 --> 02:39:58,300
 OK.

2515
02:39:58,300 --> 02:40:00,500
 So by then, you treat all of them, right?

2516
02:40:00,500 --> 02:40:02,140
 You kind of see, I want to get a small model.

2517
02:40:02,140 --> 02:40:03,740
 Then the best performance is the best.

2518
02:40:03,740 --> 02:40:04,380
 No such thing.

2519
02:40:09,860 --> 02:40:11,220
 OK.

2520
02:40:11,220 --> 02:40:11,860
 So that's a tree.

2521
02:40:11,860 --> 02:40:15,060
 Maybe I show some typical cloud-future trees.

2522
02:40:15,060 --> 02:40:16,740
 And some like a cart.

2523
02:40:16,740 --> 02:40:20,020
 Cart is actually a cloud-future regression tree.

2524
02:40:20,020 --> 02:40:21,900
 And then we have AD3.

2525
02:40:21,900 --> 02:40:28,619
 This is for, actually, the cart is a binary tree.

2526
02:40:28,619 --> 02:40:32,220
 So here, it's used a tree, multiple split.

2527
02:40:32,220 --> 02:40:34,940
 A more famous one is the C4.5.

2528
02:40:34,940 --> 02:40:36,699
 So the name is a bit strange, right?

2529
02:40:36,699 --> 02:40:37,859
 C4.5.

2530
02:40:37,859 --> 02:40:40,580
 This is very famous, a cart-future tree.

2531
02:40:40,580 --> 02:40:42,699
 So here, they use a multiple split.

2532
02:40:42,699 --> 02:40:47,020
 The split is greater than 2, no longer binary.

2533
02:40:47,020 --> 02:40:49,140
 And then, actually, they use a gain ratio.

2534
02:40:49,140 --> 02:40:50,300
 What is gain ratio?

2535
02:40:50,300 --> 02:40:57,220
 The no-scale that tree, you remember the scale that

2536
02:40:57,220 --> 02:41:00,019
 tree, the impurity reduction, right?

2537
02:41:00,019 --> 02:41:03,980
 That is the gain ratio.

2538
02:41:03,980 --> 02:41:10,140
 It's used for the selection of the features

2539
02:41:10,140 --> 02:41:14,179
 and also the determination of the significant point.

2540
02:41:14,179 --> 02:41:16,340
 So these are the commonly used.

2541
02:41:16,340 --> 02:41:18,859
 And then another very famous is the random forest.

2542
02:41:18,900 --> 02:41:22,180
 So now, these random forests become

2543
02:41:22,180 --> 02:41:25,380
 a very popular decision tree.

2544
02:41:25,380 --> 02:41:27,380
 Based on decision tree, the forest, right?

2545
02:41:27,380 --> 02:41:30,020
 Forest, that means more than one tree.

2546
02:41:30,020 --> 02:41:31,820
 So it's a forest.

2547
02:41:31,820 --> 02:41:34,940
 And by random, while random, actually, later, we

2548
02:41:34,940 --> 02:41:39,340
 will see that random mechanism, actually,

2549
02:41:39,340 --> 02:41:41,780
 underlying two paths.

2550
02:41:41,780 --> 02:41:48,060
 And so here, this diagram shows, actually, random forest.

2551
02:41:48,060 --> 02:41:49,939
 So we have multiple trees.

2552
02:41:49,939 --> 02:41:51,859
 So these are called the forest.

2553
02:41:51,859 --> 02:41:53,460
 We have multiple trees.

2554
02:41:53,460 --> 02:41:57,420
 And each of the tree, actually, is based on random

2555
02:41:57,420 --> 02:42:01,699
 selected samples, training samples, a subset of the

2556
02:42:01,699 --> 02:42:03,500
 training samples, random selected.

2557
02:42:03,500 --> 02:42:07,060
 Also, based on random selected features, not all features

2558
02:42:07,060 --> 02:42:08,619
 are used in each of the trees.

2559
02:42:08,619 --> 02:42:10,699
 All the trees use different features.

2560
02:42:10,699 --> 02:42:14,619
 So random selected samples, random selected features, are

2561
02:42:14,660 --> 02:42:18,980
 used to create each of the trees.

2562
02:42:18,980 --> 02:42:22,180
 So these are called random, multiple trees.

2563
02:42:22,180 --> 02:42:24,940
 So it's a random forest.

2564
02:42:24,940 --> 02:42:28,860
 And then, for each of the sample, we can have one result

2565
02:42:28,860 --> 02:42:33,540
 from each of the trees in the forest.

2566
02:42:33,540 --> 02:42:37,020
 Then finally, we can combine the results into one single

2567
02:42:37,020 --> 02:42:38,380
 result.

2568
02:42:38,380 --> 02:42:40,900
 So how do we determine that?

2569
02:42:40,900 --> 02:42:44,140
 We take the majority.

2570
02:42:44,140 --> 02:42:49,740
 So majority voting is used to determine the final level of

2571
02:42:49,740 --> 02:42:52,660
 the training sample.

2572
02:42:52,660 --> 02:42:58,859
 So this is actually random forest.

2573
02:42:58,859 --> 02:43:02,460
 So that's the tree we have a look of this random forest.

2574
02:43:02,460 --> 02:43:09,619
 And actually, random forest here, and so it's a use

2575
02:43:09,619 --> 02:43:13,820
 multiple trees, or multiple classified.

2576
02:43:13,860 --> 02:43:16,260
 So this is also called a tree ensemble method.

2577
02:43:16,260 --> 02:43:16,980
 Ensemble.

2578
02:43:16,980 --> 02:43:18,860
 We use multiples.

2579
02:43:18,860 --> 02:43:26,860
 And in the construction of each of the trees, they use

2580
02:43:26,860 --> 02:43:28,580
 random selected samples.

2581
02:43:28,580 --> 02:43:33,260
 So here, they use the ensemble training backing.

2582
02:43:33,260 --> 02:43:35,100
 Begging is a combination of the two parts.

2583
02:43:35,100 --> 02:43:37,140
 One is called bootstrapping.

2584
02:43:37,140 --> 02:43:41,580
 Bootstrapping is a kind of sampling technique.

2585
02:43:41,580 --> 02:43:44,460
 And aggregating would mean combining the result from

2586
02:43:44,460 --> 02:43:45,900
 multiple trees.

2587
02:43:45,900 --> 02:43:48,900
 So it's an ensemble technique, or begging.

2588
02:43:48,900 --> 02:43:52,820
 So begging is a bootstrapping for the creation of the data

2589
02:43:52,820 --> 02:43:54,420
 set.

2590
02:43:54,420 --> 02:43:57,220
 Aggregating means the combination of the result of

2591
02:43:57,220 --> 02:43:59,660
 different trees.

2592
02:43:59,660 --> 02:44:06,180
 So bootstrapping is kind of a tree sampling technique.

2593
02:44:06,180 --> 02:44:08,660
 And this sampling technique is called a tree, often when

2594
02:44:08,660 --> 02:44:13,220
 it's not stated, we call sampling with replacement.

2595
02:44:13,220 --> 02:44:17,060
 For example, I want to select one student, 10 students.

2596
02:44:17,060 --> 02:44:21,300
 I select one student, and then put it back.

2597
02:44:21,300 --> 02:44:24,020
 So this is called a tree.

2598
02:44:24,020 --> 02:44:26,140
 This is a bootstraping.

2599
02:44:26,140 --> 02:44:27,820
 So this is actually an idea.

2600
02:44:27,820 --> 02:44:31,260
 Bootstraping is called random sampling with replacement.

2601
02:44:31,260 --> 02:44:33,100
 You sample the data.

2602
02:44:33,100 --> 02:44:36,220
 When you collect the second data, you still have the

2603
02:44:36,220 --> 02:44:38,640
 chance to get this data.

2604
02:44:38,640 --> 02:44:41,119
 So these are known as sampling with replacement.

2605
02:44:41,119 --> 02:44:46,680
 You have some repeated samples, sample number 10.

2606
02:44:46,680 --> 02:44:49,560
 So in another sample, you get repeated samples.

2607
02:44:49,560 --> 02:44:52,439
 Because after sampling, you put it back, then get another one.

2608
02:44:52,439 --> 02:44:53,560
 Then this is the same one.

2609
02:44:53,560 --> 02:44:55,840
 Also have the chance to be picked in the second time,

2610
02:44:55,840 --> 02:44:57,920
 the third time.

2611
02:44:57,920 --> 02:45:00,039
 So this is a bootstrapping.

2612
02:45:00,039 --> 02:45:03,560
 And then actually, we have the random sampling without

2613
02:45:03,560 --> 02:45:05,359
 replacement.

2614
02:45:05,359 --> 02:45:07,920
 After this sample is selected, the next time only other

2615
02:45:07,920 --> 02:45:09,840
 samples could be selected.

2616
02:45:09,840 --> 02:45:12,520
 So with all the replacements.

2617
02:45:12,520 --> 02:45:15,520
 So instead today, it's quite often we have this name,

2618
02:45:15,520 --> 02:45:17,000
 right, concept term.

2619
02:45:17,000 --> 02:45:21,520
 Sampling with replacement, sampling without replacement.

2620
02:45:21,520 --> 02:45:23,160
 Then you can get another set.

2621
02:45:23,160 --> 02:45:31,360
 So here, with the bootstraping, sampling with replacement.

2622
02:45:31,360 --> 02:45:34,880
 Sampling with replacement is a bootstraping.

2623
02:45:34,880 --> 02:45:42,160
 To create the data set, a useful data set will be used to

2624
02:45:42,160 --> 02:45:45,240
 create one tree.

2625
02:45:45,240 --> 02:45:48,800
 So this is the, and then finally the tree results are

2626
02:45:48,800 --> 02:45:51,240
 aggregated, combined.

2627
02:45:51,240 --> 02:45:53,920
 So this is called a sample attending or backing, right,

2628
02:45:53,920 --> 02:45:56,400
 a backing.

2629
02:45:56,400 --> 02:45:59,080
 Bootstraping for data generation.

2630
02:45:59,080 --> 02:46:02,960
 Aggregating for the results from multiple trees.

2631
02:46:04,880 --> 02:46:09,880
 OK, so some properties.

2632
02:46:09,880 --> 02:46:11,839
 And then these are the steps.

2633
02:46:11,839 --> 02:46:14,480
 I just show you, I think this step.

2634
02:46:14,480 --> 02:46:18,599
 So we create an end data set through bootstraping.

2635
02:46:18,599 --> 02:46:20,880
 You randomly select 100 samples.

2636
02:46:20,880 --> 02:46:23,000
 Randomly select replacement 100.

2637
02:46:23,000 --> 02:46:24,679
 This is got a one data set.

2638
02:46:24,679 --> 02:46:27,240
 Then you randomly select another 100 samples.

2639
02:46:27,240 --> 02:46:30,279
 Get another one, put back, get another one, put back.

2640
02:46:30,279 --> 02:46:31,240
 Repeat 100 times.

2641
02:46:31,240 --> 02:46:32,880
 You get 100 samples.

2642
02:46:32,880 --> 02:46:34,560
 So you repeat this many times.

2643
02:46:34,560 --> 02:46:37,519
 You create end data subset.

2644
02:46:37,519 --> 02:46:41,279
 End data subset are randomly selected using the so-called

2645
02:46:41,279 --> 02:46:45,039
 sampling with replacement technique.

2646
02:46:45,039 --> 02:46:50,519
 And then based on each of the data set, we randomly select

2647
02:46:50,519 --> 02:46:51,760
 end features.

2648
02:46:51,760 --> 02:46:54,680
 Not all features are used.

2649
02:46:54,680 --> 02:46:57,160
 So these are not the randomness, right?

2650
02:46:57,160 --> 02:47:00,519
 So random features, random data.

2651
02:47:00,520 --> 02:47:04,600
 So then after a random selection of the features,

2652
02:47:04,600 --> 02:47:08,560
 and also based on this random selection on the data set,

2653
02:47:08,560 --> 02:47:11,480
 the data, then we create a tree.

2654
02:47:11,480 --> 02:47:12,720
 How to create a tree?

2655
02:47:12,720 --> 02:47:16,800
 Just follow the method we introduced today.

2656
02:47:16,800 --> 02:47:18,280
 You can create binary trees.

2657
02:47:18,280 --> 02:47:23,240
 You can create the multiple split trees.

2658
02:47:23,240 --> 02:47:27,640
 So for each of the data, we create a tree.

2659
02:47:27,640 --> 02:47:30,720
 But each of the tree uses different data set,

2660
02:47:30,720 --> 02:47:32,080
 uses different features.

2661
02:47:32,080 --> 02:47:33,920
 Of course, some of the features could be different.

2662
02:47:33,920 --> 02:47:36,000
 Some of the features would be different.

2663
02:47:36,000 --> 02:47:39,000
 So they use different features subset.

2664
02:47:39,000 --> 02:47:42,640
 And then we repeat this process until all the data set I use,

2665
02:47:42,640 --> 02:47:45,840
 until all end trees are created.

2666
02:47:50,240 --> 02:47:52,960
 So this is just the illustration of this, right?

2667
02:47:52,960 --> 02:47:55,840
 We have to subset one, create one tree, subset two, create

2668
02:47:55,840 --> 02:47:56,880
 another tree, right?

2669
02:47:56,880 --> 02:47:58,800
 So we totally end subset.

2670
02:47:58,800 --> 02:48:01,240
 We create end trees.

2671
02:48:01,240 --> 02:48:05,039
 And during the classification of a testing sample,

2672
02:48:05,039 --> 02:48:09,519
 and normally we go through each of the trees.

2673
02:48:09,519 --> 02:48:12,599
 I get one result, calculate the voting.

2674
02:48:12,599 --> 02:48:17,080
 Then we get the voting, take the majority.

2675
02:48:17,080 --> 02:48:19,720
 Then we just say, this is called aggregating, right?

2676
02:48:19,720 --> 02:48:22,480
 Bootstrap aggregating the backing.

2677
02:48:22,480 --> 02:48:24,599
 Then we can look at the majority.

2678
02:48:24,600 --> 02:48:27,480
 This is a tree, the combination, the aggregating.

2679
02:48:27,480 --> 02:48:31,040
 Then we just output this majority, the voting result,

2680
02:48:31,040 --> 02:48:37,000
 as the final result of this testing sample.

2681
02:48:37,000 --> 02:48:41,720
 So here also list some of the features of the diversity

2682
02:48:41,720 --> 02:48:44,960
 and the crest of the minority.

2683
02:48:44,960 --> 02:48:48,600
 Because we just use a subset of each tree, right?

2684
02:48:48,600 --> 02:48:54,160
 And also parallelization and some of the things that's lit.

2685
02:48:54,200 --> 02:48:57,000
 So this diagram just shows the comparison, right?

2686
02:48:57,000 --> 02:49:00,960
 Cloud-fine tree and also the random forest, right?

2687
02:49:00,960 --> 02:49:04,080
 And the bottom, cloud-fine tree is a foundation, right?

2688
02:49:04,080 --> 02:49:08,000
 And then we use this foundation to build multiple trees.

2689
02:49:08,000 --> 02:49:10,240
 Based on different randomly selected data set,

2690
02:49:10,240 --> 02:49:13,800
 and different randomly selected features subset.

2691
02:49:13,800 --> 02:49:17,840
 Finally, we combine the result from different trees.

2692
02:49:17,840 --> 02:49:21,360
 So this shows the comparison, right?

2693
02:49:21,360 --> 02:49:25,440
 This is the comparison of the features of random forest

2694
02:49:25,440 --> 02:49:26,920
 and the cloud-fine trees.

2695
02:49:26,920 --> 02:49:29,400
 For big data set in particular, right?

2696
02:49:29,400 --> 02:49:30,720
 So now data is very big.

2697
02:49:30,720 --> 02:49:33,960
 For high-dominant data and the tree.

2698
02:49:33,960 --> 02:49:36,680
 So we normally use the random forest.

2699
02:49:36,680 --> 02:49:39,600
 So that is the reason why random forest is very popular

2700
02:49:39,600 --> 02:49:40,640
 nowadays.

2701
02:49:40,640 --> 02:49:43,040
 Because nowadays the data is big, right?

2702
02:49:43,040 --> 02:49:44,600
 Domino is high.

2703
02:49:44,600 --> 02:49:46,560
 The number of samples is high.

2704
02:49:46,560 --> 02:49:47,840
 But the big data.

2705
02:49:47,840 --> 02:49:52,680
 So we prefer to use the random forest and matter.

2706
02:49:52,680 --> 02:49:56,640
 Of course, the tree, in terms of the interpretability,

2707
02:49:56,640 --> 02:49:59,480
 I think the traditional cloud-fine tree

2708
02:49:59,480 --> 02:50:00,880
 is more interpretable.

2709
02:50:05,280 --> 02:50:10,080
 I think that's all for the cloud-fine tree.

2710
02:50:10,080 --> 02:50:11,520
 Thank you very much.

2711
02:50:11,520 --> 02:50:12,520
 Thank you.

2712
02:50:12,520 --> 02:50:14,200
 Thank you.

2713
02:50:42,520 --> 02:50:43,520
 Thank you.

2714
02:51:12,520 --> 02:51:13,520
 Thank you.

2715
02:51:42,520 --> 02:51:43,520
 Thank you.

2716
02:52:12,520 --> 02:52:13,520
 Thank you.

2717
02:52:42,520 --> 02:52:44,520
 Thank you.

2718
02:53:12,520 --> 02:53:14,520
 Thank you.

2719
02:53:42,520 --> 02:53:44,520
 Thank you.

2720
02:54:12,520 --> 02:54:14,520
 Thank you.

2721
02:54:42,520 --> 02:54:44,520
 Thank you.

2722
02:55:12,520 --> 02:55:14,520
 Thank you.

2723
02:55:42,520 --> 02:55:44,520
 Thank you.

2724
02:56:12,520 --> 02:56:14,520
 Thank you.

2725
02:56:42,520 --> 02:56:45,520
 Thank you.

2726
02:57:12,520 --> 02:57:14,520
 Thank you.

2727
02:57:42,520 --> 02:57:44,520
 Thank you.

2728
02:58:12,520 --> 02:58:14,520
 Thank you.

2729
02:58:42,520 --> 02:58:45,520
 Thank you.

2730
02:59:12,520 --> 02:59:14,520
 Thank you.

2731
02:59:42,520 --> 02:59:44,520
 Thank you.

