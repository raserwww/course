1
00:02:30,000 --> 00:02:32,000
 .

2
00:03:30,000 --> 00:03:40,000
 Okay.

3
00:03:40,000 --> 00:03:42,000
 Okay.

4
00:03:42,000 --> 00:03:46,000
 Good evening, everyone.

5
00:03:46,000 --> 00:03:50,000
 So in the previous week, actually, we have studied the supra-learning,

6
00:03:50,000 --> 00:03:56,000
 and actually we have designed different types of classifiers.

7
00:03:56,000 --> 00:04:04,000
 And actually all these classified designs are based on the label training data.

8
00:04:04,000 --> 00:04:06,000
 Okay.

9
00:04:06,000 --> 00:04:14,000
 So of course, in this framework, supra-learning, based on the label training data,

10
00:04:14,000 --> 00:04:18,000
 and we have studied two main problems.

11
00:04:18,000 --> 00:04:20,000
 One is the classification problem.

12
00:04:20,000 --> 00:04:22,000
 Okay.

13
00:04:22,000 --> 00:04:30,000
 Classification, that is to categorize the samples, to assign the sample to different classes.

14
00:04:30,000 --> 00:04:32,000
 Okay.

15
00:04:32,000 --> 00:04:38,000
 So the label, right, label training, the data label here is just the class label.

16
00:04:38,000 --> 00:04:40,000
 And another problem is the regression problem.

17
00:04:40,000 --> 00:04:42,000
 Okay.

18
00:04:42,000 --> 00:04:46,000
 And then for each sample, we have a target value.

19
00:04:46,000 --> 00:04:50,000
 Now in the classification problem, the target value is just the class label.

20
00:04:50,000 --> 00:04:52,000
 Okay.

21
00:04:52,000 --> 00:04:58,000
 But here, actually, in regression problem, actually, the target value is no longer a class label.

22
00:04:58,000 --> 00:05:02,000
 Instead, it is a real value or numerical value, actually, a target variable.

23
00:05:02,000 --> 00:05:04,000
 Okay.

24
00:05:04,000 --> 00:05:06,000
 Numerical value, target variable.

25
00:05:06,000 --> 00:05:08,000
 So this is the supra-learning.

26
00:05:08,000 --> 00:05:14,000
 And today we are going to study the unsupervised learning.

27
00:05:14,000 --> 00:05:18,000
 Supra-learning is based on the unlabeled data.

28
00:05:18,000 --> 00:05:23,000
 So in supra-learning, actually, the supra-varyon signal is just the class label, right?

29
00:05:23,000 --> 00:05:36,000
 So we learn and adjust the parameters so that actually the response of the model could be very close to the supra-varyon signal.

30
00:05:36,000 --> 00:05:42,000
 So that is the target variable, the class label, or the numerical value, not target variable.

31
00:05:42,000 --> 00:05:43,000
 Okay.

32
00:05:43,000 --> 00:05:44,000
 We have a supra-varyon signal.

33
00:05:44,000 --> 00:05:45,000
 So this is supra-learning.

34
00:05:45,000 --> 00:05:48,000
 The unsupervised learning is the data unlabeled.

35
00:05:48,000 --> 00:05:53,000
 So we no longer have such a supra-varyon signal.

36
00:05:53,000 --> 00:05:58,000
 So that is the reason why this kind of learning is called unsupervised learning, right?

37
00:05:58,000 --> 00:06:00,000
 Because we have no label.

38
00:06:00,000 --> 00:06:02,000
 We have no supra-varyon signal.

39
00:06:02,000 --> 00:06:05,000
 So this kind of learning is called unlabeled.

40
00:06:05,000 --> 00:06:07,000
 Unsupervised learning.

41
00:06:07,000 --> 00:06:08,000
 Okay.

42
00:06:08,000 --> 00:06:13,000
 So this is actually a category, unsupervised learning.

43
00:06:13,000 --> 00:06:15,000
 We have two main topics.

44
00:06:15,000 --> 00:06:17,000
 One is the clustering.

45
00:06:17,000 --> 00:06:20,000
 Clustering is just a group of the data.

46
00:06:20,000 --> 00:06:21,000
 Okay.

47
00:06:21,000 --> 00:06:24,000
 And then another problem is association analysis.

48
00:06:24,000 --> 00:06:25,000
 Association.

49
00:06:25,000 --> 00:06:26,000
 Okay.

50
00:06:26,000 --> 00:06:32,000
 So actually in this course, of course, actually we focus on the clustering analysis, clustering of the data.

51
00:06:32,000 --> 00:06:33,000
 Okay.

52
00:06:33,000 --> 00:06:35,000
 And I'm not trusting this course, actually.

53
00:06:35,000 --> 00:06:45,000
 In the landscape of the machine learning, in particular the unsupervised learning, the focus is actually just the clustering analysis.

54
00:06:45,000 --> 00:06:46,000
 Okay.

55
00:06:46,000 --> 00:06:50,000
 So association analysis is just attached a bit.

56
00:06:50,000 --> 00:06:51,000
 Okay.

57
00:06:51,000 --> 00:06:52,000
 And okay.

58
00:06:52,000 --> 00:06:57,000
 So of course in this course, actually we focus on the clustering analysis.

59
00:06:57,000 --> 00:06:58,000
 Okay.

60
00:06:58,000 --> 00:07:01,000
 And the clustering analysis also have many applications.

61
00:07:01,000 --> 00:07:02,000
 Okay.

62
00:07:02,000 --> 00:07:07,000
 And some example applications include actually the marketing segmentation.

63
00:07:07,000 --> 00:07:08,000
 Okay.

64
00:07:08,000 --> 00:07:09,000
 In marketing, right.

65
00:07:09,000 --> 00:07:13,000
 And we can actually group the customers, right.

66
00:07:13,000 --> 00:07:20,000
 And then actually, you know, we know, you know, for different groups of customers, you know, we can adopt different marketing strategies.

67
00:07:20,000 --> 00:07:21,000
 Okay.

68
00:07:21,000 --> 00:07:24,000
 So this is a marketing segmentation.

69
00:07:24,000 --> 00:07:26,000
 And then another such result group.

70
00:07:26,000 --> 00:07:27,000
 Okay.

71
00:07:27,000 --> 00:07:28,000
 You can search.

72
00:07:28,000 --> 00:07:29,000
 Okay.

73
00:07:29,000 --> 00:07:30,000
 You can get a lot of results.

74
00:07:30,000 --> 00:07:33,000
 And then you can group the result, right.

75
00:07:33,000 --> 00:07:38,000
 And in the super-added learning, we put the result normally into different categories.

76
00:07:38,000 --> 00:07:41,000
 But here we don't have the concept of category.

77
00:07:41,000 --> 00:07:45,000
 Instead, we have the concept of cluster or group.

78
00:07:45,000 --> 00:07:46,000
 Okay.

79
00:07:46,000 --> 00:07:52,000
 So we into actually group, actually, or cluster, actually the session result.

80
00:07:52,000 --> 00:07:53,000
 Okay.

81
00:07:53,000 --> 00:07:54,000
 So this unsupervised learning.

82
00:07:54,000 --> 00:08:03,000
 Actually, in many applications, actually, you know, to solve one of the problems, we need to use both super-added learning and unsupervised learning.

83
00:08:03,000 --> 00:08:04,000
 Okay.

84
00:08:04,000 --> 00:08:08,000
 And the progress, actually, I remember I talked about one example, right.

85
00:08:08,000 --> 00:08:20,000
 For example, actually, you know, one bank or insurance company launch a new program, insurance program for insurance company or bank insurance.

86
00:08:20,000 --> 00:08:25,000
 And maybe a launch a new fixed deposit, actually, you know, scheme.

87
00:08:25,000 --> 00:08:26,000
 Okay.

88
00:08:26,000 --> 00:08:39,000
 And of course, actually, you know, a very smart way for marketing is that, actually, we can collect, you know, we can select some of the customers, right, from the very big pool.

89
00:08:39,000 --> 00:08:44,000
 The pool could be, you know, easily over a few million, right, in Singapore or small country.

90
00:08:44,000 --> 00:08:53,000
 The population, the number of customers in a bank could be thousands of millions or billions, right, could be that, actually, scale.

91
00:08:53,000 --> 00:08:54,000
 Okay.

92
00:08:54,000 --> 00:09:08,000
 So we can randomly, of course, we can select, no, we can select a small group of customers and then we can counter them to get their response, to see whether they are interested in the program or in the scheme or they have no interest.

93
00:09:08,000 --> 00:09:09,000
 Okay.

94
00:09:09,000 --> 00:09:12,000
 So we can, these are interests or no interest, just a label, right.

95
00:09:12,000 --> 00:09:17,000
 So we can collect such label, then we can train a model, right.

96
00:09:17,000 --> 00:09:23,000
 So then we can use this model to predict the interest of other customers.

97
00:09:23,000 --> 00:09:24,000
 Okay.

98
00:09:24,000 --> 00:09:31,000
 So, so these are the three problems here until now, three is a super learning problem, right, because we collect the response of the customer.

99
00:09:31,000 --> 00:09:33,000
 So this is a label.

100
00:09:33,000 --> 00:09:34,000
 Okay.

101
00:09:34,000 --> 00:09:39,000
 Then, actually, we use the label to learn a model, right.

102
00:09:39,000 --> 00:09:42,000
 So other classification model or sometimes regression model.

103
00:09:42,000 --> 00:09:44,000
 And then we, we learn a model.

104
00:09:44,000 --> 00:09:45,000
 Okay.

105
00:09:45,000 --> 00:09:46,000
 So this is super learning.

106
00:09:46,000 --> 00:09:54,000
 But the problem here is that, actually, you have a very big pool of customers, millions, thousands of millions, billions of customers.

107
00:09:54,000 --> 00:10:03,000
 So how to select a few thousand, for example, two thousand customers, how to select a few thousand randomly.

108
00:10:03,000 --> 00:10:04,000
 Okay.

109
00:10:04,000 --> 00:10:12,000
 And of course you can do randomly, but randomly selection may not be a good, actually, selection scheme.

110
00:10:12,000 --> 00:10:19,000
 And actually we know, you know, the model performance, actually, largely depends on the data, right, the data quality.

111
00:10:19,000 --> 00:10:25,000
 So one of the aspects of the data quality is the representativeness of the data.

112
00:10:25,000 --> 00:10:30,000
 If the data tree, the training data, span the whole space.

113
00:10:30,000 --> 00:10:36,000
 The whole training data, the training data are representative of all the data.

114
00:10:36,000 --> 00:10:44,000
 And then the model, then, based on this training data, even a small number of training data, couldn't generate well.

115
00:10:44,000 --> 00:10:45,000
 Okay.

116
00:10:45,000 --> 00:10:49,000
 But we use randomly a small group of, you know, customers.

117
00:10:49,000 --> 00:10:57,000
 And maybe, you know, this customer cannot be a good representation or good representative of all the, you know, customers.

118
00:10:57,000 --> 00:11:02,000
 So in such a scenario, the model we built will not generate well.

119
00:11:02,000 --> 00:11:03,000
 Okay.

120
00:11:03,000 --> 00:11:10,000
 So here how to select these, you know, a few thousand customers and to, and to, and to label them, right?

121
00:11:10,000 --> 00:11:11,000
 This is a key issue.

122
00:11:11,000 --> 00:11:17,000
 Actually we can solve this problem using the so-called unsupervised landing.

123
00:11:17,000 --> 00:11:18,000
 Okay.

124
00:11:18,000 --> 00:11:20,000
 And then we can group.

125
00:11:20,000 --> 00:11:26,000
 We can cluster, actually, the millions, or down the millions, or billions of customers.

126
00:11:26,000 --> 00:11:27,000
 Okay.

127
00:11:27,000 --> 00:11:34,000
 And we can put them into, for example, two thousand groups through the so-called cluster analysis method.

128
00:11:34,000 --> 00:11:39,000
 And then, you know, from each group we select one as a representative.

129
00:11:39,000 --> 00:11:40,000
 Okay.

130
00:11:40,000 --> 00:11:47,000
 If we select, you know, customers and then label them in such a way, and then the training data will be representative.

131
00:11:47,000 --> 00:11:52,000
 And then the model built upon this representative data will generate well.

132
00:11:52,000 --> 00:11:53,000
 Okay.

133
00:11:53,000 --> 00:11:58,000
 So you can say, you know, in this, you know, problem, actually, we use the boot supervised

134
00:11:58,000 --> 00:12:00,000
 landing and unsupervised landing.

135
00:12:00,000 --> 00:12:01,000
 Okay.

136
00:12:01,000 --> 00:12:02,000
 Okay.

137
00:12:02,000 --> 00:12:07,000
 So this is the application of the unsupervised landing.

138
00:12:07,000 --> 00:12:08,000
 Okay.

139
00:12:08,000 --> 00:12:11,000
 So let's talk about some basics of a cluster analysis.

140
00:12:11,000 --> 00:12:18,000
 Unsupervised landing, normally when we see unsupervised landing, normally we refer to the cluster analysis.

141
00:12:18,000 --> 00:12:19,000
 Clustering.

142
00:12:19,000 --> 00:12:21,000
 A group of the data.

143
00:12:21,000 --> 00:12:22,000
 Okay.

144
00:12:22,000 --> 00:12:29,000
 And so roughly speaking, a classroom procedure yield data description in terms of cluster

145
00:12:29,000 --> 00:12:31,000
 or grouping of groups of the data.

146
00:12:31,000 --> 00:12:32,000
 Okay.

147
00:12:32,000 --> 00:12:33,000
 Description.

148
00:12:33,000 --> 00:12:34,000
 Okay.

149
00:12:34,000 --> 00:12:38,000
 So that's reason, unsupervised landing is quite often called actually description landing.

150
00:12:38,000 --> 00:12:39,000
 Right.

151
00:12:39,000 --> 00:12:46,000
 And actually, throughout the landing, you know, we have a description of the data, or the data has some groups, right.

152
00:12:46,000 --> 00:12:49,000
 And the group of the data, we have a description of this.

153
00:12:49,000 --> 00:12:53,000
 So unsupervised landing is also called actually, you know, descriptive landing.

154
00:12:53,000 --> 00:12:57,000
 And then unsupervised landing is also called actually predictive landing.

155
00:12:57,000 --> 00:12:58,000
 Right.

156
00:12:58,000 --> 00:12:59,000
 We predict the class label.

157
00:12:59,000 --> 00:13:04,000
 We predict, you know, the numerical value, you know, target variable.

158
00:13:04,000 --> 00:13:05,000
 Okay.

159
00:13:05,000 --> 00:13:09,000
 So this is the, you know, the cluster analysis.

160
00:13:09,000 --> 00:13:12,000
 It's about the grouping of the data.

161
00:13:12,000 --> 00:13:13,000
 Okay.

162
00:13:13,000 --> 00:13:19,000
 And of course, the grouping of the data actually is not arbitrary.

163
00:13:19,000 --> 00:13:25,000
 You cannot see all these, you know, samples should be put in one group.

164
00:13:25,000 --> 00:13:28,000
 This sample should be, you know, put in another group.

165
00:13:28,000 --> 00:13:32,000
 But we should be based on some criteria, right, when we group them.

166
00:13:32,000 --> 00:13:33,000
 Okay.

167
00:13:33,000 --> 00:13:40,000
 And the criteria here is that the samples within the same group should have strong internal

168
00:13:40,000 --> 00:13:41,000
 similarities.

169
00:13:42,000 --> 00:13:45,000
 Or within group similarities.

170
00:13:45,000 --> 00:13:46,000
 Okay.

171
00:13:46,000 --> 00:13:50,000
 And, you know, in panel classification, you know, quite often we see, we hope actually

172
00:13:50,000 --> 00:13:56,000
 the data could have a strong and within class similarity, right.

173
00:13:56,000 --> 00:14:00,000
 And we could have a strong, between class, this similarity.

174
00:14:00,000 --> 00:14:04,000
 In unsupervised landing, there is no concept of class.

175
00:14:04,000 --> 00:14:06,000
 There is a concept of group.

176
00:14:06,000 --> 00:14:07,000
 Okay.

177
00:14:07,000 --> 00:14:14,000
 So actually the criteria here is that actually the samples within the same group or cluster

178
00:14:14,000 --> 00:14:17,000
 should have strong internal similarities.

179
00:14:17,000 --> 00:14:18,000
 Okay.

180
00:14:18,000 --> 00:14:19,000
 Okay.

181
00:14:19,000 --> 00:14:23,000
 So here we talk about similarity, right.

182
00:14:23,000 --> 00:14:25,000
 So how do you measure similarity?

183
00:14:25,000 --> 00:14:26,000
 Similarity.

184
00:14:26,000 --> 00:14:27,000
 Okay.

185
00:14:27,000 --> 00:14:34,000
 And actually here, the grouping, group the data and then we should have some, show strong

186
00:14:34,000 --> 00:14:35,000
 similarity, right.

187
00:14:35,000 --> 00:14:40,000
 And the grouping of the data here we see, we emphasize the natural grouping.

188
00:14:40,000 --> 00:14:44,000
 You know, actually see, oh, those, those should be belong to one group.

189
00:14:44,000 --> 00:14:47,000
 Based on your, you know, arbitrary criteria.

190
00:14:47,000 --> 00:14:49,000
 This grouping should be natural.

191
00:14:49,000 --> 00:14:50,000
 Okay.

192
00:14:50,000 --> 00:14:53,000
 So what does this mean for natural grouping?

193
00:14:53,000 --> 00:14:55,000
 So this is one issue.

194
00:14:55,000 --> 00:14:59,000
 So what do we mean if we see natural grouping?

195
00:14:59,000 --> 00:15:00,000
 Okay.

196
00:15:00,000 --> 00:15:04,000
 And then another is how to measure the similarity.

197
00:15:04,000 --> 00:15:12,000
 In what sense the sample in one group shows strong and within class or within group similarities

198
00:15:12,000 --> 00:15:15,000
 or the strong internal similarities.

199
00:15:15,000 --> 00:15:16,000
 So in what sense?

200
00:15:16,000 --> 00:15:20,000
 In other words, how to measure the similarity?

201
00:15:20,000 --> 00:15:21,000
 Okay.

202
00:15:21,000 --> 00:15:26,000
 So these are the two issues actually we need to further explain.

203
00:15:26,000 --> 00:15:34,000
 So actually the similarity measure actually similarity can be measured using the distance.

204
00:15:34,000 --> 00:15:37,000
 The distance, right, the distance.

205
00:15:37,000 --> 00:15:44,000
 In the space, in the feature space actually every point, every sample is a point, right.

206
00:15:44,000 --> 00:15:51,000
 And of course in the feature space we can calculate the distance between any point.

207
00:15:51,000 --> 00:15:57,000
 So actually this distance actually is a measure of the similarity between the two samples.

208
00:15:57,000 --> 00:15:58,000
 Okay.

209
00:15:58,000 --> 00:16:06,000
 You can imagine if two samples are very similar, an extreme case is that the two samples are exactly the same.

210
00:16:06,000 --> 00:16:07,000
 Okay.

211
00:16:07,000 --> 00:16:10,000
 Then in the space they correspond to the same point, right.

212
00:16:10,000 --> 00:16:15,000
 The distance from the same point, the point to itself, right, is zero.

213
00:16:15,000 --> 00:16:16,000
 Okay.

214
00:16:16,000 --> 00:16:21,000
 In other words actually we know the similarity can be measured using the distance.

215
00:16:21,000 --> 00:16:26,000
 And the smaller the distance actually the greater the similarity.

216
00:16:26,000 --> 00:16:27,000
 Okay.

217
00:16:27,000 --> 00:16:30,000
 So this is the measure for similarity.

218
00:16:30,000 --> 00:16:33,000
 And we use the distance.

219
00:16:33,000 --> 00:16:34,000
 Distance, okay.

220
00:16:34,000 --> 00:16:39,000
 Of course actually this is not the only metric that we can use for similarity, right.

221
00:16:39,000 --> 00:16:45,000
 I think another that we commonly use actually, simulation measure is the, normally the correlation.

222
00:16:45,000 --> 00:16:46,000
 Okay.

223
00:16:46,000 --> 00:16:47,000
 And what is correlation?

224
00:16:47,000 --> 00:16:52,000
 Correlation actually basically something like an inner product of two vectors.

225
00:16:52,000 --> 00:16:54,000
 Each sample is a vector, right.

226
00:16:54,000 --> 00:16:58,000
 So there's a correlation between two vectors.

227
00:16:58,000 --> 00:17:01,000
 And, but we need to normalize this correlation.

228
00:17:01,000 --> 00:17:02,000
 Okay.

229
00:17:02,000 --> 00:17:04,000
 So we need to normalize this correlation.

230
00:17:04,000 --> 00:17:10,000
 And normally this should be the inner product of these two vectors divided by the norm of the two vectors.

231
00:17:10,000 --> 00:17:13,000
 And actually this is actually a correlation, a normalize correlation.

232
00:17:13,000 --> 00:17:16,000
 It's also called cosine similarity.

233
00:17:16,000 --> 00:17:19,000
 Cousine similarity between the two vectors.

234
00:17:19,000 --> 00:17:20,000
 Okay.

235
00:17:20,000 --> 00:17:27,000
 And the, gemetrically the cosine similarity is just a cosine of the angle between the two vectors.

236
00:17:27,000 --> 00:17:28,000
 We have two vectors, right.

237
00:17:28,000 --> 00:17:31,000
 Of course actually there is an angle between the two vectors.

238
00:17:31,000 --> 00:17:37,000
 So the cosine similarity actually is just the cosine of the angle between the two actually,

239
00:17:37,000 --> 00:17:38,000
 the two vectors.

240
00:17:38,000 --> 00:17:45,000
 And you can imagine, right, if the two angles, if the two samples, is that the same?

241
00:17:45,000 --> 00:17:48,000
 And then the two vectors, you know, overlap, right?

242
00:17:48,000 --> 00:17:50,000
 So the angle between them is zero.

243
00:17:50,000 --> 00:17:52,000
 Then cosine zero is one.

244
00:17:52,000 --> 00:17:53,000
 Okay.

245
00:17:53,000 --> 00:18:00,000
 So these actually, cosine similarity can be used as a, actually, metric for the evaluation

246
00:18:00,000 --> 00:18:04,000
 of the similarity between the samples.

247
00:18:04,000 --> 00:18:05,000
 Okay.

248
00:18:05,000 --> 00:18:10,000
 And so here actually, but I think frequently, I think in different applications, and for

249
00:18:10,000 --> 00:18:14,000
 example, in much, you know, left-hand angle processing, and quite often actually we use

250
00:18:14,000 --> 00:18:21,000
 the, like, cosine similarity between two, you know, representations of words or representation

251
00:18:21,000 --> 00:18:22,000
 or sentence.

252
00:18:22,000 --> 00:18:24,000
 We use cosine similarity.

253
00:18:24,000 --> 00:18:25,000
 Okay.

254
00:18:25,000 --> 00:18:30,000
 And by the most applications, we use the Euclidean distance.

255
00:18:30,000 --> 00:18:31,000
 Okay.

256
00:18:31,000 --> 00:18:34,000
 Later we will see, now that we're Euclidean distance, we have a few, we have a few, we

257
00:18:34,000 --> 00:18:36,000
 have a few types of distance.

258
00:18:36,000 --> 00:18:37,000
 Okay.

259
00:18:37,000 --> 00:18:40,000
 But Euclidean distance is just the most commonly used one.

260
00:18:40,000 --> 00:18:41,000
 Okay.

261
00:18:41,000 --> 00:18:43,000
 So this is the metric.

262
00:18:43,000 --> 00:18:46,000
 This is a similarity measure.

263
00:18:46,000 --> 00:18:47,000
 Okay.

264
00:18:47,000 --> 00:18:50,000
 And then that's actually, we have another issue, right?

265
00:18:50,000 --> 00:18:51,000
 So natural grouping.

266
00:18:51,000 --> 00:18:53,000
 What does this mean, natural grouping?

267
00:18:53,000 --> 00:18:56,000
 Actually, when you put them into one group, right?

268
00:18:56,000 --> 00:19:01,000
 Actually, this natural grouping means that we, within the group, and the samples should

269
00:19:01,000 --> 00:19:06,000
 show strong similarities, or within group similarities.

270
00:19:06,000 --> 00:19:12,000
 And between groups, the sample should show strong, between group, actually distance,

271
00:19:12,000 --> 00:19:14,000
 or dissimilarities.

272
00:19:14,000 --> 00:19:17,000
 So this is called natural grouping.

273
00:19:17,000 --> 00:19:18,000
 Okay.

274
00:19:18,000 --> 00:19:21,000
 So natural grouping here, I have a tool, an S-pad, right?

275
00:19:21,000 --> 00:19:22,000
 Why is it taught about that?

276
00:19:22,000 --> 00:19:28,000
 Within group, within group, within cluster similarity.

277
00:19:28,000 --> 00:19:29,000
 Okay.

278
00:19:29,000 --> 00:19:32,000
 So this diagram just shows this.

279
00:19:32,000 --> 00:19:34,000
 All the red samples belong to one group.

280
00:19:34,000 --> 00:19:35,000
 Okay.

281
00:19:35,000 --> 00:19:40,000
 We can see the distance between the samples, actually, they're relative small.

282
00:19:40,000 --> 00:19:42,000
 And, okay.

283
00:19:42,000 --> 00:19:46,000
 And so here, the blue samples also belong to one group.

284
00:19:46,000 --> 00:19:49,000
 So the distance between the samples is relatively small.

285
00:19:49,000 --> 00:19:54,000
 But the distance between the groups is relatively big.

286
00:19:54,000 --> 00:19:55,000
 Okay.

287
00:19:55,000 --> 00:19:57,000
 So this is called natural grouping.

288
00:19:57,000 --> 00:20:01,000
 We cannot see, oh, I put this sample, and this sample into one group.

289
00:20:01,000 --> 00:20:05,000
 You define that group, but that group is not the natural grouping of the data.

290
00:20:05,000 --> 00:20:06,000
 Okay.

291
00:20:06,000 --> 00:20:09,000
 Collagen analysis tries to find the natural grouping of the data.

292
00:20:09,000 --> 00:20:10,000
 Okay.

293
00:20:10,000 --> 00:20:15,000
 After grouping of other classrooms, within cluster, the sample should have strong similarities

294
00:20:15,000 --> 00:20:17,000
 or small distance.

295
00:20:17,000 --> 00:20:18,000
 Okay.

296
00:20:18,000 --> 00:20:35,000
 So these are the two cons aspects of the natural grouping and also for the evaluation of the

297
00:20:35,000 --> 00:20:37,000
 similarities.

298
00:20:37,000 --> 00:20:38,000
 Okay.

299
00:20:38,000 --> 00:20:43,000
 So these actually just now talk about the distance, right?

300
00:20:43,000 --> 00:20:47,000
 So here, we have a very general form for the distance.

301
00:20:47,000 --> 00:20:51,000
 Here, we have two vectors.

302
00:20:51,000 --> 00:20:53,000
 One is x, another is x prime.

303
00:20:53,000 --> 00:20:54,000
 Right?

304
00:20:54,000 --> 00:21:01,000
 So how to find the distance between actually the two vectors, and we have this very general

305
00:21:01,000 --> 00:21:02,000
 definition.

306
00:21:02,000 --> 00:21:03,000
 Okay.

307
00:21:03,000 --> 00:21:05,000
 So the care, right?

308
00:21:05,000 --> 00:21:08,000
 Because the vector has many elements.

309
00:21:08,000 --> 00:21:09,000
 Okay.

310
00:21:09,000 --> 00:21:16,000
 So the care element, the care element in the two vector, the difference, and then power

311
00:21:16,000 --> 00:21:22,000
 q summation, and then power 1 q over q.

312
00:21:22,000 --> 00:21:25,000
 D here is the denominator of the vector, right?

313
00:21:25,000 --> 00:21:27,000
 How many elements in one vector?

314
00:21:27,000 --> 00:21:29,000
 So this is called the D. Okay.

315
00:21:29,000 --> 00:21:32,000
 And this is very general, right?

316
00:21:32,000 --> 00:21:33,000
 Minkowski distance.

317
00:21:33,000 --> 00:21:34,000
 Okay.

318
00:21:34,000 --> 00:21:38,000
 But actually, we have normally, we use a special scenario, right?

319
00:21:38,000 --> 00:21:43,000
 When q equals to 2, so this is just the commonly used actually, including distance.

320
00:21:43,000 --> 00:21:49,000
 If you didn't write it, we have the elements, corresponding element, different square,

321
00:21:49,000 --> 00:21:51,000
 that's summation, square roots.

322
00:21:51,000 --> 00:21:54,000
 So that is just an including distance.

323
00:21:54,000 --> 00:21:55,000
 Okay.

324
00:21:55,000 --> 00:22:00,000
 And so this is the case when q equals 1 or 2.

325
00:22:00,000 --> 00:22:05,000
 When q equals to 1, this distance is called actually Manhattan distance or city block

326
00:22:05,000 --> 00:22:06,000
 distance.

327
00:22:06,000 --> 00:22:07,000
 Okay.

328
00:22:07,000 --> 00:22:12,000
 And for example, actually, no, for example, the Euclidean distance, for example, from

329
00:22:12,000 --> 00:22:13,000
 me to this student.

330
00:22:13,000 --> 00:22:15,000
 So this is just an equal distance.

331
00:22:15,000 --> 00:22:16,000
 Okay.

332
00:22:16,000 --> 00:22:17,000
 This is the distance.

333
00:22:17,000 --> 00:22:18,000
 Okay.

334
00:22:18,000 --> 00:22:21,000
 And actually, the city block, that means actually we have a lot of blocks, there are buildings,

335
00:22:21,000 --> 00:22:23,000
 and we cannot go anywhere, right?

336
00:22:23,000 --> 00:22:29,000
 And so we need to go this way first, horizontal axis from here, and then go vertical axis.

337
00:22:29,000 --> 00:22:32,000
 So this is called x, actually, the city block distance.

338
00:22:32,000 --> 00:22:39,000
 So distance, actually, along the y-axis, then plus the distance along another axis.

339
00:22:39,000 --> 00:22:41,000
 So this is our city block.

340
00:22:41,000 --> 00:22:42,000
 Okay.

341
00:22:42,000 --> 00:22:47,000
 Because actually, we cannot go, I cannot pass through the table and go to the student, right?

342
00:22:47,000 --> 00:22:48,000
 This is the distance.

343
00:22:48,000 --> 00:22:53,000
 I need to go this way first, and then horizontally and then vertically.

344
00:22:53,000 --> 00:22:54,000
 Okay.

345
00:22:54,000 --> 00:22:59,000
 So this is called, actually, the city block distance.

346
00:22:59,000 --> 00:23:00,000
 Okay.

347
00:23:00,000 --> 00:23:09,000
 And remember, actually, in the feature subset selection part, and we use, actually, a distance

348
00:23:09,000 --> 00:23:14,000
 to evaluate the goodies of the feature subset, right?

349
00:23:14,000 --> 00:23:17,000
 And actually, we try to eliminate the redundancies.

350
00:23:17,000 --> 00:23:18,000
 Okay.

351
00:23:18,000 --> 00:23:21,000
 So that is the Mahanobid distance.

352
00:23:21,000 --> 00:23:23,000
 So progress, it will be used Mahanobid distance.

353
00:23:23,000 --> 00:23:24,000
 Okay.

354
00:23:24,000 --> 00:23:33,000
 Actually, you can also use the Mahanobid distance, actually, to know for, as a metric, and for

355
00:23:33,000 --> 00:23:36,000
 the similarity between the samples.

356
00:23:36,000 --> 00:23:37,000
 Okay.

357
00:23:37,000 --> 00:23:40,000
 And another is the Kusang similarity, right?

358
00:23:40,000 --> 00:23:43,000
 Actually, already explained is Kusang similarity.

359
00:23:43,000 --> 00:23:49,000
 So these are the, you know, so city block distance, you clean the distance, and the Mahanobid

360
00:23:49,000 --> 00:23:52,000
 distance, and the city block distance, and so forth.

361
00:23:52,000 --> 00:24:00,000
 Kusang similarity can all be used, actually, as a metric for the evaluation of the similarity

362
00:24:00,000 --> 00:24:01,000
 between the samples.

363
00:24:01,000 --> 00:24:02,000
 Okay.

364
00:24:02,000 --> 00:24:07,000
 Samples within one cluster, the, or samples between clusters.

365
00:24:07,000 --> 00:24:08,000
 Okay.

366
00:24:08,000 --> 00:24:17,000
 So these are the dysnometric, which can be used as a mirror for the similarity.

367
00:24:18,000 --> 00:24:19,000
 Okay.

368
00:24:19,000 --> 00:24:23,000
 And then we talk about the types of a collaterial algorithm.

369
00:24:23,000 --> 00:24:24,000
 Okay.

370
00:24:24,000 --> 00:24:27,000
 So there are four types of a collaterial algorithm.

371
00:24:27,000 --> 00:24:34,000
 The first part is called central-based collaterial algorithm, central-based collaterial method,

372
00:24:34,000 --> 00:24:35,000
 central, okay.

373
00:24:35,000 --> 00:24:37,000
 Central is just a center.

374
00:24:37,000 --> 00:24:39,000
 We assume there is a center.

375
00:24:39,000 --> 00:24:40,000
 Okay.

376
00:24:40,000 --> 00:24:43,000
 And this method is also called a partitioning method.

377
00:24:43,000 --> 00:24:44,000
 Okay.

378
00:24:44,000 --> 00:24:48,000
 And we partition the whole space into many parts, and each part has a center.

379
00:24:48,000 --> 00:24:53,000
 This center is a representative of that, you know, of that group, of that cluster.

380
00:24:53,000 --> 00:24:55,000
 This center is also called a non-class center.

381
00:24:55,000 --> 00:24:56,000
 Okay.

382
00:24:56,000 --> 00:24:58,000
 It's a central-based method.

383
00:24:58,000 --> 00:24:59,000
 Okay.

384
00:24:59,000 --> 00:25:03,000
 And then another method is called hierarchical collaterial.

385
00:25:03,000 --> 00:25:04,000
 Hararchical.

386
00:25:04,000 --> 00:25:11,000
 And then that type is called a G, a G non-disruption-based collaterial.

387
00:25:11,000 --> 00:25:15,000
 The final one is called density-based collaterial.

388
00:25:15,000 --> 00:25:16,000
 Okay.

389
00:25:16,000 --> 00:25:21,000
 So that's our first briefly talk about each of, you know, the basic ideas of this, you

390
00:25:21,000 --> 00:25:24,000
 know, four types of collaterial method.

391
00:25:24,000 --> 00:25:31,000
 And then after that, from each of these four types, we select one algorithm, and then to

392
00:25:31,000 --> 00:25:34,000
 study the details of that algorithm.

393
00:25:34,000 --> 00:25:35,000
 Okay.

394
00:25:35,000 --> 00:25:40,000
 So we first look at the central-based method, central-based partitioning method, right?

395
00:25:40,000 --> 00:25:44,000
 And then we look at the central-based partitioning method, actually, through the whole space.

396
00:25:44,000 --> 00:25:48,000
 Is the partition divided into many parts?

397
00:25:48,000 --> 00:25:51,000
 And each part actually have one center.

398
00:25:51,000 --> 00:25:56,000
 So this is a central-based method, right?

399
00:25:56,000 --> 00:26:02,000
 You can divide the whole pool of the customers into many groups, right?

400
00:26:02,000 --> 00:26:06,000
 And each group, you can select one representative.

401
00:26:06,000 --> 00:26:07,000
 Okay.

402
00:26:07,000 --> 00:26:12,000
 Actually, I think this representative, actually, the class center is a good representative,

403
00:26:12,000 --> 00:26:13,000
 right?

404
00:26:13,000 --> 00:26:14,000
 Yeah.

405
00:26:14,000 --> 00:26:15,000
 Okay.

406
00:26:15,000 --> 00:26:18,000
 So this is the partitioning method.

407
00:26:18,000 --> 00:26:25,000
 And actually, the very famous K-means classroom algorithm actually belongs to this type,

408
00:26:25,000 --> 00:26:30,000
 central-based method, okay, or partitioning method.

409
00:26:30,000 --> 00:26:32,000
 We assume there is a center.

410
00:26:32,000 --> 00:26:34,000
 There is a center.

411
00:26:34,000 --> 00:26:37,000
 So this center is also a centroid.

412
00:26:37,000 --> 00:26:44,000
 This center is also a representative of all the samples in that group.

413
00:26:44,000 --> 00:26:51,000
 Okay.

414
00:26:51,000 --> 00:26:56,000
 And then the second type is hierarchical classroom, actually, hierarchical.

415
00:26:56,000 --> 00:27:03,000
 And actually, hierarchical classroom were produced actually, a dendrogram, which is a tree-like

416
00:27:03,000 --> 00:27:06,000
 structure, this, like this, okay.

417
00:27:06,000 --> 00:27:12,000
 And actually, previously, we studied the classification tree, right?

418
00:27:12,000 --> 00:27:18,000
 From the rule node, and we asked one question using a tree based on one specific feature,

419
00:27:18,000 --> 00:27:19,000
 right, or attribute.

420
00:27:19,000 --> 00:27:23,000
 And then the data is divided into two parts, okay.

421
00:27:23,000 --> 00:27:29,000
 And then from the left branch or right branch, actually, we can ask another question using

422
00:27:29,000 --> 00:27:31,000
 maybe another feature.

423
00:27:31,000 --> 00:27:34,000
 Another divided is a further divided into two parts, right?

424
00:27:34,000 --> 00:27:35,000
 And then this is a tree.

425
00:27:35,000 --> 00:27:36,000
 We could group the tree.

426
00:27:36,000 --> 00:27:38,000
 So this is a tree, okay.

427
00:27:38,000 --> 00:27:40,000
 So here, similarly, right?

428
00:27:40,000 --> 00:27:41,000
 Okay.

429
00:27:41,000 --> 00:27:46,000
 And the data here, you know, can be considered, we have a whole data, and the data divided

430
00:27:46,000 --> 00:27:51,000
 into two parts, okay, into two parts.

431
00:27:51,000 --> 00:27:52,000
 Okay.

432
00:27:52,000 --> 00:27:54,000
 And the two big groups.

433
00:27:54,000 --> 00:27:58,000
 And then for each of the two groups, we can further divide them into two groups.

434
00:27:58,000 --> 00:28:01,000
 And then for them, and then we further divide them into two groups.

435
00:28:01,000 --> 00:28:02,000
 Okay.

436
00:28:02,000 --> 00:28:06,000
 So this is, like, I know, just very similar to the classification tree, okay.

437
00:28:06,000 --> 00:28:14,000
 So this is the hierarchical classification, and this is actually a diagram, and it is

438
00:28:14,000 --> 00:28:15,000
 called a dendrogram.

439
00:28:15,000 --> 00:28:16,000
 Dendrogram, okay.

440
00:28:16,000 --> 00:28:24,000
 And this dendrogram, actually, shows the similarity of the samples.

441
00:28:24,000 --> 00:28:29,000
 And the samples could be hard to know, right?

442
00:28:29,000 --> 00:28:32,000
 And normally, we cannot visualize the data directly in the space.

443
00:28:32,000 --> 00:28:36,000
 You cannot visualize, then it is hard to visualize the distance between them, right?

444
00:28:36,000 --> 00:28:42,000
 But actually, this is a hierarchical classification tree, the dendrogram, actually, gives us a

445
00:28:42,000 --> 00:28:46,000
 visualization of the distance, the similarity between the samples.

446
00:28:46,000 --> 00:28:47,000
 Okay.

447
00:28:47,000 --> 00:28:51,000
 So actually, now you can see the dendrogram, you draw a line, right?

448
00:28:51,000 --> 00:28:55,000
 Actually, the height of the line, actually, the proportional, actually, the similarity

449
00:28:55,000 --> 00:29:01,000
 or the distance, actually, between, actually, the samples.

450
00:29:01,000 --> 00:29:05,000
 Okay.

451
00:29:05,000 --> 00:29:12,000
 You can see, like, okay, so these are two samples combined, right, into one cluster.

452
00:29:12,000 --> 00:29:16,000
 And this distance, this height here, right, is small.

453
00:29:16,000 --> 00:29:20,000
 This means that these samples, two samples, actually, are very similar.

454
00:29:20,000 --> 00:29:25,000
 And in this diagram here, we see these two clusters are combined.

455
00:29:25,000 --> 00:29:28,000
 But actually, this distance is big.

456
00:29:28,000 --> 00:29:31,000
 So this means these two samples, actually, are very different.

457
00:29:31,000 --> 00:29:33,000
 They're very different.

458
00:29:33,000 --> 00:29:34,000
 Okay.

459
00:29:34,000 --> 00:29:39,000
 So this is the dendrogram, the height, actually, the proportional to the distance between,

460
00:29:39,000 --> 00:29:43,000
 actually, the clusters.

461
00:29:43,000 --> 00:29:44,000
 Okay.

462
00:29:44,000 --> 00:29:50,000
 So this is the hierarchical clustering.

463
00:29:50,000 --> 00:29:54,000
 Then we have a distribution-based clustering, distribution-based.

464
00:29:54,000 --> 00:30:02,000
 In the distribution-based method, actually, we assume the data actually comes from certain distributions.

465
00:30:02,000 --> 00:30:06,000
 Of course, actually, the most commonly used distribution is Gaussian distribution, right?

466
00:30:06,000 --> 00:30:10,000
 It's a multivariate normal distribution or Gaussian distribution.

467
00:30:10,000 --> 00:30:11,000
 Okay.

468
00:30:11,000 --> 00:30:19,000
 So actually, we can assume this, and then we can actually try to model the distribution of the data,

469
00:30:19,000 --> 00:30:21,000
 try to model, right?

470
00:30:21,000 --> 00:30:27,000
 And actually, I think, if we assume the data for a normal distribution,

471
00:30:27,000 --> 00:30:33,000
 and then, actually, this is a method, just Gaussian-Mittscher model, right?

472
00:30:33,000 --> 00:30:38,000
 Gaussian-Mittscher model, actually, remember, is used in the first part, right?

473
00:30:38,000 --> 00:30:45,000
 When we started the business decision rule, we need to build the class conditional probability-dense functions.

474
00:30:45,000 --> 00:30:46,000
 Okay.

475
00:30:46,000 --> 00:30:54,000
 In some scenarios, the samples in the same class are from different distributions, different Gaussian functions.

476
00:30:54,000 --> 00:30:55,000
 Okay.

477
00:30:55,000 --> 00:30:57,000
 So we use the Gaussian-Mittscher model, right?

478
00:30:57,000 --> 00:31:01,000
 And to model the class conditional probability-dense function.

479
00:31:01,000 --> 00:31:02,000
 Okay.

480
00:31:02,000 --> 00:31:04,000
 But actually, that's for superheated learning, right?

481
00:31:04,000 --> 00:31:08,000
 We know that sample belongs to the same group, a same class, right?

482
00:31:08,000 --> 00:31:14,000
 So for class one, class two, then we use the Gaussian-Mittscher model.

483
00:31:14,000 --> 00:31:18,000
 But in the superheated learning, we don't have the concept of class, right?

484
00:31:18,000 --> 00:31:22,000
 But actually, we can just put all the data into one group.

485
00:31:22,000 --> 00:31:23,000
 Okay.

486
00:31:23,000 --> 00:31:29,000
 Then we try to build, actually, the data, actually, using the Gaussian-Mittscher model.

487
00:31:29,000 --> 00:31:30,000
 Okay.

488
00:31:30,000 --> 00:31:35,000
 We can build the distribution, or model distribution of the data using the Gaussian-Mittscher model.

489
00:31:35,000 --> 00:31:36,000
 Okay.

490
00:31:36,000 --> 00:31:44,000
 And then after we've, you know, actually, you know, after we have got this model, and then

491
00:31:44,000 --> 00:31:49,000
 actually each Gaussian function correspond to one cluster.

492
00:31:49,000 --> 00:31:50,000
 Okay.

493
00:31:50,000 --> 00:31:57,000
 So in this example here, we have three Gaussian functions, and then we have three clusters.

494
00:31:57,000 --> 00:31:58,000
 Okay.

495
00:31:58,000 --> 00:32:01,000
 And then for Gaussian function, we have the mean vector, right?

496
00:32:01,000 --> 00:32:04,000
 Actually, the mean vector is just a class center.

497
00:32:04,000 --> 00:32:08,000
 So later, we will look at this again.

498
00:32:08,000 --> 00:32:09,000
 Okay.

499
00:32:09,000 --> 00:32:16,000
 So this is the distribution-based cluster.

500
00:32:16,000 --> 00:32:19,000
 Another is density-based cluster.

501
00:32:19,000 --> 00:32:20,000
 Okay.

502
00:32:20,000 --> 00:32:26,000
 And actually, in the central-based cluster, if we normally assume the distribution of

503
00:32:26,000 --> 00:32:32,000
 the data is like a sphere, like a ball.

504
00:32:32,000 --> 00:32:33,000
 Okay.

505
00:32:33,000 --> 00:32:38,000
 And by some scenarios, actually, this may not be true.

506
00:32:38,000 --> 00:32:39,000
 Okay.

507
00:32:39,000 --> 00:32:47,000
 And so actually, to address, actually, arbitrary shape, actually, a distribution, and then

508
00:32:47,000 --> 00:32:59,000
 we can use density-based cluster.

509
00:32:59,000 --> 00:33:00,000
 For example, right?

510
00:33:00,000 --> 00:33:04,000
 The data, the arbitrary shape, not like a sphere, right?

511
00:33:04,000 --> 00:33:06,000
 It's an arbitrary shape.

512
00:33:06,000 --> 00:33:07,000
 Okay.

513
00:33:07,000 --> 00:33:16,000
 And so for such a kind of data, and the, the, the division method, you know, the, you know,

514
00:33:16,000 --> 00:33:19,000
 the, the partition method may not work well.

515
00:33:19,000 --> 00:33:20,000
 Okay.

516
00:33:20,000 --> 00:33:24,000
 But the, actually, the density-based method actually could work well.

517
00:33:24,000 --> 00:33:25,000
 Okay.

518
00:33:25,000 --> 00:33:30,000
 For arbitrary shape, actually, in the distribution, we can use a density-based cluster.

519
00:33:30,000 --> 00:33:31,000
 Okay.

520
00:33:31,000 --> 00:33:36,000
 So later, we will also, you know, select one density-based method and then start the detail

521
00:33:36,000 --> 00:33:39,000
 of that method.

522
00:33:39,000 --> 00:33:40,000
 Okay.

523
00:33:41,000 --> 00:33:48,000
 So this is the, okay.

524
00:33:48,000 --> 00:33:54,000
 Then a brief introduction of the, the, the four types of cluster method, right?

525
00:33:54,000 --> 00:34:03,000
 So, that's actually, you know, we will actually study it through one example, you know, algorithm

526
00:34:03,000 --> 00:34:06,000
 from each of the four methods.

527
00:34:06,000 --> 00:34:07,000
 Okay.

528
00:34:07,000 --> 00:34:11,000
 So we first look at the central-based cluster, central-based.

529
00:34:11,000 --> 00:34:17,000
 That means that we assume that the data is a centroid, a center.

530
00:34:17,000 --> 00:34:18,000
 Okay.

531
00:34:18,000 --> 00:34:26,000
 And so not also we assume that the distribution of the data has a shape of a sphere, a sphere

532
00:34:26,000 --> 00:34:29,000
 shape, a distribution, like a ball.

533
00:34:29,000 --> 00:34:30,000
 Okay.

534
00:34:30,000 --> 00:34:32,000
 A ball, of course, that is the center.

535
00:34:32,000 --> 00:34:33,000
 Okay.

536
00:34:33,000 --> 00:34:35,000
 So this is the basic assumption.

537
00:34:35,000 --> 00:34:36,000
 Okay.

538
00:34:36,000 --> 00:34:42,000
 And of course, if this is the assumption, it's not actually, it's not met, right?

539
00:34:42,000 --> 00:34:44,000
 It's not satisfied.

540
00:34:44,000 --> 00:34:48,000
 And then this is the central-based method may not produce the best result.

541
00:34:48,000 --> 00:34:50,000
 And then we need to think about some other methods.

542
00:34:50,000 --> 00:34:51,000
 Okay.

543
00:34:51,000 --> 00:34:56,000
 So that's the reason, you know, for any classification problem, we need to study different methods,

544
00:34:56,000 --> 00:34:57,000
 right?

545
00:34:57,000 --> 00:35:00,000
 Because there will be no one single algorithm that performs the best in all applications.

546
00:35:00,000 --> 00:35:05,000
 So every time, actually, we need to try all the algorithms, then we find the best performing

547
00:35:05,000 --> 00:35:06,000
 one.

548
00:35:06,000 --> 00:35:07,000
 For class, we're the same.

549
00:35:07,000 --> 00:35:11,000
 So that's the reason why we study different class, you know, method.

550
00:35:11,000 --> 00:35:16,000
 If the data follow, actually, this kind of a distribution, right?

551
00:35:16,000 --> 00:35:17,000
 A sphere-shaped distribution.

552
00:35:17,000 --> 00:35:22,000
 And then the central-based method may produce the best performance.

553
00:35:22,000 --> 00:35:27,000
 But if this condition is not satisfied, then this is actually a method.

554
00:35:27,000 --> 00:35:29,000
 We are not producing the best performance.

555
00:35:29,000 --> 00:35:32,000
 Then we should try other methods.

556
00:35:32,000 --> 00:35:33,000
 Okay.

557
00:35:33,000 --> 00:35:34,000
 So, okay.

558
00:35:34,000 --> 00:35:38,000
 So you can see, actually, here, all the data belong to one group.

559
00:35:38,000 --> 00:35:39,000
 You want to another group.

560
00:35:39,000 --> 00:35:42,000
 You want to each group have a center.

561
00:35:42,000 --> 00:35:43,000
 Okay.

562
00:35:43,000 --> 00:35:51,000
 And actually, the most commonly used, actually, you know, central-based method or partition

563
00:35:51,000 --> 00:35:53,000
 method is a K-means classroom.

564
00:35:53,000 --> 00:35:54,000
 Okay.

565
00:35:54,000 --> 00:36:01,000
 Actually, you know, this is the most frequently used method, classroom method, K-means classroom.

566
00:36:01,000 --> 00:36:02,000
 Okay.

567
00:36:02,000 --> 00:36:06,000
 And this K, okay, is a hyperparameter.

568
00:36:06,000 --> 00:36:09,000
 So, later, we are, you spend this further.

569
00:36:09,000 --> 00:36:10,000
 Okay.

570
00:36:10,000 --> 00:36:18,000
 So, we are geometry, you know, we have a data set D. And the data set consists of X1, X2,

571
00:36:18,000 --> 00:36:19,000
 and Xn.

572
00:36:19,000 --> 00:36:20,000
 Okay.

573
00:36:20,000 --> 00:36:23,000
 In Schubert's learning, we have the training data.

574
00:36:23,000 --> 00:36:25,000
 We are testing data, right?

575
00:36:25,000 --> 00:36:29,000
 In the, in the, in the, in the Schubert's learning, we don't have this concept.

576
00:36:29,000 --> 00:36:30,000
 We don't have training.

577
00:36:30,000 --> 00:36:31,000
 We don't have testing.

578
00:36:31,000 --> 00:36:34,000
 Because testing, even for testing, we don't have a label, right?

579
00:36:34,000 --> 00:36:35,000
 We don't have a label.

580
00:36:35,000 --> 00:36:36,000
 You don't, you don't know.

581
00:36:36,000 --> 00:36:38,000
 We also don't have the validation data.

582
00:36:38,000 --> 00:36:40,000
 We just have some data.

583
00:36:40,000 --> 00:36:42,000
 Then you just put them into different groups.

584
00:36:42,000 --> 00:36:43,000
 Okay.

585
00:36:43,000 --> 00:36:45,000
 You provide the distribution of other groups.

586
00:36:45,000 --> 00:36:49,000
 So, that is just the, you know, classroom analysis.

587
00:36:49,000 --> 00:36:50,000
 Okay.

588
00:36:50,000 --> 00:36:52,000
 We don't have the concept of training or testing.

589
00:36:52,000 --> 00:36:53,000
 Okay.

590
00:36:53,000 --> 00:36:55,000
 So, here, we just have such a data set.

591
00:36:55,000 --> 00:36:59,000
 D is a continuing example, X1, X2.

592
00:36:59,000 --> 00:37:00,000
 Okay.

593
00:37:00,000 --> 00:37:08,000
 And then our objective is to divide them or partition them into actually case subset.

594
00:37:08,000 --> 00:37:11,000
 And this case subset should be disjoint.

595
00:37:11,000 --> 00:37:13,000
 So, be disjoint.

596
00:37:13,000 --> 00:37:14,000
 Right?

597
00:37:14,000 --> 00:37:16,000
 That means there are no overlap.

598
00:37:16,000 --> 00:37:22,000
 In other words, no sample beyond two or more actual groups.

599
00:37:22,000 --> 00:37:23,000
 Okay.

600
00:37:23,000 --> 00:37:25,000
 That means no, no overlapping, right?

601
00:37:25,000 --> 00:37:26,000
 So, disjoint.

602
00:37:26,000 --> 00:37:27,000
 Okay.

603
00:37:27,000 --> 00:37:37,000
 So, our objective of classroom is to divide the data into case disjoint subset.

604
00:37:37,000 --> 00:37:40,000
 D1, D2, and then DK.

605
00:37:40,000 --> 00:37:41,000
 DK.

606
00:37:41,000 --> 00:37:42,000
 Okay.

607
00:37:42,000 --> 00:37:49,000
 And then, of course, this division or partition of the data are not actual, right?

608
00:37:49,000 --> 00:37:53,000
 And actually, we should follow the so-called natural grouping principle.

609
00:37:53,000 --> 00:37:54,000
 Natural grouping.

610
00:37:54,000 --> 00:37:55,000
 Okay.

611
00:37:55,000 --> 00:38:01,000
 So, after you divide the data into case subset, then within each subset, the data should have

612
00:38:01,000 --> 00:38:03,000
 a strong internal similarity.

613
00:38:03,000 --> 00:38:04,000
 Okay.

614
00:38:04,000 --> 00:38:10,000
 And between the subset, the sample should estimate a strong dissimilarity.

615
00:38:10,000 --> 00:38:14,000
 So, this is the between group dissimilarity.

616
00:38:14,000 --> 00:38:21,000
 So, within group similarity, strong internal, within group similarity, or within class similarity,

617
00:38:21,000 --> 00:38:25,000
 or between a strong, between class disimilarities.

618
00:38:25,000 --> 00:38:29,000
 This is the principle of natural grouping, right?

619
00:38:29,000 --> 00:38:31,000
 Natural grouping.

620
00:38:31,000 --> 00:38:32,000
 Okay.

621
00:38:32,000 --> 00:38:39,000
 And actually, you know, here we also need to define actually a function, a optimization

622
00:38:39,000 --> 00:38:40,000
 function.

623
00:38:40,000 --> 00:38:43,000
 Actually, I repeat this a few times, right?

624
00:38:43,000 --> 00:38:46,000
 Actually, machine learning problem, like supervised learning, right?

625
00:38:46,000 --> 00:38:52,000
 So, we start from some, you know, understanding or some ideas about pattern classification.

626
00:38:52,000 --> 00:38:58,000
 And then, you know, we convert this into, or we formulate this into an optimization problem.

627
00:38:58,000 --> 00:39:00,000
 You remember, right?

628
00:39:00,000 --> 00:39:06,000
 In the suborbital machine, you know, we formulate this as a constant optimization problem.

629
00:39:06,000 --> 00:39:07,000
 Okay.

630
00:39:07,000 --> 00:39:15,000
 And in the future linear demagneticism, and we also formulate this as a maximization

631
00:39:15,000 --> 00:39:18,000
 problem, maximizing the ratio, right?

632
00:39:18,000 --> 00:39:26,000
 Fissure ratio, the ratio between, actually, the scatter, the ratio of the, you know, between

633
00:39:26,000 --> 00:39:31,000
 class scatter, you know, over the, you know, within class scatter.

634
00:39:31,000 --> 00:39:34,000
 So, it's an optimization problem.

635
00:39:34,000 --> 00:39:39,000
 And actually, the classification problem can also be formulated, actually, as an optimization

636
00:39:39,000 --> 00:39:40,000
 problem.

637
00:39:40,000 --> 00:39:41,000
 Okay.

638
00:39:41,000 --> 00:39:46,000
 So, here, and we want to derive this, actually, in our criterion, the loss function.

639
00:39:46,000 --> 00:39:49,000
 So, that minimizes the loss function.

640
00:39:49,000 --> 00:39:50,000
 Okay.

641
00:39:50,000 --> 00:39:56,000
 And so, here, actually, we define this, you know, which we call, this is actually a criterion,

642
00:39:56,000 --> 00:40:02,000
 or loss function, is called, within class, sum of squares, sum of squares.

643
00:40:02,000 --> 00:40:07,000
 This square, I mean, the square distance, square distance, or distance squares.

644
00:40:07,000 --> 00:40:08,000
 Okay.

645
00:40:08,000 --> 00:40:13,000
 Sum of, you know, within class, sum of square distance.

646
00:40:13,000 --> 00:40:14,000
 Okay.

647
00:40:14,000 --> 00:40:22,000
 So, here, actually, we assume, you know, for each subset, and we have a mean vector, right?

648
00:40:22,000 --> 00:40:23,000
 We have a mean vector.

649
00:40:23,000 --> 00:40:24,000
 Okay.

650
00:40:24,000 --> 00:40:25,000
 So, this is just the mean vector.

651
00:40:25,000 --> 00:40:26,000
 Okay.

652
00:40:26,000 --> 00:40:32,000
 And then, of course, within the same class, each of the samples are distant, and two of

653
00:40:32,000 --> 00:40:34,000
 these are no center vector, right?

654
00:40:34,000 --> 00:40:35,000
 Center vector.

655
00:40:35,000 --> 00:40:36,000
 Okay.

656
00:40:36,000 --> 00:40:42,000
 And therefore, each of the class, and we can calculate within class, get, within class,

657
00:40:42,000 --> 00:40:46,000
 within class, actually, sum of square distance, within class, okay?

658
00:40:46,000 --> 00:40:52,000
 So, for all the sample in the class, the DI, actually, we can calculate the distance to

659
00:40:52,000 --> 00:40:56,000
 the center vector, MI.

660
00:40:56,000 --> 00:40:59,000
 For all samples, within class, I, okay?

661
00:40:59,000 --> 00:41:05,000
 We can calculate the distance between each of the samples in this cluster to the center

662
00:41:05,000 --> 00:41:06,000
 of this cluster.

663
00:41:06,000 --> 00:41:11,000
 And the distance, the square distance, we get a summation.

664
00:41:11,000 --> 00:41:12,000
 Okay.

665
00:41:12,000 --> 00:41:18,000
 So, this is just for, you know, for class I, actually, we have more than one cluster,

666
00:41:18,000 --> 00:41:19,000
 right?

667
00:41:19,000 --> 00:41:22,000
 We have a class I, class I, and your class K. Okay.

668
00:41:22,000 --> 00:41:29,000
 So, then, we get a summation of the within cluster, sum of squares for all the K clusters.

669
00:41:29,000 --> 00:41:34,000
 So, from one, see, from one to K. So, here, yeah.

670
00:41:34,000 --> 00:41:40,000
 So, this is WCSS, within cluster, sum of squares.

671
00:41:40,000 --> 00:41:45,000
 And these are actually something similar to the, you know, within cluster, within cluster,

672
00:41:45,000 --> 00:41:46,000
 right?

673
00:41:46,000 --> 00:41:48,000
 You know, within cluster scatter.

674
00:41:48,000 --> 00:41:51,000
 But here, we don't have cluster.

675
00:41:51,000 --> 00:41:53,000
 We call it within cluster.

676
00:41:53,000 --> 00:41:54,000
 Okay.

677
00:41:54,000 --> 00:42:01,000
 So, this is WCSS, within cluster, sum of square distance.

678
00:42:01,000 --> 00:42:02,000
 Okay.

679
00:42:02,000 --> 00:42:10,000
 And, of course, actually, you know, when you actually, you know, the data divide differently,

680
00:42:10,000 --> 00:42:11,000
 right?

681
00:42:11,000 --> 00:42:14,000
 And here, we have some of the D1, D2, DK, right?

682
00:42:14,000 --> 00:42:17,000
 If you D1, DK, you are rendering the data in different groups, right?

683
00:42:17,000 --> 00:42:19,000
 You have a different division of the data.

684
00:42:19,000 --> 00:42:23,000
 And also, for each of the data, you have different, actually, MI, right?

685
00:42:23,000 --> 00:42:26,000
 So, then, the WCSS would be different.

686
00:42:26,000 --> 00:42:27,000
 Would be different.

687
00:42:27,000 --> 00:42:28,000
 Okay.

688
00:42:28,000 --> 00:42:32,000
 And, actually, we want to find, actually, the division of the, or partition of the data.

689
00:42:32,000 --> 00:42:38,000
 And also, actually, find this MI so that, actually, the distance WCSS could be minimized.

690
00:42:38,000 --> 00:42:39,000
 Minimized.

691
00:42:39,000 --> 00:42:40,000
 Okay.

692
00:42:40,000 --> 00:42:45,000
 So, the central base cluster here, you know, through some analysis.

693
00:42:45,000 --> 00:42:49,000
 Again, this is summarized, actually, as a minimization problem.

694
00:42:49,000 --> 00:42:51,000
 We want to partition the data.

695
00:42:51,000 --> 00:42:52,000
 Okay.

696
00:42:52,000 --> 00:43:01,000
 And so, that, actually, the, within class, the sum of squares, this actually functions

697
00:43:01,000 --> 00:43:02,000
 minimized.

698
00:43:02,000 --> 00:43:03,000
 Right?

699
00:43:03,000 --> 00:43:08,000
 So, again, the machining problem is not coming into a minimization problem.

700
00:43:08,000 --> 00:43:13,000
 Of course, actually, this is different from, you know, maybe you just have knowledge of

701
00:43:13,000 --> 00:43:14,000
 mathematics, right?

702
00:43:14,000 --> 00:43:15,000
 Like, or machination.

703
00:43:15,000 --> 00:43:17,000
 But you have no idea about classroom.

704
00:43:17,000 --> 00:43:20,000
 We must have some idea of classroom first.

705
00:43:20,000 --> 00:43:27,000
 And then we can summarize this into, formulate this into an organization problem.

706
00:43:27,000 --> 00:43:28,000
 Okay.

707
00:43:28,000 --> 00:43:29,000
 Okay.

708
00:43:29,000 --> 00:43:40,000
 And the criterion function has a simple interpretation.

709
00:43:40,000 --> 00:43:45,760
 For every given cluster, the mean vector MI is the best representation or representative

710
00:43:45,760 --> 00:43:49,000
 of the sample in this center.

711
00:43:49,000 --> 00:43:53,000
 It's the mean class, the mean, the mean, right?

712
00:43:53,000 --> 00:43:54,000
 And you have groups.

713
00:43:54,000 --> 00:43:55,000
 Okay.

714
00:43:55,000 --> 00:43:59,000
 So, what value should be the best representative of that group of data?

715
00:43:59,000 --> 00:44:03,000
 Read the mean, the mean vector.

716
00:44:03,000 --> 00:44:04,000
 Okay.

717
00:44:04,000 --> 00:44:09,360
 And if you think about that problem again, right?

718
00:44:09,360 --> 00:44:14,280
 You want to select, you know, like, two, a thousand customers from the pool of millions

719
00:44:14,280 --> 00:44:16,560
 or billions of customers, right?

720
00:44:16,560 --> 00:44:17,560
 And how to do that?

721
00:44:17,560 --> 00:44:21,560
 Actually, what we need to do actually is to select representative customers, right?

722
00:44:21,560 --> 00:44:27,320
 Actually, here, we can also, like, divide the whole, you know, pool of the customers

723
00:44:27,320 --> 00:44:29,560
 into D1, D2, and DK.

724
00:44:29,560 --> 00:44:33,040
 DK here could be 2,000, right?

725
00:44:33,040 --> 00:44:42,240
 And then from each of the, you know, the subset, or from each of the groups or cluster, and

726
00:44:42,240 --> 00:44:47,520
 we can have select actually the representative customer, right?

727
00:44:47,520 --> 00:44:57,479
 Finally, we can select actually 2,000 representative customers through this cluster, the classroom

728
00:44:57,479 --> 00:44:58,479
 analysis.

729
00:44:58,479 --> 00:44:59,479
 Okay.

730
00:44:59,479 --> 00:45:05,359
 So, the, actually, the mean vector is the best representation or representative of this,

731
00:45:05,359 --> 00:45:07,359
 you know, sample in this cluster.

732
00:45:07,359 --> 00:45:08,359
 Okay.

733
00:45:08,360 --> 00:45:13,000
 So, after the call, eventually, we have this concept called class center.

734
00:45:13,000 --> 00:45:17,640
 Class center represents the sample in that class.

735
00:45:17,640 --> 00:45:18,640
 Okay.

736
00:45:18,640 --> 00:45:20,280
 So, this is a center vector.

737
00:45:20,280 --> 00:45:22,280
 It's also called a prototype.

738
00:45:22,280 --> 00:45:23,280
 Prototype.

739
00:45:23,280 --> 00:45:24,280
 Okay.

740
00:45:24,280 --> 00:45:29,280
 So, it's a representative.

741
00:45:29,280 --> 00:45:31,280
 Okay.

742
00:45:31,280 --> 00:45:41,280
 So, the optimal partitioning is the one that minimizes WCSS.

743
00:45:41,280 --> 00:45:49,800
 So, as I mentioned, actually, our objective here is to minimize WCSS.

744
00:45:49,800 --> 00:45:52,920
 And in the K-means classroom, actually, we have this K, right?

745
00:45:52,920 --> 00:45:57,560
 And actually, this K should be preset.

746
00:45:57,560 --> 00:46:02,080
 And actually, so, this actually K is a hyper parameter.

747
00:46:02,080 --> 00:46:07,080
 Actually, in many machine learning, you know, all of them, we have a hyper, the so-called

748
00:46:07,080 --> 00:46:09,080
 hyper parameter, right?

749
00:46:09,080 --> 00:46:13,640
 Previously, when we started the support by number machine, right, you remember, actually,

750
00:46:13,640 --> 00:46:17,680
 for inseparable samples, right?

751
00:46:17,680 --> 00:46:21,400
 Now, we, the loss function contains two parts.

752
00:46:21,400 --> 00:46:27,560
 One is above the norm of the square norm of the W, right?

753
00:46:27,560 --> 00:46:31,760
 And then another part, actually, is the summation of the select variables.

754
00:46:31,760 --> 00:46:33,920
 Then we have a C, the C.

755
00:46:33,920 --> 00:46:35,760
 So, this is a hyper parameter.

756
00:46:35,760 --> 00:46:36,760
 Okay.

757
00:46:36,760 --> 00:46:39,680
 We need to actually select this hyper parameter, right?

758
00:46:39,680 --> 00:46:44,760
 So, this hyper parameter normally is not actually selected directly, you know, from the, not

759
00:46:44,760 --> 00:46:47,160
 directly submitted from the data.

760
00:46:47,160 --> 00:46:51,680
 Instead, actually, we need to determine the value based on the data, but not directly

761
00:46:51,680 --> 00:46:53,680
 estimated from the data.

762
00:46:53,680 --> 00:46:54,680
 Okay.

763
00:46:54,680 --> 00:46:57,160
 So, this kind of parameter called hyper parameter.

764
00:46:57,160 --> 00:46:59,160
 Many models have this parameter, right?

765
00:46:59,160 --> 00:47:00,160
 Hyper parameter.

766
00:47:00,160 --> 00:47:01,160
 Okay.

767
00:47:01,160 --> 00:47:03,560
 And in the regression, we have the lambda.

768
00:47:03,560 --> 00:47:04,560
 Okay.

769
00:47:04,560 --> 00:47:08,240
 So, we also have, you know, this lambda is also a hyper parameter.

770
00:47:08,240 --> 00:47:15,600
 When we build a classification tree, okay, and we also need to balance the accuracy on

771
00:47:15,600 --> 00:47:18,319
 the training data and the model size.

772
00:47:18,319 --> 00:47:19,319
 Okay.

773
00:47:19,319 --> 00:47:22,880
 So, model size, maybe, you need to have a weightage, right?

774
00:47:22,880 --> 00:47:26,920
 When you combine with the prediction area from the classification tree.

775
00:47:26,920 --> 00:47:29,920
 So, this also, again, is a hyper parameter.

776
00:47:29,920 --> 00:47:36,560
 We need to determine this hyper parameter based on the performance, based on the performance

777
00:47:36,560 --> 00:47:39,480
 on the validation data.

778
00:47:39,480 --> 00:47:42,400
 But here, we don't have the concept of validation data, right?

779
00:47:42,400 --> 00:47:44,720
 But we still can determine this K value.

780
00:47:44,839 --> 00:47:45,839
 Okay.

781
00:47:45,839 --> 00:47:50,439
 But when we talk about K means classification, so normally this K, when you execute this

782
00:47:50,439 --> 00:47:54,439
 K means classification algorithm, this K need to be preset.

783
00:47:54,439 --> 00:47:55,439
 Preset.

784
00:47:55,439 --> 00:47:56,439
 Okay.

785
00:47:56,439 --> 00:48:01,839
 And, but actually, we have measured, so determine this value of K. So later, we will talk about

786
00:48:01,839 --> 00:48:02,839
 this.

787
00:48:02,839 --> 00:48:03,839
 Okay.

788
00:48:03,839 --> 00:48:09,839
 So, this is the K means classification.

789
00:48:09,839 --> 00:48:11,839
 Okay.

790
00:48:12,840 --> 00:48:18,840
 So, what kind of a class, you know, problem, we are suited to WCSS criteria.

791
00:48:18,840 --> 00:48:22,840
 WCSS criteria, here, we assume there is a center vector.

792
00:48:22,840 --> 00:48:23,840
 There is a center vector.

793
00:48:23,840 --> 00:48:28,600
 This center vector is a good representative of all the samples within the cluster.

794
00:48:28,600 --> 00:48:30,840
 So, we have this concept, right?

795
00:48:30,840 --> 00:48:36,640
 So, if the data follows this kind of, or satisfies this kind of assumption, then actually, you

796
00:48:36,640 --> 00:48:39,280
 know, this matter will perform very well.

797
00:48:39,280 --> 00:48:40,280
 Okay.

798
00:48:40,280 --> 00:48:42,560
 Otherwise, actually, we are not performing very well.

799
00:48:42,560 --> 00:48:43,560
 Okay.

800
00:48:43,560 --> 00:48:45,080
 So, here, now give some example.

801
00:48:45,080 --> 00:48:46,080
 Okay.

802
00:48:46,080 --> 00:48:50,680
 And this is known, we see that data distribution is like a sphere shape, right?

803
00:48:50,680 --> 00:48:51,680
 Like a ball, right?

804
00:48:51,680 --> 00:48:52,680
 This is data from here.

805
00:48:52,680 --> 00:48:59,280
 Like, the distribution of the data is like a sphere or like a ball.

806
00:48:59,280 --> 00:49:00,280
 Okay.

807
00:49:00,280 --> 00:49:05,480
 So, this kind of, then for each of the cluster, actually, we can find the center, right?

808
00:49:05,480 --> 00:49:09,960
 This center could be a good representative of all the samples in that group.

809
00:49:09,960 --> 00:49:19,480
 So, this is actually a scenario and well-suit, actually, the chemist's class, you know, all

810
00:49:19,480 --> 00:49:20,480
 good.

811
00:49:20,480 --> 00:49:21,480
 Okay.

812
00:49:21,480 --> 00:49:23,480
 But now, we have another scenario.

813
00:49:23,480 --> 00:49:28,080
 In this scenario, of course, the data here, right?

814
00:49:28,080 --> 00:49:30,880
 The data here, actually, like ball shape, right?

815
00:49:30,880 --> 00:49:31,880
 Sphere shape.

816
00:49:31,880 --> 00:49:36,960
 And of course, for this part of the data, the chemist's class organ should perform

817
00:49:36,960 --> 00:49:37,960
 well.

818
00:49:37,960 --> 00:49:48,560
 But if you look at the data here, this is like an S, right?

819
00:49:48,560 --> 00:49:50,040
 Like a ship of S, right?

820
00:49:50,040 --> 00:49:53,800
 It's all, this is actually irregular shape, not a sphere shape.

821
00:49:53,800 --> 00:49:54,800
 Okay.

822
00:49:54,800 --> 00:50:00,520
 And then, the chemist's class organ will not perform well.

823
00:50:00,520 --> 00:50:03,120
 So here, we assume there is a center vector.

824
00:50:03,120 --> 00:50:07,400
 This center vector is a good representative of all the samples within the cluster.

825
00:50:07,400 --> 00:50:08,400
 Okay.

826
00:50:08,400 --> 00:50:13,840
 And for this, I think for these two, actually, clusters, there will be no problem.

827
00:50:13,840 --> 00:50:16,280
 But for the data here, right?

828
00:50:16,280 --> 00:50:20,200
 For all the data here, you find, if you want to find the center vector, the center vector

829
00:50:20,200 --> 00:50:21,200
 may be here.

830
00:50:21,200 --> 00:50:22,200
 This is the position.

831
00:50:22,200 --> 00:50:23,400
 You get a mean vector, right?

832
00:50:23,400 --> 00:50:27,520
 The mean vector, maybe, you look at the, maybe the mean vector is just here.

833
00:50:28,320 --> 00:50:33,440
 So here, if this position, the point here, certainly, it is not a good representation

834
00:50:33,440 --> 00:50:37,400
 or representative of the sample in this cluster.

835
00:50:37,400 --> 00:50:41,440
 And then, we should know, you should not use the chemist's classroom.

836
00:50:41,440 --> 00:50:42,440
 Okay.

837
00:50:42,440 --> 00:50:50,000
 So here, we, although we have not talked about the details of the chemist's classroom, but

838
00:50:50,000 --> 00:50:56,240
 we have talked about the criterion, actually, that, you know, minimized, right?

839
00:50:56,240 --> 00:51:03,839
 So in this criterion, actually, we have assumed that this mean vector is the best representative

840
00:51:03,839 --> 00:51:06,120
 of all the samples within the cluster.

841
00:51:06,120 --> 00:51:07,120
 Okay.

842
00:51:07,120 --> 00:51:14,240
 So as long as this condition is satisfied, then we can use the chemist's classroom.

843
00:51:14,240 --> 00:51:18,319
 If this, like this scenario, right?

844
00:51:18,319 --> 00:51:19,560
 This scenario.

845
00:51:19,560 --> 00:51:23,560
 But if the data, actually, does not follow this assumption, right?

846
00:51:23,560 --> 00:51:26,520
 I do not satisfy this assumption.

847
00:51:26,520 --> 00:51:31,640
 And then, we should not actually use this chemist's classroom.

848
00:51:31,640 --> 00:51:34,160
 We should not use.

849
00:51:34,160 --> 00:51:35,160
 Okay.

850
00:51:35,160 --> 00:51:40,279
 So here, you know, we, based on the criterion, right?

851
00:51:40,279 --> 00:51:44,520
 WC, I said, within class, the sum of square distance.

852
00:51:44,520 --> 00:51:45,520
 Okay.

853
00:51:45,520 --> 00:51:50,799
 And, you know, we have talked about the advantage, disadvantage, and disadvantage.

854
00:51:50,800 --> 00:51:56,640
 And when this chemist's classroom can be used, and when it should not be used.

855
00:51:56,640 --> 00:51:57,640
 Okay.

856
00:51:57,640 --> 00:52:02,400
 So, next, actually, we have a brick after brick, actually, we talked about how this

857
00:52:02,400 --> 00:52:09,480
 tree, you know, chemist's classroom algorithm works.

858
00:52:09,480 --> 00:52:11,480
 Okay.

859
01:03:11,480 --> 01:03:41,440
 So, next, actually, we look at how this chemist's classroom algorithm actually operates.

860
01:03:42,400 --> 01:03:47,760
 And actually, the chemist's classroom algorithm consists of iteration of two main operations.

861
01:03:47,760 --> 01:03:52,240
 And the first operation is to assign the sample to the nearest cluster.

862
01:03:52,240 --> 01:03:53,240
 Okay.

863
01:03:53,240 --> 01:03:54,240
 Nearest cluster.

864
01:03:54,240 --> 01:03:55,240
 Okay.

865
01:03:55,240 --> 01:03:58,800
 Nearest, that means the distance, right?

866
01:03:58,800 --> 01:04:03,720
 And to the cluster is the, nearest is the minimum.

867
01:04:03,720 --> 01:04:04,720
 Okay.

868
01:04:04,720 --> 01:04:09,240
 Actually, each sample, you know, could have a distance to all the clusters.

869
01:04:09,240 --> 01:04:12,279
 I mean, the cluster here means the center of the cluster, right?

870
01:04:12,279 --> 01:04:16,000
 From any sample to a center vector of a cluster, right?

871
01:04:16,000 --> 01:04:17,600
 Actually, we can have a distance.

872
01:04:17,600 --> 01:04:18,600
 Okay.

873
01:04:18,600 --> 01:04:21,439
 We will assign the sample to the nearest cluster.

874
01:04:21,439 --> 01:04:23,479
 So, this is one operation.

875
01:04:23,479 --> 01:04:24,479
 Okay.

876
01:04:24,479 --> 01:04:30,879
 So, after assignment of the samples into the clusters, and then we need to calculate the

877
01:04:30,879 --> 01:04:34,359
 new cluster.

878
01:04:34,360 --> 01:04:40,920
 And actually, we need to, you know, repeat these operations by many steps.

879
01:04:40,920 --> 01:04:41,920
 Okay.

880
01:04:41,920 --> 01:04:45,520
 So, this is the chemist's classroom algorithm.

881
01:04:45,520 --> 01:04:51,960
 And actually, the beginning, actually, we have no idea about the cluster, right?

882
01:04:51,960 --> 01:04:55,000
 So, we can randomly generate, okay?

883
01:04:55,000 --> 01:04:59,920
 We can randomly generate the cluster, and then, based on that, we can calculate the distance

884
01:04:59,920 --> 01:05:05,200
 from all the samples to the initial, the random, initial random generator, center vector,

885
01:05:05,200 --> 01:05:06,200
 right?

886
01:05:06,200 --> 01:05:10,680
 And then, we assign them to the corresponding clusters.

887
01:05:10,680 --> 01:05:13,320
 And then, we calculate the new, the cluster.

888
01:05:13,320 --> 01:05:16,000
 This cluster is not random generator.

889
01:05:16,000 --> 01:05:21,640
 It's based on the assignment, actually, of the samples to the different clusters.

890
01:05:21,640 --> 01:05:22,640
 Okay.

891
01:05:22,640 --> 01:05:27,800
 So, after we get a new cluster center, then, again, we will repeat our program, calculate

892
01:05:27,800 --> 01:05:31,320
 the distance from each of the samples to a new cluster center.

893
01:05:31,320 --> 01:05:35,880
 Then, we actually reassign, actually, the sample to different clusters.

894
01:05:35,880 --> 01:05:36,880
 Okay.

895
01:05:36,880 --> 01:05:44,280
 So, we will perform, actually, repeated operation of these two.

896
01:05:44,280 --> 01:05:45,280
 Okay.

897
01:05:45,280 --> 01:05:48,200
 So, this is a procedure, right?

898
01:05:48,200 --> 01:05:52,600
 The first step, actually, we need to set the K to the number of clusters.

899
01:05:52,600 --> 01:05:54,800
 So, this K already is playing.

900
01:05:54,800 --> 01:05:56,800
 So, this is a hyperparameter.

901
01:05:56,800 --> 01:05:57,800
 Okay.

902
01:05:57,800 --> 01:06:01,000
 And so, later, we will talk about the method to determine this.

903
01:06:01,000 --> 01:06:05,800
 But in the K-means classification algorithm itself, this K must be preset.

904
01:06:05,800 --> 01:06:06,800
 Okay.

905
01:06:06,800 --> 01:06:14,000
 And in some scenarios, actually, now, this K depends on, I think, our requirement, right?

906
01:06:14,000 --> 01:06:19,440
 For example, if we want to select 2,000 customers from the millions of customers, and then,

907
01:06:19,440 --> 01:06:24,680
 you can just, actually, assume there are 2,000 clusters, and then, you assign the sample

908
01:06:24,680 --> 01:06:27,680
 to one of the 2,000 or 2,000 clusters, right?

909
01:06:27,680 --> 01:06:30,080
 So, finally, you send the, find the center vectors.

910
01:06:30,080 --> 01:06:34,080
 So, then, this number of clusters is determined by you, right?

911
01:06:34,080 --> 01:06:36,080
 You determine the number of clusters.

912
01:06:36,080 --> 01:06:41,720
 But sometimes, we need to determine to discover how many groups are underlying the data.

913
01:06:41,720 --> 01:06:42,720
 Okay.

914
01:06:42,720 --> 01:06:46,240
 Then, we need to spend effort to find the value of K. Okay.

915
01:06:46,240 --> 01:06:51,680
 So, here, we assume, actually, now, we assume this K value is known, is preset.

916
01:06:51,680 --> 01:06:52,680
 Okay.

917
01:06:52,680 --> 01:06:53,680
 Set the value.

918
01:06:53,680 --> 01:06:56,680
 And then, actually, we need to look at the first operation.

919
01:06:56,680 --> 01:06:57,680
 Okay.

920
01:06:57,680 --> 01:07:00,040
 And then, to care for, no, not, okay.

921
01:07:00,040 --> 01:07:05,279
 Then the step two, we need to generate the center vectors.

922
01:07:05,279 --> 01:07:08,399
 Because, actually, if you look at the two operations, right, the first one, assign the

923
01:07:08,399 --> 01:07:10,279
 sample to the nearest cluster.

924
01:07:10,279 --> 01:07:15,680
 So, if we don't have this, you know, step two, we run and generate some class centers.

925
01:07:15,680 --> 01:07:17,640
 And then, we don't have a basis, right?

926
01:07:17,640 --> 01:07:20,480
 We don't have a foundation to do the first operation.

927
01:07:20,480 --> 01:07:21,480
 Okay.

928
01:07:21,480 --> 01:07:22,480
 We don't have the center vector.

929
01:07:22,480 --> 01:07:23,960
 How to do the assignment?

930
01:07:23,960 --> 01:07:24,960
 Okay.

931
01:07:24,960 --> 01:07:28,240
 So, at the beginning, at the first step, the initial step, right?

932
01:07:28,240 --> 01:07:33,840
 Actually, this is no, center vector could be randomly generated.

933
01:07:33,840 --> 01:07:36,320
 Random generated by two ways.

934
01:07:36,320 --> 01:07:40,480
 First, actually, we can randomly, you know, actually, generate the center vector from

935
01:07:40,480 --> 01:07:41,480
 the samples.

936
01:07:41,480 --> 01:07:45,320
 You can randomly select some of the samples as the center vectors.

937
01:07:45,320 --> 01:07:48,200
 So, this is the initial center vectors, right?

938
01:07:48,200 --> 01:07:52,359
 We can randomly actually select samples.

939
01:07:52,359 --> 01:08:00,080
 Another is that, you know, in the range of the data, or in the space of the data, right?

940
01:08:00,080 --> 01:08:02,560
 And we randomly generate the center vectors.

941
01:08:02,560 --> 01:08:05,720
 We randomly generate, actually, the numerical vectors.

942
01:08:05,720 --> 01:08:08,080
 And then, randomly generate the numerical vectors.

943
01:08:08,080 --> 01:08:11,680
 In the future space, you know, I can't see that as the center vectors.

944
01:08:11,680 --> 01:08:12,680
 Okay.

945
01:08:12,680 --> 01:08:14,759
 So, then, two ways to do that.

946
01:08:14,759 --> 01:08:15,759
 Okay.

947
01:08:15,760 --> 01:08:21,760
 So, we have the step two, and we have created the initial center vector.

948
01:08:21,760 --> 01:08:24,840
 And after that, actually, we will repeat these two operations, right?

949
01:08:24,840 --> 01:08:25,840
 The first operation.

950
01:08:25,840 --> 01:08:28,640
 I assign the sample to the nearest class test.

951
01:08:28,640 --> 01:08:29,640
 Okay.

952
01:08:29,640 --> 01:08:33,960
 So, these are all the samples, right?

953
01:08:33,960 --> 01:08:36,440
 And then, we randomly generate.

954
01:08:36,440 --> 01:08:37,440
 Okay.

955
01:08:37,440 --> 01:08:45,720
 So, these are the randomly generated, actually, point or data point, which we have been using

956
01:08:45,720 --> 01:08:48,080
 as the initial center vectors.

957
01:08:48,080 --> 01:08:51,000
 So, here, we assume that k equals two.

958
01:08:51,000 --> 01:08:52,000
 Right?

959
01:08:52,000 --> 01:08:53,000
 There are two clusters.

960
01:08:53,000 --> 01:08:58,440
 And then, we create the two, you know, generate the two random vectors.

961
01:08:58,440 --> 01:09:03,600
 And these random, two random vectors are just the initial center vectors.

962
01:09:03,600 --> 01:09:04,600
 Okay.

963
01:09:04,600 --> 01:09:07,280
 After this, actually, we calculate the distance.

964
01:09:07,280 --> 01:09:08,280
 Okay.

965
01:09:08,280 --> 01:09:09,600
 For each of the sample, right?

966
01:09:09,600 --> 01:09:14,000
 For each of the sample, we can calculate the distance to the two, to the center vectors,

967
01:09:14,000 --> 01:09:16,920
 to the initial class centers.

968
01:09:16,920 --> 01:09:17,920
 Okay.

969
01:09:17,920 --> 01:09:20,160
 And then, we can compare the distance, right?

970
01:09:20,160 --> 01:09:23,160
 For example, after example, here, right?

971
01:09:23,160 --> 01:09:26,720
 And then, once I'm, we do this for every sample.

972
01:09:26,720 --> 01:09:27,720
 Okay.

973
01:09:27,720 --> 01:09:30,600
 For this sample, we can calculate the distance to this class center.

974
01:09:30,600 --> 01:09:33,680
 We can also calculate the distance to this class center.

975
01:09:33,680 --> 01:09:36,399
 And we find this distance is smaller, right?

976
01:09:36,399 --> 01:09:43,880
 And then, we will assign this sample to this cluster centered at this initial center.

977
01:09:44,760 --> 01:09:45,760
 Okay.

978
01:09:45,760 --> 01:09:50,920
 So, for each of the sample, we do this, we calculate the distance to the two cluster

979
01:09:50,920 --> 01:09:51,920
 centers.

980
01:09:51,920 --> 01:09:59,120
 And then, we assign the sample to the closest or to the nearest class centers.

981
01:09:59,120 --> 01:10:00,120
 Okay.

982
01:10:00,120 --> 01:10:02,320
 So, this is the result, actually.

983
01:10:02,320 --> 01:10:04,760
 After, actually, you know, okay.

984
01:10:04,760 --> 01:10:07,680
 So, assign to this result.

985
01:10:07,680 --> 01:10:11,880
 So, this is actually all the, you know, the sample here, right?

986
01:10:11,880 --> 01:10:15,320
 All the sample actually are closer to this one.

987
01:10:15,320 --> 01:10:26,160
 All the blue samples that we hear, sorry.

988
01:10:26,160 --> 01:10:28,280
 All the blue sample here, right?

989
01:10:28,280 --> 01:10:31,160
 Actually are closer to another center vector.

990
01:10:31,160 --> 01:10:32,160
 Okay.

991
01:10:32,160 --> 01:10:38,680
 So, this is the, you know, actually the assignment, the assignment or the sample to all the,

992
01:10:38,680 --> 01:10:40,680
 you know, to the cluster.

993
01:10:41,680 --> 01:10:43,040
 This is step three.

994
01:10:43,040 --> 01:10:49,440
 And then, you know, after that, we will, based on this new assignment to create a new center

995
01:10:49,440 --> 01:10:50,440
 vector.

996
01:10:50,440 --> 01:10:53,440
 Actually, the first step, the center vector, actually, is random generated.

997
01:10:53,440 --> 01:10:57,920
 But now, based on the assignment, you know, of the sample in each of the cluster, then

998
01:10:57,920 --> 01:11:01,440
 we create or generate a new center vector.

999
01:11:01,440 --> 01:11:05,440
 New center vector is just the mean vector for all the samples here, right?

1000
01:11:05,440 --> 01:11:10,600
 For the red sample, you know, we get a random vector.

1001
01:11:11,520 --> 01:11:15,120
 For all the blue samples, actually, we get a mean vector.

1002
01:11:15,120 --> 01:11:17,120
 So, this is the new center vector.

1003
01:11:17,120 --> 01:11:18,120
 Okay.

1004
01:11:18,120 --> 01:11:21,920
 So, this new center vector certainly is much better than the initial center vector, which

1005
01:11:21,920 --> 01:11:23,520
 are generated randomly, right?

1006
01:11:23,520 --> 01:11:29,440
 You can see, indeed, they are closer to these samples and they are located around the center,

1007
01:11:29,440 --> 01:11:36,320
 the central region of this, right, of all the sample in each of the cluster.

1008
01:11:36,320 --> 01:11:37,320
 Okay.

1009
01:11:37,320 --> 01:11:40,080
 So, after this, we will repeat this operation again.

1010
01:11:40,080 --> 01:11:45,800
 For each of the samples, we will calculate this into the new cluster, the new cluster.

1011
01:11:45,800 --> 01:11:46,800
 Okay.

1012
01:11:46,800 --> 01:11:53,800
 Then we look at which one is smaller, then we assign this sample to the nearest cluster.

1013
01:11:53,800 --> 01:11:54,800
 Okay.

1014
01:11:54,800 --> 01:12:02,120
 So, we will repeat this operation until, actually, the stopping criterion is satisfied.

1015
01:12:02,120 --> 01:12:08,040
 The stopping criterion could be no change in the centroid, no change in the centroid.

1016
01:12:08,040 --> 01:12:14,000
 Of course, in such a scenario, there is also no change in the assignment of the samples.

1017
01:12:14,000 --> 01:12:15,000
 Okay.

1018
01:12:15,000 --> 01:12:17,000
 So, then we can stop.

1019
01:12:17,000 --> 01:12:20,720
 So, this is the final, you know, collection result.

1020
01:12:20,720 --> 01:12:25,640
 Actually, now, from this example, as you can see, just through a few steps, right, and

1021
01:12:25,640 --> 01:12:30,519
 then the sample actually can come to this, actually, in the center vector.

1022
01:12:30,519 --> 01:12:31,519
 Indeed.

1023
01:12:31,519 --> 01:12:32,519
 Okay.

1024
01:12:32,519 --> 01:12:37,320
 And actually, if you program, if you do some real data, you can find it.

1025
01:12:37,320 --> 01:12:44,960
 Maybe in most applications, maybe just like after 10 or 20 iterations, then the collection

1026
01:12:44,960 --> 01:12:48,799
 process is completed, just about 10.

1027
01:12:48,799 --> 01:12:49,799
 Right?

1028
01:12:49,799 --> 01:12:53,599
 In this example, actually, just after one or two rounds, right, actually, we can get,

1029
01:12:53,599 --> 01:12:56,880
 actually, very good collection results.

1030
01:12:56,880 --> 01:13:03,000
 But in general, actually, when there will be no change in the class center, or if there

1031
01:13:03,000 --> 01:13:07,640
 will be no change in the assignment of the sample to different classes, right, is the

1032
01:13:07,640 --> 01:13:11,840
 sum of the sample to the cluster, and then we can stop.

1033
01:13:11,840 --> 01:13:12,840
 Okay.

1034
01:13:12,840 --> 01:13:16,840
 So, of course, I know this is just for visualization purpose, right?

1035
01:13:16,840 --> 01:13:20,160
 In space, hard-enlarged space, you cannot see the data distribution.

1036
01:13:20,160 --> 01:13:25,160
 But from this, actually, we should have confidence, right, to the chemist's classroom algorithm.

1037
01:13:25,160 --> 01:13:30,600
 Even if we cannot see the data distribution, we believe or trust the algorithm, actually,

1038
01:13:30,600 --> 01:13:39,280
 to find the center of the clusters, of all the samples in that cluster.

1039
01:13:39,280 --> 01:13:42,600
 So, this is just a centroid of the cluster.

1040
01:13:42,600 --> 01:13:46,040
 Actually, it is a mean vector.

1041
01:13:46,040 --> 01:13:47,040
 Okay.

1042
01:13:47,040 --> 01:13:50,400
 So, this is a chemist's classroom algorithm.

1043
01:13:50,400 --> 01:13:54,280
 So, this is the most popular classroom algorithm, most popular.

1044
01:13:54,280 --> 01:13:55,280
 Okay.

1045
01:13:55,280 --> 01:14:00,200
 In any package, right, when you talk about the classroom, so, they always know that the

1046
01:14:00,200 --> 01:14:01,200
 chemist is the first choice.

1047
01:14:01,200 --> 01:14:02,200
 Okay.

1048
01:14:02,200 --> 01:14:07,200
 It is the most popular method.

1049
01:14:07,200 --> 01:14:08,200
 Okay.

1050
01:14:08,200 --> 01:14:09,200
 Okay.

1051
01:14:09,200 --> 01:14:13,960
 So, that is actually, I think, a very important issue, the determination of the care value,

1052
01:14:13,960 --> 01:14:14,960
 right?

1053
01:14:14,960 --> 01:14:16,599
 Chemist's classroom, this is care.

1054
01:14:16,599 --> 01:14:22,000
 You assume we know this care when we actually perform chemist's classroom, right?

1055
01:14:22,000 --> 01:14:23,480
 We assume we know this care.

1056
01:14:23,480 --> 01:14:27,360
 But in practice, actually, normally, we don't know this care, right?

1057
01:14:27,719 --> 01:14:31,519
 Then how to determine the super value of care?

1058
01:14:31,519 --> 01:14:33,519
 Okay.

1059
01:14:33,519 --> 01:14:38,280
 And actually, no, we can actually use the trauma and the error method.

1060
01:14:38,280 --> 01:14:40,120
 Trauma and error, right?

1061
01:14:40,120 --> 01:14:44,799
 Actually, we can set the care to different values, starting from one.

1062
01:14:44,799 --> 01:14:46,400
 Even you can start from one, right?

1063
01:14:46,400 --> 01:14:47,400
 K equals one.

1064
01:14:47,400 --> 01:14:50,240
 That means, one, that means all the samples belong to one cluster, right?

1065
01:14:50,240 --> 01:14:53,920
 Then the cluster is just the mean vector for all the samples.

1066
01:14:53,920 --> 01:14:54,920
 Okay.

1067
01:14:54,920 --> 01:14:57,040
 You can start from K, right?

1068
01:14:57,040 --> 01:15:02,480
 And then, actually, we can, based on this center vector, we can evaluate, we can calculate

1069
01:15:02,480 --> 01:15:03,480
 WCSS.

1070
01:15:03,480 --> 01:15:11,000
 You remember, we see chemist's classroom trying to minimize WCSS within cluster sum of squared

1071
01:15:11,000 --> 01:15:13,280
 distance, right?

1072
01:15:13,280 --> 01:15:18,720
 Certainly we can use the WCSS to evaluate the classroom results, right?

1073
01:15:18,720 --> 01:15:26,720
 So when we set K equals one, we can calculate the center vector and we can calculate WCSS.

1074
01:15:27,440 --> 01:15:32,040
 And then we can actually set K to two.

1075
01:15:32,040 --> 01:15:34,080
 We assume there are two clusters, right?

1076
01:15:34,080 --> 01:15:39,000
 Then through the chemist's classroom, actually, we can get the classroom result.

1077
01:15:39,000 --> 01:15:44,120
 We can get the centroid, and also we can get the assignment of the sample to each of the

1078
01:15:44,120 --> 01:15:45,120
 two clusters.

1079
01:15:45,120 --> 01:15:46,120
 Okay.

1080
01:15:46,120 --> 01:15:54,320
 So based on this, we can calculate within cluster sum of squared distance, WCSS.

1081
01:15:54,320 --> 01:15:56,560
 So this is for the WCSS, right?

1082
01:15:56,560 --> 01:16:01,440
 Then you can set K to three, K to four, K to five.

1083
01:16:01,440 --> 01:16:03,960
 So I don't know you have done all of these, right?

1084
01:16:03,960 --> 01:16:06,680
 Then you can actually plot a curve.

1085
01:16:06,680 --> 01:16:13,680
 This curve, actually, this curve, actually, for each of the K, we calculate the WCSS,

1086
01:16:14,400 --> 01:16:15,400
 right?

1087
01:16:15,400 --> 01:16:20,040
 Then we plot a curve of WCSS value against K values.

1088
01:16:20,040 --> 01:16:21,040
 Okay.

1089
01:16:21,040 --> 01:16:23,040
 So just this.

1090
01:16:24,040 --> 01:16:30,640
 And actually, you can know the tree from here, right?

1091
01:16:30,640 --> 01:16:36,560
 And previously, now we see, now we have like a hyper parameter, and actually we can determine

1092
01:16:36,560 --> 01:16:40,640
 the value of hyper parameter based on the performance of the validation data.

1093
01:16:40,640 --> 01:16:43,600
 Normally, we are training, we are validation, we are testing, right?

1094
01:16:43,600 --> 01:16:46,920
 But in the classroom, we don't have the validation data.

1095
01:16:46,920 --> 01:16:47,920
 We don't have the validation data.

1096
01:16:47,920 --> 01:16:51,760
 Then how to determine the hyper parameter?

1097
01:16:52,760 --> 01:16:54,960
 Actually, we use the training data.

1098
01:16:54,960 --> 01:16:56,800
 Here we only have the data, training data, right?

1099
01:16:56,800 --> 01:16:59,360
 We don't have the testing, we don't have validation.

1100
01:16:59,360 --> 01:17:00,360
 Okay.

1101
01:17:00,360 --> 01:17:07,680
 And for the training data, the more class that we use, the smaller WCSS.

1102
01:17:07,680 --> 01:17:09,320
 You can imagine, right?

1103
01:17:09,320 --> 01:17:16,320
 If each of the sample is, if the number of class are equal to the number of samples,

1104
01:17:16,320 --> 01:17:18,960
 then each sample belongs to one class, right?

1105
01:17:18,960 --> 01:17:20,560
 Then the center is just a sample itself.

1106
01:17:20,560 --> 01:17:24,600
 The distance of the sample to the class center is zero.

1107
01:17:24,600 --> 01:17:26,600
 Then WCSS is zero.

1108
01:17:26,600 --> 01:17:27,600
 Okay.

1109
01:17:27,600 --> 01:17:29,360
 So these are extremely key, right?

1110
01:17:29,360 --> 01:17:35,400
 So you can imagine tree, the WCSS actually is decreasing with the increase in actually,

1111
01:17:35,400 --> 01:17:37,880
 you know, the care value.

1112
01:17:37,880 --> 01:17:38,880
 Okay.

1113
01:17:38,880 --> 01:17:43,360
 Certainly, we cannot determine care based on the minimum WCSS.

1114
01:17:43,360 --> 01:17:49,520
 We cannot determine care based on the minimum WCSS because when the care equal to the number

1115
01:17:49,520 --> 01:17:52,280
 of samples, these value will be the minimum.

1116
01:17:52,280 --> 01:17:53,280
 Okay.

1117
01:17:53,280 --> 01:17:55,320
 Because it's on the training data, right?

1118
01:17:55,320 --> 01:17:56,800
 On the training data.

1119
01:17:56,800 --> 01:17:57,800
 Okay.

1120
01:17:57,800 --> 01:18:00,360
 And actually, they have to determine super value.

1121
01:18:00,360 --> 01:18:04,360
 Actually, we can use this so-called Able method.

1122
01:18:04,360 --> 01:18:06,520
 Able, right?

1123
01:18:06,520 --> 01:18:13,040
 So from here, right, just like this, the tree, the drop is very fast, right?

1124
01:18:13,040 --> 01:18:17,440
 But after Able, this slope, just like this, right?

1125
01:18:17,440 --> 01:18:18,440
 Significant drop at the beginning.

1126
01:18:18,440 --> 01:18:23,759
 So after a certain point, critical point, actually, the decrease is slowed down.

1127
01:18:23,759 --> 01:18:26,519
 So we can just based on that Able, right?

1128
01:18:26,519 --> 01:18:28,200
 Just like this Able point.

1129
01:18:28,200 --> 01:18:29,799
 Very fast, then very slow.

1130
01:18:29,799 --> 01:18:31,799
 This point.

1131
01:18:31,799 --> 01:18:33,799
 Three.

1132
01:18:33,799 --> 01:18:37,360
 We can see that.

1133
01:18:37,360 --> 01:18:43,559
 From here, you can see when a number of classes set to one, then the WCSS is around 8,000,

1134
01:18:43,559 --> 01:18:44,559
 right?

1135
01:18:44,560 --> 01:18:48,720
 So if we set class in the 2, 2, then the WCSS is nearly 3,000.

1136
01:18:48,720 --> 01:18:49,720
 Okay.

1137
01:18:49,720 --> 01:18:57,080
 So we increase the number of classes by one, then the reduction of the WCSS is nearly 5,000.

1138
01:18:57,080 --> 01:18:59,480
 So this is a very significant reduction, right?

1139
01:18:59,480 --> 01:19:05,400
 Then we continue to increase the class number from 2 to 3, then the WCSS is dropped to around

1140
01:19:05,400 --> 01:19:07,920
 maybe like 800 or 900.

1141
01:19:07,920 --> 01:19:08,920
 Okay.

1142
01:19:08,920 --> 01:19:15,720
 Then we can see from 2 to 3, just one increment of the class number, right?

1143
01:19:15,720 --> 01:19:24,600
 But the reduction of WCSS is from 3,000 to 800 over 2,000, this is a reduction, but just

1144
01:19:24,600 --> 01:19:28,720
 at one class center.

1145
01:19:28,720 --> 01:19:34,120
 But after 3, right, if you continue to increase the class center to 4, and then the reduction

1146
01:19:34,120 --> 01:19:36,680
 is not significant, right?

1147
01:19:36,680 --> 01:19:41,440
 From 800 probably to like 500, something like that.

1148
01:19:41,440 --> 01:19:44,440
 The reduction is only like 500.

1149
01:19:44,440 --> 01:19:48,720
 It's relatively small.

1150
01:19:48,720 --> 01:19:49,720
 Okay.

1151
01:19:49,720 --> 01:19:59,720
 So based on this so-called Able method, Able method, we can determine the suitable value

1152
01:19:59,720 --> 01:20:00,720
 of k.

1153
01:20:00,720 --> 01:20:01,720
 Okay.

1154
01:20:01,720 --> 01:20:03,280
 And actually this Able method not just used here.

1155
01:20:03,280 --> 01:20:08,679
 In many applications, if you want to determine the value of hyperparameter based on the training

1156
01:20:08,679 --> 01:20:11,880
 data, always we can use this.

1157
01:20:11,880 --> 01:20:13,280
 Okay.

1158
01:20:13,280 --> 01:20:19,960
 And actually in classification, in regression, the larger the model, the more complex the

1159
01:20:19,960 --> 01:20:20,960
 model, right?

1160
01:20:20,960 --> 01:20:24,440
 Then the smaller the training area, smaller the training area.

1161
01:20:24,440 --> 01:20:30,440
 So normally we don't use the training area to determine the value of hyperparameter.

1162
01:20:30,440 --> 01:20:32,240
 We use the validation data, right?

1163
01:20:32,240 --> 01:20:40,320
 But actually we can also use the training data to determine the hyperparameters.

1164
01:20:40,320 --> 01:20:44,760
 But just similar to this, we use the Able method.

1165
01:20:44,760 --> 01:20:45,760
 Able.

1166
01:20:45,760 --> 01:20:46,760
 Okay.

1167
01:20:46,760 --> 01:20:49,200
 So then after a certain point, drop is slow.

1168
01:20:49,200 --> 01:20:52,360
 We'll just use that point.

1169
01:20:52,360 --> 01:20:53,360
 Okay.

1170
01:20:53,360 --> 01:20:59,559
 So this is the determination for the k values in the k-means class.

1171
01:20:59,559 --> 01:21:01,559
 Able method.

1172
01:21:02,560 --> 01:21:08,200
 So this point, the sharp point of the band of the plot, it can see that's the best value

1173
01:21:08,200 --> 01:21:09,200
 of k.

1174
01:21:09,200 --> 01:21:10,200
 Okay.

1175
01:21:10,200 --> 01:21:13,720
 So this is the k-means classification.

1176
01:21:13,720 --> 01:21:22,280
 Next, we talk about another second method, hierarchical classification.

1177
01:21:22,280 --> 01:21:27,240
 Actually, as I explained before, actually, hierarchical classification produces the so-called

1178
01:21:27,240 --> 01:21:28,240
 dendrogram.

1179
01:21:28,240 --> 01:21:33,360
 The dendrogram, actually dendrogram, actually provides the visualization of the similarities

1180
01:21:33,360 --> 01:21:34,360
 between the samples.

1181
01:21:34,360 --> 01:21:35,360
 Okay.

1182
01:21:35,360 --> 01:21:39,760
 Of course, in the original space, right, we can look at the distance between the samples.

1183
01:21:39,760 --> 01:21:42,840
 We can look at the location of the samples in the space.

1184
01:21:42,840 --> 01:21:45,760
 Then we can determine whether the two samples are similar or not.

1185
01:21:45,760 --> 01:21:46,760
 Okay.

1186
01:21:46,760 --> 01:21:49,040
 But actually, this space is low dimensional.

1187
01:21:49,040 --> 01:21:50,040
 We can, right?

1188
01:21:50,040 --> 01:21:54,440
 But if the space is higher than three, we cannot visualize the data distribution.

1189
01:21:54,639 --> 01:21:57,759
 Then, of course, we cannot see the location of the data.

1190
01:21:57,759 --> 01:22:01,919
 Again, we cannot see the similarity between the data.

1191
01:22:01,919 --> 01:22:04,759
 But the dendrogram provides such a visualization.

1192
01:22:04,759 --> 01:22:05,759
 Okay.

1193
01:22:05,759 --> 01:22:09,040
 And for the distance between the samples.

1194
01:22:09,040 --> 01:22:10,040
 Okay.

1195
01:22:10,040 --> 01:22:18,919
 So the hierarchical classification, and we already have seen this kind of tree-like structure,

1196
01:22:18,919 --> 01:22:19,919
 right?

1197
01:22:19,919 --> 01:22:21,559
 So this is the dendrogram.

1198
01:22:21,560 --> 01:22:25,200
 And here we have a sample, ABCDEFGH, right?

1199
01:22:25,200 --> 01:22:28,840
 And then, actually, we can see the combination of the samples.

1200
01:22:28,840 --> 01:22:29,840
 Okay.

1201
01:22:29,840 --> 01:22:35,320
 And B and C combination from one cluster.

1202
01:22:35,320 --> 01:22:38,280
 And the CD combination from one cluster.

1203
01:22:38,280 --> 01:22:42,400
 And then finally, ABCDE, actually join, join, right?

1204
01:22:42,400 --> 01:22:46,280
 Actually, combine, actually, to form one cluster.

1205
01:22:46,280 --> 01:22:47,280
 Okay.

1206
01:22:47,280 --> 01:22:49,960
 So this is actually the hierarchical tree.

1207
01:22:49,960 --> 01:22:54,240
 And actually, if you look at the tree, then we can interpret the tree from two different

1208
01:22:54,240 --> 01:22:55,240
 perspectives.

1209
01:22:55,240 --> 01:22:59,240
 The first perspective is from the division perspective, the top down.

1210
01:22:59,240 --> 01:23:00,240
 Okay.

1211
01:23:00,240 --> 01:23:03,760
 Actually, we can see the data from the very top, right?

1212
01:23:03,760 --> 01:23:06,120
 The data is divided into two parts.

1213
01:23:06,120 --> 01:23:07,600
 Just like classification tree.

1214
01:23:07,600 --> 01:23:08,800
 Now, we have a rule node.

1215
01:23:08,800 --> 01:23:11,080
 But here we don't have a rule node concept.

1216
01:23:11,080 --> 01:23:12,080
 But here the data, right?

1217
01:23:12,080 --> 01:23:13,680
 Actually, divided into two parts.

1218
01:23:13,680 --> 01:23:17,280
 This part and this part.

1219
01:23:17,280 --> 01:23:18,280
 Okay.

1220
01:23:18,280 --> 01:23:19,599
 And then the data is further.

1221
01:23:19,599 --> 01:23:23,599
 For the lab branch, right, the data in this subset, and then the data is further divided

1222
01:23:23,599 --> 01:23:24,599
 into two subsets.

1223
01:23:24,599 --> 01:23:25,599
 This is one subset.

1224
01:23:25,599 --> 01:23:26,599
 And this isdu subset.

1225
01:23:26,599 --> 01:23:31,080
 Then for each of the subset, the other two subset.

1226
01:23:31,080 --> 01:23:36,860
 So this is, like, a, you know, like a, a division, right, a division of the whole data set into

1227
01:23:36,860 --> 01:23:37,860
 two parts.

1228
01:23:37,860 --> 01:23:40,719
 Then for each of the two parts, we divide them into two parts.

1229
01:23:40,719 --> 01:23:43,360
 Then for each of the parts, we divide them into two parts.

1230
01:23:43,360 --> 01:23:44,360
 Okay.

1231
01:23:44,360 --> 01:23:49,460
 Finally, each sample corresponds to one cluster.

1232
01:23:49,460 --> 01:23:50,540
 At the beginning of the order, this

1233
01:23:50,540 --> 01:23:52,059
 have been onto one cluster.

1234
01:23:52,059 --> 01:23:54,740
 We divide them into two clusters.

1235
01:23:54,740 --> 01:23:56,019
 Then for each of the two clusters,

1236
01:23:56,019 --> 01:23:58,059
 we divide them into two clusters.

1237
01:23:58,059 --> 01:24:00,620
 OK, so this is a kind of a division.

1238
01:24:00,620 --> 01:24:03,700
 From the full dataset, we divide them into,

1239
01:24:03,700 --> 01:24:08,540
 to know, finally, we divide into n, actually, no, groups.

1240
01:24:08,540 --> 01:24:10,139
 n here, in number of samples.

1241
01:24:10,139 --> 01:24:13,820
 Each sample corresponds to one group.

1242
01:24:14,099 --> 01:24:17,340
 So this is an interpretation of this dendrogram

1243
01:24:17,340 --> 01:24:21,219
 from the division perspective.

1244
01:24:21,219 --> 01:24:25,540
 Another perspective is from the group, the combination.

1245
01:24:25,540 --> 01:24:28,940
 And that is the bottom up method.

1246
01:24:28,940 --> 01:24:30,860
 From the bottom up, in the bottom up,

1247
01:24:30,860 --> 01:24:36,820
 actually, each of the sample here is considered as one cluster.

1248
01:24:36,820 --> 01:24:39,059
 And then we merge the clusters.

1249
01:24:39,059 --> 01:24:43,380
 We merge, actually, the sample A and B merge together.

1250
01:24:43,380 --> 01:24:46,100
 C and D merge together.

1251
01:24:46,100 --> 01:24:48,060
 F and G merge together.

1252
01:24:48,060 --> 01:24:53,940
 Finally, all the samples are merged into one single cluster.

1253
01:24:53,940 --> 01:24:55,940
 So this is another perspective.

1254
01:24:55,940 --> 01:24:59,580
 So based on these two perspectives,

1255
01:24:59,580 --> 01:25:02,300
 and they have developed two different methods

1256
01:25:02,300 --> 01:25:07,940
 for constructing such a hierarchical tree.

1257
01:25:07,940 --> 01:25:11,580
 And so the first one, actually, I just

1258
01:25:11,580 --> 01:25:14,140
 talk about the divisive algorithm.

1259
01:25:14,140 --> 01:25:17,059
 So it's a top down.

1260
01:25:17,059 --> 01:25:18,380
 Divide the data into two.

1261
01:25:18,380 --> 01:25:22,940
 This is quite like the group of the classification tree,

1262
01:25:22,940 --> 01:25:26,059
 the group of the tree.

1263
01:25:26,059 --> 01:25:27,700
 And then the second method, I just

1264
01:25:27,700 --> 01:25:31,980
 talk about is a generative algorithm, also known as bottom

1265
01:25:31,980 --> 01:25:33,380
 up.

1266
01:25:33,380 --> 01:25:38,700
 It's a combination, merging, the merge of two clusters.

1267
01:25:38,700 --> 01:25:40,580
 Then another two clusters merge together.

1268
01:25:40,580 --> 01:25:46,059
 So finally, all the samples merge into one single cluster.

1269
01:25:46,059 --> 01:25:48,580
 So one is the divisive method.

1270
01:25:48,580 --> 01:25:50,860
 It's a top down.

1271
01:25:50,860 --> 01:25:52,660
 It's a divisive process.

1272
01:25:52,660 --> 01:25:55,059
 Another is a generative method.

1273
01:25:55,059 --> 01:25:56,180
 It's a bottom up.

1274
01:25:56,180 --> 01:26:01,260
 It's a merging process, merge the clusters.

1275
01:26:01,260 --> 01:26:03,980
 OK, so actually, in this code, we just

1276
01:26:03,980 --> 01:26:06,780
 look at the generative algorithm,

1277
01:26:06,780 --> 01:26:10,300
 which is the so-called bottom up method.

1278
01:26:10,340 --> 01:26:12,020
 So if you have an interest, actually,

1279
01:26:12,020 --> 01:26:16,180
 you can search on the internet about the divisive method

1280
01:26:16,180 --> 01:26:18,220
 for hierarchical class.

1281
01:26:26,620 --> 01:26:28,860
 And we look at this algorithm.

1282
01:26:28,860 --> 01:26:35,540
 Actually, the algorithm actually started from,

1283
01:26:35,540 --> 01:26:39,940
 OK, here we look at this example.

1284
01:26:39,940 --> 01:26:42,580
 Through this example, I explained the procedure

1285
01:26:42,580 --> 01:26:46,339
 of principles in this agglomerative algorithm

1286
01:26:46,339 --> 01:26:49,660
 for hierarchical clustering.

1287
01:26:49,660 --> 01:26:55,219
 And here we have six samples, S1, S2, S3, S4, S5, S6.

1288
01:26:55,219 --> 01:26:57,019
 So we have six samples.

1289
01:26:57,019 --> 01:27:04,019
 Each sample tree is denoted by four attribute, right?

1290
01:27:04,019 --> 01:27:06,540
 a1, a2, a3, a4, a4 value.

1291
01:27:06,540 --> 01:27:09,219
 In other words, each sample is a four dimensional vector.

1292
01:27:10,100 --> 01:27:12,420
 And so how to combine them?

1293
01:27:12,420 --> 01:27:14,140
 They're just the data.

1294
01:27:14,140 --> 01:27:15,220
 How to combine them?

1295
01:27:15,220 --> 01:27:19,500
 From here, you cannot see how to combine.

1296
01:27:19,500 --> 01:27:21,900
 It's very hard to see how to combine.

1297
01:27:21,900 --> 01:27:24,540
 But actually, in the agglomerative algorithm,

1298
01:27:24,540 --> 01:27:29,420
 actually, the first step is to calculate the pairwise distance.

1299
01:27:29,420 --> 01:27:30,540
 Pairwise.

1300
01:27:30,540 --> 01:27:35,580
 That means sample number one, sample S1 to S2, S1 to S3,

1301
01:27:35,580 --> 01:27:38,620
 S1 to S4, S1 to S6.

1302
01:27:38,660 --> 01:27:40,500
 Then S2 to S3, S2.

1303
01:27:40,500 --> 01:27:42,180
 So this is a pairwise distance.

1304
01:27:42,180 --> 01:27:49,980
 So first, actually, we need to have such a similarity

1305
01:27:49,980 --> 01:27:52,260
 matrix.

1306
01:27:52,260 --> 01:27:55,019
 So a similarity matrix can see the pairwise similarity

1307
01:27:55,019 --> 01:27:58,140
 between the samples.

1308
01:27:58,140 --> 01:27:59,940
 And of course, the similarity measure

1309
01:27:59,940 --> 01:28:04,900
 can be based on the Euclidean distance.

1310
01:28:04,900 --> 01:28:07,420
 So here we use Euclidean distance.

1311
01:28:07,420 --> 01:28:10,420
 In other words, actually, we calculate the Euclidean distance

1312
01:28:10,420 --> 01:28:12,140
 between the samples.

1313
01:28:12,140 --> 01:28:15,860
 Then we can have such a metric.

1314
01:28:15,860 --> 01:28:18,500
 This is actually an S1, S2.

1315
01:28:18,500 --> 01:28:24,740
 And actually, you can see that this is a metric, a 6 by 6,

1316
01:28:24,740 --> 01:28:26,820
 because we have a 6 sample.

1317
01:28:26,820 --> 01:28:27,980
 And we have 6.

1318
01:28:27,980 --> 01:28:29,340
 It's a 6 by 6.

1319
01:28:29,340 --> 01:28:33,820
 And this metric is symmetric because a distance from S1

1320
01:28:33,820 --> 01:28:38,179
 to S2 is the same as a distance from S2 to S1.

1321
01:28:38,179 --> 01:28:40,780
 So these are symmetric metrics.

1322
01:28:40,780 --> 01:28:45,900
 And also, we know that all the elements on the metangular

1323
01:28:45,900 --> 01:28:48,299
 are zeros.

1324
01:28:48,299 --> 01:28:51,740
 Actually, all the elements mean the distance from sample

1325
01:28:51,740 --> 01:28:54,900
 number one to number one itself, S2 to S2 itself.

1326
01:28:54,900 --> 01:28:57,139
 I'll call the distance zero.

1327
01:28:57,139 --> 01:29:00,940
 So the elements on the metangular

1328
01:29:01,139 --> 01:29:06,940
 actually shows the distance from the sample to itself.

1329
01:29:06,940 --> 01:29:10,540
 So this is the first step in the agglomerative algorithm

1330
01:29:10,540 --> 01:29:18,580
 is to obtain such a promiscuity or similarity metrics.

1331
01:29:18,580 --> 01:29:22,660
 And of course, we can use different metrics

1332
01:29:22,660 --> 01:29:24,339
 for the similarity measure.

1333
01:29:24,339 --> 01:29:26,820
 But here, we use Euclidean distance.

1334
01:29:26,820 --> 01:29:30,299
 OK.

1335
01:29:30,299 --> 01:29:31,900
 So this is a distance.

1336
01:29:31,900 --> 01:29:34,139
 And I suppose you know how to calculate

1337
01:29:34,139 --> 01:29:37,660
 the distance between the sample, Euclidean distance.

1338
01:29:37,660 --> 01:29:39,620
 You know how to do that.

1339
01:29:39,620 --> 01:29:45,980
 And then, actually, we need to start the merge process.

1340
01:29:45,980 --> 01:29:49,259
 Actually, in the agglomerative algorithm,

1341
01:29:49,259 --> 01:29:56,620
 at each step, we merge two more similar clusters.

1342
01:29:57,420 --> 01:30:00,380
 And in the first step, at the beginning,

1343
01:30:00,380 --> 01:30:03,740
 each sample is considered as a cluster.

1344
01:30:03,740 --> 01:30:05,460
 Each sample is considered a cluster.

1345
01:30:05,460 --> 01:30:07,380
 In other words, actually, in the first step,

1346
01:30:07,380 --> 01:30:11,580
 we merge two more similar samples.

1347
01:30:11,580 --> 01:30:13,380
 Because each sample is a cluster.

1348
01:30:13,380 --> 01:30:17,260
 So actually, this is a special case.

1349
01:30:17,260 --> 01:30:19,900
 So if you look at this, actually,

1350
01:30:19,900 --> 01:30:23,620
 there's a distance between the sample and the sample.

1351
01:30:24,180 --> 01:30:25,380
 Look at this, actually.

1352
01:30:25,380 --> 01:30:28,900
 There's a distance of metrics or similarity metrics.

1353
01:30:28,900 --> 01:30:32,780
 And you can find which distance, which value is the smallest.

1354
01:30:32,780 --> 01:30:34,580
 And then, this distance, actually,

1355
01:30:34,580 --> 01:30:38,180
 corresponds to two more similar samples.

1356
01:30:41,059 --> 01:30:43,940
 So from here, you can just inspect the values, right?

1357
01:30:43,940 --> 01:30:45,740
 Actually, two points something.

1358
01:30:45,740 --> 01:30:48,900
 And then, we have one is 1.29 something.

1359
01:30:48,900 --> 01:30:51,019
 Another is 1.288.

1360
01:30:51,020 --> 01:30:54,700
 Actually, the 1.288 is the smallest value, right?

1361
01:30:54,700 --> 01:30:56,100
 We exclude a zero line.

1362
01:30:56,100 --> 01:30:58,100
 Because the zero just represents the distance

1363
01:30:58,100 --> 01:30:59,740
 to the sample itself, right?

1364
01:30:59,740 --> 01:31:03,660
 We don't need to know this point.

1365
01:31:03,660 --> 01:31:07,340
 So 1.288 is the minimum value.

1366
01:31:07,340 --> 01:31:11,260
 And actually, this is the distance between S5 and S6.

1367
01:31:11,260 --> 01:31:17,340
 In other words, S5 and S6 are the most similar samples.

1368
01:31:17,340 --> 01:31:20,820
 And then, in the first step, we merge these two samples.

1369
01:31:20,820 --> 01:31:23,019
 Or these two clusters.

1370
01:31:23,019 --> 01:31:24,460
 This is the first step.

1371
01:31:24,460 --> 01:31:25,460
 OK.

1372
01:31:29,460 --> 01:31:31,660
 So we merge these two.

1373
01:31:31,660 --> 01:31:34,179
 S5, S6.

1374
01:31:34,179 --> 01:31:38,299
 So because these samples, S5, S6, are more similar.

1375
01:31:38,299 --> 01:31:41,099
 And after merging, we get a new cluster, right?

1376
01:31:41,099 --> 01:31:43,420
 Initially, we have two clusters, S5, S6.

1377
01:31:43,420 --> 01:31:45,019
 Although they are not two samples,

1378
01:31:45,019 --> 01:31:47,380
 we can get two clusters, OK?

1379
01:31:47,380 --> 01:31:48,660
 Special key.

1380
01:31:48,660 --> 01:31:51,660
 Then we merge these two clusters to obtain new cluster.

1381
01:31:51,660 --> 01:31:55,860
 We name the new cluster S5, S6.

1382
01:31:55,860 --> 01:31:57,740
 So then we link them, right?

1383
01:31:57,740 --> 01:32:00,260
 We just link the two, connect them.

1384
01:32:00,260 --> 01:32:02,260
 But the height of this, right?

1385
01:32:02,260 --> 01:32:06,540
 The height should be proportional to the distance.

1386
01:32:06,540 --> 01:32:09,340
 Proponent to the distance.

1387
01:32:09,340 --> 01:32:11,180
 So later, when you see, or the distance is small,

1388
01:32:11,180 --> 01:32:13,139
 you know, or the two samples are quite similar.

1389
01:32:13,139 --> 01:32:15,180
 If the distance isn't very big, then you

1390
01:32:15,180 --> 01:32:17,460
 know the two samples are very, or two clusters

1391
01:32:17,460 --> 01:32:19,500
 are very different.

1392
01:32:19,500 --> 01:32:20,140
 OK.

1393
01:32:20,140 --> 01:32:24,140
 So this actually, basically, this height

1394
01:32:24,140 --> 01:32:28,140
 is measured between cluster distance, right?

1395
01:32:28,140 --> 01:32:30,660
 Between cluster distance.

1396
01:32:30,660 --> 01:32:31,220
 OK.

1397
01:32:31,220 --> 01:32:34,580
 So this height should be proportional to the distance.

1398
01:32:34,580 --> 01:32:37,140
 You just get 1.288, right?

1399
01:32:37,140 --> 01:32:41,980
 S5, S. So after the merging of these two clusters, S5, S6,

1400
01:32:41,980 --> 01:32:47,620
 we obtain actually five clusters, S1, S2, S3, S4,

1401
01:32:47,620 --> 01:32:49,379
 and S5, S6.

1402
01:32:49,379 --> 01:32:52,179
 We have five clusters.

1403
01:32:52,179 --> 01:32:52,780
 OK.

1404
01:32:52,780 --> 01:32:54,780
 And then we should repeat the process, right?

1405
01:32:54,780 --> 01:32:57,660
 Among the five clusters, so which two clusters

1406
01:32:57,660 --> 01:32:58,900
 are the most similar?

1407
01:32:58,900 --> 01:33:00,900
 Then we should merge them, right?

1408
01:33:00,900 --> 01:33:03,139
 Then how do you evaluate which cluster is the most similar?

1409
01:33:03,139 --> 01:33:05,299
 Then we look at the distance.

1410
01:33:05,299 --> 01:33:06,540
 The distance.

1411
01:33:06,540 --> 01:33:08,459
 But then one issue comes, right?

1412
01:33:08,459 --> 01:33:10,019
 If one sample to another sample,

1413
01:33:10,060 --> 01:33:12,180
 we can easily calculate the distance.

1414
01:33:12,180 --> 01:33:15,380
 But now you can imagine the cluster.

1415
01:33:15,380 --> 01:33:18,140
 Even each cluster contains more than two samples,

1416
01:33:18,140 --> 01:33:19,540
 more than one sample, right?

1417
01:33:19,540 --> 01:33:21,820
 How to calculate the distance between two clusters?

1418
01:33:21,820 --> 01:33:24,700
 That's an issue.

1419
01:33:24,700 --> 01:33:27,500
 So after the first step, each cluster just

1420
01:33:27,500 --> 01:33:28,180
 has one sample.

1421
01:33:28,180 --> 01:33:29,460
 We don't have this issue.

1422
01:33:29,460 --> 01:33:31,660
 But after the first step, actually, we

1423
01:33:31,660 --> 01:33:35,460
 have one cluster that has two samples.

1424
01:33:35,460 --> 01:33:39,140
 So how to calculate the distance from cluster S1 to cluster S5,

1425
01:33:39,140 --> 01:33:40,260
 S6.

1426
01:33:40,260 --> 01:33:43,500
 S5, S6 have two samples.

1427
01:33:43,500 --> 01:33:47,540
 How to calculate the distance between these?

1428
01:33:47,540 --> 01:33:52,460
 And actually, we have a different definition here.

1429
01:33:52,460 --> 01:33:57,980
 And here, actually, we have the distance between two clusters,

1430
01:33:57,980 --> 01:33:58,700
 right?

1431
01:33:58,700 --> 01:34:02,180
 And here we have the so-called single linkage method.

1432
01:34:02,180 --> 01:34:06,540
 Single linkage for the distance between two clusters.

1433
01:34:06,540 --> 01:34:10,700
 Each cluster contains more than one sample.

1434
01:34:10,700 --> 01:34:13,620
 So we look at this.

1435
01:34:13,620 --> 01:34:15,940
 So there are two clusters, right?

1436
01:34:15,940 --> 01:34:17,580
 Sphereship cluster.

1437
01:34:17,580 --> 01:34:20,460
 Each of the cluster has multiple samples.

1438
01:34:20,460 --> 01:34:23,060
 How to calculate the distance?

1439
01:34:23,060 --> 01:34:27,340
 How to define this distance between the two clusters?

1440
01:34:27,340 --> 01:34:31,980
 In the single linkage method, the two nearest samples

1441
01:34:31,980 --> 01:34:34,140
 in the two cluster, right?

1442
01:34:34,140 --> 01:34:37,740
 So these two samples are the nearest sample

1443
01:34:37,740 --> 01:34:41,500
 in the two clusters, nearest between cluster, right?

1444
01:34:41,500 --> 01:34:43,420
 Between cluster, not within cluster.

1445
01:34:43,420 --> 01:34:48,660
 Between cluster, these are the two nearest samples.

1446
01:34:48,660 --> 01:34:53,300
 Then this distance between these two nearest samples

1447
01:34:53,300 --> 01:34:57,700
 is defined as the distance between these two clusters.

1448
01:34:57,700 --> 01:34:59,620
 So this is a single linkage.

1449
01:34:59,620 --> 01:35:01,180
 Single linkage.

1450
01:35:01,180 --> 01:35:03,500
 Of course, actually, you can have a different definition,

1451
01:35:04,260 --> 01:35:04,980
 right?

1452
01:35:04,980 --> 01:35:09,740
 So that is the other kind of linkage.

1453
01:35:09,740 --> 01:35:11,780
 But in the single linkage method,

1454
01:35:11,780 --> 01:35:18,220
 the nearest samples is defined as the distance.

1455
01:35:18,220 --> 01:35:20,220
 The distance between the two nearest samples

1456
01:35:20,220 --> 01:35:24,500
 is defined as the distance between the two clusters.

1457
01:35:24,500 --> 01:35:25,140
 OK.

1458
01:35:25,140 --> 01:35:27,940
 So now if we go back to the original, right,

1459
01:35:27,940 --> 01:35:33,420
 to the symmetric, now S5, S6 are one cluster.

1460
01:35:33,420 --> 01:35:37,700
 If you think about distance from cluster S1 to cluster S5,

1461
01:35:37,700 --> 01:35:41,340
 S6, S5, S6 contain two samples, right?

1462
01:35:41,340 --> 01:35:46,300
 So we can see S1 to S5 distance is 3.263.

1463
01:35:46,300 --> 01:35:49,620
 Distance from S1 to S6 is 2.651.

1464
01:35:49,620 --> 01:35:53,460
 So certainly, S1, this sample, S6,

1465
01:35:53,460 --> 01:35:58,540
 these are the two nearest samples in the two clusters.

1466
01:35:58,540 --> 01:36:00,500
 Then this distance should be defined

1467
01:36:00,500 --> 01:36:05,980
 as the distance between cluster S1 and cluster S5, S6.

1468
01:36:05,980 --> 01:36:08,780
 This method is the single linkage method, right?

1469
01:36:08,780 --> 01:36:10,620
 So based on the single linkage method,

1470
01:36:10,620 --> 01:36:15,540
 the distance S1 to S5, S5, S6 is 2.651.

1471
01:36:18,420 --> 01:36:19,020
 OK.

1472
01:36:19,020 --> 01:36:21,020
 So this is the distance.

1473
01:36:21,020 --> 01:36:24,980
 So now actually we see the single linkage, right?

1474
01:36:24,980 --> 01:36:29,260
 So for example, we think about the S3 and S5, S6, right?

1475
01:36:30,260 --> 01:36:36,140
 Actually, we look at the distance between S3 and S6, right?

1476
01:36:36,140 --> 01:36:37,980
 Distance between S3 and S5, then we

1477
01:36:37,980 --> 01:36:39,580
 look at the minimum value, right?

1478
01:36:39,580 --> 01:36:40,380
 Minimum value.

1479
01:36:40,380 --> 01:36:42,380
 So this is 3.111.

1480
01:36:42,380 --> 01:36:46,940
 1.311 is considered the distance between the cluster S3

1481
01:36:46,940 --> 01:36:50,260
 and the cluster S5, 6.

1482
01:36:50,260 --> 01:36:50,940
 OK.

1483
01:36:50,940 --> 01:36:53,460
 So the distance between two nearest samples

1484
01:36:53,460 --> 01:36:57,900
 in the two clusters is defined as the distance

1485
01:36:57,900 --> 01:36:59,220
 between clusters.

1486
01:36:59,260 --> 01:37:02,340
 If this is a case, my single linkage is used.

1487
01:37:02,340 --> 01:37:03,820
 Single linkage.

1488
01:37:03,820 --> 01:37:06,660
 So besides this, actually, definition

1489
01:37:06,660 --> 01:37:09,620
 of the distance between clusters,

1490
01:37:09,620 --> 01:37:12,540
 we have another distance definition.

1491
01:37:12,540 --> 01:37:15,020
 So this is called complete linkage.

1492
01:37:15,020 --> 01:37:19,420
 Complete linkage is the furthest neighbor.

1493
01:37:19,420 --> 01:37:23,020
 I think, yeah, so I think this looks at that, right?

1494
01:37:23,020 --> 01:37:25,620
 So these are the sample in the two clusters.

1495
01:37:25,620 --> 01:37:28,260
 And then we look at them between the two clusters.

1496
01:37:28,260 --> 01:37:33,380
 Which two samples are the furthest, right?

1497
01:37:33,380 --> 01:37:35,580
 So then this distance is considered

1498
01:37:35,580 --> 01:37:39,500
 the distance between two clusters.

1499
01:37:39,500 --> 01:37:41,900
 Just different measures, different measures.

1500
01:37:41,900 --> 01:37:44,540
 Sometimes, actually, we are very conservative, right?

1501
01:37:44,540 --> 01:37:45,820
 Or you say they are very similar.

1502
01:37:45,820 --> 01:37:47,180
 No, I want to look at the nearest.

1503
01:37:50,900 --> 01:37:53,140
 Sometimes, the furthest distance should be considered

1504
01:37:53,140 --> 01:37:54,300
 the similar distance.

1505
01:37:55,300 --> 01:37:57,660
 Right?

1506
01:37:57,660 --> 01:38:02,020
 So yeah, sometimes you may evaluate something.

1507
01:38:02,020 --> 01:38:04,340
 Maybe you say, OK, how about this student?

1508
01:38:04,340 --> 01:38:04,940
 He's very good.

1509
01:38:04,940 --> 01:38:08,180
 He's one of the course, he's known as like a hundred marks.

1510
01:38:08,180 --> 01:38:11,740
 Sometimes, actually, you just based on his lowest result

1511
01:38:11,740 --> 01:38:15,420
 to judge a student, right?

1512
01:38:15,420 --> 01:38:16,540
 Although, no, this is not right.

1513
01:38:16,540 --> 01:38:18,660
 We should not judge a person based on this, right?

1514
01:38:18,660 --> 01:38:21,060
 But just this example, right?

1515
01:38:21,060 --> 01:38:23,940
 So he's not that good.

1516
01:38:23,940 --> 01:38:25,940
 He's called Jessica 16 marks.

1517
01:38:25,940 --> 01:38:29,339
 You're based on the worst result to judge a person.

1518
01:38:29,339 --> 01:38:33,139
 So this is just a different kind of method, right?

1519
01:38:33,139 --> 01:38:36,179
 But they are all reasonable method.

1520
01:38:36,179 --> 01:38:39,219
 So this is a method called complete linkage.

1521
01:38:39,219 --> 01:38:42,259
 Complete linkage.

1522
01:38:42,259 --> 01:38:44,460
 And then if a complete linkage is used,

1523
01:38:44,460 --> 01:38:47,299
 then again, we look at the example S3, S5, S6, right?

1524
01:38:47,299 --> 01:38:50,219
 Then we should look at the maximum distance.

1525
01:38:50,220 --> 01:38:55,220
 And then the distance is 1.3165.

1526
01:38:55,220 --> 01:38:57,220
 OK.

1527
01:38:57,220 --> 01:39:01,060
 And then in the next week, when we talk about the evaluation

1528
01:39:01,060 --> 01:39:04,060
 of the clusters, and basically the cluster evaluation

1529
01:39:04,060 --> 01:39:05,980
 is also based on the concept, right?

1530
01:39:05,980 --> 01:39:10,380
 Within cluster distance and between cluster distance.

1531
01:39:10,380 --> 01:39:10,940
 OK.

1532
01:39:10,940 --> 01:39:14,940
 But actually, we have a few evaluation metrics

1533
01:39:14,940 --> 01:39:17,180
 for cluster evaluation.

1534
01:39:17,180 --> 01:39:20,620
 And all the evaluation criteria is based on the concept,

1535
01:39:20,620 --> 01:39:25,100
 like within cluster similarity, between cluster similarity.

1536
01:39:25,100 --> 01:39:27,540
 And if we use the distance, how you

1537
01:39:27,540 --> 01:39:32,020
 measure the between cluster distance?

1538
01:39:32,020 --> 01:39:33,540
 Just like the same case, linkers,

1539
01:39:33,540 --> 01:39:35,500
 the different kind of linkage.

1540
01:39:35,500 --> 01:39:37,700
 We have different ways to evaluate the cluster

1541
01:39:37,700 --> 01:39:39,180
 within cluster distance.

1542
01:39:39,180 --> 01:39:42,500
 We also have different ways to evaluate the between cluster

1543
01:39:42,500 --> 01:39:44,140
 distance.

1544
01:39:44,140 --> 01:39:46,420
 And then we combine these two.

1545
01:39:46,420 --> 01:39:48,060
 Then we can have a different metrics

1546
01:39:48,060 --> 01:39:50,420
 for the evaluation of the cluster result.

1547
01:39:50,420 --> 01:39:50,620
 OK.

1548
01:39:50,620 --> 01:39:52,340
 So these are all reasonable, right?

1549
01:39:52,340 --> 01:39:53,620
 We just have a different measure.

1550
01:39:53,620 --> 01:39:56,580
 Different measures from different perspectives.

1551
01:39:56,580 --> 01:39:57,900
 They are all reasonable measures.

1552
01:39:57,900 --> 01:40:00,060
 They are all correct.

1553
01:40:00,060 --> 01:40:00,700
 OK.

1554
01:40:00,700 --> 01:40:04,860
 So here, this is the complete linkage.

1555
01:40:04,860 --> 01:40:08,700
 And then, next actually, it's called the central linkage.

1556
01:40:08,700 --> 01:40:10,220
 Central linkage.

1557
01:40:10,220 --> 01:40:13,500
 So just now, we see, oh, you think by the distance, right?

1558
01:40:13,500 --> 01:40:16,060
 So actually, we know the centroid actually

1559
01:40:16,060 --> 01:40:18,140
 is a representative, right?

1560
01:40:18,140 --> 01:40:21,300
 It's a representative of all the sample in each cluster.

1561
01:40:21,300 --> 01:40:25,300
 So why not use the distance between the two centroid

1562
01:40:25,300 --> 01:40:28,100
 as the distance of the between the two clusters?

1563
01:40:28,100 --> 01:40:30,020
 Yes, I think that's reasonable, right?

1564
01:40:30,020 --> 01:40:32,220
 So that is just the central linkage.

1565
01:40:34,900 --> 01:40:35,580
 OK.

1566
01:40:35,580 --> 01:40:38,340
 We use the distance between the two cluster centers.

1567
01:40:43,540 --> 01:40:46,260
 So from this code, from here, you can also see there

1568
01:40:46,260 --> 01:40:47,340
 is nothing to see.

1569
01:40:47,340 --> 01:40:48,180
 Oh, these are correct.

1570
01:40:48,180 --> 01:40:49,260
 Other, not correct.

1571
01:40:49,260 --> 01:40:51,060
 We cannot see that, right?

1572
01:40:51,060 --> 01:40:54,500
 We have just have different ways to evaluate, different ways

1573
01:40:54,500 --> 01:40:55,980
 to describe.

1574
01:40:55,980 --> 01:40:57,340
 All correct, right?

1575
01:40:57,340 --> 01:40:59,540
 So these are all very reasonable, right?

1576
01:40:59,540 --> 01:41:02,980
 You look at the distance between the two cluster centers.

1577
01:41:02,980 --> 01:41:03,820
 OK.

1578
01:41:03,820 --> 01:41:06,820
 So this is called central linkage.

1579
01:41:06,820 --> 01:41:08,780
 Central linkage.

1580
01:41:08,780 --> 01:41:10,660
 And then, it's such a scenario.

1581
01:41:10,660 --> 01:41:12,700
 And then, after you do the cluster, right,

1582
01:41:12,700 --> 01:41:14,780
 you merge the cluster, for example.

1583
01:41:14,780 --> 01:41:18,940
 Then you need to look at the distance, the centroid

1584
01:41:18,940 --> 01:41:20,420
 of each of the cluster.

1585
01:41:20,420 --> 01:41:23,420
 Then, based on the centroid, you calculate the distance.

1586
01:41:23,420 --> 01:41:29,820
 And for example, I'm doing here, and we have no S5, S6, right?

1587
01:41:29,820 --> 01:41:32,740
 Then when the two clusters or two samples

1588
01:41:32,740 --> 01:41:35,860
 are merged into one single cluster,

1589
01:41:35,860 --> 01:41:37,580
 then what is the centroid?

1590
01:41:37,580 --> 01:41:42,139
 Centroid is just the average of the two clusters, right?

1591
01:41:42,140 --> 01:41:43,980
 Or the two samples.

1592
01:41:43,980 --> 01:41:45,580
 Just the mean vector.

1593
01:41:45,580 --> 01:41:48,460
 The summation of the two vector divided by 2,

1594
01:41:48,460 --> 01:41:50,740
 then we can get this C S5, right?

1595
01:41:50,740 --> 01:41:54,980
 So this is the central of the cluster, S5, S6.

1596
01:41:54,980 --> 01:42:00,300
 And then, we can use the distance between S5 and S6.

1597
01:42:00,300 --> 01:42:01,660
 Basically, I use this to.

1598
01:42:12,140 --> 01:42:32,820
 So this is the transport.

1599
01:42:32,820 --> 01:42:35,940
 I want to actually use this as a transport.

1600
01:42:35,940 --> 01:42:38,340
 It's an inner product of the two vectors, right?

1601
01:42:38,340 --> 01:42:40,940
 So these are just the inner product of the two vectors.

1602
01:42:40,940 --> 01:42:44,700
 And then transport.

1603
01:42:44,700 --> 01:42:47,740
 Then we have this metric, the distance between the two vectors.

1604
01:42:47,740 --> 01:42:49,099
 So this is just a calculator, the distance

1605
01:42:49,099 --> 01:42:50,540
 between the two vectors, right?

1606
01:42:50,540 --> 01:42:53,419
 The inner product of the two vectors, right?

1607
01:42:53,419 --> 01:42:59,900
 So this is the distance between the cluster S3 and S5,

1608
01:42:59,900 --> 01:43:00,620
 6.

1609
01:43:00,620 --> 01:43:01,860
 This is a central of this method, right?

1610
01:43:01,860 --> 01:43:05,940
 We use the C S5, 6.

1611
01:43:05,940 --> 01:43:06,940
 So this is the.

1612
01:43:14,379 --> 01:43:17,580
 Then the next method is called the average linkage.

1613
01:43:17,580 --> 01:43:18,500
 Every linkage.

1614
01:43:18,500 --> 01:43:20,620
 What's the basic idea of the average linkage?

1615
01:43:20,620 --> 01:43:22,339
 In the average linkage, right?

1616
01:43:22,339 --> 01:43:25,820
 Actually, we specifically calculate the pairwise distance

1617
01:43:25,820 --> 01:43:26,740
 from the.

1618
01:43:32,099 --> 01:43:33,500
 We use this diagram, right?

1619
01:43:33,940 --> 01:43:37,220
 For the sample in this cluster, we calculate the distance

1620
01:43:37,220 --> 01:43:41,460
 from all the samples in another cluster, right?

1621
01:43:41,460 --> 01:43:44,100
 We calculate the pairwise distance between the two

1622
01:43:44,100 --> 01:43:45,180
 clusters, right?

1623
01:43:45,180 --> 01:43:46,740
 Finally, we take the average.

1624
01:43:46,740 --> 01:43:51,020
 So this average is used at distance between the two clusters.

1625
01:43:51,020 --> 01:43:53,260
 So this is also a reasonable measure, right?

1626
01:43:53,260 --> 01:43:55,140
 For the distance between the two clusters.

1627
01:43:55,140 --> 01:43:57,820
 So this kind of method is called average linkage.

1628
01:44:01,020 --> 01:44:02,060
 Every linkage.

1629
01:44:02,100 --> 01:44:06,140
 So in the average linkage, and the average,

1630
01:44:06,140 --> 01:44:10,140
 we calculate the pairwise distance between the sample

1631
01:44:10,140 --> 01:44:11,740
 in the two cluster, right?

1632
01:44:11,740 --> 01:44:13,780
 Then we take the average distance.

1633
01:44:13,780 --> 01:44:15,580
 So the average distance is considered

1634
01:44:15,580 --> 01:44:17,580
 the distance between the two clusters.

1635
01:44:17,580 --> 01:44:20,180
 So this kind of linkage is called average linkage.

1636
01:44:26,860 --> 01:44:30,700
 So in the above example, and actually the distance from S3

1637
01:44:30,740 --> 01:44:31,940
 to S5, right?

1638
01:44:31,940 --> 01:44:34,980
 Because in class S3, just one sample, right?

1639
01:44:34,980 --> 01:44:37,019
 And the class 5, 6 has two samples.

1640
01:44:37,019 --> 01:44:39,179
 So we have a pairwise of two pairs, right?

1641
01:44:39,179 --> 01:44:42,580
 S3 to S5, S3 to S6, OK?

1642
01:44:42,580 --> 01:44:43,980
 Then we have this distance.

1643
01:44:43,980 --> 01:44:48,059
 Then we take the average of the two, right?

1644
01:44:48,059 --> 01:44:49,860
 Summation divided by two.

1645
01:44:49,860 --> 01:44:52,059
 This distance is defined as distance between these two

1646
01:44:52,059 --> 01:44:53,179
 clusters.

1647
01:44:53,179 --> 01:44:57,059
 So this is called the average linkage.

1648
01:44:57,059 --> 01:44:57,660
 OK.

1649
01:44:57,660 --> 01:44:59,740
 So we have different linkage methods.

1650
01:44:59,740 --> 01:45:07,500
 And then actually, we have different metrics

1651
01:45:07,500 --> 01:45:13,300
 to evaluate the similarity between actually the two

1652
01:45:13,300 --> 01:45:14,820
 clusters.

1653
01:45:14,820 --> 01:45:16,540
 OK.

1654
01:45:16,540 --> 01:45:17,179
 OK.

1655
01:45:17,179 --> 01:45:20,260
 So now, as you might say, the single linkage is used, right?

1656
01:45:20,260 --> 01:45:25,340
 If a single linkage is used, then actually,

1657
01:45:25,340 --> 01:45:26,860
 after merging, right?

1658
01:45:26,860 --> 01:45:29,620
 After merging, we have five clusters.

1659
01:45:29,620 --> 01:45:33,099
 And then we need to list these metrics,

1660
01:45:33,099 --> 01:45:34,380
 similarity metrics.

1661
01:45:34,380 --> 01:45:39,740
 This is the similarity between the five clusters.

1662
01:45:39,740 --> 01:45:42,340
 And initially, it's the distance between the six clusters,

1663
01:45:42,340 --> 01:45:43,140
 right?

1664
01:45:43,140 --> 01:45:45,460
 Actually, it's between six samples, actually.

1665
01:45:45,460 --> 01:45:48,740
 But now, it's the distance between the five clusters.

1666
01:45:48,740 --> 01:45:49,180
 OK.

1667
01:45:49,180 --> 01:45:52,500
 So from each of them, S1 to S2, right?

1668
01:45:52,500 --> 01:45:53,940
 We have no change, right?

1669
01:45:53,940 --> 01:45:55,940
 But S1 to S5, 6.

1670
01:45:55,940 --> 01:45:58,340
 Because S1 to S5, 6 have two samples.

1671
01:45:58,340 --> 01:46:01,460
 Then we, based on the single linkage method,

1672
01:46:01,460 --> 01:46:03,140
 we look at the nearest sample, right?

1673
01:46:03,140 --> 01:46:04,460
 Then we get the distance.

1674
01:46:04,460 --> 01:46:05,020
 OK.

1675
01:46:05,020 --> 01:46:07,940
 So we get these S5 by 5 metrics.

1676
01:46:07,940 --> 01:46:08,980
 And then the next step, actually,

1677
01:46:08,980 --> 01:46:12,220
 we need to identify which value is the minimum value, right?

1678
01:46:12,220 --> 01:46:15,780
 That means these two clusters are more similar.

1679
01:46:15,780 --> 01:46:18,060
 So from this, actually, a metric,

1680
01:46:18,060 --> 01:46:25,380
 we can identify the minimum value is 1.28, right?

1681
01:46:25,380 --> 01:46:27,900
 1.294.

1682
01:46:28,219 --> 01:46:32,500
 1.290, actually.

1683
01:46:32,500 --> 01:46:34,900
 1.290 is the minimum value, right?

1684
01:46:34,900 --> 01:46:35,500
 Minimum value.

1685
01:46:35,500 --> 01:46:38,139
 Actually, this is the distance between S4 and S1,

1686
01:46:38,139 --> 01:46:40,219
 or S1 and S4.

1687
01:46:40,219 --> 01:46:41,860
 So in the next step, actually, we

1688
01:46:41,860 --> 01:46:46,500
 should merge these two clusters, S1, S4.

1689
01:46:46,500 --> 01:46:49,019
 So every step, we merge the two clusters,

1690
01:46:49,019 --> 01:46:52,099
 the two more similar clusters, right?

1691
01:46:52,099 --> 01:46:56,179
 So from here, we identify, actually, S1, S4

1692
01:46:56,180 --> 01:46:57,820
 have the minimum distance.

1693
01:46:57,820 --> 01:46:59,500
 And then we should merge them.

1694
01:46:59,500 --> 01:47:05,100
 And then we merge them and name the new cluster S1, 5 and S1, 4.

1695
01:47:05,100 --> 01:47:05,780
 OK.

1696
01:47:05,780 --> 01:47:08,100
 So then we have this.

1697
01:47:08,100 --> 01:47:12,220
 You also sketch this dendrogram.

1698
01:47:12,220 --> 01:47:14,380
 And actually, you look at the height, right?

1699
01:47:14,380 --> 01:47:16,740
 So this height S1, 4 is certainly

1700
01:47:16,740 --> 01:47:19,700
 no higher than the S5, 6.

1701
01:47:19,700 --> 01:47:21,700
 So this means that the distance, actually,

1702
01:47:21,700 --> 01:47:27,660
 between S1 and 4 is larger than distance between S5 and 6.

1703
01:47:27,660 --> 01:47:27,940
 OK.

1704
01:47:27,940 --> 01:47:30,820
 So you must draw this, actually, in a proportion

1705
01:47:30,820 --> 01:47:34,660
 to the distance between the clusters.

1706
01:47:34,660 --> 01:47:35,179
 OK.

1707
01:47:35,179 --> 01:47:36,940
 So these are the two steps.

1708
01:47:36,940 --> 01:47:42,139
 And then we have, actually, up to the four clusters, S1, 4,

1709
01:47:42,139 --> 01:47:45,940
 S5, 6, and then S2, S3.

1710
01:47:46,940 --> 01:47:49,419
 OK.

1711
01:47:49,419 --> 01:47:53,059
 And then, actually, we need to perform another merging,

1712
01:47:53,059 --> 01:47:53,700
 right?

1713
01:47:53,700 --> 01:47:54,740
 Another run, actually.

1714
01:47:54,740 --> 01:48:01,820
 This run should be merge the two more similar clusters.

1715
01:48:01,820 --> 01:48:05,860
 Before that, certainly, we need to have these similarity

1716
01:48:05,860 --> 01:48:07,219
 metrics, right?

1717
01:48:07,219 --> 01:48:09,259
 And between the four clusters.

1718
01:48:09,259 --> 01:48:09,900
 OK.

1719
01:48:09,900 --> 01:48:13,900
 So this is the similarity metrics between the four

1720
01:48:13,900 --> 01:48:15,900
 clusters.

1721
01:48:16,339 --> 01:48:19,740
 And if the single link is used or complete link is used,

1722
01:48:19,740 --> 01:48:22,099
 actually, you don't need to do calculation, right?

1723
01:48:22,099 --> 01:48:24,179
 You just need to inspect which value is bigger, which

1724
01:48:24,179 --> 01:48:25,460
 value is small, right?

1725
01:48:25,460 --> 01:48:27,299
 Then you can get these metrics.

1726
01:48:27,299 --> 01:48:27,540
 OK.

1727
01:48:27,540 --> 01:48:29,500
 Because we don't need to carry the centroid.

1728
01:48:29,500 --> 01:48:32,379
 We don't need to carry the average.

1729
01:48:32,379 --> 01:48:33,420
 OK.

1730
01:48:33,420 --> 01:48:33,920
 OK.

1731
01:48:33,920 --> 01:48:37,059
 So now, we look at the similarity metrics

1732
01:48:37,059 --> 01:48:39,860
 between the four clusters.

1733
01:48:39,860 --> 01:48:45,860
 And then we see, actually, 1.3.

1734
01:48:45,900 --> 01:48:48,620
 1.311 is the minimum value, right?

1735
01:48:48,620 --> 01:48:55,019
 And actually, the 1.311 is the distance between S3 and S5,

1736
01:48:55,019 --> 01:48:55,940
 6.

1737
01:48:55,940 --> 01:48:56,179
 OK.

1738
01:48:56,179 --> 01:48:59,460
 So the next step, we should merge S3 and S5, 6.

1739
01:48:59,460 --> 01:49:02,540
 Because this distance is the minimum.

1740
01:49:02,540 --> 01:49:03,339
 OK.

1741
01:49:03,339 --> 01:49:03,980
 Then we merge.

1742
01:49:07,780 --> 01:49:10,099
 S3 and S5, 6.

1743
01:49:10,099 --> 01:49:15,540
 But the height here should be proportional to the distance.

1744
01:49:15,540 --> 01:49:20,420
 The distance here, actually, is 1.311.

1745
01:49:20,420 --> 01:49:21,580
 So we draw this.

1746
01:49:21,580 --> 01:49:26,180
 And then we name the new cluster as 356.

1747
01:49:26,180 --> 01:49:27,780
 OK.

1748
01:49:27,780 --> 01:49:30,660
 So up this step, we have three clusters, right?

1749
01:49:30,660 --> 01:49:34,100
 Again, we need to obtain the distance

1750
01:49:34,100 --> 01:49:35,940
 metric for the three clusters.

1751
01:49:35,940 --> 01:49:40,580
 S14, S2, and S356, the three clusters.

1752
01:49:43,220 --> 01:49:43,540
 OK.

1753
01:49:43,540 --> 01:49:46,220
 So this is the distance metric.

1754
01:49:46,220 --> 01:49:47,060
 OK.

1755
01:49:47,060 --> 01:49:49,660
 And then we look at the smallest value, right?

1756
01:49:49,660 --> 01:49:51,060
 2.29.

1757
01:49:51,060 --> 01:49:52,060
 2.29.

1758
01:49:52,060 --> 01:49:56,300
 So that's the distance between S14 and S356.

1759
01:49:56,300 --> 01:49:57,540
 Then we should merge them.

1760
01:50:00,220 --> 01:50:00,820
 OK.

1761
01:50:00,820 --> 01:50:04,740
 So S14 and S356 are merged.

1762
01:50:04,740 --> 01:50:05,740
 OK.

1763
01:50:05,740 --> 01:50:07,460
 And after this step, actually, we just

1764
01:50:07,460 --> 01:50:09,660
 have two clusters, right?

1765
01:50:09,660 --> 01:50:11,420
 Certainly, we should merge them.

1766
01:50:11,420 --> 01:50:12,060
 OK.

1767
01:50:12,060 --> 01:50:15,540
 And here, we still need to get the distance metric.

1768
01:50:15,540 --> 01:50:20,980
 Because we need to know the distance between S2 and S13456.

1769
01:50:20,980 --> 01:50:23,100
 Because we need to know the height should be proportional

1770
01:50:23,100 --> 01:50:25,900
 to the distance between the two clusters.

1771
01:50:25,900 --> 01:50:26,500
 OK.

1772
01:50:26,500 --> 01:50:29,460
 So after this, we still need to get this metric.

1773
01:50:32,460 --> 01:50:34,820
 The distance metric is between the two clusters.

1774
01:50:34,820 --> 01:50:44,059
 Actually, we know this is 2.327, right?

1775
01:50:44,059 --> 01:50:47,219
 So then, actually, when we get the dendrogram,

1776
01:50:47,219 --> 01:50:50,340
 the height should be proportional to this.

1777
01:50:50,340 --> 01:50:52,059
 OK.

1778
01:50:52,059 --> 01:50:54,059
 And finally, we have the result.

1779
01:50:54,059 --> 01:50:57,099
 All the data points are merged into one single cluster.

1780
01:50:57,099 --> 01:51:00,980
 So that is as 1, 2, 3, 4, 5, 6.

1781
01:51:00,980 --> 01:51:02,420
 So this is a new cluster.

1782
01:51:02,420 --> 01:51:04,660
 So this is the process.

1783
01:51:04,860 --> 01:51:06,300
 From bottom, right?

1784
01:51:06,300 --> 01:51:08,900
 From each single sample.

1785
01:51:08,900 --> 01:51:12,300
 Of course, each sample is considered as a cluster, right?

1786
01:51:12,300 --> 01:51:14,700
 This sample is considered as a cluster.

1787
01:51:14,700 --> 01:51:18,900
 Then we progressively merge two clusters at each step.

1788
01:51:18,900 --> 01:51:24,059
 Finally, all the clusters are merged into one single cluster.

1789
01:51:24,059 --> 01:51:24,660
 OK.

1790
01:51:24,660 --> 01:51:29,620
 Then the whole process is completed.

1791
01:51:29,620 --> 01:51:35,220
 So this is a hierarchical tree, right?

1792
01:51:35,220 --> 01:51:38,140
 The agglomerative, actually, in the augurant.

1793
01:51:38,140 --> 01:51:41,940
 And for the construction of this hierarchical tree,

1794
01:51:41,940 --> 01:51:43,340
 classifying tree.

1795
01:51:43,340 --> 01:51:45,059
 Sorry.

1796
01:51:45,059 --> 01:51:46,820
 No, classifying tree, right?

1797
01:51:46,820 --> 01:51:49,340
 The classifying tree.

1798
01:51:49,340 --> 01:51:51,140
 And I already mentioned it, right?

1799
01:51:51,140 --> 01:51:55,980
 So this tree, distance tree, give us a good indication,

1800
01:51:56,740 --> 01:51:59,700
 visualization of the similarity between the samples.

1801
01:52:03,059 --> 01:52:06,740
 No matter what the denominator of the sample,

1802
01:52:06,740 --> 01:52:08,900
 even a high-demonar, we don't care.

1803
01:52:08,900 --> 01:52:11,459
 Because here we just show the distance between them.

1804
01:52:11,459 --> 01:52:13,860
 The distance between them is a scalar.

1805
01:52:13,860 --> 01:52:17,620
 We don't need to show the original adrenal feature space.

1806
01:52:17,620 --> 01:52:22,099
 We can still visualize the similarity between the samples.

1807
01:52:23,100 --> 01:52:25,740
 OK.

1808
01:52:25,740 --> 01:52:28,380
 So this diagram shows another scenario, right?

1809
01:52:28,380 --> 01:52:32,460
 And the tree we see, S5, S6, S3, right?

1810
01:52:32,460 --> 01:52:35,700
 So S5, S6, merge, right?

1811
01:52:35,700 --> 01:52:39,860
 As then merge with sample 3, they have a cluster.

1812
01:52:39,860 --> 01:52:42,140
 Then the distance between them is 1.4.

1813
01:52:42,140 --> 01:52:42,780
 1.3.

1814
01:52:42,780 --> 01:52:45,220
 This is a relatively small value, right?

1815
01:52:45,220 --> 01:52:47,540
 And then another scenario, S1, S4,

1816
01:52:47,540 --> 01:52:50,340
 is merged into one single cluster, right?

1817
01:52:50,340 --> 01:52:51,980
 And then, OK.

1818
01:52:52,019 --> 01:52:55,259
 So then when these two clusters are merged together,

1819
01:52:55,259 --> 01:52:58,620
 you look at this, the distance, right?

1820
01:52:58,620 --> 01:53:02,179
 Changed from 1.3 to 2.2.

1821
01:53:02,179 --> 01:53:08,780
 This means that the cluster 3, S5, this cluster, and 1.4,

1822
01:53:08,780 --> 01:53:11,820
 this cluster, are very different.

1823
01:53:11,820 --> 01:53:14,099
 Because the distance is very big, right?

1824
01:53:14,099 --> 01:53:15,740
 You can see it's very high, right?

1825
01:53:15,740 --> 01:53:16,339
 They're very high.

1826
01:53:16,339 --> 01:53:19,860
 This means that they are very different.

1827
01:53:19,860 --> 01:53:20,459
 OK.

1828
01:53:20,459 --> 01:53:21,379
 Very different, actually.

1829
01:53:21,380 --> 01:53:24,060
 Then, actually, when you see when you merge them, right?

1830
01:53:24,060 --> 01:53:29,900
 So this means that when we see the natural grouping, right?

1831
01:53:29,900 --> 01:53:32,340
 From this diagram, roughly we can judge,

1832
01:53:32,340 --> 01:53:38,620
 at least we see S1, S4, and S5, S6, S3 should not be one cluster.

1833
01:53:38,620 --> 01:53:39,900
 Although in the hierarchical cluster,

1834
01:53:39,900 --> 01:53:42,340
 we're finding all of them merged together, right?

1835
01:53:42,340 --> 01:53:43,980
 But if you view from here, actually,

1836
01:53:43,980 --> 01:53:46,980
 diagram, you can roughly judge how many clusters

1837
01:53:46,980 --> 01:53:49,460
 underlying the data.

1838
01:53:49,500 --> 01:53:51,420
 3, 5, 6, very similar.

1839
01:53:51,420 --> 01:53:53,180
 We can put them into one cluster.

1840
01:53:53,180 --> 01:53:54,580
 1.4, very similar.

1841
01:53:54,580 --> 01:53:57,020
 Can put in the cluster, right?

1842
01:53:57,020 --> 01:53:57,820
 OK.

1843
01:53:57,820 --> 01:54:02,180
 And then, actually, 2 maybe can put into another cluster.

1844
01:54:05,060 --> 01:54:07,420
 So this, actually, is a data diagram.

1845
01:54:07,420 --> 01:54:09,980
 It shows a lot of information to us, right?

1846
01:54:09,980 --> 01:54:12,020
 And actually, in my lab, actually,

1847
01:54:12,020 --> 01:54:14,820
 we can use this function.

1848
01:54:14,820 --> 01:54:16,660
 Actually, I don't require you to know my lab.

1849
01:54:16,660 --> 01:54:20,380
 I just want to show you that it is not a software package,

1850
01:54:20,380 --> 01:54:21,059
 right?

1851
01:54:21,059 --> 01:54:23,460
 And we can use some of the functions

1852
01:54:23,460 --> 01:54:27,460
 and to construct this hierarchical tree, right?

1853
01:54:27,460 --> 01:54:30,019
 And so we use linkage, this function.

1854
01:54:30,019 --> 01:54:31,780
 Then we have the data.

1855
01:54:31,780 --> 01:54:34,580
 Then we have the single linkage or complete linkage

1856
01:54:34,580 --> 01:54:37,340
 or we need to specify what linkage we use, right?

1857
01:54:37,340 --> 01:54:40,139
 Because this will determine the distance between clusters.

1858
01:54:40,139 --> 01:54:42,019
 And then the next distance.

1859
01:54:42,019 --> 01:54:43,139
 What distance we should use?

1860
01:54:43,139 --> 01:54:46,300
 City block, monobis, Kuna similarity,

1861
01:54:46,300 --> 01:54:49,980
 all the, you know, the, the, the, the, including distance.

1862
01:54:49,980 --> 01:54:50,140
 OK.

1863
01:54:50,140 --> 01:54:55,020
 We also specify what distance we use, what linkage we use.

1864
01:54:55,020 --> 01:54:57,780
 And then, actually, finally, then we

1865
01:54:57,780 --> 01:54:59,780
 can have this kind of tree.

1866
01:54:59,780 --> 01:55:00,060
 OK.

1867
01:55:00,060 --> 01:55:01,580
 Then you can plot, right?

1868
01:55:01,580 --> 01:55:03,740
 Dendrogram, this cluster.

1869
01:55:03,740 --> 01:55:04,980
 You can just get this plot.

1870
01:55:07,580 --> 01:55:08,540
 OK.

1871
01:55:08,540 --> 01:55:11,700
 So this is the, you know, hierarchical tree,

1872
01:55:11,700 --> 01:55:13,500
 hierarchical cluster.

1873
01:55:13,700 --> 01:55:19,620
 Finally, we have one, actually, you know, the dendrogram,

1874
01:55:19,620 --> 01:55:22,580
 which can, which shows, actually, the similarity

1875
01:55:22,580 --> 01:55:23,940
 between the samples.

1876
01:55:23,940 --> 01:55:26,740
 And also from this dendrogram, we can also roughly

1877
01:55:26,740 --> 01:55:30,020
 judge how many natural groups are underlying the data.

1878
01:55:30,020 --> 01:55:32,740
 Natural group means, actually, between groups,

1879
01:55:32,740 --> 01:55:35,420
 between clusters, actually, that should be a big distance.

1880
01:55:35,420 --> 01:55:38,660
 Within clusters, that should be a small distance.

1881
01:55:38,660 --> 01:55:39,220
 OK.

1882
01:55:39,220 --> 01:55:42,380
 So roughly, you know, give us such information, right?

1883
01:55:42,380 --> 01:55:45,740
 About the natural groups underlying the data.

1884
01:55:45,740 --> 01:55:47,740
 So this, actually, the hierarchical cluster,

1885
01:55:47,740 --> 01:55:49,620
 I think, is very useful.

1886
01:55:49,620 --> 01:55:50,780
 OK.

1887
01:55:50,780 --> 01:55:54,300
 So, OK.

1888
01:55:54,300 --> 01:55:55,540
 Yeah, this is a summary, right?

1889
01:55:55,540 --> 01:55:57,860
 Summary of the hierarchical cluster.

1890
01:55:57,860 --> 01:56:02,980
 And, yeah, I think, or this is plainly the precept, right?

1891
01:56:02,980 --> 01:56:06,020
 Every step, actually, we should start from the, you know,

1892
01:56:06,020 --> 01:56:07,180
 distance metric, right?

1893
01:56:07,180 --> 01:56:10,020
 Then we identify the closest clusters.

1894
01:56:10,020 --> 01:56:10,620
 OK.

1895
01:56:10,740 --> 01:56:12,700
 Every step before merging, right,

1896
01:56:12,700 --> 01:56:19,220
 we all need to have the distance metric between the clusters.

1897
01:56:19,220 --> 01:56:20,059
 OK.

1898
01:56:20,059 --> 01:56:21,740
 So, OK.

1899
01:56:21,740 --> 01:56:25,180
 And finally, all of them are merged into one single one.

1900
01:56:25,180 --> 01:56:25,900
 OK.

1901
01:56:25,900 --> 01:56:31,059
 So this is a generative organ for hierarchical cluster.

1902
01:56:31,059 --> 01:56:32,500
 So, necessarily, we have a break.

1903
01:56:32,500 --> 01:56:36,300
 After break, we will actually talk about other two types

1904
01:56:36,300 --> 01:56:39,099
 of cluster, distribution-based cluster,

1905
01:56:39,100 --> 01:56:41,780
 and also the density-based cluster.

1906
01:56:41,780 --> 01:56:42,780
 OK.

1907
01:56:42,780 --> 01:56:43,280
 Right.

1908
01:57:09,100 --> 01:57:10,100
 OK.

1909
01:57:39,100 --> 01:57:40,100
 OK.

1910
01:58:09,100 --> 01:58:10,100
 OK.

1911
01:58:39,100 --> 01:58:40,100
 OK.

1912
01:59:09,100 --> 01:59:10,100
 OK.

1913
01:59:39,100 --> 01:59:40,100
 OK.

1914
02:00:09,100 --> 02:00:10,100
 OK.

1915
02:00:39,100 --> 02:00:40,100
 OK.

1916
02:01:09,100 --> 02:01:10,100
 OK.

1917
02:01:39,100 --> 02:01:40,100
 OK.

1918
02:02:09,100 --> 02:02:10,100
 OK.

1919
02:02:39,100 --> 02:02:40,100
 OK.

1920
02:03:09,100 --> 02:03:11,100
 OK.

1921
02:03:39,100 --> 02:03:41,100
 OK.

1922
02:04:09,100 --> 02:04:11,100
 OK.

1923
02:04:39,100 --> 02:04:41,100
 OK.

1924
02:05:09,100 --> 02:05:11,100
 OK.

1925
02:05:39,100 --> 02:05:41,100
 OK.

1926
02:06:09,100 --> 02:06:11,100
 OK.

1927
02:06:39,100 --> 02:06:41,100
 OK.

1928
02:07:09,100 --> 02:07:11,100
 OK.

1929
02:07:39,100 --> 02:07:41,100
 OK.

1930
02:08:10,060 --> 02:08:15,060
 OK.

1931
02:08:15,060 --> 02:08:17,780
 And next actually we look at the distribution-based cluster.

1932
02:08:17,780 --> 02:08:23,460
 And so in this matter, actually, we assume the data tree

1933
02:08:23,460 --> 02:08:28,160
 generated from different distributions, right.

1934
02:08:28,160 --> 02:08:33,860
 And often, we assume, actually, each of the clusters correspond

1935
02:08:33,860 --> 02:08:35,940
 to one Gaussian function.

1936
02:08:35,940 --> 02:08:42,940
 function. And then overall, the data actually is from multiple Gaussian functions. Then

1937
02:08:42,940 --> 02:08:49,940
 we can use the Gaussian-Mister model to model actually this function. This is actually the

1938
02:08:49,940 --> 02:08:56,339
 data, right? So this is a division based on a cluster. Actually, we use the Gaussian-Mister

1939
02:08:56,339 --> 02:09:03,339
 model. And actually, the basic idea of the Gaussian-Mister model is to, I think, to,

1940
02:09:04,340 --> 02:09:10,140
 actually, this is the same as the other type of class-candino probability function using

1941
02:09:10,140 --> 02:09:16,660
 one single Gaussian function. Actually, we try to actually, you know, maximize the likelihood

1942
02:09:16,660 --> 02:09:23,660
 of having this kind of set data. And actually, we can have different, we can have multiple

1943
02:09:25,180 --> 02:09:29,820
 actually Gaussian functions with different, actually, the center vector and the correction

1944
02:09:29,820 --> 02:09:36,820
 matrix. And we want to find this value of the current matrix and also the center vector

1945
02:09:37,059 --> 02:09:43,059
 of the Gaussian-Founder mean vector so that the data we have has the maximum likelihood.

1946
02:09:43,059 --> 02:09:50,059
 Okay, so this is the basic idea of the, for the parameter estimation for the Gaussian-Mister

1947
02:09:50,700 --> 02:09:57,700
 model. We maximize the likelihood of having this set of data. Okay. And, okay, so we have

1948
02:09:59,820 --> 02:10:04,380
 the, and so then the data distribution, right, we assume it from multiple, actually, you

1949
02:10:04,380 --> 02:10:10,820
 know, the normal function. And here, I mean the normal function, normal or Gaussian function.

1950
02:10:10,820 --> 02:10:17,820
 And each Gaussian function has its own mean vector and the current matrix. Right? Okay.

1951
02:10:17,860 --> 02:10:24,860
 So assume, totally, there are m Gaussian functions. Okay. And this m is also hyper parameter.

1952
02:10:25,580 --> 02:10:30,860
 We don't know the m value, right? And actually, we assume there are m clusters. So this m

1953
02:10:30,860 --> 02:10:35,740
 should be known, right? And each of the cluster, cluster to one Gaussian function. So we use

1954
02:10:35,740 --> 02:10:42,740
 the m Gaussian functions to model actually the data distribution. Okay. So, and for each

1955
02:10:43,139 --> 02:10:50,139
 of the cluster or each of the Gaussian function, we have a corresponding weight, alpha i. Okay.

1956
02:10:51,140 --> 02:10:58,140
 So, so this is the, so this is the, you know, the Gaussian-Mister model. And so once this

1957
02:11:00,740 --> 02:11:05,740
 model is aligned, right, once this model is aligned and we know, I do the mean vectors,

1958
02:11:05,740 --> 02:11:10,740
 right? And then I treat them, these mean vectors are just the class centers, class centers,

1959
02:11:10,740 --> 02:11:15,180
 right? Then based on the distance, then we can calculate the distance from each of the

1960
02:11:15,220 --> 02:11:20,220
 sample to each of the, you know, the class center, right? Then finally we can determine

1961
02:11:20,220 --> 02:11:27,220
 the class, the, the, the, the class, which class is the belongs to. Okay. So this is

1962
02:11:27,220 --> 02:11:33,220
 the Gaussian-Mister model. And of course, actually, you know, to estimate the parameters,

1963
02:11:33,220 --> 02:11:40,220
 actually the mu i, sigma i, and alpha i, I think it's quite challenging, right? Although

1964
02:11:40,620 --> 02:11:47,620
 the principle here is to maximize the likelihood, but the process, I think, is not, is not a

1965
02:11:47,620 --> 02:11:54,620
 trivial. And we need to use the expectation-maximization method, right? Expectation-maximization. Okay.

1966
02:12:01,900 --> 02:12:08,900
 So in this expectation-maximization, you know, we need to actually perform two steps. One,

1967
02:12:10,220 --> 02:12:15,220
 a step, we call that the expectation step, right? Another step, we call the expectation-maximization

1968
02:12:15,220 --> 02:12:21,220
 step. So we wrote this argument called expectation-maximization method. So if you forgot that, then you

1969
02:12:21,220 --> 02:12:28,220
 can go back and look back at our previous material on the Gaussian-Mister model, right?

1970
02:12:28,220 --> 02:12:34,220
 But here I also treat, you know, maybe copy some of the slides, and then we look at this

1971
02:12:34,220 --> 02:12:41,220
 again. Okay. So we set the number m. So this m must be preset, preset, right? Because

1972
02:12:41,220 --> 02:12:45,220
 this m is just the number of class tests, because we assume each class that corresponds

1973
02:12:45,220 --> 02:12:51,220
 to one Gaussian function. Okay. And then we need to set the initial values, right? Initial

1974
02:12:51,220 --> 02:12:58,220
 values for the alpha i, and then, and also the sigma i, the, the, no, mu i. Mu i is the

1975
02:12:59,220 --> 02:13:06,220
 mean vector, right? The sigma i is the current matrix. Okay. We need to give the initial

1976
02:13:06,220 --> 02:13:14,220
 values. Okay. And then based on these initial values, okay, we estimate the, no, parameters,

1977
02:13:14,220 --> 02:13:20,220
 right? The parameters like a gamma, you remember, right? A gamma, okay. And then, so the next

1978
02:13:20,220 --> 02:13:25,220
 step, actually, based on the gamma element, then we can carry with the mean vectors and

1979
02:13:25,220 --> 02:13:30,220
 the current matrix, even the alpha values. Okay. So we need to repeat this preset, right?

1980
02:13:30,220 --> 02:13:35,220
 So, and here actually, you know, actually, you know, this actually is a preset stabilized,

1981
02:13:35,220 --> 02:13:41,220
 stabilized, okay? And actually in the, in my lab, we have this function, a fit, fit gm

1982
02:13:44,220 --> 02:13:50,220
 like Gaussian-Mister and DIST, this function, okay, to fit a Gaussian-Mister model. And

1983
02:13:51,220 --> 02:13:57,220
 this function, right? So you can just actually use this function like this, fit gm, so we

1984
02:14:00,220 --> 02:14:07,220
 are doing this, DIST, fit gm, DIST, data, then the number of clusters, right? So this is,

1985
02:14:09,220 --> 02:14:15,220
 I think the, yeah, the main lab for us to solve this problem, right? You can, the other,

1986
02:14:16,220 --> 02:14:22,220
 now package, probably you can use other functions actually to find the Gaussian-Mister model.

1987
02:14:23,220 --> 02:14:29,220
 Okay. So once this Gaussian-Mister model is determined, then you can know the class

1988
02:14:30,220 --> 02:14:35,220
 and test, right? You know the class and test. Okay. So, so these are the examples we have

1989
02:14:35,220 --> 02:14:41,220
 used before, then we have 40 samples. So, so if we assume there are two clusters, right?

1990
02:14:42,220 --> 02:14:48,220
 Then we use this function, F-R-T-G-M-D-I-S-T, right? So we are missing D-I-S-T. And then

1991
02:14:49,220 --> 02:14:55,220
 actually we are, the result is this, the G-M-M actually, there are two components, right? Two

1992
02:14:56,220 --> 02:15:01,220
 Gaussian components. And here the list actually, first actually give the alpha value, this

1993
02:15:02,220 --> 02:15:07,220
 alpha value, the missing proportion, 0.49 something, right? 50% or half of that, right? So then this

1994
02:15:08,220 --> 02:15:14,220
 is the mean vector, this is the mean vector, okay? And also for component number two, they

1995
02:15:15,220 --> 02:15:21,220
 also give this, you know, the mean vector, the mean vector, right? This is the mean vector.

1996
02:15:22,220 --> 02:15:28,220
 And also give the proportion, 50% something percent. Actually the two, no missing proportion

1997
02:15:29,220 --> 02:15:34,220
 summation should be one, should be one. Okay. So like if you use two clusters, actually each

1998
02:15:34,220 --> 02:15:41,220
 cluster has around 50%, 50% of the data, right? And so this is the case when M equals two.

1999
02:15:44,220 --> 02:15:49,220
 So, and then this is the position of the two, Gaussian-Mission model. Indeed actually the

2000
02:15:50,220 --> 02:15:55,220
 Gaussian-Mission model, indeed can find the center, the centroid of the two clusters. Okay.

2001
02:15:56,220 --> 02:16:03,220
 So this is the cluster result actually, the center vector and for the two clusters, the

2002
02:16:04,220 --> 02:16:09,220
 center vector, we assume M equal to two. And then actually for another scenario, right?

2003
02:16:10,220 --> 02:16:15,220
 And so of course then we assign them to the closed-class center, right? Then the black,

2004
02:16:16,220 --> 02:16:21,220
 or the blue or black sample to this cluster, or the red sample to this cluster. Okay.

2005
02:16:22,220 --> 02:16:28,220
 And then if we assume M equal to two, right? Three. Because we have no idea whether or

2006
02:16:28,220 --> 02:16:35,220
 not it should be two or two, three, right? Then we should also look at the three. And

2007
02:16:36,220 --> 02:16:43,220
 then we have the three components and the proportion and also the data percentage, right? And also

2008
02:16:44,220 --> 02:16:51,220
 the center vector, the centroid for each of the three Gaussian components. Okay. And then

2009
02:16:51,219 --> 02:16:58,219
 if we sketch this, if we plot this, right? So these are not the class center. These are

2010
02:16:59,219 --> 02:17:04,219
 class center. These are class center. The three class centers. Okay. So from here you can see

2011
02:17:05,219 --> 02:17:11,219
 actually the, the, no, the two of the clusters we center are very, very close to each other,

2012
02:17:12,219 --> 02:17:17,219
 right? And also actually one of the clusters.

2013
02:17:22,219 --> 02:17:28,219
 One of the class actually, you know, two of the class actually have this actually like

2014
02:17:29,219 --> 02:17:35,219
 very close to each other, right? And then they have the, one of them also covers small number

2015
02:17:36,219 --> 02:17:42,219
 of samples. Actually if you see them, right, it's a, it's a 15, right? Yeah. 15 percent.

2016
02:17:51,219 --> 02:17:57,219
 Okay. So then let's look at the density based method. And of course you practice, I think

2017
02:17:58,219 --> 02:18:02,219
 it's very hard to determine the suitable value of m, right? But here again actually, you know,

2018
02:18:03,219 --> 02:18:08,219
 after you get the m value, and then actually you put the sample to different clusters. So again

2019
02:18:09,219 --> 02:18:16,219
 actually you can calculate the WCSS, right? The WCSS. And then again you can use actually

2020
02:18:16,219 --> 02:18:22,219
 like the Able method, right? To determine the suitable value of m. But you can, some actually

2021
02:18:23,219 --> 02:18:29,219
 previously, when we talk about the Gaussian feature model, how many m we should use actually,

2022
02:18:30,219 --> 02:18:34,219
 some scenarios, the alpha value is very small. Just cover a few number of samples, right? A few

2023
02:18:35,219 --> 02:18:40,219
 number, just a few, right? And, but sometimes actually, for example, in this scenario just now

2024
02:18:40,219 --> 02:18:46,219
 it's 15 percent of the data is covered by that. So it's very hard to see, you know, we should

2025
02:18:47,219 --> 02:18:53,219
 remove this cluster. We just have two, right? It's hard to see. Okay. So I think in practice the

2026
02:18:54,219 --> 02:18:58,219
 determination of the number of clusters, I think this is a challenging issue. In particular,

2027
02:18:59,219 --> 02:19:04,219
 there are no ground truths. There are no ground truths. There are no actually, you cannot

2028
02:19:04,219 --> 02:19:10,219
 wither the data. Because it's hard to see, right? So it's very hard to see that. Okay. So it's quite

2029
02:19:11,219 --> 02:19:18,219
 challenging actually. Okay. Okay. So, so let's actually look at the density based cluster.

2030
02:19:19,219 --> 02:19:28,219
 Density based. Okay. Actually, you know, in the previous method like, you know, KMS

2031
02:19:28,219 --> 02:19:34,219
 classroom and also the hierarchical classroom, we assume, you know, we have a center vector,

2032
02:19:35,219 --> 02:19:41,219
 right? We have centers. And so this works very well. But for some scenario, either data like this,

2033
02:19:42,219 --> 02:19:48,219
 actually two scenarios. One is an arbitrary shape, arbitrary shape, right? So even this, right,

2034
02:19:49,220 --> 02:19:56,220
 this is actually arbitrary shape. Okay. We cannot, this is not a severe shape, right? So this is

2035
02:19:56,220 --> 02:20:03,220
 actually not a severe shape. Okay. So arbitrary shape, irregular shape. Okay. And then actually

2036
02:20:04,220 --> 02:20:13,220
 some scenario we have noise. We have noise like this. We have noise. Okay. And then actually,

2037
02:20:14,220 --> 02:20:20,220
 this is all know, you know, input problem, right? Or bring problem to the KMS classroom. In the

2038
02:20:20,220 --> 02:20:26,220
 KMS classroom, actually every sample is assigned to one cluster. And the cluster center is

2039
02:20:27,220 --> 02:20:34,220
 average, the mean vector, right? So the noise that also actually contribute to the calculation

2040
02:20:35,220 --> 02:20:40,220
 of the mean vector, right? So in other words, the mean vector will be affected by the noise.

2041
02:20:41,220 --> 02:20:48,220
 Okay. So this means that if we have these two issues, then the KMS classroom, even hierarchical

2042
02:20:48,220 --> 02:20:54,220
 classroom, will not work well, very well. Okay. Certainly for the Gaussian-Mister model,

2043
02:20:55,220 --> 02:20:59,220
 it will know if we are not very well, very well, right? Actually, when we assume we can use a

2044
02:21:00,220 --> 02:21:03,220
 distribution based method, we can assume they follow Gaussian distribution. Certainly Gaussian

2045
02:21:04,220 --> 02:21:09,220
 distribution is a more like shape. Okay. So this actually, if we have such kind of

2046
02:21:10,220 --> 02:21:16,220
 distribution, then the previous problem that we, you know, the method cannot solve this kind of

2047
02:21:16,220 --> 02:21:22,220
 data well, right? So here, actually, now to address this problem, they have proposed a

2048
02:21:23,220 --> 02:21:37,220
 so-called density-based method. Okay. So this is a very famous actually method called DBSCAN.

2049
02:21:38,220 --> 02:21:44,220
 Okay. DBSCAN is a spatial classification of applications with noise. Then the short name is

2050
02:21:44,220 --> 02:21:49,220
 DBSCAN. These are very famous algorithms. Okay. So this is now the density-based method.

2051
02:21:50,220 --> 02:21:58,220
 And this DBSCAN algorithm actually can cluster the sample with arbitrary shape distribution,

2052
02:21:59,220 --> 02:22:04,220
 just like the one I showed you, right? And also, it can handle the noise problem, the

2053
02:22:04,220 --> 02:22:21,220
 auto-lias. Okay. And actually, okay, so here, actually, the DBSCAN algorithm has two main

2054
02:22:22,220 --> 02:22:29,220
 parameters. One parameter actually is called M-I-N-P-T-S, the minimum points. Okay. And

2055
02:22:29,220 --> 02:22:36,220
 actually, another tree is Epsilon. Okay. And actually, in the DBSCAN algorithm, they have the so-called

2056
02:22:37,220 --> 02:22:46,220
 neighborhood. Okay. And Epsilon actually defines the neighborhood size. Okay. And for each sample,

2057
02:22:47,220 --> 02:22:50,220
 now we can, of course, have a neighborhood, right? Based on the Epsilon, we can define, we can have a

2058
02:22:50,220 --> 02:22:59,220
 neighborhood. Okay. And within the neighborhood of a sample, and if there are many samples, for

2059
02:23:00,220 --> 02:23:05,220
 example, if the number of samples is, you see a certain value, a threshold value, for example, this value

2060
02:23:06,220 --> 02:23:12,220
 called minimum point. And then this point actually could be called actually a core point. Okay. So if the

2061
02:23:13,220 --> 02:23:17,220
 number of points within the neighborhood is less than a certain threshold value, and then it could be a

2062
02:23:17,220 --> 02:23:26,220
 border point, or outline, yes. Okay. So this is actually just actually the three, the two parameters, the

2063
02:23:27,220 --> 02:23:34,220
 neighborhood size, and also the number of points within the neighborhood. These two parameters determine the

2064
02:23:35,220 --> 02:23:42,220
 three levels of data point, the core point. Okay. Even in the neighborhood, okay. And the number of

2065
02:23:42,220 --> 02:23:50,220
 points, you see the minimum point, right? Then this point is called a core point. A core point. Okay. So I

2066
02:23:51,220 --> 02:23:57,220
 just show you this. Okay. So here, actually, for example, this is one point. Okay. And based on the Epsilon, we

2067
02:23:58,220 --> 02:24:02,220
 can define the neighborhood. So this is a neighborhood. You can just draw a circle, right? In a hard

2068
02:24:03,220 --> 02:24:08,220
 amount of space, this could be a sphere, right? Okay. And then you can have a neighborhood. And then you

2069
02:24:08,220 --> 02:24:14,220
 can call it a home sample within the neighborhood. Of course, in practice, you don't really need to draw this

2070
02:24:15,220 --> 02:24:21,220
 neighborhood, right? You don't need to check actually how many samples, you know, have a distance less than the

2071
02:24:22,220 --> 02:24:27,220
 Epsilon, right? Less than the Epsilon. Then you can find the home, you can find this, right? Whether this

2072
02:24:28,220 --> 02:24:34,220
 point is a core point or not, right? For every point, actually, you can keep the distance to all other points,

2073
02:24:34,220 --> 02:24:41,220
 other samples, right? Then you can find how many samples, you know, have a distance less than the threshold value Epsilon.

2074
02:24:42,220 --> 02:24:49,220
 Okay. If this number of points, actually, is greater than these minimum points, then this is called a core point.

2075
02:24:52,220 --> 02:25:03,220
 Okay. So this is a core point. Another thing is called a border point. Border point. And the border point is

2076
02:25:04,220 --> 02:25:10,220
 actually a point which has a few minimum points within its neighborhood. Then this is called a border point.

2077
02:25:11,220 --> 02:25:22,220
 Okay. And for example, and we have one here. Okay. And then you can draw on all the sphere. And then within the

2078
02:25:23,220 --> 02:25:27,220
 neighborhood, only one sample. So then this point, actually, could be called a border point.

2079
02:25:28,220 --> 02:25:36,220
 Okay. So based on the neighborhood and the number of samples within the neighborhood, and then we can define the

2080
02:25:37,220 --> 02:25:45,220
 points as a core point or a border point. And then the type of point is called a noise autolyze.

2081
02:25:46,220 --> 02:25:55,220
 A point which is not a core point or border point. And then it is called actually a noise.

2082
02:25:55,220 --> 02:26:02,220
 Okay. For example, for this sample, right, for this point, each point is a sample, right? Actually, you can also, you know,

2083
02:26:03,220 --> 02:26:08,220
 based on the Epsilon, you can find how many samples are within the neighborhood, defined by the Epsilon.

2084
02:26:09,220 --> 02:26:15,220
 Even no point, right? Even no point within the neighborhood. Okay. And then it's not a core point, right?

2085
02:26:15,220 --> 02:26:22,220
 It's not a core point, right? Then here, actually, these are autolyze. Okay. So that means, actually, this point is far away from other

2086
02:26:23,220 --> 02:26:31,220
 points. Okay. A core point, actually, that means, actually, this part, actually, you know, is a, you know, is a, the data is densely, actually,

2087
02:26:32,220 --> 02:26:39,220
 you know, disputed. Because within the neighborhood, there are a lot of points. Okay. So the border point, actually, also, actually,

2088
02:26:39,220 --> 02:26:45,220
 maybe the density is not as high as actually, you know, this is actually that part, actually, that's a core point, but still have some number of

2089
02:26:46,220 --> 02:26:55,220
 samples within the neighborhood. Okay. So this is actually the, you know, the three types of points. So next, actually, we have some other

2090
02:26:56,220 --> 02:27:05,220
 tree, like a concept. Okay. So like, you know, a visibility of connectivity. Okay. So visibility, and here we have three types of

2091
02:27:05,220 --> 02:27:16,220
 visibility. Directly, okay. Density, actually, reachable, and density reachable, and density connected. So these are the concepts in this

2092
02:27:17,220 --> 02:27:24,220
 DPSN algorithm. Okay. So what is the directly density reachable? Okay. So we look at the scenario. Okay. A point, P, is a direct reachable,

2093
02:27:25,220 --> 02:27:34,220
 density reachable from point Q. You P is within the neighborhood of the core point Q. Okay. So here Q is a core point, core point,

2094
02:27:35,220 --> 02:27:42,220
 right? So that means within the neighborhood, the number of samples is greater than the minimum point. Okay. So if the number of

2095
02:27:43,220 --> 02:27:52,220
 minimum points is five, right? So within the neighborhood, one, two, three, four, I think five, right? Greater than or equal, okay. So this is actually, so P, Q is

2096
02:27:53,220 --> 02:28:03,220
 a neighborhood, is a core point, and P is within the neighborhood of Q. Then we see P is a, is a not directly density reachable from P, Q. So this is

2097
02:28:03,220 --> 02:28:12,220
 a kind definition, right? So any point within the neighborhood of a core point is a density, directly density reachable from the core point.

2098
02:28:13,220 --> 02:28:30,220
 Okay. So this is the, this is the, the, the, the, this concept, directly density reachable. Okay. So another type is called actually density reachable. We don't have it directly, right? So that

2099
02:28:30,220 --> 02:28:43,220
 means actually the point, actually, if P is a density reachable from Q, if there are a set of a core points leading from Q to P, okay. So here we can see, okay. So Q here is a, is a core point, right?

2100
02:28:44,220 --> 02:28:59,220
 Core point. And this P is not within the neighborhood of Q. Q here, the neighborhood is, you know, is just a circle here, right? So P is an outside circle. So P is not directly reachable from Q. Okay. But actually from here, actually, P is the

2101
02:29:00,220 --> 02:29:20,220
 direct reachable from O. O is a center core point. Within the neighborhood, it has a very point, right? So P is directly reachable from O. Okay. And O is a direct reachable from Q. Then we see the Q, P is a reachable from Q.

2102
02:29:20,220 --> 02:29:48,220
 Okay. So here actually, P and Q are linked, right? Reachable, reachable through two core points. One is P, Q, another is O. Okay. So P is the direct reachable from O. O is the direct reachable from Q. Then we see P is the reachable from Q. So this is a, you know, a concept about the reachability, density reachability.

2103
02:29:48,220 --> 02:30:04,220
 Okay. So this is the second type of the relationship, right? Between those points. And then the next type is called density connected. Two points are called density connected. If there is a core point, then the both points are density reachable from the core points.

2104
02:30:04,220 --> 02:30:20,220
 Density reachable, right? First, actually, the highest level is the directly density reachable within the neighborhood. But if through a few, through a few, you know, core point is a density reachable, you know, directly density reachable, right?

2105
02:30:20,220 --> 02:30:49,220
 But now we have that, actually, concept, density connected. Okay. So if P and Q, right, and the two points are called density connected, right? If there is a core point, then the both points are density reachable from the core point. Okay. So from here, right, actually, we know that we have O here, right? We have O here. And actually, the, you know, this point is density reachable, direct reachable from Q.

2106
02:30:50,220 --> 02:31:13,220
 And this Q is direct reachable from this point. So we see that Q is reachable from O. Q reachable from O, right? And similarly, and at this point, it's direct reachable from this point. P is reachable from O, right? So finally, we see our Q and P are actually density connected.

2107
02:31:13,220 --> 02:31:38,220
 So your points, two points are density reachable from the core point. Then we see, yeah, connected, density connected. So these are the concepts that we're using in this, actually, the density base classroom, density, right? So actually, these are the different names that just say, on this part of the region, actually, the data actually densely distributed.

2108
02:31:39,220 --> 02:31:55,220
 Okay. And some of the part of the data actually, actually, sparsely distributed. And then, you know, that means that there is a new cluster from there. Because from dense area to another dense area, there must be some region where the samples are sparsely distributed.

2109
02:31:56,220 --> 02:32:12,220
 So sparsely distributed is just something like a boundary between the two clusters. Okay. So these are the concepts we're using in the dB scan algorithm. And so these actually, we talked about the parameter settings.

2110
02:32:13,220 --> 02:32:35,220
 Okay. So here we have two parameters, right? One is the neighborhood size, the epsilon. Okay. Another is the number, the threshold, minimum number of points, right? Okay. So the mean number of points actually should be, normally, I think, should be at least actually the d plus 1, d is the denominator of the data, the denominator of the data, right?

2111
02:32:36,220 --> 02:32:49,220
 Because each data, each sample is a vector, right? So how many elements in the vector? So this is the denominator of the data. Okay. So this d plus 1, the minimum should be at least d plus 1. And also, this number should be at least 3.

2112
02:32:50,220 --> 02:33:02,220
 Okay. So at least in the region, there are three samples. Then we see this, actually, this part, actually, then we can determine the core point, right? Then we see, oh, this part, the data densely distributed. At least there's 3.

2113
02:33:02,220 --> 02:33:25,220
 Okay. Normally, it should be d plus 1. Okay. d is the denominator of the data. d equal to 4 dimensions, right? Then this mean, I answer, be like, greater than or equal to 5. Okay. So this is the minimum point selection. And also, actually, in practice, the larger value, are you really better for data set with noise?

2114
02:33:26,220 --> 02:33:44,220
 Noise. Right? Sometimes, actually, we have few points. And they are very close to each other. They are all noise, the otlias. Okay. So if this number is too small, right? Then we thought maybe like 3 or they thought they are, this sample, actually, there is a cluster there.

2115
02:33:44,220 --> 02:34:04,220
 Okay. So to address this problem, so in practice, actually, we can adopt the rule of thumb. That is, d should be 2 times d. If d equal to 4, right? So now this is 2 times 4 should be 8, right? Then 5. Okay. So, actually, we increase the value of this minimum point.

2116
02:34:04,220 --> 02:34:20,220
 Because in practice, the denominator normally is higher, right? And normally, we don't have the 2-dimensional data or 3-dimensional. We have much higher dimension. So we use 2 times d. This, actually, is the minimum point. This is just the rule of thumb, right?

2117
02:34:21,220 --> 02:34:45,220
 Then we know, see, or we must use this value. So this is, give us a card line. Okay. We should use a value that is twice of the denominator. Okay. So this is for the M-R-N. And then another parameter is the neighborhood side, right? The neighborhood, the epsilon. Okay. And how to define m- epsilon? And actually, we can, here, we also use the so-called Able method.

2118
02:34:45,220 --> 02:35:09,220
 The value for the, this, you know, the epsilon can be chosen by plotting the distance to the k nearest neighbors. Okay. So what's the meaning of that? Okay. And so, through the k, right? The k, minimum, actually, you can set this minimum value to different value, right?

2119
02:35:09,220 --> 02:35:31,220
 And then for each of the sample, you can calculate the nearest, actually, you know, a k, you know, samples. Okay. Then you can look at the distance. Okay. For every sample, we have a nearest, you know, the distance, right? And then, for all the samples, we can take the average. So we can get the distance. This distance is the distance of one sample to the nearest k samples.

2120
02:35:31,220 --> 02:36:00,220
 Okay. Then for each of these unknown k value or each of the M-R-N points, value, we can get one value. Then we increase this value. For example, M-R-N equals three, for example, right? Then k equals two. For each sample, we find the nearest two samples. Then we look at the distance, right? For each sample, we have this value. Then we take the average, for example, right? Okay. Then we find the value. And then the necessary, we can also increase the value.

2121
02:36:01,220 --> 02:36:29,220
 M-R-N from two to three, from three to four. Then the k equals three, right? Then we select the top three, you know, nearest sample. Again, we calculate the distance. We can get one point. And finally, we can also sketch, plot, actually, the sketch plot, right? Actually, it's the distance against, actually, the minimum number of points, M-R-N points. Okay. Then you can also find, actually, some A-able, right? A-able point.

2122
02:36:30,220 --> 02:36:43,220
 Decreasing, decreasing. Then after the center point, and the decreasing is very slow. Then we can determine the epsilon based on this value. Okay. This is actually the, yeah.

2123
02:36:43,220 --> 02:37:12,220
 Okay. Okay. So this is actually the DBSCAN algorithm. And if the epsilon neighborhood of the data point contains a more than one minimum number of points, a new cluster, new cluster, right? So actually, in the precedent, actually, if the number of samples is less than one, so, certainly, this is a non-standard.

2124
02:37:13,220 --> 02:37:35,220
 And this is not a core point, right? Certainly, this should not be a part of the cluster. This may be a noise or something like that. Okay. Or at least this is the end of another cluster. Okay. Then we should select another point. Then we will start the process. Start the process. Okay. So this is basically, I think, the DBSCAN algorithm. And maybe we can have a look of this.

2125
02:37:44,220 --> 02:37:45,220
 Okay.

2126
02:37:45,220 --> 02:37:46,220
 Okay.

2127
02:37:46,220 --> 02:37:47,220
 Okay.

2128
02:37:47,220 --> 02:37:48,220
 Okay.

2129
02:37:48,220 --> 02:38:07,220
 Okay.

2130
02:38:07,220 --> 02:38:14,220
 I think this shows the, you know, it's a DB scan classroom preset.

2131
02:38:14,220 --> 02:38:18,580
 They are connected, right?

2132
02:38:18,580 --> 02:38:22,939
 Because the density is on the neighborhood, reachable, and then, yeah, density connected,

2133
02:38:22,939 --> 02:38:23,939
 then they, okay.

2134
02:38:23,939 --> 02:38:27,779
 So at the end, actually, yeah, then they start from another point, okay.

2135
02:38:27,779 --> 02:38:31,779
 They cannot connect extra, then they start from another point.

2136
02:38:31,779 --> 02:38:35,619
 So finally, some of the point where we consider, no connected with others, maybe we consider

2137
02:38:35,619 --> 02:38:36,619
 noise.

2138
02:38:36,860 --> 02:38:38,860
 So this is a DB scan algorithm.

2139
02:38:38,860 --> 02:38:39,860
 Yeah.

2140
02:38:39,860 --> 02:38:41,860
 So these are all in the neighborhood at some point, right?

2141
02:38:41,860 --> 02:38:45,180
 Then they all connect them together because they are density connected.

2142
02:38:45,180 --> 02:38:54,180
 They are, they belong to the same cluster, okay.

2143
02:38:54,180 --> 02:38:56,060
 Okay, then we have some noise, right?

2144
02:38:56,060 --> 02:38:58,180
 So this is the DB scan algorithm.

2145
02:38:58,180 --> 02:39:02,620
 And actually the, the, the, okay, so necessary we look at some advantage or disadvantage

2146
02:39:02,620 --> 02:39:04,300
 of this method.

2147
02:39:04,300 --> 02:39:12,380
 And if you run this experiment repeatedly, actually every time, actually, we will start

2148
02:39:12,380 --> 02:39:15,380
 from a different initial point.

2149
02:39:15,380 --> 02:39:19,699
 And then the issue with that, actually, we may get different, actually, classroom results.

2150
02:39:19,699 --> 02:39:22,300
 So if we look at the procedure first, okay.

2151
02:39:22,300 --> 02:39:24,300
 This is the procedure.

2152
02:39:24,300 --> 02:39:26,300
 Step one, we set the value, right?

2153
02:39:26,300 --> 02:39:28,220
 This is, you know, the hyper parameter, right?

2154
02:39:28,220 --> 02:39:33,300
 We need to set the value for this to define the neighborhood size and also to define,

2155
02:39:33,300 --> 02:39:37,300
 actually, these threshold values so that we can define which is a core point.

2156
02:39:37,300 --> 02:39:38,300
 Okay.

2157
02:39:38,300 --> 02:39:48,300
 And then, actually, we, we pick randomly, pick a data point as starting point, random

2158
02:39:48,300 --> 02:39:49,300
 point, right?

2159
02:39:49,300 --> 02:39:51,300
 Then we judge, actually, if this point is a core point, right?

2160
02:39:51,300 --> 02:39:54,300
 And then we just, actually, you know, then connect to other.

2161
02:39:54,300 --> 02:39:58,300
 So we find the recursion will always density connected point.

2162
02:39:58,300 --> 02:39:59,300
 Okay.

2163
02:39:59,300 --> 02:40:04,300
 And then we send them to the same cluster as the core point.

2164
02:40:04,300 --> 02:40:05,300
 Okay.

2165
02:40:05,300 --> 02:40:07,300
 Of course, actually, if this point is a core point, right?

2166
02:40:07,300 --> 02:40:12,300
 If the starting point is a core point, and then we just, you know, connect all the non-density

2167
02:40:12,300 --> 02:40:13,300
 connected point.

2168
02:40:13,300 --> 02:40:15,300
 And then we put them into one cluster.

2169
02:40:15,300 --> 02:40:18,300
 So this cluster is a core point cluster.

2170
02:40:18,300 --> 02:40:19,300
 Okay.

2171
02:40:19,300 --> 02:40:22,300
 And some, actually, okay, so the next step, okay.

2172
02:40:22,300 --> 02:40:27,300
 And then, actually, randomly choose another point among all the points.

2173
02:40:27,300 --> 02:40:32,300
 And then, actually, we don't need to know being wasted in the previous step.

2174
02:40:32,300 --> 02:40:34,300
 Actually, we, all of them is connected, right?

2175
02:40:34,300 --> 02:40:40,300
 If it cannot be connected, okay, then this means this cluster, you know, for this cluster,

2176
02:40:40,300 --> 02:40:41,300
 no, is completed.

2177
02:40:41,300 --> 02:40:45,300
 And then we need to actually find another cluster, second cluster.

2178
02:40:45,300 --> 02:40:48,300
 So something like at the beginning, we first find one cluster.

2179
02:40:48,300 --> 02:40:53,300
 Then the second, in the next step, we find the second cluster, actually, progressively

2180
02:40:53,300 --> 02:40:54,300
 find the cluster.

2181
02:40:54,300 --> 02:40:55,060
 now, right?

2182
02:40:55,060 --> 02:40:58,500
 We find the first cluster, then other clusters.

2183
02:40:58,500 --> 02:40:59,380
 So random is chosen.

2184
02:40:59,380 --> 02:41:00,920
 Another point among the points that

2185
02:41:00,920 --> 02:41:03,460
 have not been visited in the previous step.

2186
02:41:03,460 --> 02:41:06,939
 Then we repeat this process to find all the density connected

2187
02:41:06,939 --> 02:41:08,619
 points and put them into one cluster.

2188
02:41:13,060 --> 02:41:14,340
 So this process is finished.

2189
02:41:14,340 --> 02:41:15,779
 All points are visited.

2190
02:41:15,779 --> 02:41:18,859
 So those points that don't belong to any cluster

2191
02:41:18,859 --> 02:41:20,900
 are considered as noise.

2192
02:41:20,900 --> 02:41:22,699
 It's just an illustration now, right?

2193
02:41:22,700 --> 02:41:26,060
 Some of the points are not actually assigned

2194
02:41:26,060 --> 02:41:27,060
 to any of the cluster.

2195
02:41:27,060 --> 02:41:30,380
 They are just considered as noise.

2196
02:41:30,380 --> 02:41:33,860
 OK, so this is basically the DB scan algorithm.

2197
02:41:33,860 --> 02:41:38,700
 I think the implementation of this is not that straightforward.

2198
02:41:38,700 --> 02:41:40,100
 It's not that straightforward.

2199
02:41:40,100 --> 02:41:44,860
 And every time, because the point is random, right?

2200
02:41:44,860 --> 02:41:47,300
 And actually, you make a different result.

2201
02:41:47,300 --> 02:41:50,740
 So this is actually one issue of this DB scan algorithm.

2202
02:41:50,740 --> 02:41:55,140
 If you repeat, actually, many models,

2203
02:41:55,140 --> 02:41:56,539
 actually, we have this problem.

2204
02:41:56,539 --> 02:41:58,260
 Even for neural networks, actually,

2205
02:41:58,260 --> 02:42:00,619
 if you start from a different point,

2206
02:42:00,619 --> 02:42:03,220
 I find that the values you get are actually different.

2207
02:42:03,220 --> 02:42:05,060
 Actually, the DB scan algorithm is the same.

2208
02:42:08,780 --> 02:42:10,619
 So the advantage is that, actually,

2209
02:42:10,619 --> 02:42:15,300
 so we don't need to present a number of clusters, right?

2210
02:42:15,300 --> 02:42:16,740
 We don't need to present a number of clusters.

2211
02:42:16,740 --> 02:42:19,020
 But the disadvantage is that we still

2212
02:42:19,020 --> 02:42:20,900
 have some hyperparameters.

2213
02:42:20,900 --> 02:42:24,660
 In the k-means, for example, we have a hyperparameter k, right?

2214
02:42:24,660 --> 02:42:29,980
 But actually, in the Gordon-Mr. model, we have the number m.

2215
02:42:29,980 --> 02:42:32,180
 But in here, we don't have this number,

2216
02:42:32,180 --> 02:42:34,380
 but we have other hyperparameters.

2217
02:42:34,380 --> 02:42:37,940
 So this also can be considered a disadvantage, actually.

2218
02:42:37,940 --> 02:42:41,100
 So advantage is that we don't need to present a number of

2219
02:42:41,100 --> 02:42:41,900
 clusters.

2220
02:42:41,900 --> 02:42:44,700
 But a disadvantage is that, actually, we

2221
02:42:44,700 --> 02:42:48,300
 need to set the epsilon value and also the minimum point

2222
02:42:48,580 --> 02:42:48,800
 value.

2223
02:42:48,800 --> 02:42:53,060
 Actually, this value is not that straightforward to set.

2224
02:42:53,060 --> 02:42:56,619
 So this is an advantage to disadvantage, right?

2225
02:42:56,619 --> 02:42:58,980
 And also, the DB scan algorithm, actually,

2226
02:42:58,980 --> 02:43:00,699
 is robust for other areas.

2227
02:43:00,699 --> 02:43:03,660
 So this is an advantage, a robust advantage,

2228
02:43:03,660 --> 02:43:06,580
 because the algorithm that we use, the DB scan,

2229
02:43:06,580 --> 02:43:09,019
 is particular to address two issues, right?

2230
02:43:09,019 --> 02:43:12,099
 One is the noise, another is the irregular shape.

2231
02:43:12,780 --> 02:43:13,280
 OK.

2232
02:43:17,580 --> 02:43:21,360
 So I think that the Australian characteristic of the DB

2233
02:43:21,360 --> 02:43:25,420
 scan algorithm, I think it's, first, actually, irregular shape.

2234
02:43:25,420 --> 02:43:27,820
 And then the second, actually, the noise.

2235
02:43:27,820 --> 02:43:28,820
 OK.

2236
02:43:28,820 --> 02:43:31,700
 So yeah.

2237
02:43:31,700 --> 02:43:32,500
 OK.

2238
02:43:32,500 --> 02:43:34,740
 So I think that's all for the DB scan.

2239
02:43:34,740 --> 02:43:37,140
 And these are all for the non-antibiotic linear.

2240
02:43:37,140 --> 02:43:40,580
 And no, no, no, actually, the next week, actually, we still

2241
02:43:40,660 --> 02:43:46,860
 have a few slides to talk about the evaluation of the classroom.

2242
02:43:46,860 --> 02:43:47,660
 You have the algorithm.

2243
02:43:47,660 --> 02:43:48,420
 You have the data.

2244
02:43:48,420 --> 02:43:51,100
 Then you do the performance classroom.

2245
02:43:51,100 --> 02:43:52,580
 You get the result.

2246
02:43:52,580 --> 02:43:55,580
 But how about the validity of the classroom result?

2247
02:43:55,580 --> 02:43:57,300
 So we have the evaluation.

2248
02:43:57,300 --> 02:43:57,900
 OK.

2249
02:43:57,900 --> 02:43:59,780
 So next week, we will talk about the metrics

2250
02:43:59,780 --> 02:44:02,180
 for the evaluation of the classroom result.

2251
02:44:02,180 --> 02:44:09,940
 And so on Saturday, we have the quiz starting from 11 to 12.

2252
02:44:09,940 --> 02:44:12,300
 And already I've loaded the file.

2253
02:44:12,300 --> 02:44:17,060
 And then you can check the file to see which room, which

2254
02:44:17,060 --> 02:44:20,620
 lecture data you should go to take your quiz.

2255
02:44:20,620 --> 02:44:21,500
 OK.

2256
02:44:21,500 --> 02:44:26,940
 And so please be seated at least 10 minutes before 11.

2257
02:44:26,940 --> 02:44:28,900
 And to the students, helpers, we'll

2258
02:44:28,900 --> 02:44:37,020
 start to do the quiz paper from 10.15 or 10.45.

2259
02:44:37,060 --> 02:44:43,380
 So please make sure you are there before 10.15.

2260
02:44:43,380 --> 02:44:46,380
 OK.

2261
02:44:46,380 --> 02:44:47,380
 OK.

2262
02:44:47,380 --> 02:44:51,860
 The scope of the quiz already I mentioned in the announcement.

2263
02:44:51,860 --> 02:44:56,140
 So I think you just follow those in the announcement

2264
02:44:56,140 --> 02:44:58,820
 to prepare for the quiz.

2265
02:44:58,820 --> 02:45:00,900
 We could have a description question.

2266
02:45:00,900 --> 02:45:04,260
 We could have a calculation question.

2267
02:45:04,260 --> 02:45:06,980
 But actually, I think one hour should

2268
02:45:06,980 --> 02:45:09,500
 be sufficient to finish the questions.

2269
02:45:09,500 --> 02:45:11,100
 OK.

2270
02:45:11,100 --> 02:45:14,980
 And then on the next week, I think the next Tuesday class,

2271
02:45:14,980 --> 02:45:16,980
 I think first I will finish the slides

2272
02:45:16,980 --> 02:45:18,740
 for the evaluation of the classroom.

2273
02:45:18,740 --> 02:45:22,780
 And then actually, I will have a revision of all the content

2274
02:45:22,780 --> 02:45:24,260
 in the machine learning part.

2275
02:45:24,260 --> 02:45:24,980
 OK.

2276
02:45:24,980 --> 02:45:26,780
 So after that, actually, maybe you

2277
02:45:26,780 --> 02:45:28,500
 can have a Q&A session.

2278
02:45:28,500 --> 02:45:30,340
 So if you want to ask questions, please,

2279
02:45:30,340 --> 02:45:32,580
 actually bring the laptop.

2280
02:45:32,580 --> 02:45:36,900
 Maybe I can, like, you know, in the Zoom, right?

2281
02:45:36,900 --> 02:45:39,940
 You can just type your question on the Zoom.

2282
02:45:39,940 --> 02:45:42,380
 And then I can shoot a question on the board.

2283
02:45:42,380 --> 02:45:44,460
 And then I'll answer your question.

2284
02:45:44,460 --> 02:45:45,260
 OK.

2285
02:45:45,260 --> 02:45:47,020
 So that's it for the next week.

2286
02:45:47,020 --> 02:45:47,520
 OK.

2287
02:45:47,520 --> 02:45:48,020
 Thank you.

2288
02:45:48,020 --> 02:45:50,020
 Yeah.

2289
02:45:50,020 --> 02:45:50,520
 Yeah.

2290
02:45:50,520 --> 02:45:51,020
 Thank you.

2291
02:45:51,020 --> 02:45:52,020
 Thank you.

2292
02:45:52,020 --> 02:45:53,020
 Thank you.

2293
02:46:02,580 --> 02:46:03,580
 Thank you.

2294
02:46:32,580 --> 02:46:33,580
 Thank you.

2295
02:47:02,580 --> 02:47:03,580
 Thank you.

2296
02:47:32,580 --> 02:47:33,580
 Thank you.

2297
02:48:02,580 --> 02:48:03,580
 Thank you.

2298
02:48:32,580 --> 02:48:33,580
 Thank you.

2299
02:49:02,580 --> 02:49:03,580
 Thank you.

2300
02:49:32,580 --> 02:49:34,580
 Thank you.

2301
02:50:02,580 --> 02:50:04,580
 Thank you.

2302
02:50:32,580 --> 02:50:34,580
 Thank you.

2303
02:51:02,580 --> 02:51:04,580
 Thank you.

2304
02:51:32,580 --> 02:51:34,580
 Thank you.

2305
02:52:02,580 --> 02:52:05,580
 Thank you.

2306
02:52:32,580 --> 02:52:34,580
 Thank you.

2307
02:53:02,580 --> 02:53:05,580
 Thank you.

2308
02:53:32,580 --> 02:53:34,580
 Thank you.

2309
02:54:02,580 --> 02:54:04,580
 Thank you.

2310
02:54:32,580 --> 02:54:35,580
 Thank you.

2311
02:55:02,580 --> 02:55:04,580
 Thank you.

2312
02:55:32,580 --> 02:55:35,580
 Thank you.

2313
02:56:02,580 --> 02:56:04,580
 Thank you.

2314
02:56:32,580 --> 02:56:34,580
 Thank you.

2315
02:57:02,580 --> 02:57:05,580
 Thank you.

2316
02:57:32,580 --> 02:57:34,580
 Thank you.

2317
02:58:02,580 --> 02:58:05,580
 Thank you.

2318
02:58:32,580 --> 02:58:34,580
 Thank you.

2319
02:59:02,580 --> 02:59:05,580
 Thank you.

2320
02:59:32,580 --> 02:59:34,580
 Thank you.

