1
00:01:30,000 --> 00:01:50,000
 Okay.

2
00:01:50,000 --> 00:01:52,000
 Good evening everyone.

3
00:01:52,000 --> 00:01:56,000
 So in the last week, we started the piece decision theory.

4
00:01:56,000 --> 00:02:05,000
 So in the piece decision theory, we need to make decisions based on the posterior probability.

5
00:02:05,000 --> 00:02:14,000
 Or we can use the variant of the posterior probability, such as the drawn probability of the input and the class labels.

6
00:02:14,000 --> 00:02:20,000
 So this is the formula for the posterior probability.

7
00:02:20,000 --> 00:02:29,000
 Given the x, given the input, given the feature vector, what is the probability of belonging to class omega j?

8
00:02:29,000 --> 00:02:36,000
 Okay. And actually, so in the numerator part, it's a drawn probability, right, of the x and omega j.

9
00:02:36,000 --> 00:02:42,000
 So this equals to the given omega j, the probability of x times the probability of p omega j,

10
00:02:42,000 --> 00:02:46,000
 and the probability of p x. p x is the total probability of x.

11
00:02:46,000 --> 00:02:48,000
 So this is a common factor.

12
00:02:48,000 --> 00:02:54,000
 And for all the posterior probability of omega 1, omega 2, and omega c, right?

13
00:02:54,000 --> 00:02:57,000
 Okay. So normally, we can ignore this part.

14
00:02:57,000 --> 00:03:07,000
 And for the numerator part, actually, so we have a p x omega j, which is the class conditional density function.

15
00:03:08,000 --> 00:03:16,000
 And actually, we assume actually the x, the feature, the continuous feature, and the following normal distribution.

16
00:03:16,000 --> 00:03:28,000
 So we use a multi-variable, multi-variable normal function to construct or to build or to approximate this density function.

17
00:03:28,000 --> 00:03:36,000
 Okay. And either multi-variable normal function or Gaussian function, we have two parameters.

18
00:03:36,000 --> 00:03:41,000
 One is the mean vector, and then another is the current matrix.

19
00:03:41,000 --> 00:03:49,000
 Okay. So whether the minority is low, for example, like a few dimension, right, a ten dimension, okay,

20
00:03:49,000 --> 00:03:53,000
 I think the number of unknown parameters is not much, right?

21
00:03:53,000 --> 00:04:01,000
 I think because actually the unknown values or unknown parameters in the current matrix, d by d, right, divided by two.

22
00:04:01,000 --> 00:04:10,000
 Okay. d, if d is just ten, d by d is 100 divided by two is 50, and y divided by two because the matrix is symmetric.

23
00:04:10,000 --> 00:04:12,000
 Okay. So that's the current matrix.

24
00:04:12,000 --> 00:04:21,000
 And then in the feature, in the mean vector, and we have two, and we have ten values, right?

25
00:04:21,000 --> 00:04:25,000
 So totally, we have about 60 unknown parameters.

26
00:04:25,000 --> 00:04:28,000
 Okay. So I think this is low dimension data.

27
00:04:28,000 --> 00:04:35,000
 And there's no problem, no issue to estimate these two type of parameters, the mean vector and the current matrix.

28
00:04:35,000 --> 00:04:47,000
 But you can imagine if the data is a high dimensional, for example, d, like 100, okay, this is actually quite normal, right, in pentacletic equation in machine learning.

29
00:04:47,000 --> 00:04:57,000
 Okay. And then d times d, 100 times 100, and then 10,000, and divide by two, 50,000, 5,000.

30
00:04:58,000 --> 00:05:10,000
 Okay. And then in the mean vector, we still, we also have actually like a vector, 100 dimensional vector, right?

31
00:05:10,000 --> 00:05:13,000
 So there are 100 unknown parameters.

32
00:05:13,000 --> 00:05:21,000
 So together, we could have actually like 5,000 and then 100 unknown parameters.

33
00:05:21,000 --> 00:05:30,000
 Okay. And actually, if we just have a small number of, you know, training samples, like even a few thousand, right?

34
00:05:30,000 --> 00:05:35,000
 The estimation of these parameters could be inaccurate.

35
00:05:35,000 --> 00:05:44,000
 Although, you know, in the last week, we have not derived the maximum likelihood parameter estimation, right, from the mean vector for the current matrix.

36
00:05:44,000 --> 00:05:51,000
 But if the number of samples is small, and this estimation for the current matrix, I think could not be accurate.

37
00:05:51,000 --> 00:06:07,000
 Okay. So in practice, if the demagnitude is very high, and normally we don't use the, you know, the Gaussian function, right, to estimate the density function.

38
00:06:07,000 --> 00:06:15,000
 Okay. But in such a scenario, actually, we can actually assume the interdependence between the features.

39
00:06:15,000 --> 00:06:29,000
 Okay. And then for each interdependence, and then the class kernel of density function becomes the multiplication of these, actually, d-density functions.

40
00:06:29,000 --> 00:06:32,000
 For each dimension, we have a density function.

41
00:06:32,000 --> 00:06:44,000
 Okay. So, I mean, each density function, we have two parameters, because actually for one scale and for one feature, actually, one parameter is the mean value, right?

42
00:06:44,000 --> 00:06:51,000
 Another is a variance or standard deviation. So, totally, we have two parameters, right?

43
00:06:51,000 --> 00:07:00,000
 So, if we have a d Gaussian function here, so, totally, we only have a 2-d, right? 2-d parameters.

44
00:07:01,000 --> 00:07:10,000
 Okay. You can see for each kernel, this number from, you know, d squared divided by 2 plus d, reduce 2-d.

45
00:07:10,000 --> 00:07:16,000
 Okay. Even the d is 100. Then we only have about 200, actually, unknown parameters, right?

46
00:07:16,000 --> 00:07:29,000
 Okay. So, actually, in practice, if the demagnitude is very high, and often we don't use the multivariant normal density function to estimate the distribution of the data.

47
00:07:29,000 --> 00:07:35,000
 Okay. Instead, actually, we can use, actually, the naive base.

48
00:07:35,000 --> 00:07:48,000
 Okay. And in the last week, actually, we have talked about this point, the naive base, and we assume, actually, the independence of the features.

49
00:07:49,000 --> 00:07:59,000
 That means that we all need a current matrix to, all the, you know, the off-diagonal elements are zeros.

50
00:07:59,000 --> 00:08:07,000
 Of course, this is not the scenario, right? In practice, the data could be, you know, the features could be independent.

51
00:08:07,000 --> 00:08:16,000
 But, actually, even this assumption is violated in practice. But, actually, the naive base shows good performance.

52
00:08:17,000 --> 00:08:24,000
 Okay. So, that's the reason, you know, I think in most of the toolbox, they have the naive base that we classify.

53
00:08:24,000 --> 00:08:32,000
 Okay. And so, last week, I've talked about the three types of the naive base.

54
00:08:32,000 --> 00:08:39,000
 One is the Gaussian naive base. So, here, this, you know, we assume the features are continuous, right?

55
00:08:39,000 --> 00:08:46,000
 And then, for each feature, we can use Gaussian function to construct, actually, the density function.

56
00:08:46,000 --> 00:08:50,000
 Okay. And then, another is, you know, Bernoulli and naive base.

57
00:08:50,000 --> 00:08:57,000
 So, this is used, actually, when the features are discrete, in particular, when the features are binary.

58
00:08:57,000 --> 00:09:02,000
 For example, when we talk about features, right, if a gender is a feature, we only have two values, right?

59
00:09:02,000 --> 00:09:06,000
 Male or female. So, actually, this is a binary.

60
00:09:06,000 --> 00:09:15,000
 Okay. And if it's about another feature, for example, like weather students, they come today or not coming, right?

61
00:09:15,000 --> 00:09:18,000
 So, yes or no, right? So, also, two values.

62
00:09:18,000 --> 00:09:21,000
 Okay. Then, we can use the Bernoulli naive base.

63
00:09:21,000 --> 00:09:27,000
 And then, if the features could take multiple values, and then we have the multinomial naive base.

64
00:09:27,000 --> 00:09:32,000
 And this method is quite often used, actually, in the test class, efficient task.

65
00:09:32,000 --> 00:09:39,000
 So, later, we will look at all these methods, okay, through the examples.

66
00:09:39,000 --> 00:09:48,000
 Okay. I think the first part, actually, for the first method, naive, Gaussian naive base, actually, we assume the features are continuous features, right?

67
00:09:48,000 --> 00:09:59,000
 Continuous features. And then, we already know, you know, actually, for continuous features, and for one dimension, we can use the univariate normal function, right, to build, actually, the density function.

68
00:09:59,000 --> 00:10:06,000
 Okay. So, this is actually, this is the class conditional density function, right?

69
00:10:06,000 --> 00:10:14,000
 Given omega j, the class. So, what is the distribution of the x? What is the density of x? What is the probability of x?

70
00:10:14,000 --> 00:10:19,000
 So, here, this is the Gaussian function, right? We have two unknown parameters.

71
00:10:19,000 --> 00:10:27,000
 One is the mu j for class j, right? Which is the mean value, okay, of the feature, right, of my specific feature.

72
00:10:27,000 --> 00:10:33,000
 And sigma j is a standard division of covariance square, right?

73
00:10:33,000 --> 00:10:39,000
 Sigma square is a variance. Okay. So, I think this is for univariate. I think we have no problem, right?

74
00:10:39,000 --> 00:10:45,000
 So, okay, so we just, yeah, I think we, you know, these are very special cases, right?

75
00:10:45,000 --> 00:10:55,000
 Actually, of a multivariate normal function. This is the case when the denominator equals one.

76
00:10:55,000 --> 00:11:01,000
 Okay. Let's look at the Bernoulli-Naeve base. Okay.

77
00:11:01,000 --> 00:11:10,000
 So, the features can only have two possible values, right? One is the, one, another is minus one.

78
00:11:10,000 --> 00:11:18,000
 One is zero, one another is zero. So, one is a fill, another is a pass. One is male, another is female.

79
00:11:18,000 --> 00:11:31,000
 Okay. So, this is kind of a binary feature. And for binary features, and if the probability that the random variable takes one value, it's r.

80
00:11:31,000 --> 00:11:38,000
 And then the probability of taking another value is one minus r. Okay. Because the summation should be equals one, right?

81
00:11:38,000 --> 00:11:46,000
 Should be equals one. Okay. So, this is the Bernoulli distribution.

82
00:11:46,000 --> 00:11:55,000
 And, okay. So, that's actually, we, we explained this Bernoulli-Naeve base through one example.

83
00:11:55,000 --> 00:12:02,000
 Okay. So, here, we have actually one, two, three, four, five, actually, training data for illustration purpose, right?

84
00:12:02,000 --> 00:12:05,000
 We can really have a small number of samples here.

85
00:12:05,000 --> 00:12:10,000
 In practice, no number could be, you know, a thousand, right? A hundred, a thousand. Okay.

86
00:12:10,000 --> 00:12:19,000
 And then in this data set, we have three features. And each of the three features actually is a binary feature, discrete.

87
00:12:19,000 --> 00:12:26,000
 Okay. So, the feature name is Confident. And then, study, a SICK. Okay.

88
00:12:26,000 --> 00:12:30,000
 So, whether a student is a Confident, yes or no, right? Two values.

89
00:12:30,000 --> 00:12:35,000
 Whether a student spends time on this work, on this course, study or not, right?

90
00:12:35,000 --> 00:12:40,000
 So, it's a yes or no. And then, during the exam, whether a student is sick or not, right? Yes or no.

91
00:12:40,000 --> 00:12:46,000
 So, each of the features is a binary feature. Okay.

92
00:12:46,000 --> 00:12:53,000
 And then, the last column, the result, actually, is a class label. Okay.

93
00:12:53,000 --> 00:12:58,000
 It's a result that students feel or pass the exam, right? Okay.

94
00:12:58,000 --> 00:13:03,000
 And so, now we have a new sample. This sample, we can see a testing sample, right?

95
00:13:03,000 --> 00:13:09,000
 So, whether or not a Confident is a yes, a study is a yes, SICK, no.

96
00:13:09,000 --> 00:13:12,000
 It's not SICK during the exam time. Okay.

97
00:13:12,000 --> 00:13:22,000
 So, we want to find the probability that this sample, okay, pass the exam or fill the exam, right?

98
00:13:22,000 --> 00:13:27,000
 So, finally, we can predict the result of the exam. Okay.

99
00:13:27,000 --> 00:13:33,000
 So, actually, here we assume actually, you know, the three features, right?

100
00:13:33,000 --> 00:13:38,000
 Confident, study, SICK, adenutrient by X1, X2, X3, right?

101
00:13:38,000 --> 00:13:40,000
 So, these are the three features. Okay.

102
00:13:40,000 --> 00:13:51,000
 And so, based on the theorem, so, we need to calculate the posterior probability.

103
00:13:52,000 --> 00:13:59,000
 So, the X is a vector and the value is a yes, yes, no, right?

104
00:13:59,000 --> 00:14:03,000
 So, this is the X, okay?

105
00:14:03,000 --> 00:14:10,000
 And so, here, actually, we want to use the naive base.

106
00:14:10,000 --> 00:14:17,000
 Okay. And so, we assume the interdependence between the features, right?

107
00:14:17,000 --> 00:14:22,000
 So, whether it's in the confident, whether it's in the studied or whether it's in the SICK, actually,

108
00:14:22,000 --> 00:14:25,000
 they are uncorrelated, right?

109
00:14:25,000 --> 00:14:29,000
 So, these features are actually independent, okay?

110
00:14:29,000 --> 00:14:33,000
 And then, we can use the naive base method.

111
00:14:33,000 --> 00:14:38,000
 Okay. So, we don't need to calculate, evaluate this, you know, density function.

112
00:14:38,000 --> 00:14:43,000
 Instead, we can evaluate the density for each feature.

113
00:14:44,000 --> 00:14:49,000
 Okay. Then, multiply the three functions.

114
00:14:49,000 --> 00:14:51,000
 Okay. And then, time the p omega j.

115
00:14:51,000 --> 00:14:55,000
 p omega j is the probability.

116
00:14:55,000 --> 00:14:57,000
 Okay. So, we want to solve the problem, right?

117
00:14:57,000 --> 00:15:08,000
 So, here, actually, we need to find, actually, the pxi omega j.

118
00:15:09,000 --> 00:15:12,000
 For p omega j, I think it's easy to estimate, right?

119
00:15:12,000 --> 00:15:14,000
 To estimate, okay?

120
00:15:14,000 --> 00:15:19,000
 So, actually, now, if we look at the table, like, if we look at the table,

121
00:15:19,000 --> 00:15:25,000
 and, actually, the probability, you know, we can count the number of samples that pass the exam,

122
00:15:25,000 --> 00:15:28,000
 the number of samples that fill the exam,

123
00:15:28,000 --> 00:15:37,000
 and then, we can calculate, you know, the probability of passing the exam or fill the exam.

124
00:15:37,000 --> 00:15:44,000
 Okay. So, actually, if we look at the example, and so, there are five students and two students fill,

125
00:15:44,000 --> 00:15:47,000
 and so, three students pass.

126
00:15:47,000 --> 00:15:52,000
 So, if we assume omega y is the path, and omega 2 is fill, right?

127
00:15:52,000 --> 00:15:58,000
 Then, the probability of omega y is 3 fifths, so, 0.6, right?

128
00:15:58,000 --> 00:16:06,000
 And then, the p omega 2 fill, the probability is 2 fifths, is 0.4.

129
00:16:06,000 --> 00:16:10,000
 Okay. So, from this table, now, we can easily, you know,

130
00:16:10,000 --> 00:16:15,000
 count the number of samples fill the exam, and the number of students pass the exam.

131
00:16:15,000 --> 00:16:18,000
 Then, we look at the percentage, okay?

132
00:16:18,000 --> 00:16:21,000
 Then, that is just the probability.

133
00:16:21,000 --> 00:16:23,000
 So, this is easy to estimate.

134
00:16:23,000 --> 00:16:29,000
 Okay. And then, let's assume, to estimate the class conditional probability,

135
00:16:29,000 --> 00:16:35,000
 class conditional, let me assume, actually, this student fill the exam.

136
00:16:35,000 --> 00:16:39,000
 And then, we look at the distribution of the values.

137
00:16:39,000 --> 00:16:45,000
 Okay. We look at the probability of X1, you know, take a value of yes,

138
00:16:45,000 --> 00:16:48,000
 and the probability, you know, of X2 to the value of yes.

139
00:16:48,000 --> 00:16:53,000
 The probability of X3 takes the value of no.

140
00:16:53,000 --> 00:16:56,000
 You can look at this. Okay.

141
00:16:56,000 --> 00:17:00,000
 So, we'll calculate this.

142
00:17:00,000 --> 00:17:05,000
 When the student fill the exam, what is the probability of X1 equal to yes?

143
00:17:05,000 --> 00:17:09,000
 What is the probability of X2, right?

144
00:17:09,000 --> 00:17:14,000
 Study equal to the value of yes, and X3, whether it's in the 6 or not, right?

145
00:17:14,000 --> 00:17:17,000
 And take a value of no. We want to look at these probabilities.

146
00:17:17,000 --> 00:17:19,000
 Yes, yes, no, right?

147
00:17:19,000 --> 00:17:24,000
 And the different, given different conditions, pass the exam, fill the exam.

148
00:17:24,000 --> 00:17:28,000
 Okay. So, let's look at the table.

149
00:17:28,000 --> 00:17:35,000
 Okay. So, we want to calculate, you know, the conditional, the class conditional density,

150
00:17:35,000 --> 00:17:39,000
 or class conditional probabilities for pass.

151
00:17:39,000 --> 00:17:42,000
 We just look at the three students that pass the exam.

152
00:17:42,000 --> 00:17:44,000
 Okay. Just look at the three students.

153
00:17:44,000 --> 00:17:51,000
 The first student, the second student pass, the fourth student pass, the fifth student pass.

154
00:17:51,000 --> 00:17:54,000
 Only the three examples.

155
00:17:54,000 --> 00:17:56,000
 Okay. The three samples, the passing exam.

156
00:17:56,000 --> 00:18:00,000
 The three samples in class omega one.

157
00:18:00,000 --> 00:18:05,000
 Okay. And we look at, and then among the three students, then actually the first student,

158
00:18:05,000 --> 00:18:08,000
 the confidence, actually the feature number one is yes.

159
00:18:08,000 --> 00:18:12,000
 And then the second student pass the exam is no.

160
00:18:12,000 --> 00:18:17,000
 And then the third pass the exam has a value of yes.

161
00:18:17,000 --> 00:18:22,000
 Okay. So, under this condition, the student is a student passing the exam.

162
00:18:22,000 --> 00:18:28,000
 And under this condition, the confidence, actually two has a value of confidence.

163
00:18:28,000 --> 00:18:30,000
 One has a value of no.

164
00:18:30,000 --> 00:18:37,000
 So, that means actually the value, actually the probability that the confidence,

165
00:18:37,000 --> 00:18:41,000
 take a value of yes is to the, under this condition,

166
00:18:41,000 --> 00:18:44,000
 given the student pass the exam.

167
00:18:44,000 --> 00:18:50,000
 Okay. So, we just look at the student pass the exam in this class omega one sample.

168
00:18:50,000 --> 00:18:53,000
 And then we look at the third part, right?

169
00:18:53,000 --> 00:18:56,000
 How many have a value of yes and how many have a value of no.

170
00:18:56,000 --> 00:18:59,000
 And then finally we find the percentage.

171
00:18:59,000 --> 00:19:02,000
 So, that is the probability.

172
00:19:02,000 --> 00:19:06,000
 Okay. So, for the first feature, and the student pass.

173
00:19:06,000 --> 00:19:08,000
 Okay. So, we look at the third sample.

174
00:19:08,000 --> 00:19:13,000
 The first feature that take a value of yes, actually is to that.

175
00:19:13,000 --> 00:19:16,000
 Okay. And then we look at the second feature, studied.

176
00:19:16,000 --> 00:19:21,000
 And among the three students pass into one value of no.

177
00:19:21,000 --> 00:19:25,000
 And other two values, yes, yes. Right? Yes, yes.

178
00:19:25,000 --> 00:19:35,000
 So, again, actually the value, the probability of taking a value of yes for feature number two, studied, is to that.

179
00:19:35,000 --> 00:19:39,000
 Okay. So, this is the second, actually, probability, right?

180
00:19:39,000 --> 00:19:45,000
 The class conditional probability for feature two, that is studied.

181
00:19:45,000 --> 00:19:48,000
 And then for feature three, the sick, right?

182
00:19:48,000 --> 00:19:56,000
 The sick, actually, we look at all the three samples, actually, once, yes, another no, right?

183
00:19:56,000 --> 00:20:04,000
 And then another yes. Sooner sick, probability, actually one done.

184
00:20:04,000 --> 00:20:08,000
 Because only one student, no, did not sick, right?

185
00:20:08,000 --> 00:20:11,000
 Is not sick, actually, during the exam, soon as no.

186
00:20:11,000 --> 00:20:14,000
 But this is yes, this is yes. Okay.

187
00:20:14,000 --> 00:20:22,000
 So, the probability of taking a value of no for that feature sick, actually, is one done.

188
00:20:22,000 --> 00:20:36,000
 Okay. So, actually, from the table, from the given sample, we can easily estimate, actually, the probability and the class conditional probabilities.

189
00:20:36,000 --> 00:20:39,000
 Right? Okay.

190
00:20:39,000 --> 00:20:42,000
 So, we don't use a tree like a Gaussian function.

191
00:20:42,000 --> 00:20:46,000
 Instead, we call the number of samples in each class.

192
00:20:46,000 --> 00:20:52,000
 Or the number of samples are taken on one of the two values.

193
00:20:52,000 --> 00:20:57,000
 And then we look at the percentage of taking one of the two values.

194
00:20:57,000 --> 00:21:01,000
 So, that is the class conditional probabilities.

195
00:21:01,000 --> 00:21:03,000
 Okay.

196
00:21:03,000 --> 00:21:09,000
 Okay. So, now, actually, from this, actually, no table, we can easily find a tree, this, no, tree fifth, right?

197
00:21:09,000 --> 00:21:11,000
 Because three single parts are two single fields.

198
00:21:11,000 --> 00:21:14,000
 So, this is the probability of one, six, okay.

199
00:21:14,000 --> 00:21:18,000
 And also, just now, we have analyzed and we have calculated, right?

200
00:21:18,000 --> 00:21:25,000
 The probability for feature one to take a value of yes to that.

201
00:21:25,000 --> 00:21:31,000
 Feature number two, taking a value of two, and the yes has a probability of two, that.

202
00:21:31,000 --> 00:21:39,000
 Feature number three, taking a value of no, has a probability of one, that.

203
00:21:39,000 --> 00:21:47,000
 Okay. So, we have, similarly, for class two, we can also estimate the probability, right?

204
00:21:47,000 --> 00:21:54,000
 Of x1, take a tree, value of yes, x2, take a value of yes, x3, take a no.

205
00:21:55,000 --> 00:22:02,000
 And, or given, the samples are between the omega two samples.

206
00:22:02,000 --> 00:22:06,000
 Okay. So, I think then, just look at the table, right?

207
00:22:06,000 --> 00:22:10,000
 Again, for class two, we have two samples here.

208
00:22:10,000 --> 00:22:12,000
 Field, field, right?

209
00:22:12,000 --> 00:22:18,000
 And then the value of taking yes, 50%.

210
00:22:18,000 --> 00:22:20,000
 Because one is yes, one another no, right?

211
00:22:20,000 --> 00:22:24,000
 And then for feature two study, the two field students, no.

212
00:22:24,000 --> 00:22:26,000
 And then another is yes.

213
00:22:26,000 --> 00:22:32,000
 The probability of taking a value of yes is also one half, right?

214
00:22:32,000 --> 00:22:33,000
 One half.

215
00:22:33,000 --> 00:22:35,000
 Then for the third one, take a value of no.

216
00:22:35,000 --> 00:22:39,000
 So, here is one, the part is sick, another is yes.

217
00:22:39,000 --> 00:22:42,000
 So, it's also actually one half.

218
00:22:42,000 --> 00:22:47,000
 Okay. So, we can easily estimate all these values, right?

219
00:22:47,000 --> 00:22:52,000
 So, this is conditional probabilities for class two, right?

220
00:22:52,000 --> 00:22:58,000
 Class two, conditional probabilities of taking yes, yes, and no.

221
00:22:58,000 --> 00:23:01,000
 Okay. And then based on these, actually, you know,

222
00:23:01,000 --> 00:23:06,000
 conditional probabilities for each individual feature, right?

223
00:23:06,000 --> 00:23:08,000
 And then we can multiply them together, right?

224
00:23:08,000 --> 00:23:12,000
 The three, actually, you know, conditional probabilities.

225
00:23:12,000 --> 00:23:14,000
 We multiply them together, right?

226
00:23:14,000 --> 00:23:19,000
 Because here we assume the interdependence between the features.

227
00:23:19,000 --> 00:23:24,000
 And then we can calculate the joint probability of x and omega one, right?

228
00:23:24,000 --> 00:23:25,000
 Yes, yes, no.

229
00:23:25,000 --> 00:23:27,000
 Actually, this is x.

230
00:23:27,000 --> 00:23:30,000
 So, to that, to that, okay?

231
00:23:30,000 --> 00:23:38,000
 And this is the joint probability of omega two with a given x at 0.05.

232
00:23:38,000 --> 00:23:39,000
 Okay.

233
00:23:39,000 --> 00:23:46,000
 And actually, we can make decisions based on this joint probability, right?

234
00:23:46,000 --> 00:23:51,000
 Without having to use actually the posterior probability, okay?

235
00:23:51,000 --> 00:23:54,000
 Because the Px is common, it's a scaling factor, right?

236
00:23:54,000 --> 00:23:56,000
 We can, you know.

237
00:23:56,000 --> 00:23:58,000
 And then we compare the two values.

238
00:23:58,000 --> 00:24:06,000
 And actually, we can find that the joint probability of the x and omega one is greater, right?

239
00:24:06,000 --> 00:24:13,000
 So, we can find that the joint probability of the x and omega one is greater than 0.08.

240
00:24:13,000 --> 00:24:14,000
 Okay.

241
00:24:14,000 --> 00:24:15,000
 Another 0.05.

242
00:24:15,000 --> 00:24:19,000
 So, actually, we classify the student to class omega one.

243
00:24:19,000 --> 00:24:24,000
 In other words, the student, we predict the student will pass the exam.

244
00:24:24,000 --> 00:24:25,000
 Okay.

245
00:24:25,000 --> 00:24:35,000
 So, I think this is just, this is the, see the, now based, okay, when the features,

246
00:24:35,000 --> 00:24:43,000
 okay, let's look at the multinomial, nine base, okay?

247
00:24:43,000 --> 00:24:47,000
 And actually, here the features also discrete, discrete.

248
00:24:47,000 --> 00:24:51,000
 But the value they take is more than two, not just binary.

249
00:24:51,000 --> 00:24:53,000
 Can have different values, right?

250
00:24:53,000 --> 00:24:54,000
 And they have different values.

251
00:24:54,000 --> 00:24:55,000
 But it is discrete.

252
00:24:55,000 --> 00:24:56,000
 Discrete.

253
00:24:56,000 --> 00:24:57,000
 Okay.

254
00:24:57,000 --> 00:25:02,000
 And this is quite often, actually, they use, actually, in the test classification.

255
00:25:02,000 --> 00:25:03,000
 Okay.

256
00:25:03,000 --> 00:25:08,000
 And so, here, now, we have an example of test classification.

257
00:25:08,000 --> 00:25:15,000
 So, these are labeled in detail.

258
00:25:15,000 --> 00:25:16,000
 Okay.

259
00:25:16,000 --> 00:25:20,000
 So, the first rule is test and is an input, right?

260
00:25:20,000 --> 00:25:24,000
 And the second column, the first column is an input.

261
00:25:24,000 --> 00:25:29,000
 The second column, which is the class label.

262
00:25:29,000 --> 00:25:30,000
 Okay.

263
00:25:30,000 --> 00:25:34,000
 So, the question of the class is, this is, we only have two possible values.

264
00:25:34,000 --> 00:25:36,000
 One is the spot, not the non-spot.

265
00:25:36,000 --> 00:25:37,000
 Okay.

266
00:25:37,000 --> 00:25:42,000
 So, this is the two class classification problems.

267
00:25:42,000 --> 00:25:45,000
 And the inputs are tests.

268
00:25:45,000 --> 00:25:46,000
 Okay.

269
00:25:46,000 --> 00:25:53,000
 So, this kind of classification is quite, you know, is a quite a standard question in

270
00:25:53,000 --> 00:25:56,000
 the natural language procession, the classification of a given test.

271
00:25:56,000 --> 00:26:00,000
 For example, we have news, right, the news from different categories.

272
00:26:00,000 --> 00:26:06,000
 Now, we kind of classify a news into, like, a category of education, into a category

273
00:26:06,000 --> 00:26:12,000
 of, you know, sports, into a category about, you know, the fitness, into a category about,

274
00:26:12,000 --> 00:26:14,000
 you know, the entertainment.

275
00:26:14,000 --> 00:26:18,000
 And then, it can be put into a category of technology and science, right?

276
00:26:18,000 --> 00:26:20,000
 So, we can put them in different categories.

277
00:26:20,000 --> 00:26:22,000
 This is a news classification.

278
00:26:22,000 --> 00:26:23,000
 Okay.

279
00:26:23,000 --> 00:26:31,000
 So, based on the content of the, based on the word, the wording of the, of the, of the,

280
00:26:31,000 --> 00:26:33,000
 of the, in the test, right?

281
00:26:33,000 --> 00:26:34,000
 And we classify.

282
00:26:34,000 --> 00:26:37,000
 And also, like, you know, like, sentiment analysis.

283
00:26:37,000 --> 00:26:38,000
 Okay.

284
00:26:38,000 --> 00:26:47,000
 So, based on your comment, right, on a certain policy, and if you can classify your opinion

285
00:26:47,000 --> 00:26:53,000
 or sentiment into, like, positive sentiment, a negative sentiment, or neutral.

286
00:26:53,000 --> 00:26:54,000
 Okay.

287
00:26:54,000 --> 00:27:00,000
 These are actually, you know, standard, actually, class class, feeling problem.

288
00:27:00,000 --> 00:27:01,000
 Okay.

289
00:27:01,000 --> 00:27:06,000
 So, here, we are given this tree, one, two, three, four, five, training samples.

290
00:27:06,000 --> 00:27:12,000
 And so, these are the label training samples, because it's a learning problem, right?

291
00:27:12,000 --> 00:27:16,000
 And based on these training samples, we want to classify a new sample, a test sample.

292
00:27:16,000 --> 00:27:17,000
 Okay.

293
00:27:17,000 --> 00:27:26,000
 So, this sample is to classify the testing sample, a very close game.

294
00:27:26,000 --> 00:27:28,000
 So, this, so this is the input.

295
00:27:28,000 --> 00:27:34,000
 So, we want to classify this input into one of the two categories, sports or non-sports.

296
00:27:34,000 --> 00:27:36,000
 Of course, for us, you know, we have no problem.

297
00:27:36,000 --> 00:27:38,000
 We're trying to look at this, right?

298
00:27:38,000 --> 00:27:39,000
 We have to classify.

299
00:27:39,000 --> 00:27:40,000
 Okay.

300
00:27:40,000 --> 00:27:43,000
 And now, we want to use, you know, use a machine learning model, right?

301
00:27:43,000 --> 00:27:51,000
 And to classify, and this actually input, this test, okay, into one of the two categories,

302
00:27:51,000 --> 00:27:53,000
 sports or non-sports.

303
00:27:53,000 --> 00:27:54,000
 Okay.

304
00:27:54,000 --> 00:27:58,000
 And so, here, again, we can start from the posterior probability, right?

305
00:27:58,000 --> 00:28:03,000
 So, we need to know, calculate the posterior probability, or the general probability of

306
00:28:03,000 --> 00:28:04,000
 this input.

307
00:28:04,000 --> 00:28:07,000
 The input is just a very close game.

308
00:28:07,000 --> 00:28:09,000
 So, this is the input.

309
00:28:09,000 --> 00:28:10,000
 Okay.

310
00:28:10,000 --> 00:28:17,000
 And we want to calculate, you know, this actually the probability, the posterior probability

311
00:28:17,000 --> 00:28:26,000
 of class one, sports, and the posterior probability of class two, non-sports, okay, given this

312
00:28:26,000 --> 00:28:27,000
 input.

313
00:28:27,000 --> 00:28:28,000
 So, this is just the X.

314
00:28:28,000 --> 00:28:32,000
 The X here is a short sentence.

315
00:28:32,000 --> 00:28:33,000
 Okay.

316
00:28:33,000 --> 00:28:34,000
 Okay.

317
00:28:34,000 --> 00:28:39,000
 So, here, based on the, actually, Na'yi, based, based on the, based on the, based

318
00:28:39,000 --> 00:28:40,000
 theorem, right?

319
00:28:40,000 --> 00:28:51,000
 So, actually, the, you know, the omega one, given X, so the X is a various, you know,

320
00:28:51,000 --> 00:28:52,000
 close game, right?

321
00:28:52,000 --> 00:29:00,000
 So, this should be equal to the general probability of X one, X and omega, omega, sport, right?

322
00:29:00,000 --> 00:29:01,000
 Omega one.

323
00:29:01,000 --> 00:29:02,000
 Okay.

324
00:29:02,000 --> 00:29:08,000
 So, a very close game, given sports, and times the probability of sports.

325
00:29:08,000 --> 00:29:09,000
 Okay.

326
00:29:09,000 --> 00:29:14,000
 And so, here, actually, of course, actually, a very close game, right?

327
00:29:14,000 --> 00:29:17,000
 Given sports, and we have a few words, one, two, three, four.

328
00:29:17,000 --> 00:29:18,000
 Okay.

329
00:29:18,000 --> 00:29:21,000
 So, we assume that these four words are independent.

330
00:29:21,000 --> 00:29:22,000
 Are independent.

331
00:29:22,000 --> 00:29:27,000
 Of course, in the generation of a sentence, then they may not be no independent, right?

332
00:29:27,000 --> 00:29:34,000
 Actually, normally, actually, we know the words that follow, actually, you know.

333
00:29:34,000 --> 00:29:38,000
 It's, actually, the word, actually, like a game, right?

334
00:29:38,000 --> 00:29:43,000
 Maybe, and it's related to some of the words, you know, like close.

335
00:29:43,000 --> 00:29:44,000
 Okay.

336
00:29:44,000 --> 00:29:46,000
 So, they are not really no independent.

337
00:29:46,000 --> 00:29:47,000
 Okay.

338
00:29:47,000 --> 00:29:50,000
 But we, here, we assume they are independent.

339
00:29:50,000 --> 00:29:51,000
 Okay.

340
00:29:51,000 --> 00:29:56,000
 And, actually, they are not, right?

341
00:29:56,000 --> 00:30:01,000
 Because the large-lang model, the model, the language model, we use the sequence of words

342
00:30:01,000 --> 00:30:03,000
 to predict the next word.

343
00:30:03,000 --> 00:30:06,000
 So, that means, actually, the words are not really independent.

344
00:30:06,000 --> 00:30:09,000
 But here, we assume they are independent.

345
00:30:09,000 --> 00:30:10,000
 Okay.

346
00:30:10,000 --> 00:30:16,000
 And then, these, actually, just equal to the probability of a given sports, probability

347
00:30:16,000 --> 00:30:24,000
 of a very given sports, times the probability of a close given sports, probably of a game,

348
00:30:24,000 --> 00:30:26,000
 actually, given sports.

349
00:30:26,000 --> 00:30:27,000
 Okay.

350
00:30:27,000 --> 00:30:30,000
 Then, times the probability of a sports.

351
00:30:30,000 --> 00:30:31,000
 Okay.

352
00:30:31,000 --> 00:30:35,000
 And, soon, that's actually, we need to estimate the probability of a, right?

353
00:30:35,000 --> 00:30:39,000
 And, the probability of a sports.

354
00:30:39,000 --> 00:30:42,000
 I think this, actually, can be easily estimated, right?

355
00:30:42,000 --> 00:30:49,000
 And, based on the number of samples in the category of sports.

356
00:30:49,000 --> 00:30:56,000
 And, then, we calculate the percentage, right, of the sports news.

357
00:30:56,000 --> 00:30:57,000
 Okay.

358
00:30:57,000 --> 00:31:06,000
 And, then, actually, we can find this, actually, the probability of a sports.

359
00:31:06,000 --> 00:31:07,000
 Okay.

360
00:31:07,000 --> 00:31:11,000
 And, then, how to calculate a given sports?

361
00:31:11,000 --> 00:31:14,000
 How to calculate a given sports?

362
00:31:14,000 --> 00:31:22,000
 I think, just now, we already, you know, answered this right through an example in the classification,

363
00:31:22,000 --> 00:31:25,000
 the single field part, right?

364
00:31:25,000 --> 00:31:29,000
 So, given the class sports, that means we're just going to look at the sample in the sports

365
00:31:29,000 --> 00:31:30,000
 in this category.

366
00:31:30,000 --> 00:31:33,000
 So, what is the probability of a, right?

367
00:31:33,000 --> 00:31:38,000
 So, how to calculate probability, that means that we need to count how many words, actually,

368
00:31:38,000 --> 00:31:44,000
 in this, you know, all the news, all the tests, under the category of sports.

369
00:31:44,000 --> 00:31:46,000
 And, we look at the total words.

370
00:31:46,000 --> 00:31:49,000
 And, then, we look at the percentage of words, a.

371
00:31:49,000 --> 00:31:51,000
 So, this is the probability, right?

372
00:31:51,000 --> 00:31:53,000
 This is the probability.

373
00:31:53,000 --> 00:32:00,000
 So, for a very given sport, right, we also need to look at all the news under the category

374
00:32:00,000 --> 00:32:01,000
 of sports.

375
00:32:01,000 --> 00:32:06,000
 And, then, we look at how many words, and the words appear how many times.

376
00:32:06,000 --> 00:32:07,000
 Okay.

377
00:32:07,000 --> 00:32:12,000
 We count, actually, the word, the word, right?

378
00:32:12,000 --> 00:32:16,000
 So, then, we also look at the percentage.

379
00:32:16,000 --> 00:32:17,000
 Okay.

380
00:32:17,000 --> 00:32:22,000
 So, this is the estimation of the class conditional probabilities.

381
00:32:22,000 --> 00:32:23,000
 Okay.

382
00:32:23,000 --> 00:32:26,000
 So, then, we go back to this, right?

383
00:32:26,000 --> 00:32:27,000
 Okay.

384
00:32:27,000 --> 00:32:32,000
 So, for the sports, number, sample number one, number three, number four, right?

385
00:32:32,000 --> 00:32:34,000
 Number one, number three, number four, okay.

386
00:32:34,000 --> 00:32:36,000
 So, how many words?

387
00:32:36,000 --> 00:32:37,000
 One, two, three, right?

388
00:32:37,000 --> 00:32:39,000
 For sport, the first one.

389
00:32:39,000 --> 00:32:42,000
 The second one, very clean match, three, right?

390
00:32:42,000 --> 00:32:44,000
 And, now, we have six words.

391
00:32:44,000 --> 00:32:48,000
 And, then, for sports, one, two, three, four, five.

392
00:32:48,000 --> 00:32:52,000
 So, together, we have, you know, six plus five, 11 words.

393
00:32:52,000 --> 00:32:55,000
 We have 11 words.

394
00:32:55,000 --> 00:33:00,000
 And, for the training samples, right, in the category of sports.

395
00:33:00,000 --> 00:33:02,000
 So, these are total number of words.

396
00:33:02,000 --> 00:33:03,000
 Okay.

397
00:33:03,000 --> 00:33:06,000
 And, then, we look at how many is here.

398
00:33:06,000 --> 00:33:08,000
 A very, a great game.

399
00:33:08,000 --> 00:33:09,000
 We have one, right?

400
00:33:09,000 --> 00:33:11,000
 And, then, we don't have here.

401
00:33:11,000 --> 00:33:12,000
 We don't have here.

402
00:33:12,000 --> 00:33:18,000
 And, then, also, we have a clean but forgettable, actually, a game.

403
00:33:18,000 --> 00:33:21,000
 Actually, the answer appears twice.

404
00:33:21,000 --> 00:33:22,000
 Okay.

405
00:33:22,000 --> 00:33:25,000
 So, totally, we have 11 words, right?

406
00:33:25,000 --> 00:33:28,000
 And, by this A, actually, now, the count is two.

407
00:33:28,000 --> 00:33:29,000
 Okay.

408
00:33:29,000 --> 00:33:36,000
 So, the probability of A given sports, actually, is a two, 11th.

409
00:33:36,000 --> 00:33:37,000
 Okay.

410
00:33:37,000 --> 00:33:43,000
 So, we just know the estimation of the class, actually, you know, conditional probability.

411
00:33:43,000 --> 00:33:46,000
 And, similarly, we can calculate the worry, right?

412
00:33:46,000 --> 00:33:48,000
 What is the how many worry here?

413
00:33:48,000 --> 00:33:50,000
 No worry in the first sample.

414
00:33:50,000 --> 00:33:54,000
 And, no, one worry in the second sample.

415
00:33:54,000 --> 00:33:56,000
 Only one worry, right?

416
00:33:56,000 --> 00:33:59,000
 So, the count is one, 11th.

417
00:33:59,000 --> 00:34:00,000
 Okay.

418
00:34:00,000 --> 00:34:01,000
 Close.

419
00:34:01,000 --> 00:34:04,000
 No close.

420
00:34:04,000 --> 00:34:07,000
 The count is zero.

421
00:34:07,000 --> 00:34:10,000
 So, whether the probability is zero.

422
00:34:10,000 --> 00:34:15,000
 If the probability is zero, then the multiplication of the three, actually, no, four, actually,

423
00:34:15,000 --> 00:34:20,000
 class, conditional probability, actually, will be zero.

424
00:34:20,000 --> 00:34:22,000
 Actually, this is one issue, actually.

425
00:34:22,000 --> 00:34:23,000
 Okay.

426
00:34:23,000 --> 00:34:26,000
 And, then, the next, actually, is for the game, right?

427
00:34:26,000 --> 00:34:27,000
 Game, actually.

428
00:34:27,000 --> 00:34:29,000
 The first, we have a game here.

429
00:34:29,000 --> 00:34:32,000
 And, then, the second, actually, we don't have a game.

430
00:34:32,000 --> 00:34:34,000
 And, then, actually, we have a game.

431
00:34:34,000 --> 00:34:35,000
 Okay.

432
00:34:35,000 --> 00:34:36,000
 So, it's twice.

433
00:34:36,000 --> 00:34:37,000
 Okay.

434
00:34:37,000 --> 00:34:38,000
 So, it's a two, 11th.

435
00:34:38,000 --> 00:34:39,000
 Okay.

436
00:34:39,000 --> 00:34:45,000
 And, but, actually, the first word, as the word close, actually, did not appear right here.

437
00:34:45,000 --> 00:34:48,000
 Then, the probability is zero.

438
00:34:48,000 --> 00:34:50,000
 And, then, we could, you can't have a problem, right?

439
00:34:50,000 --> 00:34:52,000
 Because of the mission of these words.

440
00:34:52,000 --> 00:34:58,000
 And, then, the probabilities of a very close note, these would be zeroes.

441
00:34:58,000 --> 00:34:59,000
 Okay.

442
00:34:59,000 --> 00:35:02,000
 So, this, actually, is a problem that we need to address.

443
00:35:02,000 --> 00:35:03,000
 Okay.

444
00:35:03,000 --> 00:35:06,000
 So, how to address this problem?

445
00:35:06,000 --> 00:35:09,000
 So, these are the count, right?

446
00:35:09,000 --> 00:35:13,000
 So, you've already not seen in the training data.

447
00:35:13,000 --> 00:35:18,000
 And, then, the conditional probability, actually, will be zero, right?

448
00:35:18,000 --> 00:35:19,000
 Will be zero.

449
00:35:19,000 --> 00:35:22,000
 So, this will lead to the zero plus zero probability.

450
00:35:22,000 --> 00:35:25,000
 So, this is, actually, a problem.

451
00:35:25,000 --> 00:35:26,000
 Okay.

452
00:35:26,000 --> 00:35:33,000
 So, to address this problem, actually, we can use the, actually, the so-called Laplace,

453
00:35:33,000 --> 00:35:36,000
 Laplace, you know, smoothing.

454
00:35:36,000 --> 00:35:40,000
 Laplace smoothing, actually, so, we can, you know, add one.

455
00:35:40,000 --> 00:35:42,000
 We can count, count the word, right?

456
00:35:42,000 --> 00:35:44,000
 And, the category of omega-j.

457
00:35:44,000 --> 00:35:45,000
 Spouse.

458
00:35:45,000 --> 00:35:46,000
 Okay.

459
00:35:46,000 --> 00:35:48,000
 We, we all know plus one.

460
00:35:48,000 --> 00:35:49,000
 Okay.

461
00:35:49,000 --> 00:35:52,000
 Then, now, we have, well, no, no longer have a zero, right?

462
00:35:52,000 --> 00:35:53,000
 A one.

463
00:35:53,000 --> 00:35:54,000
 Okay.

464
00:35:54,000 --> 00:35:57,000
 So, we have, actually, we're all into a constant.

465
00:35:57,000 --> 00:35:58,000
 Okay.

466
00:35:58,000 --> 00:36:03,000
 So, here, constant xk, d.

467
00:36:03,000 --> 00:36:04,000
 Okay.

468
00:36:04,000 --> 00:36:07,000
 So, d, actually, is a unique word.

469
00:36:07,000 --> 00:36:08,000
 Unique words.

470
00:36:08,000 --> 00:36:09,000
 Okay.

471
00:36:09,000 --> 00:36:14,000
 Actually, a unique word, we have defined this d.

472
00:36:14,000 --> 00:36:19,000
 D is a number of unique words in all the training data.

473
00:36:19,000 --> 00:36:21,000
 The unique words.

474
00:36:21,000 --> 00:36:22,000
 Okay.

475
00:36:22,000 --> 00:36:27,000
 And, actually, if we count this, actually, in all the training data here, all the training

476
00:36:27,000 --> 00:36:32,000
 data here, the unique words, actually, is, you know, 14.

477
00:36:32,000 --> 00:36:33,000
 It's 14.

478
00:36:33,000 --> 00:36:34,000
 Okay.

479
00:36:34,000 --> 00:36:41,000
 So, so here, in this formula, and then, actually, this, actually, so the numerator part, for

480
00:36:41,000 --> 00:36:43,000
 each word, we plus a one.

481
00:36:43,000 --> 00:36:48,000
 And then, the denominator part, actually, so this, actually, the total words, and then

482
00:36:48,000 --> 00:36:49,000
 plus a d.

483
00:36:49,000 --> 00:36:54,000
 The words, okay, in this class, okay, then plus a d.

484
00:36:54,000 --> 00:36:55,000
 Okay.

485
00:36:55,000 --> 00:37:00,000
 So, then, you know, to calculate the class, a conditional probability of close, you know,

486
00:37:00,000 --> 00:37:02,000
 given the spot, right?

487
00:37:02,000 --> 00:37:07,000
 So, now, it should be the, in the numerator part, we should plus a one.

488
00:37:07,000 --> 00:37:08,000
 Okay.

489
00:37:08,000 --> 00:37:10,000
 In the denominator part, we should plus the d.

490
00:37:10,000 --> 00:37:15,000
 The d is the total number of unique words in all the training data.

491
00:37:15,000 --> 00:37:19,000
 So, here, in all the training data means the training data in both classes.

492
00:37:19,000 --> 00:37:20,000
 Okay.

493
00:37:20,000 --> 00:37:23,000
 So, actually, there are 14, actually, unique words.

494
00:37:23,000 --> 00:37:24,000
 Okay.

495
00:37:24,000 --> 00:37:30,000
 Then, so, finally, there's a probability equal to one over 25.

496
00:37:30,000 --> 00:37:31,000
 Okay.

497
00:37:31,000 --> 00:37:40,000
 So, through this way, actually, now, we can address, you know, the zero problem, right?

498
00:37:40,000 --> 00:37:41,000
 Okay.

499
00:37:41,000 --> 00:37:42,000
 Okay.

500
00:37:42,000 --> 00:37:47,000
 So, similarly, for all the words, right, based on a, you know, two, right, then we also

501
00:37:47,000 --> 00:37:48,000
 need to modify, right?

502
00:37:48,000 --> 00:37:49,000
 Although it's not zero, right?

503
00:37:49,000 --> 00:37:50,000
 Actually, we modify for all.

504
00:37:50,000 --> 00:37:51,000
 Okay.

505
00:37:51,000 --> 00:37:54,000
 Then, two, for, you know, two plus one.

506
00:37:54,000 --> 00:37:56,000
 And then, 11 plus 14, right?

507
00:37:56,000 --> 00:37:57,000
 Yeah.

508
00:37:57,000 --> 00:37:59,000
 For very, only once, right?

509
00:37:59,000 --> 00:38:00,000
 Only once.

510
00:38:00,000 --> 00:38:02,000
 One plus one.

511
00:38:02,000 --> 00:38:03,000
 Okay.

512
00:38:03,000 --> 00:38:06,000
 For every node, close, also plus one, right?

513
00:38:06,000 --> 00:38:08,000
 And every one, call it one.

514
00:38:08,000 --> 00:38:09,000
 Okay.

515
00:38:09,000 --> 00:38:13,000
 So, we can modify, actually, the class-cannibal probabilities.

516
00:38:13,000 --> 00:38:14,000
 Okay.

517
00:38:14,000 --> 00:38:20,000
 And this is the probabilities after the Laplace smoothing.

518
00:38:20,000 --> 00:38:21,000
 Okay.

519
00:38:21,000 --> 00:38:26,000
 And then, now, we can calculate the probabilities, right?

520
00:38:26,000 --> 00:38:33,000
 And so, for the new real part, which is the John probability of the X, a very close game

521
00:38:33,000 --> 00:38:38,000
 with the, you know, sparse, you know, the John probability, right?

522
00:38:38,000 --> 00:38:39,000
 Equals this.

523
00:38:39,000 --> 00:38:51,000
 I think, you know, I think I missed a value here, right?

524
00:38:51,000 --> 00:38:56,000
 This should be times the p probability, p, p sparse, right?

525
00:38:56,000 --> 00:38:59,000
 P sparse equals, I missed a one, one value.

526
00:38:59,000 --> 00:39:02,000
 I think p sparse equal to 0.6, right?

527
00:39:02,000 --> 00:39:06,000
 Because there are three sparse, three, sparse news.

528
00:39:06,000 --> 00:39:08,000
 And then, it's 0.6.

529
00:39:08,000 --> 00:39:09,000
 Okay.

530
00:39:09,000 --> 00:39:12,000
 So, here, maybe I missed some value.

531
00:39:12,000 --> 00:39:13,000
 Okay.

532
00:39:13,000 --> 00:39:14,000
 Okay.

533
00:39:14,000 --> 00:39:22,000
 So, this is something, and then, this should be multiplied by 0.4, 0.6.

534
00:39:22,000 --> 00:39:23,000
 0.6.

535
00:39:23,000 --> 00:39:24,000
 Okay.

536
00:39:24,000 --> 00:39:31,000
 And then, similarly, we can calculate another value, 0.4, also times 0.4.

537
00:39:31,000 --> 00:39:32,000
 Okay.

538
00:39:32,000 --> 00:39:35,000
 And, actually, we can find the John probability in the new real part, right?

539
00:39:35,000 --> 00:39:41,000
 So, here is a 4.61 times 6, 10 power minus 5.

540
00:39:41,000 --> 00:39:47,000
 So, here is a 1.43 times 6 times 10 power minus 5.

541
00:39:47,000 --> 00:39:56,000
 So, certainly, I read the posterior probability or the John probability of the input x, a

542
00:39:56,000 --> 00:40:00,000
 very close game with the omega 1 sparse.

543
00:40:00,000 --> 00:40:01,000
 Okay.

544
00:40:01,000 --> 00:40:04,000
 So, this John probability is greater, right?

545
00:40:04,000 --> 00:40:12,000
 So, based on this tree, we can finally classify this test into the class sparse.

546
00:40:12,000 --> 00:40:13,000
 Okay.

547
00:40:13,000 --> 00:40:21,000
 So, I think through this example tree, we can see, you know, if the x is discrete, so

548
00:40:21,000 --> 00:40:23,000
 we no longer try to use a function, right?

549
00:40:23,000 --> 00:40:29,000
 Like a Gaussian function to build, to find the class-calibri probabilities.

550
00:40:29,000 --> 00:40:33,000
 So, instead, you know, based on the cost of the values, right?

551
00:40:33,000 --> 00:40:40,000
 Based on the cost of the values, based on the percentage of the sample that are taking

552
00:40:40,000 --> 00:40:43,000
 a value, taking a specific value.

553
00:40:43,000 --> 00:40:46,000
 We first count this and then look at the percentage.

554
00:40:46,000 --> 00:40:47,000
 Okay.

555
00:40:47,000 --> 00:40:48,000
 The proportion.

556
00:40:48,000 --> 00:40:52,000
 So, that's just probably estimation.

557
00:40:52,000 --> 00:40:53,000
 Okay.

558
00:40:53,000 --> 00:41:02,000
 So, yeah, so this is the multinomial, not your base.

559
00:41:02,000 --> 00:41:04,000
 That is, you know, we have a discrete, right?

560
00:41:04,000 --> 00:41:06,000
 The features actually are discrete.

561
00:41:06,000 --> 00:41:08,000
 Why discrete?

562
00:41:08,000 --> 00:41:13,000
 Because the number of features, right, can have different values.

563
00:41:13,000 --> 00:41:18,000
 One can take different values, can take two or can even more, right?

564
00:41:18,000 --> 00:41:20,000
 So, actually, you know, discrete.

565
00:41:20,000 --> 00:41:23,000
 So, you can take more than two values, not binary.

566
00:41:23,000 --> 00:41:24,000
 Okay.

567
00:41:24,000 --> 00:41:30,000
 And then, you can use this method to estimate the class-calibri probabilities.

568
00:41:30,000 --> 00:41:38,000
 But then, based on that, finally, we can make the decision, right, to predict the class.

569
00:41:38,000 --> 00:41:39,000
 Okay.

570
00:41:39,000 --> 00:41:40,000
 Okay.

571
00:41:40,000 --> 00:41:45,000
 So, that's actually the summary for the 9-base.

572
00:41:45,000 --> 00:41:50,000
 So, a 9-base, actually, we know, is based on the assumption, right?

573
00:41:50,000 --> 00:41:53,000
 So, that is all the features are independent.

574
00:41:53,000 --> 00:41:54,000
 Okay.

575
00:41:54,000 --> 00:42:03,000
 And so, sometimes, you see that this assumption is a kind of oversimplified assumption.

576
00:42:03,000 --> 00:42:04,000
 Okay.

577
00:42:04,000 --> 00:42:11,000
 And so, in other words, assumption is actually, the assumption is a violated in practice.

578
00:42:11,000 --> 00:42:12,000
 Okay.

579
00:42:12,000 --> 00:42:19,000
 But actually, the performance of this kind of model, right, simply from our model, often,

580
00:42:19,000 --> 00:42:24,000
 I think, you know, is better than our expected.

581
00:42:24,000 --> 00:42:25,000
 Okay.

582
00:42:25,000 --> 00:42:28,000
 So, we think, oh, this model is based on this assumption.

583
00:42:28,000 --> 00:42:36,000
 And then, actually, you know, we, the practical data, what is this, actually, you know, condition,

584
00:42:36,000 --> 00:42:37,000
 this assumption.

585
00:42:37,000 --> 00:42:40,000
 And then, the performance could be very bad.

586
00:42:40,000 --> 00:42:41,000
 Okay.

587
00:42:41,000 --> 00:42:42,000
 So, it's not a bad expectation, right?

588
00:42:42,000 --> 00:42:47,000
 But in practice, the result, actually, is surprisingly good.

589
00:42:47,000 --> 00:42:48,000
 Okay.

590
00:42:48,000 --> 00:42:55,000
 So, I think in the hard-denome data, and if you want to use the, you know, statistical

591
00:42:55,000 --> 00:43:01,000
 approach, so probably you can think about it as a 9-base.

592
00:43:01,000 --> 00:43:06,000
 Okay.

593
00:43:06,000 --> 00:43:11,000
 And so, here, actually, also, actually, 9-base, actually, can be extremely, you know, faster.

594
00:43:11,000 --> 00:43:12,000
 Okay.

595
00:43:12,000 --> 00:43:16,000
 So, you know, if you count the word, right, so, of course, this operation, I think, could

596
00:43:16,000 --> 00:43:17,000
 be very fast.

597
00:43:17,000 --> 00:43:18,000
 Okay.

598
00:43:18,000 --> 00:43:24,000
 And so, there's a reason, you know, we need to understand the model.

599
00:43:24,000 --> 00:43:30,000
 And also, actually, in our assumption, right, the assumption, actually, I also, actually,

600
00:43:30,000 --> 00:43:35,000
 ask you, you know, to, to, to, to practice, right, to practice.

601
00:43:35,000 --> 00:43:36,000
 Okay.

602
00:43:36,000 --> 00:43:41,000
 But actually, when you look at the data, maybe the data, you know, is all continuous, but

603
00:43:41,000 --> 00:43:43,000
 continuous, and then you don't use the count, right?

604
00:43:43,000 --> 00:43:49,000
 You don't, we use another Gaussian function, you will read Gaussian function to, you know,

605
00:43:49,000 --> 00:43:52,000
 calculate class conditional, conditional probabilities.

606
00:43:52,000 --> 00:43:53,000
 Okay.

607
00:43:53,000 --> 00:44:01,000
 But these methods, I think, we need to understand it, and also, we need to know how to use it.

608
00:44:01,000 --> 00:44:02,000
 Okay.

609
00:44:02,000 --> 00:44:09,000
 So, the example, I could give you, give you know, one example like this, right, as you

610
00:44:09,000 --> 00:44:10,000
 to calculate.

611
00:44:10,000 --> 00:44:11,000
 Okay.

612
00:44:11,000 --> 00:44:12,000
 Okay.

613
00:44:12,000 --> 00:44:15,000
 So, this is the set of our approach for classification.

614
00:44:15,000 --> 00:44:20,000
 So, basically, we know we make decisions, right, and based on the probabilities.

615
00:44:20,000 --> 00:44:21,000
 Okay.

616
00:44:21,000 --> 00:44:23,000
 So, it's a base decision theory.

617
00:44:23,000 --> 00:44:24,000
 Okay.

618
00:44:24,000 --> 00:44:27,000
 And we can see the difference in our, right.

619
00:44:27,000 --> 00:44:34,000
 So, the continuous features and also discrete features, discrete features, when you take

620
00:44:34,000 --> 00:44:38,000
 binary values, oh, no, you can take multiple values.

621
00:44:38,000 --> 00:44:39,000
 Okay.

622
00:44:39,000 --> 00:44:40,000
 Okay.

623
00:44:40,000 --> 00:44:41,000
 Okay.

624
00:44:41,000 --> 00:44:42,000
 Okay.

625
00:44:42,000 --> 00:44:43,000
 Okay.

626
00:44:43,000 --> 00:44:59,000
 Next, actually, we study the second method for, you know, pattern classification.

627
00:44:59,000 --> 00:45:00,000
 Okay.

628
00:45:00,000 --> 00:45:03,000
 So, of course, this approach is not based on the probabilities.

629
00:45:03,000 --> 00:45:04,000
 Okay.

630
00:45:04,000 --> 00:45:09,000
 And the ideas are these, they're very good for these linear adjustment analysis.

631
00:45:09,000 --> 00:45:10,000
 Okay.

632
00:45:10,000 --> 00:45:12,000
 So, linear adjustment analysis, linear.

633
00:45:12,000 --> 00:45:17,000
 So, certainly, you know, this actually classifies a linear classifier.

634
00:45:17,000 --> 00:45:24,000
 So, for all the classifiers, actually, you know, actually, we can put them into two categories.

635
00:45:24,000 --> 00:45:25,000
 Okay.

636
00:45:25,000 --> 00:45:27,000
 Why is it nonlinear, right?

637
00:45:27,000 --> 00:45:33,000
 The nonlinear classifier, you know, of course, actually, the simpler method is a linear classifier.

638
00:45:33,000 --> 00:45:37,000
 So, the linear adjustment analysis belongs to the second group, linear classifier.

639
00:45:37,000 --> 00:45:38,000
 Okay.

640
00:45:38,000 --> 00:45:42,000
 So, the nonlinear classifier doesn't mean that this function is a linear equation of

641
00:45:42,000 --> 00:45:43,000
 the input x.

642
00:45:43,000 --> 00:45:44,000
 Okay.

643
00:45:44,000 --> 00:45:53,500
 And even if a nonlinear classifier, the dissimilar function, actually, is a nonlinear function

644
00:45:53,500 --> 00:45:55,000
 of the input x.

645
00:45:55,000 --> 00:45:56,000
 Okay.

646
00:45:56,000 --> 00:46:02,000
 Based on this, actually, we can think about the base decision rule.

647
00:46:02,000 --> 00:46:03,000
 Okay.

648
00:46:03,000 --> 00:46:07,000
 If we use normal function, right, we use actually the Gaussian function.

649
00:46:07,000 --> 00:46:10,000
 And to approximate the density function.

650
00:46:10,000 --> 00:46:11,000
 Okay.

651
00:46:11,000 --> 00:46:16,000
 And then we know, actually, the x is either Gaussian function, right, in the isponera function.

652
00:46:16,000 --> 00:46:22,000
 In other words, actually, the dissimilar function is a nonlinear function of the x, the input

653
00:46:22,000 --> 00:46:23,000
 vector, right.

654
00:46:23,000 --> 00:46:32,000
 So, in other words, actually, the base decision rule, right, if we adopt the Gaussian function,

655
00:46:32,000 --> 00:46:35,000
 right, as a density function.

656
00:46:35,000 --> 00:46:39,000
 And then, actually, the classifier is a nonlinear classifier.

657
00:46:39,000 --> 00:46:43,000
 A nonlinear classifier, actually, then the decision boundary, actually, is a curve.

658
00:46:43,000 --> 00:46:44,000
 It's a curve.

659
00:46:44,000 --> 00:46:46,000
 Actually, we have noted this, right.

660
00:46:46,000 --> 00:46:52,000
 In the last week, actually, when we show, actually, in the example, we know, actually,

661
00:46:52,000 --> 00:46:54,000
 the decision boundary is a curve.

662
00:46:54,000 --> 00:46:58,000
 Although the curve could be very smooth, but it is a curve.

663
00:46:58,000 --> 00:46:59,000
 Okay.

664
00:46:59,000 --> 00:47:02,000
 So, it is because this is a nonlinear classifier.

665
00:47:02,000 --> 00:47:03,000
 Okay.

666
00:47:03,000 --> 00:47:08,000
 And in the linear classifier, and then the dissimilar function is a linear function

667
00:47:08,000 --> 00:47:09,000
 of x.

668
00:47:09,000 --> 00:47:16,000
 A linear function of x could be a straight line in the two-dimensional space, or a hyperplane

669
00:47:16,000 --> 00:47:18,000
 in the two-dimensional space.

670
00:47:18,000 --> 00:47:20,000
 It's no longer a curve or a hyper surface.

671
00:47:20,000 --> 00:47:26,000
 Actually, it's a hyperplane in the hard-dimensional space, or just one straight line in the,

672
00:47:26,000 --> 00:47:28,000
 actually, the two-dimensional space.

673
00:47:28,000 --> 00:47:29,000
 Okay.

674
00:47:29,000 --> 00:47:34,000
 So, for this problem, and if we use a linear classifier to classify, and then, actually,

675
00:47:34,000 --> 00:47:40,000
 we just need to have such a straight line, right, to separate the data in the two classes.

676
00:47:40,000 --> 00:47:43,000
 So, this is a nonlinear classifier.

677
00:47:43,000 --> 00:47:47,000
 This straight line is just a decision boundary.

678
00:47:47,000 --> 00:47:49,000
 This is a straight line.

679
00:47:49,000 --> 00:47:52,000
 So, the classifier is a linear classifier.

680
00:47:52,000 --> 00:47:58,000
 So, linear classifier corresponds to a hyperplane, or a straight line, here.

681
00:47:58,000 --> 00:47:59,000
 Okay.

682
00:47:59,000 --> 00:48:04,000
 Nonlinear classifier is a curve or a hyper surface.

683
00:48:04,000 --> 00:48:10,000
 So, this belongs to the second category, right, linear classifier.

684
00:48:10,000 --> 00:48:16,000
 The dissimilar function is a linear function of the input x.

685
00:48:16,000 --> 00:48:18,000
 Then, what function can be a linear function, right?

686
00:48:18,000 --> 00:48:19,000
 Something like that.

687
00:48:19,000 --> 00:48:21,000
 If we have x1, x2, right?

688
00:48:21,000 --> 00:48:24,000
 x is a feature vector, and we have x1, x2.

689
00:48:24,000 --> 00:48:28,000
 For example, x3, any number, right?

690
00:48:28,000 --> 00:48:33,000
 And then, the dissimilar function should be a linear function.

691
00:48:33,000 --> 00:48:41,000
 Linear function, that means, actually, it could be like a function like y equals to a, w1, x1, plus w2, x2, plus w3, x3, right?

692
00:48:41,000 --> 00:48:43,000
 And then, plus w0.

693
00:48:43,000 --> 00:48:48,000
 So, this is a linear combination of all the features.

694
00:48:48,000 --> 00:48:52,000
 So, this is a linear function of the input x.

695
00:48:52,000 --> 00:48:53,000
 Okay.

696
00:48:53,000 --> 00:48:59,000
 And actually, for all linear classifiers, you take this form.

697
00:48:59,000 --> 00:49:03,000
 The dissimilar function is a linear function of the x.

698
00:49:03,000 --> 00:49:13,000
 It is the form of w1, x1, plus w2, x2, plus w3, x3, plus until wdxd, and then, plus w0.

699
00:49:13,000 --> 00:49:15,000
 So, this is a general form.

700
00:49:15,000 --> 00:49:16,000
 Okay.

701
00:49:16,000 --> 00:49:18,000
 Any nonlinear classifier has this form.

702
00:49:18,000 --> 00:49:19,000
 Okay.

703
00:49:19,000 --> 00:49:26,000
 Of course, actually, we can write this form into this, w1, x1, plus w2, right?

704
00:49:26,000 --> 00:49:32,000
 Into the product of this width vector and the feature vector x.

705
00:49:32,000 --> 00:49:33,000
 Okay.

706
00:49:33,000 --> 00:49:39,000
 All linear classifiers, all linear classifiers have this form.

707
00:49:39,000 --> 00:49:44,000
 But just different classifiers have different ways, have different methods, right?

708
00:49:44,000 --> 00:49:54,000
 To find, actually, this w and the w0 by optimizing some different loss functions.

709
00:49:54,000 --> 00:49:55,000
 Okay.

710
00:49:55,000 --> 00:50:00,000
 And actually, now, the classification problem can be interpreted from many perspectives.

711
00:50:00,000 --> 00:50:01,000
 Okay.

712
00:50:01,000 --> 00:50:06,000
 From each angle, you can summarize a certain loss function.

713
00:50:06,000 --> 00:50:15,000
 And then, now, we try our best to find the parameters of w and w0 to optimize this loss

714
00:50:15,000 --> 00:50:16,000
 function.

715
00:50:16,000 --> 00:50:18,000
 So, this is a general procedure, right?

716
00:50:18,000 --> 00:50:22,000
 And for designing, you know, a linear classifier.

717
00:50:22,000 --> 00:50:23,000
 Okay.

718
00:50:23,000 --> 00:50:28,000
 Any other problem within what should be a good classifier?

719
00:50:28,000 --> 00:50:29,000
 Okay.

720
00:50:29,000 --> 00:50:31,000
 And we have this concept.

721
00:50:31,000 --> 00:50:36,000
 And then, finally, we use this concept into a mathematical expression.

722
00:50:36,000 --> 00:50:37,000
 Okay.

723
00:50:37,000 --> 00:50:41,000
 And then, finally, we convert this into an optimization problem.

724
00:50:41,000 --> 00:50:47,000
 And then, we solve this optimization problem to find the values of w and w0.

725
00:50:47,000 --> 00:50:52,000
 So, that is just another process of designing a linear classifier.

726
00:50:52,000 --> 00:50:53,000
 Okay.

727
00:50:53,000 --> 00:50:57,000
 So, in this course, we are introduced to a linear classifier.

728
00:50:57,000 --> 00:50:59,000
 Why is a linear discipline analysis?

729
00:50:59,000 --> 00:51:00,000
 Okay.

730
00:51:00,000 --> 00:51:02,000
 Because it is a linear support-bound machine.

731
00:51:02,000 --> 00:51:03,000
 Okay.

732
00:51:03,000 --> 00:51:06,000
 And both, not just these two, right?

733
00:51:06,000 --> 00:51:08,000
 Oh, I have this form.

734
00:51:08,000 --> 00:51:14,000
 But just that we have found the w and w0 based on different methods, right?

735
00:51:14,000 --> 00:51:17,000
 Based on different ways.

736
00:51:17,000 --> 00:51:18,000
 Okay.

737
00:51:18,000 --> 00:51:19,000
 What time?

738
00:51:19,000 --> 00:51:20,000
 Okay.

739
00:51:20,000 --> 00:51:23,000
 Maybe that's actually, we have a break.

740
00:51:23,000 --> 00:51:29,000
 For other break, actually, now, we study how to find actually the values of w and x.

741
00:51:29,000 --> 00:51:33,000
 And so that, you know, they can separate the data into two classes.

742
00:51:33,000 --> 00:51:35,000
 Of course, actually, here is just one example, right?

743
00:51:35,000 --> 00:51:37,000
 In the space, we can, in this 2D, right?

744
00:51:37,000 --> 00:51:40,000
 In this 2D space, we can see the distribution.

745
00:51:40,000 --> 00:51:43,000
 And we can find a line to separate the sum of the two classes.

746
00:51:43,000 --> 00:51:46,000
 But the practical problem is the high-dimensional, right?

747
00:51:46,000 --> 00:51:48,000
 So, we cannot see the distribution.

748
00:51:48,000 --> 00:51:52,000
 So, how to find this line to separate the sum of the two classes?

749
00:51:52,000 --> 00:51:56,000
 So, this is the objective of, you know, the classified design.

750
00:51:56,000 --> 00:52:01,000
 You cannot see the distribution, but you need to find the tree, a line, to separate the

751
00:52:01,000 --> 00:52:06,000
 sum of the two classes with high accuracy.

752
00:52:06,000 --> 00:52:07,000
 Okay.

753
00:52:07,000 --> 00:52:09,000
 So, that's how we have a break.

754
00:52:09,000 --> 00:52:11,000
 We have 10 minutes break.

755
00:52:26,000 --> 00:52:28,000
 Okay.

756
00:52:56,000 --> 00:53:01,000
 So, we have to find the, you know, the distribution model.

757
00:53:01,000 --> 00:53:04,000
 And we can do better performance.

758
00:53:04,000 --> 00:53:12,000
 But I don't have the answer, so I don't know how many, you know, how many questions.

759
00:53:12,000 --> 00:53:15,000
 Yeah, this is what you need to do, right?

760
00:53:15,000 --> 00:53:17,000
 You don't know the answer, right?

761
00:53:17,000 --> 00:53:18,000
 Yeah.

762
00:53:18,000 --> 00:53:21,000
 You need to find the tree, the single number of the problem, and the question from the

763
00:53:21,000 --> 00:53:22,000
 question.

764
00:53:22,000 --> 00:53:25,000
 If you want to use the question, you should know that.

765
00:53:25,000 --> 00:53:26,000
 Yeah.

766
00:53:26,000 --> 00:53:29,000
 Based on the question, you need to find the tree, the single number.

767
00:53:29,000 --> 00:53:30,000
 Oh, okay.

768
00:53:30,000 --> 00:53:31,000
 Yeah.

769
00:53:31,000 --> 00:53:32,000
 Yeah.

770
00:53:32,000 --> 00:53:40,000
 Yeah, yeah, I'm already, but I miss the very, right?

771
00:53:40,000 --> 00:53:41,000
 Yeah, yeah.

772
00:53:41,000 --> 00:53:42,000
 Okay.

773
00:54:11,000 --> 00:54:13,000
 Okay.

774
00:54:13,000 --> 00:54:14,000
 Okay.

775
00:54:14,000 --> 00:54:15,000
 Okay.

776
00:54:15,000 --> 00:54:16,000
 Okay.

777
00:54:16,000 --> 00:54:17,000
 Okay.

778
00:54:17,000 --> 00:54:18,000
 Okay.

779
00:54:18,000 --> 00:54:19,000
 Okay.

780
00:54:19,000 --> 00:54:20,000
 Okay.

781
00:54:20,000 --> 00:54:21,000
 Okay.

782
00:54:21,000 --> 00:54:22,000
 Okay.

783
00:54:22,000 --> 00:54:23,000
 Okay.

784
00:54:23,000 --> 00:54:24,000
 Okay.

785
00:54:24,000 --> 00:54:25,000
 Okay.

786
00:54:25,000 --> 00:54:26,000
 Okay.

787
00:54:26,000 --> 00:54:27,000
 Okay.

788
00:54:27,000 --> 00:54:28,000
 Okay.

789
00:54:28,000 --> 00:54:29,000
 Okay.

790
00:54:29,000 --> 00:54:30,000
 Okay.

791
00:54:30,000 --> 00:54:31,000
 Okay.

792
00:54:31,000 --> 00:54:32,000
 Okay.

793
00:54:32,000 --> 00:54:33,000
 Okay.

794
00:54:33,000 --> 00:54:34,000
 Okay.

795
00:54:34,000 --> 00:54:35,000
 Okay.

796
00:54:35,000 --> 00:54:36,000
 Okay.

797
00:54:36,000 --> 00:54:37,000
 Okay.

798
00:54:37,000 --> 00:54:38,000
 Okay.

799
00:54:38,000 --> 00:54:39,000
 Okay.

800
00:54:39,000 --> 00:54:40,000
 Okay.

801
00:54:40,000 --> 00:54:41,000
 Okay.

802
00:55:10,000 --> 00:55:12,000
 Okay.

803
00:55:12,000 --> 00:55:13,000
 Okay.

804
00:55:40,000 --> 00:55:42,000
 Okay.

805
00:55:42,000 --> 00:55:43,000
 Okay.

806
00:56:10,000 --> 00:56:12,000
 Okay.

807
00:56:40,000 --> 00:56:42,000
 Okay.

808
00:57:10,000 --> 00:57:12,000
 Okay.

809
00:57:12,000 --> 00:57:13,000
 Okay.

810
00:57:13,000 --> 00:57:14,000
 Okay.

811
00:57:14,000 --> 00:57:15,000
 Okay.

812
00:57:15,000 --> 00:57:16,000
 Okay.

813
00:57:16,000 --> 00:57:17,000
 Okay.

814
00:57:17,000 --> 00:57:18,000
 Okay.

815
00:57:18,000 --> 00:57:19,000
 Okay.

816
00:57:19,000 --> 00:57:20,000
 Okay.

817
00:57:20,000 --> 00:57:21,000
 Okay.

818
00:57:21,000 --> 00:57:22,000
 Okay.

819
00:57:22,000 --> 00:57:23,000
 Okay.

820
00:57:23,000 --> 00:57:24,000
 Okay.

821
00:57:24,000 --> 00:57:25,000
 Okay.

822
00:57:25,000 --> 00:57:26,000
 Okay.

823
00:57:26,000 --> 00:57:27,000
 Okay.

824
00:57:27,000 --> 00:57:28,000
 Okay.

825
00:57:28,000 --> 00:57:29,000
 Okay.

826
00:57:29,000 --> 00:57:30,000
 Okay.

827
00:57:30,000 --> 00:57:31,000
 Okay.

828
00:57:31,000 --> 00:57:32,000
 Okay.

829
00:57:32,000 --> 00:57:33,000
 Okay.

830
00:57:33,000 --> 00:57:34,000
 Okay.

831
00:57:34,000 --> 00:57:35,000
 Okay.

832
00:57:35,000 --> 00:57:36,000
 Okay.

833
00:57:36,000 --> 00:57:37,000
 Okay.

834
00:57:37,000 --> 00:57:38,000
 Okay.

835
00:57:38,000 --> 00:57:39,000
 Okay.

836
00:57:39,000 --> 00:57:40,000
 Okay.

837
00:58:09,000 --> 00:58:10,000
 Okay.

838
00:58:39,000 --> 00:58:40,000
 Okay.

839
00:59:09,000 --> 00:59:10,000
 Okay.

840
00:59:39,000 --> 00:59:40,000
 Okay.

841
01:00:09,000 --> 01:00:11,000
 Okay.

842
01:00:11,000 --> 01:00:12,000
 Okay.

843
01:00:12,000 --> 01:00:13,000
 Okay.

844
01:00:13,000 --> 01:00:14,000
 Okay.

845
01:00:14,000 --> 01:00:15,000
 Okay.

846
01:00:15,000 --> 01:00:16,000
 Okay.

847
01:00:16,000 --> 01:00:17,000
 Okay.

848
01:00:17,000 --> 01:00:18,000
 Okay.

849
01:00:18,000 --> 01:00:19,000
 Okay.

850
01:00:19,000 --> 01:00:20,000
 Okay.

851
01:00:20,000 --> 01:00:21,000
 Okay.

852
01:00:21,000 --> 01:00:22,000
 Okay.

853
01:00:22,000 --> 01:00:23,000
 Okay.

854
01:00:23,000 --> 01:00:24,000
 Okay.

855
01:00:24,000 --> 01:00:25,000
 Okay.

856
01:00:25,000 --> 01:00:26,000
 Okay.

857
01:00:26,000 --> 01:00:27,000
 Okay.

858
01:00:27,000 --> 01:00:28,000
 Okay.

859
01:00:28,000 --> 01:00:29,000
 Okay.

860
01:00:29,000 --> 01:00:30,000
 Okay.

861
01:00:30,000 --> 01:00:31,000
 Okay.

862
01:00:31,000 --> 01:00:32,000
 Okay.

863
01:00:32,000 --> 01:00:33,000
 Okay.

864
01:00:33,000 --> 01:00:34,000
 Okay.

865
01:00:34,000 --> 01:00:35,000
 Okay.

866
01:00:35,000 --> 01:00:36,000
 Okay.

867
01:00:36,000 --> 01:00:37,000
 Okay.

868
01:00:37,000 --> 01:00:38,000
 Okay.

869
01:00:38,000 --> 01:00:39,000
 Okay.

870
01:01:08,000 --> 01:01:09,000
 Okay.

871
01:01:38,000 --> 01:01:39,000
 Okay.

872
01:02:08,000 --> 01:02:09,000
 Okay.

873
01:02:38,000 --> 01:02:39,000
 Okay.

874
01:03:08,000 --> 01:03:09,000
 Okay.

875
01:03:38,000 --> 01:03:42,160
 Okay.

876
01:03:42,160 --> 01:03:43,540
 Okay.

877
01:03:43,540 --> 01:03:44,880
 Okay.

878
01:03:44,880 --> 01:03:45,880
 Yes.

879
01:03:45,880 --> 01:03:46,880
 Okay.

880
01:03:46,880 --> 01:03:47,880
 Okay.

881
01:03:47,880 --> 01:03:48,880
 Okay.

882
01:03:48,880 --> 01:03:49,880
 Okay.

883
01:03:49,880 --> 01:03:50,880
 Okay.

884
01:03:50,880 --> 01:03:51,880
 Okay.

885
01:03:51,880 --> 01:03:52,880
 Okay.

886
01:03:52,880 --> 01:03:53,880
 Okay.

887
01:03:53,880 --> 01:03:54,880
 Okay.

888
01:03:54,880 --> 01:03:55,880
 Okay.

889
01:03:55,880 --> 01:03:56,880
 Okay.

890
01:03:56,880 --> 01:03:57,880
 Okay.

891
01:03:57,880 --> 01:03:58,880
 Okay.

892
01:03:58,880 --> 01:03:59,880
 Okay.

893
01:03:59,880 --> 01:04:03,880
 On this linear class file and the decimal function Gx equals w transpose x right, plus

894
01:04:03,880 --> 01:04:04,880
 w 0.

895
01:04:04,880 --> 01:04:05,880
 There are two parameters.

896
01:04:05,880 --> 01:04:06,880
 Why is w?

897
01:04:06,880 --> 01:04:07,880
 Which vector?

898
01:04:07,880 --> 01:04:09,200
 classes.

899
01:04:09,200 --> 01:04:11,400
 And actually, this straight line is called the decision

900
01:04:11,400 --> 01:04:12,400
 boundary.

901
01:04:12,400 --> 01:04:14,360
 The decision boundary.

902
01:04:14,360 --> 01:04:17,760
 And actually, this decision boundary is a line,

903
01:04:17,760 --> 01:04:20,440
 or hyperplane.

904
01:04:20,440 --> 01:04:24,320
 Why is the direction or orientation of this straight line?

905
01:04:24,320 --> 01:04:26,040
 Or this hyperplane?

906
01:04:26,040 --> 01:04:28,360
 And actually, the orientation or the direction of this straight

907
01:04:28,360 --> 01:04:34,320
 line is determined by the width back of the w.

908
01:04:34,320 --> 01:04:37,640
 And then for this direction, you can imagine a straight line.

909
01:04:37,640 --> 01:04:40,359
 Actually, we have the same direction,

910
01:04:40,359 --> 01:04:44,279
 but we can have many lines in different location positions.

911
01:04:44,279 --> 01:04:50,080
 And actually, this line position is determined by the w0,

912
01:04:50,080 --> 01:04:52,000
 which is called a bias term.

913
01:04:52,000 --> 01:04:53,560
 Bias.

914
01:04:53,560 --> 01:04:54,080
 OK.

915
01:04:54,080 --> 01:04:57,120
 Actually, these are constant terms.

916
01:04:57,120 --> 01:04:59,799
 This also equals the w0 times the constant 1,

917
01:04:59,799 --> 01:05:01,759
 the input is the 1.

918
01:05:01,759 --> 01:05:06,600
 So this is the two parameters in this model.

919
01:05:06,600 --> 01:05:09,680
 And later, we will see to determine

920
01:05:09,680 --> 01:05:13,279
 the two parameters in the linear adjustment analysis.

921
01:05:13,279 --> 01:05:16,799
 And then we normally determine the w first.

922
01:05:16,799 --> 01:05:21,319
 We determine the direction, orientation of the line first.

923
01:05:21,319 --> 01:05:25,839
 And then, next, we will determine the location of the line,

924
01:05:25,839 --> 01:05:26,880
 or the hyperplane.

925
01:05:33,080 --> 01:05:35,400
 And for a two-class continental problem,

926
01:05:35,400 --> 01:05:37,400
 and actually, we here, we just have one actually,

927
01:05:37,400 --> 01:05:38,720
 a dissimilar function.

928
01:05:38,720 --> 01:05:43,240
 Actually, in the last week, when we

929
01:05:43,240 --> 01:05:46,360
 introduced the concept of dissimilar function,

930
01:05:46,360 --> 01:05:49,880
 actually, for each class, we have one dissimilar function.

931
01:05:49,880 --> 01:05:53,400
 The dissimilar function could be the posterior probability,

932
01:05:53,400 --> 01:05:54,840
 could be the general probability,

933
01:05:54,840 --> 01:05:57,360
 could be the log of the general probability.

934
01:05:57,360 --> 01:06:01,360
 So all the three functions could be used

935
01:06:01,360 --> 01:06:02,360
 as a dissimilar function.

936
01:06:02,360 --> 01:06:05,760
 So each class has one dissimilar function.

937
01:06:05,760 --> 01:06:07,880
 In the Fisher-Linux linear dissimilar analysis,

938
01:06:07,880 --> 01:06:10,240
 we have only one dissimilar function.

939
01:06:10,240 --> 01:06:12,600
 This can be considered, this dissimilar function

940
01:06:12,600 --> 01:06:18,280
 is the difference of two different dissimilar functions.

941
01:06:18,280 --> 01:06:21,520
 So here, we just have one dissimilar function.

942
01:06:21,520 --> 01:06:23,600
 This different function can be interpreted

943
01:06:23,600 --> 01:06:30,360
 as the difference of two dissimilar functions.

944
01:06:30,680 --> 01:06:35,520
 In the normal case, we see g1x greater than g2x,

945
01:06:35,520 --> 01:06:39,400
 then we classify into class 1.

946
01:06:39,400 --> 01:06:44,640
 So you assume this gx is just g1x minus g2x.

947
01:06:44,640 --> 01:06:48,840
 That means when this gx, which is the difference between g1

948
01:06:48,840 --> 01:06:53,800
 and g2, is greater than 0, that means g1 is greater than g2.

949
01:06:53,800 --> 01:06:56,600
 Then we classify the sample to class 1.

950
01:06:56,640 --> 01:07:02,360
 So here, we design omega 1 if the gx is greater than 0.

951
01:07:02,360 --> 01:07:05,720
 gx can be interpreted as the difference of two dissimilar

952
01:07:05,720 --> 01:07:09,920
 functions, g1 minus g2.

953
01:07:09,920 --> 01:07:15,080
 And if gx is less than 0, that means g2 is greater than.

954
01:07:15,080 --> 01:07:20,400
 So actually, then we see that x belongs to class 2, omega 2.

955
01:07:20,400 --> 01:07:23,360
 And gx could be 0.

956
01:07:24,120 --> 01:07:25,240
 So gx could be 0.

957
01:07:25,240 --> 01:07:30,240
 That means actually the probability belonging to class 1,

958
01:07:30,240 --> 01:07:31,560
 class 2, the same.

959
01:07:31,560 --> 01:07:33,160
 Actually, we cannot classify.

960
01:07:33,160 --> 01:07:34,240
 We cannot decide.

961
01:07:34,240 --> 01:07:37,840
 And actually, this point is on the dissimilar boundary.

962
01:07:37,840 --> 01:07:39,960
 Just now, we have seen the dissimilar boundary

963
01:07:39,960 --> 01:07:40,880
 with the straight line.

964
01:07:40,880 --> 01:07:42,960
 Any point on this straight line have a 0 gx.

965
01:07:46,600 --> 01:07:48,600
 So the dissimilar boundary, so just now,

966
01:07:48,600 --> 01:07:51,840
 we see actually, is gx equals 0.

967
01:07:52,200 --> 01:07:55,600
 If this gx equals 0, this is just

968
01:07:55,600 --> 01:07:57,840
 the equation of the straight line.

969
01:07:57,840 --> 01:08:03,080
 W transpose times x plus w0 equals 0.

970
01:08:03,080 --> 01:08:05,600
 So this equation is just the line.

971
01:08:05,600 --> 01:08:06,360
 Just the line.

972
01:08:06,360 --> 01:08:08,560
 The line equation is just gx equals 0.

973
01:08:11,400 --> 01:08:15,160
 So in one side of the line, actually, gx is greater than 0.

974
01:08:15,160 --> 01:08:19,160
 In another side of the line, actually, gx is less than 0.

975
01:08:19,160 --> 01:08:22,559
 In other words, in one side, just greater than 0,

976
01:08:22,559 --> 01:08:25,200
 the sample belongs to class 1.

977
01:08:25,200 --> 01:08:26,800
 And then another side, actually, the sample

978
01:08:26,800 --> 01:08:28,639
 belongs to class 2.

979
01:08:28,639 --> 01:08:31,760
 So quite often, in the two-class problem,

980
01:08:31,760 --> 01:08:35,160
 and the class label for omega 1, class 1 is 1,

981
01:08:35,160 --> 01:08:38,840
 the class label for omega 2 is just minus 1.

982
01:08:38,840 --> 01:08:42,319
 So we can just, based on the sign, positive or negative,

983
01:08:42,319 --> 01:08:43,920
 to decide the class label.

984
01:08:43,920 --> 01:08:47,399
 In positive class 1, negative class minus 1,

985
01:08:47,439 --> 01:08:50,319
 is 0 on the dissonant boundary.

986
01:08:50,319 --> 01:08:53,359
 So actually, the equation for the dissonant boundary

987
01:08:53,359 --> 01:08:55,960
 is just gx equals 0.

988
01:08:55,960 --> 01:09:01,040
 Because any points on the dissonant boundary, actually,

989
01:09:01,040 --> 01:09:02,279
 gx is 0.

990
01:09:02,279 --> 01:09:04,759
 So this is the equation for the dissonant boundary.

991
01:09:09,960 --> 01:09:12,679
 So I didn't use this.

992
01:09:12,679 --> 01:09:14,839
 So I used to plan this, actually, from the,

993
01:09:14,840 --> 01:09:17,160
 this is a geometric interpretation, right?

994
01:09:17,160 --> 01:09:18,520
 In the geometric view.

995
01:09:18,520 --> 01:09:22,319
 And so we plot the data in a feature space.

996
01:09:22,319 --> 01:09:24,040
 And then we explain this concept.

997
01:09:29,000 --> 01:09:31,560
 And this side, also, actually, the positive side

998
01:09:31,560 --> 01:09:32,360
 of these three lines.

999
01:09:32,360 --> 01:09:35,400
 So here, these three lines, any point on these three lines,

1000
01:09:35,400 --> 01:09:36,880
 actually, could be 0.

1001
01:09:36,880 --> 01:09:38,480
 The dissonant function is 0.

1002
01:09:38,480 --> 01:09:40,800
 And this side, gx is greater than 0.

1003
01:09:40,800 --> 01:09:44,520
 So this side is called the positive side.

1004
01:09:44,520 --> 01:09:46,600
 And then another side is called negative side.

1005
01:09:46,600 --> 01:09:49,680
 Because in another side, for any arbitrary x,

1006
01:09:49,680 --> 01:09:53,440
 the dissonant function, actually, equals g is negative.

1007
01:09:53,440 --> 01:09:56,000
 So this side is called negative side,

1008
01:09:56,000 --> 01:09:57,320
 or the positive side.

1009
01:09:57,320 --> 01:10:01,160
 And then on the dissonant boundary.

1010
01:10:01,160 --> 01:10:03,560
 And then now we assume there are two vectors.

1011
01:10:03,560 --> 01:10:04,720
 Two vectors.

1012
01:10:04,720 --> 01:10:07,400
 And one is x1, another is x2.

1013
01:10:07,400 --> 01:10:10,320
 And both vectors are, or both vectors,

1014
01:10:10,320 --> 01:10:12,720
 is just a data point.

1015
01:10:12,720 --> 01:10:15,560
 Both points, actually, are on the dissonant boundary.

1016
01:10:15,560 --> 01:10:16,800
 On the dissonant boundary, that means

1017
01:10:16,800 --> 01:10:19,840
 if we substitute x1 into the dissonant function,

1018
01:10:19,840 --> 01:10:21,520
 then this should be 0.

1019
01:10:21,520 --> 01:10:25,760
 That means w transpose x1 plus w should be 0.

1020
01:10:25,760 --> 01:10:27,920
 x2 is also on the dissonant boundary.

1021
01:10:27,920 --> 01:10:31,240
 That means, actually, w transpose x2 plus w0

1022
01:10:31,240 --> 01:10:32,880
 should also be 0.

1023
01:10:32,880 --> 01:10:36,440
 In other words, w1 transpose x1 plus w0

1024
01:10:36,440 --> 01:10:40,400
 equals w transpose x2 plus w0.

1025
01:10:40,400 --> 01:10:42,560
 And equal 0.

1026
01:10:43,240 --> 01:10:45,720
 So now, actually, this w0 in the 2-th side

1027
01:10:45,720 --> 01:10:47,120
 can be canceled.

1028
01:10:47,120 --> 01:10:50,920
 Then we move this w transpose x2 to the left-hand side.

1029
01:10:50,920 --> 01:10:54,680
 Then we can get this result.

1030
01:10:54,680 --> 01:10:56,360
 So what does this mean?

1031
01:10:56,360 --> 01:10:58,800
 W transpose times x1 minus x2.

1032
01:10:58,800 --> 01:11:03,640
 x1, x2, actually, are on the dissonant boundary.

1033
01:11:03,640 --> 01:11:09,480
 And x1 minus x2 is a vector, also, on the dissonant boundary.

1034
01:11:09,480 --> 01:11:13,599
 So now, w transpose multiplied a vector on the dissonant

1035
01:11:13,599 --> 01:11:15,240
 boundary equals 0.

1036
01:11:15,240 --> 01:11:21,919
 So this means that w is perpendicular to x1 minus x2.

1037
01:11:21,919 --> 01:11:25,879
 Or xw transpose w is perpendicular

1038
01:11:25,879 --> 01:11:28,759
 to the dissonant boundary.

1039
01:11:28,759 --> 01:11:29,759
 This is a menu.

1040
01:11:34,719 --> 01:11:37,679
 So these x1, x2 are two points, right?

1041
01:11:37,680 --> 01:11:40,200
 Two points on the dissonant boundary.

1042
01:11:40,200 --> 01:11:43,800
 S2 minus x1 is also on the dissonant boundary.

1043
01:11:43,800 --> 01:11:48,000
 But now, we see w transpose multiplied by this x1 minus 2

1044
01:11:48,000 --> 01:11:49,840
 or x2 minus 0.

1045
01:11:49,840 --> 01:11:55,000
 So this means that w is perpendicular to s2 minus x1.

1046
01:11:55,000 --> 01:11:57,480
 S2 and minus y are actually the same direction

1047
01:11:57,480 --> 01:11:59,160
 as the dissonant boundary, right?

1048
01:11:59,160 --> 01:12:04,040
 So this means that w is perpendicular to the dissonant

1049
01:12:04,040 --> 01:12:04,680
 boundary.

1050
01:12:04,680 --> 01:12:06,280
 When you look at the equation, right?

1051
01:12:06,280 --> 01:12:10,360
 So w transpose, actually, x plus w is equal to 0.

1052
01:12:10,360 --> 01:12:14,559
 We see that w determine this dissonant boundary.

1053
01:12:14,559 --> 01:12:17,240
 But that does not mean, actually, that w

1054
01:12:17,240 --> 01:12:20,120
 is the same direction with the dissonant boundary.

1055
01:12:20,120 --> 01:12:26,519
 Actually, the w is perpendicular to the dissonant boundary.

1056
01:12:26,519 --> 01:12:28,559
 So this is a w.

1057
01:12:28,559 --> 01:12:32,280
 And actually, the w, of course, when we talk about a vector,

1058
01:12:32,280 --> 01:12:33,880
 we have a direction.

1059
01:12:33,880 --> 01:12:38,240
 And actually, this w point to the positive side,

1060
01:12:38,240 --> 01:12:39,480
 point to the class 1.

1061
01:12:42,520 --> 01:12:45,400
 So this one, we just now know, this is a positive side

1062
01:12:45,400 --> 01:12:46,200
 of this.

1063
01:12:46,200 --> 01:12:47,200
 This is a negative side.

1064
01:12:47,200 --> 01:12:52,760
 And actually, this w point to the positive side.

1065
01:12:52,760 --> 01:12:57,640
 And it is perpendicular to the dissonant boundary.

1066
01:12:57,640 --> 01:12:59,960
 So next time, we think about this w, right?

1067
01:12:59,960 --> 01:13:01,720
 Oh, this is the dissonant boundary, right?

1068
01:13:01,720 --> 01:13:03,600
 But actually, w is perpendicular to this.

1069
01:13:03,640 --> 01:13:08,400
 W is not in the same direction as the dissonant boundary.

1070
01:13:08,400 --> 01:13:08,900
 OK.

1071
01:13:11,200 --> 01:13:13,600
 So yeah, so here, this is the reason here.

1072
01:13:13,600 --> 01:13:17,240
 We say w is normal, or perpendicular, right?

1073
01:13:17,240 --> 01:13:17,740
 Yeah.

1074
01:13:21,960 --> 01:13:23,120
 OK, so this is w.

1075
01:13:23,120 --> 01:13:25,560
 Now, we are going to send the meaning of w, right?

1076
01:13:25,560 --> 01:13:30,800
 W, actually, determine the direction of the dissonant

1077
01:13:30,800 --> 01:13:31,640
 boundary.

1078
01:13:31,640 --> 01:13:34,680
 Because the dissonant boundary should be perpendicular to this

1079
01:13:34,680 --> 01:13:36,680
 w.

1080
01:13:36,680 --> 01:13:39,760
 And so actually, when we add time, actually,

1081
01:13:39,760 --> 01:13:42,840
 when we have arbitrary x, then we substitute this x

1082
01:13:42,840 --> 01:13:44,160
 into the dissonant function, gs.

1083
01:13:44,160 --> 01:13:45,560
 We can have a value, right?

1084
01:13:45,560 --> 01:13:47,360
 We can have a value.

1085
01:13:47,360 --> 01:13:50,840
 This value could be negative value, could be positive value.

1086
01:13:50,840 --> 01:13:54,120
 So what is the meaning of this value?

1087
01:13:54,120 --> 01:13:57,720
 What's the geometric meaning of this value?

1088
01:13:57,720 --> 01:14:01,640
 And actually, this gs is kind of a measure

1089
01:14:01,640 --> 01:14:06,520
 of the distance from the point to the dissonant boundary.

1090
01:14:06,520 --> 01:14:08,000
 OK.

1091
01:14:08,000 --> 01:14:09,520
 So that's actually we analyze this.

1092
01:14:14,080 --> 01:14:16,000
 So this is a point to the positive side, right?

1093
01:14:16,000 --> 01:14:17,120
 Just now we analyzed.

1094
01:14:17,120 --> 01:14:18,600
 So this region is the positive side,

1095
01:14:18,600 --> 01:14:20,480
 this region is negative side.

1096
01:14:20,480 --> 01:14:21,120
 OK.

1097
01:14:21,120 --> 01:14:22,840
 So that's actually, we look at what

1098
01:14:22,840 --> 01:14:25,880
 is gs w times s plus wj.

1099
01:14:25,880 --> 01:14:27,960
 What is the meaning of this, right?

1100
01:14:27,960 --> 01:14:30,360
 And actually, now we analyze.

1101
01:14:30,360 --> 01:14:32,400
 When we calculate every time, we substitute x

1102
01:14:32,400 --> 01:14:34,640
 into a dissonant function, we can get a value.

1103
01:14:34,640 --> 01:14:37,320
 And we know when positive, we put it into the positive class.

1104
01:14:37,320 --> 01:14:39,120
 When negative, we put it in negative class.

1105
01:14:39,120 --> 01:14:41,920
 But this positive, sometimes the value could be big.

1106
01:14:41,920 --> 01:14:43,960
 Sometimes the value could be small, right?

1107
01:14:43,960 --> 01:14:45,640
 And for negative, sometimes the value

1108
01:14:45,640 --> 01:14:47,560
 could be closing to zero.

1109
01:14:47,560 --> 01:14:49,400
 But sometimes it could be very negative.

1110
01:14:49,400 --> 01:14:51,760
 So what's the meaning of these values?

1111
01:14:51,760 --> 01:14:52,260
 OK.

1112
01:14:52,260 --> 01:14:55,000
 So that's we analyze this meaning of this gx.

1113
01:14:55,800 --> 01:15:00,320
 And so here, actually, x, we have x here.

1114
01:15:00,320 --> 01:15:02,400
 And then, of course, we kind of project

1115
01:15:02,400 --> 01:15:05,960
 this x onto this dissonant boundary, onto this dissonant

1116
01:15:05,960 --> 01:15:07,280
 boundary.

1117
01:15:07,280 --> 01:15:11,840
 So this is the, and this projection is just xp.

1118
01:15:11,840 --> 01:15:14,240
 This point is on the dissonant boundary.

1119
01:15:14,240 --> 01:15:16,520
 And this is a projection, a perpendicular projection

1120
01:15:16,520 --> 01:15:18,680
 of x onto this xp.

1121
01:15:18,680 --> 01:15:22,760
 So x here is arbitrary, no, these are points.

1122
01:15:23,720 --> 01:15:25,360
 So this xp.

1123
01:15:25,360 --> 01:15:27,160
 So if you know for geometrically,

1124
01:15:27,160 --> 01:15:29,120
 we know this is a vector x, right?

1125
01:15:34,600 --> 01:15:35,720
 This is x.

1126
01:15:35,720 --> 01:15:37,040
 So this is the x.

1127
01:15:43,040 --> 01:15:44,240
 OK, so this is x, right?

1128
01:15:44,240 --> 01:15:45,720
 This is x.

1129
01:15:45,720 --> 01:15:48,800
 And actually, these are vector from this origin, right?

1130
01:15:48,800 --> 01:15:50,800
 From origin to this.

1131
01:15:50,800 --> 01:15:54,520
 So this is the origin, you know, this x is a vector, right?

1132
01:15:54,520 --> 01:15:57,880
 We can, this actually corresponds to a, you know,

1133
01:15:57,880 --> 01:15:59,680
 this line, right?

1134
01:15:59,680 --> 01:16:01,280
 This is a vector, OK?

1135
01:16:01,280 --> 01:16:02,400
 This is a vector.

1136
01:16:02,400 --> 01:16:03,880
 And actually, from this, this vector

1137
01:16:03,880 --> 01:16:07,640
 equal to this vector xp plus another vector.

1138
01:16:07,640 --> 01:16:10,400
 This vector is just x minus xp.

1139
01:16:10,400 --> 01:16:12,720
 Of course, actually, if we just based on the mathematical

1140
01:16:12,720 --> 01:16:17,560
 calculation, xp equal to x equal to xp plus x minus xp,

1141
01:16:17,560 --> 01:16:17,760
 right?

1142
01:16:18,720 --> 01:16:21,520
 So of course, actually, this xp is canceled, right?

1143
01:16:21,520 --> 01:16:24,600
 But actually, we know this xp is just a projection of x

1144
01:16:24,600 --> 01:16:27,800
 onto the, onto the decision boundary.

1145
01:16:27,800 --> 01:16:32,680
 So even geometrically, we know that the vector summation,

1146
01:16:32,680 --> 01:16:33,160
 right?

1147
01:16:33,160 --> 01:16:37,720
 So this vector x here equal to this vector xp plus another

1148
01:16:37,720 --> 01:16:38,800
 vector, this vector.

1149
01:16:38,800 --> 01:16:42,240
 Actually, this vector is just x minus xp.

1150
01:16:42,240 --> 01:16:45,640
 And when we talk about a vector, actually, normally for a vector,

1151
01:16:45,640 --> 01:16:47,680
 first, actually, we have a length of the vector, right?

1152
01:16:47,680 --> 01:16:50,760
 And then we have a direction of the vector, OK?

1153
01:16:50,760 --> 01:16:57,600
 And actually, just now, we see w is perpendicular to this

1154
01:16:57,600 --> 01:16:59,320
 decision boundary, OK?

1155
01:16:59,320 --> 01:17:03,320
 And here, xp is a projection.

1156
01:17:03,320 --> 01:17:05,280
 It's also perpendicular, right?

1157
01:17:05,280 --> 01:17:10,680
 So this x minus xp should be actually parallel to w.

1158
01:17:10,680 --> 01:17:12,320
 Should be parallel to w, right?

1159
01:17:12,320 --> 01:17:13,800
 Should be parallel to w.

1160
01:17:13,800 --> 01:17:18,560
 And then, actually, this x minus xp can be expressed by two

1161
01:17:18,560 --> 01:17:19,160
 things.

1162
01:17:19,160 --> 01:17:23,040
 First, actually, the length of x minus xp.

1163
01:17:23,040 --> 01:17:24,880
 So we use the length r here.

1164
01:17:24,880 --> 01:17:28,080
 The r could be positive, could be negative here, OK?

1165
01:17:28,080 --> 01:17:29,800
 And then another is the direction.

1166
01:17:29,800 --> 01:17:31,480
 The direction here, we'll use the w.

1167
01:17:31,480 --> 01:17:33,920
 Just now, we see parallel to w, right?

1168
01:17:33,920 --> 01:17:37,400
 And actually, so this is in the same direction of w.

1169
01:17:37,400 --> 01:17:42,480
 And then w, of course, actually, even now, we just use the

1170
01:17:42,679 --> 01:17:44,599
 direction of the w.

1171
01:17:44,599 --> 01:17:47,080
 We don't consider the length of the w.

1172
01:17:47,080 --> 01:17:48,639
 So we normalize the w.

1173
01:17:48,639 --> 01:17:52,200
 So w divided by the normal w.

1174
01:17:52,200 --> 01:17:54,839
 So now, this vector just represents the direction

1175
01:17:54,839 --> 01:17:57,879
 with a unit length.

1176
01:17:57,879 --> 01:18:01,759
 Then we times the length of x minus xp.

1177
01:18:01,759 --> 01:18:04,320
 So this, we just know from this, actually, we

1178
01:18:04,320 --> 01:18:07,240
 express x into a different form, right?

1179
01:18:07,240 --> 01:18:10,639
 x equal to the projection of x onto the projection,

1180
01:18:10,639 --> 01:18:12,160
 onto the decision boundary.

1181
01:18:12,840 --> 01:18:16,040
 So we use the length of the vector, right?

1182
01:18:16,040 --> 01:18:23,040
 That is, xp plus, actually, the distance of the length of the

1183
01:18:23,040 --> 01:18:24,040
 vector, right?

1184
01:18:24,040 --> 01:18:25,440
 From x to xp.

1185
01:18:25,440 --> 01:18:28,040
 Then times the direction.

1186
01:18:28,040 --> 01:18:31,040
 Direction parallel to w, right?

1187
01:18:31,040 --> 01:18:34,240
 So w divided by the normal.

1188
01:18:34,240 --> 01:18:35,240
 OK.

1189
01:18:35,240 --> 01:18:36,639
 So this is x.

1190
01:18:36,639 --> 01:18:39,840
 So now, we substitute this x into the discriminant function.

1191
01:18:42,360 --> 01:18:43,360
 Into this gx.

1192
01:18:46,760 --> 01:18:48,360
 So this is a 3D view.

1193
01:18:52,360 --> 01:18:53,480
 So we substitute.

1194
01:18:53,480 --> 01:18:55,599
 W trying to put x plus w0.

1195
01:18:55,599 --> 01:19:01,519
 So this x is xp plus r times w divided by the normal of w.

1196
01:19:01,519 --> 01:19:03,120
 Normal length, right?

1197
01:19:03,120 --> 01:19:05,040
 So w divided by the normal of length.

1198
01:19:05,040 --> 01:19:10,040
 Just know it's a vector, a directional vector, with a

1199
01:19:10,040 --> 01:19:12,880
 union length.

1200
01:19:12,880 --> 01:19:14,519
 And then plus w0.

1201
01:19:14,519 --> 01:19:21,720
 And then we can put w trying to put xp, w trying to put

1202
01:19:21,720 --> 01:19:25,000
 plus r, w, and then w from normal.

1203
01:19:25,000 --> 01:19:26,280
 Because r is a scalar, right?

1204
01:19:26,280 --> 01:19:29,680
 So we can w trying to put w together here.

1205
01:19:29,680 --> 01:19:35,080
 And w trying to put xp plus w0, it's just a gxp.

1206
01:19:35,080 --> 01:19:38,720
 And then plus r, w trying to put w actually

1207
01:19:38,720 --> 01:19:41,280
 is the square of the norm.

1208
01:19:41,280 --> 01:19:44,360
 And then divided by the norm, so become r times

1209
01:19:44,360 --> 01:19:47,920
 the norm of the w.

1210
01:19:47,920 --> 01:19:50,559
 And the xp is on the descent boundary, right?

1211
01:19:50,559 --> 01:19:53,320
 So gxp actually is 0.

1212
01:19:53,320 --> 01:19:59,360
 So finally, actually the gx equal to r times the normal w.

1213
01:19:59,360 --> 01:20:08,759
 So r is a distance from the point and from the sample x

1214
01:20:08,759 --> 01:20:10,839
 to the descent boundary.

1215
01:20:10,839 --> 01:20:14,000
 So actually, in other words, actually, so every time when

1216
01:20:14,000 --> 01:20:18,559
 you get the gx, if the gx divided by the norm of w,

1217
01:20:18,559 --> 01:20:19,639
 we obtain r.

1218
01:20:19,639 --> 01:20:23,880
 This r actually is a distance from the point

1219
01:20:23,880 --> 01:20:26,480
 to the descent boundary.

1220
01:20:26,480 --> 01:20:27,799
 This r can be positive.

1221
01:20:27,799 --> 01:20:29,040
 It could be negative.

1222
01:20:29,040 --> 01:20:32,519
 If this r is on the native side of the hyperplane,

1223
01:20:32,519 --> 01:20:36,400
 then actually this sample, then this r is negative.

1224
01:20:36,400 --> 01:20:40,760
 If r is positive, then this point is on the positive side.

1225
01:20:40,760 --> 01:20:44,360
 But this represents a distance.

1226
01:20:44,360 --> 01:20:47,080
 So next time when you calculate gx in that equation,

1227
01:20:47,080 --> 01:20:49,280
 the gx divided by the norm of w is

1228
01:20:49,280 --> 01:20:53,320
 a distance from x to the descent boundary.

1229
01:20:53,320 --> 01:20:55,720
 Normally, this value is bigger, right?

1230
01:20:55,720 --> 01:20:59,000
 So that's a further away from the descent boundary.

1231
01:20:59,000 --> 01:21:00,840
 And then we have a higher confidence

1232
01:21:00,840 --> 01:21:04,440
 to classify the sample into the classes.

1233
01:21:04,440 --> 01:21:07,840
 If gx is very small, or the r is very small,

1234
01:21:07,840 --> 01:21:11,160
 that means the sample is very close to the descent boundary.

1235
01:21:11,160 --> 01:21:17,120
 We classify them into one of the two classes,

1236
01:21:17,120 --> 01:21:18,120
 but with low confidence.

1237
01:21:20,640 --> 01:21:22,120
 So this is not this r.

1238
01:21:22,120 --> 01:21:25,640
 So this r, the gx, actually give us the confidence.

1239
01:21:25,640 --> 01:21:29,320
 But of course, we need to divide by the norm of w, right?

1240
01:21:29,320 --> 01:21:35,560
 Actually, the w, we just see the main function

1241
01:21:35,560 --> 01:21:38,920
 is to the determination of the direction

1242
01:21:38,920 --> 01:21:41,520
 of the descent boundary.

1243
01:21:41,520 --> 01:21:45,480
 And for this w, if you multiply a constant value,

1244
01:21:45,480 --> 01:21:47,200
 you can change the value of w, but you

1245
01:21:47,200 --> 01:21:50,240
 don't change the direction of w.

1246
01:21:50,240 --> 01:21:53,480
 So after that, we must normalize this gx

1247
01:21:53,480 --> 01:22:01,400
 by the norm of w.

1248
01:22:01,400 --> 01:22:04,120
 If you don't divide by the norm of w,

1249
01:22:04,120 --> 01:22:09,839
 then the gx, actually, sometimes it's not that meaningful.

1250
01:22:09,839 --> 01:22:14,879
 You must divide by this gx by this norm of w.

1251
01:22:14,879 --> 01:22:18,599
 Then you can get a distance from the x

1252
01:22:18,599 --> 01:22:19,799
 to the descent boundary.

1253
01:22:19,799 --> 01:22:21,839
 And then based on this value, whether you're

1254
01:22:21,880 --> 01:22:24,920
 very close to zero or something like that,

1255
01:22:24,920 --> 01:22:27,640
 or very negative or very positive,

1256
01:22:27,640 --> 01:22:31,600
 then this can decide not to give us a confidence.

1257
01:22:31,600 --> 01:22:35,120
 Just like you apply 20, you assume

1258
01:22:35,120 --> 01:22:37,080
 we have a threshold, right?

1259
01:22:37,080 --> 01:22:43,520
 If a GPA is greater than 3.5, then you pass, right?

1260
01:22:43,520 --> 01:22:45,400
 You get offered, right?

1261
01:22:45,400 --> 01:22:50,480
 If your GPA is lower than 3.5, then you fail, right?

1262
01:22:50,480 --> 01:22:52,440
 Then you cannot get offered, right?

1263
01:22:52,440 --> 01:22:54,360
 Just this is like classified.

1264
01:22:54,360 --> 01:22:59,759
 If your GPA is far away from this, then your GPA is 4.

1265
01:22:59,759 --> 01:23:02,080
 Very far away from this 3.5, right?

1266
01:23:02,080 --> 01:23:04,719
 Then you have high confidence.

1267
01:23:04,719 --> 01:23:10,839
 Even if you actually have a very low value line, very far away

1268
01:23:10,839 --> 01:23:14,919
 from 3, then you have very high confidence

1269
01:23:14,919 --> 01:23:16,040
 to be rejected, right?

1270
01:23:16,040 --> 01:23:19,240
 So this is not like very close to the boundary.

1271
01:23:19,240 --> 01:23:20,240
 So this is a meaning, right?

1272
01:23:20,240 --> 01:23:24,400
 So from the GX, actually, we can judge.

1273
01:23:24,400 --> 01:23:26,639
 Actually, in practice, actually, all the data

1274
01:23:26,639 --> 01:23:29,400
 in the hard-demolished space, we cannot see the distribution.

1275
01:23:29,400 --> 01:23:32,360
 We don't know where the position of each of the sample.

1276
01:23:32,360 --> 01:23:35,840
 But actually, through the GX, we can roughly understand.

1277
01:23:35,840 --> 01:23:37,760
 We can just imagine, right, in the space,

1278
01:23:37,760 --> 01:23:39,160
 there is a certain boundary.

1279
01:23:39,160 --> 01:23:41,679
 Or the GX is very small, or it's very close to that,

1280
01:23:41,679 --> 01:23:43,760
 just like a 2D case, right?

1281
01:23:43,760 --> 01:23:46,719
 And very far away, all of this, very high confidence.

1282
01:23:46,719 --> 01:23:49,240
 It belongs to class, 1 or class 2.

1283
01:23:49,240 --> 01:23:51,440
 OK, so this is a meaning of GX, right?

1284
01:23:51,440 --> 01:23:55,480
 GX divided by the W is just the normal W, just the distance

1285
01:23:55,480 --> 01:23:57,679
 from the point X to the distance boundary.

1286
01:24:06,880 --> 01:24:09,160
 And actually, for a special point,

1287
01:24:09,160 --> 01:24:13,639
 for a special point, that is just the origin, right?

1288
01:24:13,639 --> 01:24:15,960
 For origin, actually, the X is 0, right?

1289
01:24:15,960 --> 01:24:17,000
 It's a 0 vector, right?

1290
01:24:17,000 --> 01:24:17,840
 0 vector.

1291
01:24:17,840 --> 01:24:20,520
 Then we substitute W transpose times 0.

1292
01:24:20,520 --> 01:24:21,760
 Of course, 0.

1293
01:24:21,760 --> 01:24:26,480
 Then the GX equals W0, OK?

1294
01:24:26,480 --> 01:24:29,040
 And so actually, the W0, of course,

1295
01:24:29,040 --> 01:24:30,920
 can be positive, can be negative, right?

1296
01:24:30,920 --> 01:24:33,080
 Can be positive, can be negative.

1297
01:24:33,080 --> 01:24:39,560
 And if this W0 is positive, that means

1298
01:24:39,560 --> 01:24:43,000
 when we look at W transpose X plus W0, W0,

1299
01:24:43,040 --> 01:24:47,720
 which is a scalar, right?

1300
01:24:47,720 --> 01:24:48,320
 It's a scalar.

1301
01:24:48,320 --> 01:24:50,480
 When we look at this value, and then we can also

1302
01:24:50,480 --> 01:24:52,960
 imagine the position of the line, right?

1303
01:24:52,960 --> 01:24:54,640
 Except the hyperplane.

1304
01:24:54,640 --> 01:25:02,320
 If W0 is positive, and then this means that the origin

1305
01:25:02,320 --> 01:25:07,040
 is on the positive side of the distance boundary, positive

1306
01:25:07,040 --> 01:25:08,800
 side, OK?

1307
01:25:08,800 --> 01:25:11,440
 If W is negative, that means the origin

1308
01:25:11,440 --> 01:25:15,280
 is on the negative side, right?

1309
01:25:15,280 --> 01:25:20,000
 W0 negative, the origin is on the negative side, OK?

1310
01:25:20,000 --> 01:25:23,160
 So from this, actually, there's a W0,

1311
01:25:23,160 --> 01:25:26,160
 we can get this point, right?

1312
01:25:26,160 --> 01:25:33,919
 So this, actually, so the W0 determine, actually,

1313
01:25:33,919 --> 01:25:38,200
 the location on the position of the hyperplane, OK?

1314
01:25:38,639 --> 01:25:46,880
 And W determine the orientation of the hyperplane, OK?

1315
01:25:46,880 --> 01:25:51,639
 If W0 positive, origin on the positive side, OK?

1316
01:25:51,639 --> 01:25:57,960
 If W0, 0, then the hyperplane passes through the origin.

1317
01:25:57,960 --> 01:26:02,280
 And so, actually, we know when we perform the data separation

1318
01:26:02,280 --> 01:26:03,720
 by a line, right?

1319
01:26:03,720 --> 01:26:05,200
 So the line position, not necessarily,

1320
01:26:05,200 --> 01:26:07,040
 will pass through the origin.

1321
01:26:07,080 --> 01:26:08,840
 So that's the reason why we're always

1322
01:26:08,840 --> 01:26:20,160
 to include a bias term, W0, OK, in the dismal function.

1323
01:26:20,160 --> 01:26:22,840
 And actually, this is often, even in other models,

1324
01:26:22,840 --> 01:26:24,240
 like in neural networks, of course,

1325
01:26:24,240 --> 01:26:27,320
 this class is not taught by neural networks.

1326
01:26:27,320 --> 01:26:30,000
 But in neural networks, normally, I think for a neural,

1327
01:26:30,000 --> 01:26:32,920
 the first step is also to perform

1328
01:26:32,920 --> 01:26:35,880
 a kind of linear operation like this.

1329
01:26:35,880 --> 01:26:37,920
 W transpose X plus W0.

1330
01:26:37,920 --> 01:26:39,680
 And then we perform a nonlinear function

1331
01:26:39,680 --> 01:26:41,280
 on this dismal function.

1332
01:26:41,280 --> 01:26:43,800
 In dismal function, it's called activation function,

1333
01:26:43,800 --> 01:26:46,920
 activation value signal in the neural networks, right?

1334
01:26:46,920 --> 01:26:49,680
 So we also need to put a W0.

1335
01:26:49,680 --> 01:26:53,320
 But some don't really understand why we put a W0, right?

1336
01:26:53,320 --> 01:26:58,440
 So if you don't have a W0, that means W0 equals 0, OK?

1337
01:26:58,440 --> 01:27:01,080
 That means, actually, you are designing something

1338
01:27:01,080 --> 01:27:03,400
 that passes through the origin.

1339
01:27:03,400 --> 01:27:05,480
 Actually, the location is almost determined, right?

1340
01:27:05,480 --> 01:27:07,000
 It must pass through 0.

1341
01:27:07,000 --> 01:27:09,799
 But this is not a reasonable assumption, right?

1342
01:27:09,799 --> 01:27:12,799
 So we must use a W0.

1343
01:27:12,799 --> 01:27:17,799
 We use data to determine the value of W0.

1344
01:27:17,799 --> 01:27:23,040
 So we can see the function of the W0, right?

1345
01:27:23,040 --> 01:27:26,200
 The use of W0, then the position or the location

1346
01:27:26,200 --> 01:27:28,240
 of the hyperplan can be any point.

1347
01:27:28,240 --> 01:27:30,639
 We can, at this point, the position

1348
01:27:30,639 --> 01:27:34,160
 is decided by the data.

1349
01:27:34,160 --> 01:27:37,559
 If you don't use the W0, this is a bias term.

1350
01:27:37,559 --> 01:27:39,360
 And then you are designing something

1351
01:27:39,360 --> 01:27:42,280
 that passes through the origin, OK?

1352
01:27:47,800 --> 01:27:49,599
 OK, so that's actually what we're talking about,

1353
01:27:49,599 --> 01:27:51,599
 the determination of W and W0.

1354
01:27:51,599 --> 01:27:54,120
 And first, we're talking about the determination of W.

1355
01:27:54,120 --> 01:27:55,519
 So these are key points, right?

1356
01:27:55,519 --> 01:27:58,720
 So here, I'll object to design a pen and classifier

1357
01:27:58,720 --> 01:28:01,080
 and the linear dismal analysis on classifier,

1358
01:28:01,120 --> 01:28:04,640
 the FD classifier, to separate data in the two classes.

1359
01:28:04,640 --> 01:28:07,440
 Design a classifier, actually, the main objective

1360
01:28:07,440 --> 01:28:13,720
 is to determine the value for W with vector and also the W0.

1361
01:28:13,720 --> 01:28:16,640
 So that's actually how we decide that.

1362
01:28:16,640 --> 01:28:19,960
 We learn how to decide this position W.

1363
01:28:19,960 --> 01:28:25,160
 And so here, I show you some data points.

1364
01:28:25,160 --> 01:28:28,320
 The sample in the two class, OK?

1365
01:28:28,320 --> 01:28:30,600
 So this sample in this class, this one

1366
01:28:30,640 --> 01:28:32,280
 is another class.

1367
01:28:32,280 --> 01:28:39,520
 And actually, we actually want to find a W, right?

1368
01:28:39,520 --> 01:28:42,000
 What is W transpose X?

1369
01:28:42,000 --> 01:28:50,240
 And actually, we can have such a W. So this is W.

1370
01:28:50,240 --> 01:28:52,800
 And then we can W transpose X, actually,

1371
01:28:52,800 --> 01:28:54,640
 come to something where we put this, right?

1372
01:28:54,640 --> 01:28:59,240
 Make this project, I did the X, onto this W.

1373
01:28:59,280 --> 01:29:04,280
 That's the meaning of W transpose X, project X,

1374
01:29:04,280 --> 01:29:08,040
 or map this X on this W. OK.

1375
01:29:08,040 --> 01:29:11,599
 So for then, after project on this W,

1376
01:29:11,599 --> 01:29:15,400
 and the value and all the location of the sample

1377
01:29:15,400 --> 01:29:21,360
 in this class on this segment of this axis, W axis, right?

1378
01:29:21,360 --> 01:29:24,679
 And the project, the sample of the projection,

1379
01:29:25,120 --> 01:29:33,720
 the sample in this class on this side of this W.

1380
01:29:33,720 --> 01:29:37,000
 And actually, from here, we can see, after projection,

1381
01:29:37,000 --> 01:29:39,560
 the samples are well separated.

1382
01:29:39,560 --> 01:29:42,040
 Because we can see a gap, right?

1383
01:29:42,040 --> 01:29:43,440
 There is a gap.

1384
01:29:43,440 --> 01:29:45,400
 So from here to here.

1385
01:29:45,400 --> 01:29:47,080
 So there is a gap.

1386
01:29:47,080 --> 01:29:48,680
 In one side, OK.

1387
01:29:48,680 --> 01:29:54,080
 So this side, all the samples are actually from this class.

1388
01:29:54,080 --> 01:29:57,680
 And then another side, all the samples are from this side.

1389
01:29:57,680 --> 01:29:59,960
 So this is the 1W.

1390
01:29:59,960 --> 01:30:06,840
 So now I show you another W. So this is another W.

1391
01:30:06,840 --> 01:30:11,280
 And then W transpose X is also against the projection of X

1392
01:30:11,280 --> 01:30:15,280
 onto the axis W. So this is the case.

1393
01:30:15,280 --> 01:30:18,080
 After projection, and then the values

1394
01:30:18,080 --> 01:30:21,519
 that we project to this part.

1395
01:30:21,519 --> 01:30:23,200
 And then the sample in this class

1396
01:30:23,200 --> 01:30:25,160
 also project the sample here.

1397
01:30:25,160 --> 01:30:26,920
 The project sample here.

1398
01:30:26,920 --> 01:30:29,440
 And actually, after projection, we

1399
01:30:29,440 --> 01:30:33,240
 see the samples actually are mixed.

1400
01:30:33,240 --> 01:30:42,240
 Amidst of the project of W. So which W is better?

1401
01:30:42,240 --> 01:30:43,320
 Which W is better?

1402
01:30:47,440 --> 01:30:48,599
 We don't have a W, right?

1403
01:30:48,600 --> 01:30:52,120
 Actually, our goal, our objective,

1404
01:30:52,120 --> 01:30:55,320
 is to find a good W. But this year, first thing

1405
01:30:55,320 --> 01:30:59,440
 we need to know what is a good W, right?

1406
01:30:59,440 --> 01:31:02,720
 So what is the carry-on ratio of a good W?

1407
01:31:02,720 --> 01:31:05,040
 So among the two, the two, right?

1408
01:31:05,040 --> 01:31:08,120
 So which is better to think?

1409
01:31:08,120 --> 01:31:11,280
 Which is better?

1410
01:31:11,280 --> 01:31:12,800
 Which one?

1411
01:31:12,800 --> 01:31:13,960
 The left one or the right one?

1412
01:31:13,960 --> 01:31:17,400
 The left side or the right side?

1413
01:31:17,440 --> 01:31:18,599
 Left side, right?

1414
01:31:18,599 --> 01:31:20,200
 So why is the left side actually better?

1415
01:31:23,639 --> 01:31:26,599
 So of course, we can see better.

1416
01:31:26,599 --> 01:31:29,080
 If you're in practice, we will see something better.

1417
01:31:29,080 --> 01:31:34,160
 We must have an evaluation function to measure the goodness.

1418
01:31:34,160 --> 01:31:35,920
 And then based on the evaluation function,

1419
01:31:35,920 --> 01:31:38,559
 then we can see one is better than another one, right?

1420
01:31:38,559 --> 01:31:42,920
 So here you see the left one is better.

1421
01:31:42,920 --> 01:31:45,799
 Then how to, based on what criteria,

1422
01:31:46,000 --> 01:31:47,880
 you see it is better?

1423
01:31:47,880 --> 01:31:48,480
 Based on what?

1424
01:31:53,600 --> 01:31:56,440
 We can see actually the samples actually in here,

1425
01:31:56,440 --> 01:31:58,360
 they are separated, right?

1426
01:31:58,360 --> 01:32:00,400
 And by here, you know, after the project on this W,

1427
01:32:00,400 --> 01:32:04,440
 this is actually the sample, actually a mix.

1428
01:32:04,440 --> 01:32:06,400
 Mix.

1429
01:32:06,400 --> 01:32:10,200
 So you know, they're separated.

1430
01:32:10,200 --> 01:32:11,960
 But how to use practice?

1431
01:32:16,320 --> 01:32:20,280
 And actually in the first one, actually after projection,

1432
01:32:20,280 --> 01:32:24,800
 and the sample in the two classes are separated.

1433
01:32:24,800 --> 01:32:26,520
 When we talk about the separation, here,

1434
01:32:26,520 --> 01:32:30,040
 actually we see actually the distance

1435
01:32:30,040 --> 01:32:34,840
 or the difference between the samples of the projection.

1436
01:32:34,840 --> 01:32:37,240
 This difference is big.

1437
01:32:37,240 --> 01:32:39,720
 They are very different after projection, right?

1438
01:32:39,720 --> 01:32:43,120
 Because of the other projection, all the samples in this,

1439
01:32:43,120 --> 01:32:47,559
 actually in one class are located in this segment,

1440
01:32:47,559 --> 01:32:49,559
 or in this part of the axis.

1441
01:32:49,559 --> 01:32:53,920
 And in this part of the axis is for another class.

1442
01:32:53,920 --> 01:32:55,559
 They are separated.

1443
01:32:55,559 --> 01:32:58,360
 They are very different.

1444
01:32:58,360 --> 01:33:00,440
 Of course, actually we cannot just use one sample

1445
01:33:00,440 --> 01:33:01,840
 to measure the difference, right?

1446
01:33:01,840 --> 01:33:05,800
 Whether the sample is very different from another, right?

1447
01:33:05,800 --> 01:33:08,280
 So here, actually, we can use the difference

1448
01:33:08,280 --> 01:33:10,440
 between the two mean values.

1449
01:33:10,440 --> 01:33:13,440
 We can look at the mean value of the sample

1450
01:33:13,440 --> 01:33:16,240
 after projection, maybe position here.

1451
01:33:16,240 --> 01:33:18,120
 And then after projection, the sample here,

1452
01:33:18,120 --> 01:33:19,440
 the mean value here.

1453
01:33:19,440 --> 01:33:23,559
 These two mean values are very different.

1454
01:33:23,559 --> 01:33:25,719
 So this is a good value.

1455
01:33:25,719 --> 01:33:27,200
 This is actually a difference called

1456
01:33:27,200 --> 01:33:30,519
 between class difference.

1457
01:33:30,519 --> 01:33:36,120
 The bigger or the greater the better between class.

1458
01:33:36,120 --> 01:33:39,200
 So this is the one, actually, the between class difference,

1459
01:33:39,200 --> 01:33:39,879
 right?

1460
01:33:40,320 --> 01:33:42,760
 We hope, actually, the right hand side.

1461
01:33:42,760 --> 01:33:47,720
 If you look at the case in the right hand side,

1462
01:33:47,720 --> 01:33:48,960
 which is the data mix, right?

1463
01:33:48,960 --> 01:33:52,200
 The mix, actually, then you look at the mean value,

1464
01:33:52,200 --> 01:33:55,120
 maybe for one class, for this cross sample class,

1465
01:33:55,120 --> 01:33:56,600
 the mean value is here.

1466
01:33:56,600 --> 01:33:58,080
 And then for the sample in another class,

1467
01:33:58,080 --> 01:33:59,840
 the mean value is here.

1468
01:33:59,840 --> 01:34:01,560
 One is the mean value is here.

1469
01:34:01,560 --> 01:34:03,360
 Another mean value maybe here.

1470
01:34:03,360 --> 01:34:05,480
 The difference is small.

1471
01:34:05,480 --> 01:34:08,080
 So when we see one is better than another one, right?

1472
01:34:08,280 --> 01:34:11,800
 So in the tree, we must use a mathematical criterion

1473
01:34:11,800 --> 01:34:16,200
 to evaluate the goodness.

1474
01:34:16,200 --> 01:34:18,400
 So here, in the pattern classification,

1475
01:34:18,400 --> 01:34:23,360
 first, we need to have an idea of what is a good classifier.

1476
01:34:23,360 --> 01:34:25,320
 So what is a good W?

1477
01:34:25,320 --> 01:34:27,080
 You must have some ideas, right?

1478
01:34:27,080 --> 01:34:29,360
 Doesn't mean that you need to have some understanding

1479
01:34:29,360 --> 01:34:31,160
 of the pattern classification.

1480
01:34:31,160 --> 01:34:33,519
 Second, based on the understanding,

1481
01:34:33,520 --> 01:34:39,280
 how to summarize this concept using mathematical equations?

1482
01:34:39,280 --> 01:34:41,280
 Because you see one is better than another one,

1483
01:34:41,280 --> 01:34:43,960
 we must have some measures, right?

1484
01:34:43,960 --> 01:34:47,560
 This measure could be the between class difference.

1485
01:34:47,560 --> 01:34:49,040
 This is between class difference, right?

1486
01:34:49,040 --> 01:34:50,720
 Because there is a gap.

1487
01:34:50,720 --> 01:34:53,480
 It can be measured based on the difference

1488
01:34:53,480 --> 01:34:56,120
 between two mean values.

1489
01:34:56,120 --> 01:34:58,800
 So here, actually, the x is a vector, right?

1490
01:34:58,800 --> 01:35:01,640
 But I have a projection onto this W,

1491
01:35:01,640 --> 01:35:03,520
 you know, the W transpose x.

1492
01:35:03,520 --> 01:35:04,680
 So this is a scalar.

1493
01:35:04,680 --> 01:35:07,520
 So we have a mean value for sample in class 1

1494
01:35:07,520 --> 01:35:08,880
 for sample in class 2.

1495
01:35:08,880 --> 01:35:11,200
 So this difference, the greater the better.

1496
01:35:11,200 --> 01:35:15,000
 So this could be a measure for the separability, right?

1497
01:35:15,000 --> 01:35:16,520
 This measure could be used to judge

1498
01:35:16,520 --> 01:35:19,080
 whether W is good or not.

1499
01:35:19,080 --> 01:35:22,280
 The difference between two mean values,

1500
01:35:22,280 --> 01:35:27,480
 this is called between class difference, the larger the better.

1501
01:35:27,480 --> 01:35:30,280
 The larger, the easier to separate the sample in the two

1502
01:35:30,280 --> 01:35:32,440
 classes.

1503
01:35:32,440 --> 01:35:36,400
 In the second case, the difference is small.

1504
01:35:36,400 --> 01:35:39,240
 And then it's quite challenging to separate the sample

1505
01:35:39,240 --> 01:35:41,160
 in the two classes.

1506
01:35:41,160 --> 01:35:43,280
 OK, so this is one point.

1507
01:35:43,280 --> 01:35:46,559
 And then another point is that when we,

1508
01:35:46,559 --> 01:35:49,040
 for easy classification, and just now we

1509
01:35:49,040 --> 01:35:50,559
 talk about the between class, actually,

1510
01:35:50,559 --> 01:35:53,519
 there are also some requirements on the all

1511
01:35:53,560 --> 01:35:57,840
 measures on the within class difference.

1512
01:35:57,840 --> 01:35:59,720
 Within class difference.

1513
01:35:59,720 --> 01:36:02,800
 And even the within class difference, actually,

1514
01:36:02,800 --> 01:36:04,880
 the smaller the better.

1515
01:36:04,880 --> 01:36:08,840
 We see this class, they all were similar.

1516
01:36:08,840 --> 01:36:11,560
 For another class, they all were similar.

1517
01:36:11,560 --> 01:36:14,840
 Then the classification problem is easier.

1518
01:36:14,840 --> 01:36:19,200
 OK, so this is actually another measure, right?

1519
01:36:19,200 --> 01:36:22,160
 Another expectation, another measure for good,

1520
01:36:22,360 --> 01:36:26,480
 actually, now, W. That means after projection,

1521
01:36:26,480 --> 01:36:32,599
 we hope the sample has a small within class difference.

1522
01:36:32,599 --> 01:36:35,599
 The difference is small.

1523
01:36:35,599 --> 01:36:38,120
 So here, OK, so this is another measure,

1524
01:36:38,120 --> 01:36:40,200
 within class difference.

1525
01:36:40,200 --> 01:36:43,200
 And we hope to find that W, the between class difference

1526
01:36:43,200 --> 01:36:45,120
 could be big.

1527
01:36:45,120 --> 01:36:47,720
 The within class difference could be small.

1528
01:36:47,720 --> 01:36:51,960
 But we want to find such a W. OK?

1529
01:36:51,960 --> 01:36:53,920
 So this is our objective.

1530
01:36:53,920 --> 01:36:57,840
 We want to find a W so that the within class difference

1531
01:36:57,840 --> 01:37:00,840
 could be great, could be big.

1532
01:37:00,840 --> 01:37:02,600
 Between class difference could be big.

1533
01:37:02,600 --> 01:37:06,000
 The within class difference could be small.

1534
01:37:06,000 --> 01:37:08,160
 And actually, we want to find a W that

1535
01:37:08,160 --> 01:37:12,640
 could maximize the ratio of the two,

1536
01:37:12,640 --> 01:37:15,440
 the ratio of the difference between class difference

1537
01:37:15,440 --> 01:37:16,720
 to the within class difference.

1538
01:37:16,720 --> 01:37:19,160
 Because we hope the numerator part could be

1539
01:37:19,160 --> 01:37:20,640
 the greater the better, right?

1540
01:37:20,640 --> 01:37:23,840
 The denominator, within class, the smaller the better.

1541
01:37:23,840 --> 01:37:28,280
 So why not to use the ratio of the two

1542
01:37:28,280 --> 01:37:32,240
 as actually the measure for the goodness of the W?

1543
01:37:35,240 --> 01:37:39,600
 OK, so this is the basic idea of the Fisher-Linus-Jiffm-Lenz.

1544
01:37:39,600 --> 01:37:42,720
 And so the Fisher-Linus-Jiffm-Lenz analysis,

1545
01:37:42,720 --> 01:37:45,120
 that try to find a W, actually, it

1546
01:37:45,120 --> 01:37:51,000
 could actually maximize this M1 and M2.

1547
01:37:51,000 --> 01:37:56,480
 M1 is the mean vector of samples in class 1

1548
01:37:56,480 --> 01:38:03,960
 under projection to the W. M2 is the mean value for samples

1549
01:38:03,960 --> 01:38:06,880
 in class 2 under projection onto the W.

1550
01:38:06,880 --> 01:38:10,480
 So this difference, the better, the bigger, the better.

1551
01:38:10,480 --> 01:38:12,080
 Of course, this is known.

1552
01:38:12,120 --> 01:38:18,360
 M1 tied, M2 tied, both are functions of W.

1553
01:38:18,360 --> 01:38:20,240
 Because when we have different Ws,

1554
01:38:20,240 --> 01:38:22,320
 and then we can have a different projection,

1555
01:38:22,320 --> 01:38:24,840
 of course, the mean value would be different.

1556
01:38:24,840 --> 01:38:29,000
 And actually, this is the mean tied, right?

1557
01:38:29,000 --> 01:38:31,080
 Mi, M1, M2, right?

1558
01:38:31,080 --> 01:38:33,120
 And this is the projection.

1559
01:38:33,120 --> 01:38:36,559
 W transpose X are the projecting on this.

1560
01:38:36,559 --> 01:38:38,680
 And actually, W transpose is the constant.

1561
01:38:38,680 --> 01:38:41,720
 We can move out of the summation operation, right?

1562
01:38:42,680 --> 01:38:45,480
 Then move out, then that is the summation

1563
01:38:45,480 --> 01:38:47,000
 of X divided by Mi.

1564
01:38:47,000 --> 01:38:51,160
 That is just the mean vector, mean vector

1565
01:38:51,160 --> 01:38:52,960
 of the sampling class i.

1566
01:38:52,960 --> 01:38:56,360
 So in other words, actually, the mean value

1567
01:38:56,360 --> 01:38:58,800
 for the sampling class i, actually,

1568
01:38:58,800 --> 01:39:02,000
 equal to the projection of the mean vector of class i

1569
01:39:02,000 --> 01:39:08,000
 to the under W. So this is Mi, right?

1570
01:39:08,000 --> 01:39:11,600
 So Mi just the average of the mean vector, right?

1571
01:39:11,600 --> 01:39:13,760
 So based on this, actually, then we

1572
01:39:13,760 --> 01:39:18,840
 see the difference between the two mean values.

1573
01:39:18,840 --> 01:39:21,320
 Of course, M1 could be greater than M2,

1574
01:39:21,320 --> 01:39:23,520
 or M1 could be less than M2.

1575
01:39:23,520 --> 01:39:28,320
 So actually, we use the absolute value.

1576
01:39:28,320 --> 01:39:30,400
 So later, we don't use the S blue value.

1577
01:39:30,400 --> 01:39:33,480
 We use the square of this measure.

1578
01:39:33,480 --> 01:39:39,240
 So to see the difference between the two mean values.

1579
01:39:40,200 --> 01:39:42,760
 So this W transpose Mi minus M2.

1580
01:39:42,760 --> 01:39:45,280
 So we look at the difference between the two mean vectors.

1581
01:39:45,280 --> 01:39:48,800
 And then we map or project the difference of the two mean

1582
01:39:48,800 --> 01:39:51,200
 vectors on this W. So this is just

1583
01:39:51,200 --> 01:39:52,599
 the between class difference.

1584
01:39:57,840 --> 01:40:01,080
 And that's actually what we look at within class difference,

1585
01:40:01,080 --> 01:40:01,719
 right?

1586
01:40:01,719 --> 01:40:03,080
 Within class difference, here, they

1587
01:40:03,080 --> 01:40:06,880
 use the so-called concept of scatter.

1588
01:40:06,880 --> 01:40:09,960
 And this scatter tree is something like a variance.

1589
01:40:09,960 --> 01:40:15,480
 So further, the projection of x belong to class i, right?

1590
01:40:15,480 --> 01:40:17,000
 So in the data set, the di.

1591
01:40:17,000 --> 01:40:21,360
 Di is the data set for class i.

1592
01:40:21,360 --> 01:40:25,640
 So this is the Gx projection onto the W.

1593
01:40:25,640 --> 01:40:30,440
 Minus Mi minus the mean value, then square, summation.

1594
01:40:30,440 --> 01:40:32,680
 If they divide by the number of samples,

1595
01:40:32,680 --> 01:40:36,400
 this would be the variance.

1596
01:40:36,400 --> 01:40:39,519
 But here, they don't divide by the number of samples.

1597
01:40:39,519 --> 01:40:40,799
 They just a summation.

1598
01:40:40,799 --> 01:40:42,599
 So this is called a scatter.

1599
01:40:42,599 --> 01:40:43,599
 The scatter.

1600
01:40:43,599 --> 01:40:47,960
 The scatter tree is a measure for the spread.

1601
01:40:47,960 --> 01:40:50,519
 Actually, you remember, in the week two,

1602
01:40:50,519 --> 01:40:54,040
 when we talk about the data preparation for modeling,

1603
01:40:54,040 --> 01:40:56,200
 actually, we need to explore the data.

1604
01:40:56,200 --> 01:40:57,679
 And for data, we have a tool.

1605
01:40:57,679 --> 01:40:59,599
 One is in the central tendency.

1606
01:40:59,599 --> 01:41:00,839
 That's the mean value.

1607
01:41:00,839 --> 01:41:02,559
 Another is the spread.

1608
01:41:02,559 --> 01:41:04,839
 The spread of the scatter.

1609
01:41:04,840 --> 01:41:08,360
 So that is a variance of standard of agent.

1610
01:41:08,360 --> 01:41:10,880
 So this is just a measure of the standard of agent,

1611
01:41:10,880 --> 01:41:14,120
 of the variance, the spread.

1612
01:41:14,120 --> 01:41:17,680
 But here, they use the concept of a scatter.

1613
01:41:17,680 --> 01:41:19,040
 Not the variance.

1614
01:41:19,040 --> 01:41:22,040
 If this is divided by n or n minus 1, actually,

1615
01:41:22,040 --> 01:41:24,840
 it's just a variance.

1616
01:41:24,840 --> 01:41:25,440
 OK.

1617
01:41:25,440 --> 01:41:26,680
 So this is called scatter.

1618
01:41:26,680 --> 01:41:28,520
 For unit class, we can have a scatter.

1619
01:41:28,520 --> 01:41:33,240
 A scatter, the scatter here, is just a spread.

1620
01:41:33,240 --> 01:41:38,200
 It's just a measure of the samples with the different,

1621
01:41:38,200 --> 01:41:40,440
 the similarity or difference of the sample

1622
01:41:40,440 --> 01:41:41,880
 within a semi-class.

1623
01:41:41,880 --> 01:41:45,719
 The bigger or the greater the s i squared,

1624
01:41:45,719 --> 01:41:48,679
 the more different the samples.

1625
01:41:48,679 --> 01:41:51,200
 Actually, we hope the s i squared, the scatter,

1626
01:41:51,200 --> 01:41:53,519
 could be as small as possible.

1627
01:41:53,519 --> 01:41:55,320
 That means the sample in the semi-class

1628
01:41:55,320 --> 01:41:56,760
 are quite similar.

1629
01:41:56,760 --> 01:41:59,960
 Then it's easier to classify.

1630
01:41:59,960 --> 01:42:01,120
 OK.

1631
01:42:01,160 --> 01:42:02,680
 So this is the scatter.

1632
01:42:06,920 --> 01:42:10,280
 And this scatter, of course, is also a function of w.

1633
01:42:10,280 --> 01:42:13,200
 Because the gx is a function of w,

1634
01:42:13,200 --> 01:42:16,680
 the mi-tide, which is the mean value,

1635
01:42:16,680 --> 01:42:17,680
 are present on w.

1636
01:42:17,680 --> 01:42:20,160
 It's also a function of w.

1637
01:42:20,160 --> 01:42:23,360
 So this is a tree.

1638
01:42:23,360 --> 01:42:28,120
 For the class 1, we can have a spread of scatter.

1639
01:42:28,120 --> 01:42:32,480
 For class 2, we can have a spread or a scatter.

1640
01:42:32,480 --> 01:42:33,880
 So we can put them together.

1641
01:42:33,880 --> 01:42:36,400
 So this is called the total scatter.

1642
01:42:36,400 --> 01:42:38,680
 Total scatter, the scatter for two classes.

1643
01:42:38,680 --> 01:42:40,920
 For each of the two classes, we calculate

1644
01:42:40,920 --> 01:42:42,800
 the within class difference, which

1645
01:42:42,800 --> 01:42:46,160
 is measured by the scatter.

1646
01:42:46,160 --> 01:42:49,280
 All the spread, actually, the variance, no variance,

1647
01:42:49,280 --> 01:42:51,640
 variance times the number of samples.

1648
01:42:51,640 --> 01:42:54,000
 And OK, so this is the total scatter.

1649
01:42:54,000 --> 01:42:55,599
 For unit class, we have a scatter, right?

1650
01:42:56,560 --> 01:42:58,200
 Total scatter.

1651
01:42:58,200 --> 01:43:01,240
 And actually, we hope this scatter could be as small as

1652
01:43:01,240 --> 01:43:02,920
 possible.

1653
01:43:02,920 --> 01:43:07,360
 And we hope the mi-m2, the difference,

1654
01:43:07,360 --> 01:43:10,320
 could be as big as possible.

1655
01:43:10,320 --> 01:43:12,320
 Why is it as big as possible?

1656
01:43:12,320 --> 01:43:14,160
 Another is as small as possible.

1657
01:43:14,160 --> 01:43:20,960
 So actually, we can try to maximize the difference

1658
01:43:20,960 --> 01:43:24,280
 between the two and the ratio of the two.

1659
01:43:24,320 --> 01:43:30,120
 So the numerator here is just the between class difference,

1660
01:43:30,120 --> 01:43:31,480
 the greater the better.

1661
01:43:31,480 --> 01:43:33,960
 And the denominator part, actually, the within class

1662
01:43:33,960 --> 01:43:35,040
 scatter, right?

1663
01:43:35,040 --> 01:43:36,559
 And the smaller the better.

1664
01:43:36,559 --> 01:43:38,960
 The within class scatter is a matter of the within class

1665
01:43:38,960 --> 01:43:41,440
 difference, the smaller the better.

1666
01:43:41,440 --> 01:43:46,360
 So now we try to actually find the W so that this ratio could

1667
01:43:46,360 --> 01:43:48,440
 be maximized.

1668
01:43:48,440 --> 01:43:50,440
 Racial could be maximized, right?

1669
01:43:50,440 --> 01:43:53,559
 And actually, this ratio is called a fissure ratio.

1670
01:43:53,560 --> 01:43:56,080
 So that's the reason why this matter is called fissure

1671
01:43:56,080 --> 01:43:57,840
 linear decimal analysis.

1672
01:43:57,840 --> 01:44:00,000
 Because this ratio is called fissure ratio.

1673
01:44:00,000 --> 01:44:05,560
 And this measure is often used actually in feature selection.

1674
01:44:05,560 --> 01:44:08,640
 I think in one week, I think we will talk about,

1675
01:44:08,640 --> 01:44:11,800
 for which week, we will talk about the feature selection

1676
01:44:11,800 --> 01:44:14,000
 for the minority reduction.

1677
01:44:14,000 --> 01:44:18,440
 And we want to select good features, good attributes.

1678
01:44:18,440 --> 01:44:21,640
 So how to measure the goodness of the attribute?

1679
01:44:21,640 --> 01:44:25,000
 Actually, we can use this fissure ratio to measure, right?

1680
01:44:25,000 --> 01:44:28,840
 So here, we know that data is projecting on W, then we measure.

1681
01:44:28,840 --> 01:44:30,440
 But actually, in feature selection,

1682
01:44:30,440 --> 01:44:32,480
 we don't do the projection, right?

1683
01:44:32,480 --> 01:44:36,000
 We measure the separability of the data directly

1684
01:44:36,000 --> 01:44:42,040
 along the features using this fissure ratio.

1685
01:44:42,040 --> 01:44:44,000
 OK, so this is a ratio.

1686
01:44:44,000 --> 01:44:46,320
 And this ratio is a function of W. OK,

1687
01:44:46,320 --> 01:44:50,200
 we need to find such a W so that this W is maximized.

1688
01:44:52,600 --> 01:44:54,400
 You see from this process, right?

1689
01:44:54,400 --> 01:44:56,160
 First, we have some understanding

1690
01:44:56,160 --> 01:44:58,240
 about the pentacletophenic problem.

1691
01:44:58,240 --> 01:45:00,560
 We think about what is a good classifier.

1692
01:45:00,560 --> 01:45:02,400
 So what is a good W?

1693
01:45:02,400 --> 01:45:07,360
 OK, then we express our ideas using mathematical equations.

1694
01:45:07,360 --> 01:45:10,440
 OK, then finally, actually, we summarize the problem

1695
01:45:10,440 --> 01:45:12,880
 into an homo-azition problem.

1696
01:45:12,880 --> 01:45:14,000
 So this is machine learning.

1697
01:45:14,000 --> 01:45:16,280
 Machine learning is an homo-azition problem, right?

1698
01:45:16,280 --> 01:45:18,800
 But you need to have some foundation knowledge

1699
01:45:18,800 --> 01:45:20,840
 about pentacletophenic.

1700
01:45:20,840 --> 01:45:26,240
 Otherwise, you won't be able to summarize such a function.

1701
01:45:26,240 --> 01:45:28,760
 We should have knowledge of pentacletophenic.

1702
01:45:28,760 --> 01:45:29,880
 And then, because of knowledge, we

1703
01:45:29,880 --> 01:45:32,680
 can summarize actually a loss function.

1704
01:45:32,680 --> 01:45:36,160
 And then, actually, this is an homo-azition problem,

1705
01:45:36,160 --> 01:45:39,040
 actually, it's converting into an homo-azition problem, right?

1706
01:45:39,040 --> 01:45:44,560
 So next step, the objective is to solve this homo-azition

1707
01:45:44,560 --> 01:45:46,560
 problem, right?

1708
01:45:46,560 --> 01:45:50,440
 And then, we need to find the W so that this JW is maximized.

1709
01:45:54,320 --> 01:45:57,840
 And so next, actually, when we look at what is s,

1710
01:45:57,840 --> 01:46:01,680
 tides squared, s2 squared, how each of these are related to W,

1711
01:46:01,680 --> 01:46:02,280
 right?

1712
01:46:02,280 --> 01:46:05,760
 Then we substitute this as the total scatter

1713
01:46:05,760 --> 01:46:10,520
 and also the square of the mean difference into this JW.

1714
01:46:10,520 --> 01:46:14,720
 We express JW into a function of W.

1715
01:46:14,720 --> 01:46:21,480
 And previously, we already have seen this M1, M2, right?

1716
01:46:21,480 --> 01:46:23,840
 Actually, this is the difference.

1717
01:46:23,840 --> 01:46:28,080
 It's just a difference after actually the difference

1718
01:46:28,080 --> 01:46:29,320
 between the two vectors.

1719
01:46:29,320 --> 01:46:32,200
 And then, project this difference of the two vectors

1720
01:46:32,200 --> 01:46:34,160
 into the tablet.

1721
01:46:34,160 --> 01:46:43,360
 So for the scatter part, right?

1722
01:46:44,320 --> 01:46:47,240
 Here, we define a scatter matrix.

1723
01:46:47,240 --> 01:46:49,040
 Scatter matrix, this.

1724
01:46:49,040 --> 01:46:53,280
 So for each class, actually, we have mean vector, right?

1725
01:46:53,280 --> 01:46:57,280
 So x minus, these are in the original space, not projected

1726
01:46:57,280 --> 01:46:58,320
 yet, right?

1727
01:46:58,320 --> 01:47:01,799
 And so x minus Mi, so this is a column vector.

1728
01:47:01,799 --> 01:47:06,679
 Then x minus Si, trying to put this in the matrix.

1729
01:47:06,679 --> 01:47:11,160
 Summation of all of this, this is called scatter matrix.

1730
01:47:11,200 --> 01:47:13,360
 Scatter matrix for class i.

1731
01:47:13,360 --> 01:47:15,440
 We have class i and class two, right?

1732
01:47:15,440 --> 01:47:19,639
 So the summation is just the total scatter.

1733
01:47:19,639 --> 01:47:22,519
 So this scatter is called within class scatter.

1734
01:47:22,519 --> 01:47:26,120
 Because S1 or S2, right?

1735
01:47:26,120 --> 01:47:28,599
 Measures, actually, the within class i,

1736
01:47:28,599 --> 01:47:31,040
 within class two, no scatter.

1737
01:47:31,040 --> 01:47:33,240
 So this is called SW here.

1738
01:47:33,240 --> 01:47:36,280
 This W mean within class.

1739
01:47:36,280 --> 01:47:38,519
 Within class.

1740
01:47:38,519 --> 01:47:40,360
 It's 1 plus S2.

1741
01:47:40,360 --> 01:47:40,880
 OK.

1742
01:47:40,880 --> 01:47:43,520
 So this SW is called within class scatter matrix.

1743
01:47:48,520 --> 01:47:51,000
 And then let's look at the relationship, right?

1744
01:47:51,000 --> 01:47:59,280
 Between Si times square and with the W. And actually Si times

1745
01:47:59,280 --> 01:48:04,720
 square equals to W, trying to put Si, W. So this is the Si times

1746
01:48:04,720 --> 01:48:10,080
 square is related to the W. Through this, actually, the

1747
01:48:10,280 --> 01:48:11,280
 scatter matrix Si.

1748
01:48:14,160 --> 01:48:19,360
 And then this, and we get a summation of Si,

1749
01:48:19,360 --> 01:48:22,400
 S1 times square, S2 times square, right?

1750
01:48:22,400 --> 01:48:28,600
 So this, actually, finally, this is equal to W transpose SW.

1751
01:48:28,600 --> 01:48:32,800
 So this SW, just the summation of S1, S2, right?

1752
01:48:32,800 --> 01:48:36,800
 This is the total within class scatter matrix.

1753
01:48:36,800 --> 01:48:37,360
 OK.

1754
01:48:37,360 --> 01:48:39,800
 So now, for the official ratio, right?

1755
01:48:39,840 --> 01:48:44,080
 The married part is expressed into a function of W.

1756
01:48:44,080 --> 01:48:46,640
 And for the denominator part, actually, we also

1757
01:48:46,640 --> 01:48:49,480
 express it into a function of W, right?

1758
01:48:49,480 --> 01:48:52,320
 So, yeah.

1759
01:48:52,320 --> 01:48:55,000
 Also, this is actually SR times, right?

1760
01:48:55,000 --> 01:48:57,440
 So this is actually the difference.

1761
01:48:57,440 --> 01:49:01,840
 It's related to the W by this SB.

1762
01:49:01,840 --> 01:49:04,120
 SB is the between class scatter matrix.

1763
01:49:07,120 --> 01:49:09,200
 And Y minus M2.

1764
01:49:09,639 --> 01:49:10,880
 This is a column vector.

1765
01:49:10,880 --> 01:49:13,280
 And Y minus M2 times transpose.

1766
01:49:13,280 --> 01:49:14,760
 So this becomes matrix.

1767
01:49:14,760 --> 01:49:16,960
 So this is called between class scatter matrix.

1768
01:49:16,960 --> 01:49:20,280
 So, actually, based on the two mean vectors,

1769
01:49:20,280 --> 01:49:22,599
 we calculate the scatter matrix.

1770
01:49:22,599 --> 01:49:26,880
 So this is called between class scatter matrix, between class.

1771
01:49:26,880 --> 01:49:27,800
 OK.

1772
01:49:27,800 --> 01:49:34,599
 So now, actually, we can see this cost function, loss

1773
01:49:34,599 --> 01:49:37,280
 function, Jw, the numerator part,

1774
01:49:37,280 --> 01:49:40,880
 which is between class difference, right?

1775
01:49:40,880 --> 01:49:43,040
 The square of the between class difference.

1776
01:49:43,040 --> 01:49:46,719
 The square of the difference between the two mean values

1777
01:49:46,719 --> 01:49:48,240
 of the two classes.

1778
01:49:48,240 --> 01:49:53,519
 So this W transpose SB, W. SB, B, the mean between.

1779
01:49:53,519 --> 01:49:55,040
 So between class.

1780
01:49:55,040 --> 01:49:55,679
 OK.

1781
01:49:55,679 --> 01:49:57,800
 And then, the numerator part is the within class scatter,

1782
01:49:57,800 --> 01:49:58,480
 right?

1783
01:49:58,480 --> 01:50:00,639
 Within class, no difference.

1784
01:50:00,639 --> 01:50:05,559
 So W transpose SW times W. OK.

1785
01:50:05,560 --> 01:50:09,920
 So this is the Jw.

1786
01:50:09,920 --> 01:50:13,560
 So finally, through the medical manipulation,

1787
01:50:13,560 --> 01:50:17,040
 we found that we have this form.

1788
01:50:17,040 --> 01:50:20,440
 So this form, the Jw, is a function of W's.

1789
01:50:20,440 --> 01:50:21,760
 Numeric part, right?

1790
01:50:21,760 --> 01:50:24,080
 So we have the between class.

1791
01:50:24,080 --> 01:50:26,640
 Because the between class is the bigger the better, right?

1792
01:50:26,640 --> 01:50:27,400
 Difference.

1793
01:50:27,400 --> 01:50:30,800
 The numerator part, actually, evaluated within class scatter,

1794
01:50:30,800 --> 01:50:32,160
 within class difference.

1795
01:50:32,160 --> 01:50:34,720
 The smaller the better.

1796
01:50:34,760 --> 01:50:38,680
 So we have W transpose SB, W, right?

1797
01:50:38,680 --> 01:50:44,680
 Then, SW transpose SW, SW here, mean within class,

1798
01:50:44,680 --> 01:50:47,000
 between class, SB.

1799
01:50:47,000 --> 01:50:52,600
 So our objective, Ness, is to solve this homogenization

1800
01:50:52,600 --> 01:50:59,200
 problem, to find the W, so that the Jw is a maximized.

1801
01:50:59,200 --> 01:51:02,360
 So this is the homogenization problem, right?

1802
01:51:02,400 --> 01:51:05,480
 So actually, Ness, actually, we have, after break,

1803
01:51:05,480 --> 01:51:09,200
 we look at the solution to this problem.

1804
01:51:09,200 --> 01:51:11,880
 So I think the, why I introduce Fisher's ratio,

1805
01:51:11,880 --> 01:51:14,519
 I think this, the idea here is great.

1806
01:51:14,519 --> 01:51:16,799
 Actually, in many classifications,

1807
01:51:16,799 --> 01:51:19,280
 we need to emphasize, actually, the within class,

1808
01:51:19,280 --> 01:51:20,559
 between class, right?

1809
01:51:20,559 --> 01:51:22,519
 So this matter, I think, is very basic.

1810
01:51:22,519 --> 01:51:25,120
 So this gives us a very good foundation,

1811
01:51:25,120 --> 01:51:27,599
 good understanding about pattern classification.

1812
01:51:27,599 --> 01:51:30,679
 So what is considered good classifier?

1813
01:51:30,680 --> 01:51:35,160
 So what is considered as a good W?

1814
01:51:35,160 --> 01:51:38,600
 So here, we have the concept of between class difference

1815
01:51:38,600 --> 01:51:41,680
 and within class difference.

1816
01:51:41,680 --> 01:51:45,800
 The between class difference should be as great as possible,

1817
01:51:45,800 --> 01:51:46,300
 right?

1818
01:51:46,300 --> 01:51:47,080
 Should be as great as possible.

1819
01:51:47,080 --> 01:51:52,160
 And within class scatter should be as small as possible.

1820
01:51:52,160 --> 01:51:57,440
 So in many scenarios, actually, we need to use this concept.

1821
01:51:57,440 --> 01:51:57,880
 OK.

1822
01:51:57,920 --> 01:51:59,440
 So Ness, we have a break.

1823
01:51:59,440 --> 01:52:00,000
 Time in this.

1824
01:52:27,880 --> 01:52:28,380
 OK.

1825
01:52:57,880 --> 01:52:58,380
 OK.

1826
01:53:27,880 --> 01:53:28,380
 OK.

1827
01:53:57,880 --> 01:53:58,380
 OK.

1828
01:54:27,880 --> 01:54:28,380
 OK.

1829
01:54:57,880 --> 01:54:58,380
 OK.

1830
01:55:27,880 --> 01:55:28,380
 OK.

1831
01:55:57,880 --> 01:55:58,380
 OK.

1832
01:56:27,880 --> 01:56:28,380
 OK.

1833
01:56:57,880 --> 01:56:58,380
 OK.

1834
01:57:27,880 --> 01:57:28,380
 OK.

1835
01:57:57,880 --> 01:57:58,380
 OK.

1836
01:58:27,880 --> 01:58:28,380
 OK.

1837
01:58:57,880 --> 01:58:58,380
 OK.

1838
01:59:27,880 --> 01:59:28,380
 OK.

1839
01:59:57,880 --> 01:59:58,380
 OK.

1840
02:00:27,880 --> 02:00:28,380
 OK.

1841
02:00:57,880 --> 02:00:58,380
 OK.

1842
02:01:27,880 --> 02:01:28,380
 OK.

1843
02:01:52,380 --> 02:01:52,880
 OK.

1844
02:01:52,880 --> 02:01:55,880
 So based on the within class scatter

1845
02:01:55,880 --> 02:01:59,380
 and also between class scatter and finally,

1846
02:01:59,380 --> 02:02:05,000
 and we have summarized the desire actually

1847
02:02:05,000 --> 02:02:09,640
 the W into such an omniscient problem, right?

1848
02:02:09,640 --> 02:02:15,280
 And we want to find such a W so that the JW is a maximized.

1849
02:02:15,280 --> 02:02:19,400
 And so how to solve this omniscient problem?

1850
02:02:19,400 --> 02:02:23,840
 And actually, I think good thing that no mathematician can

1851
02:02:23,840 --> 02:02:24,800
 help, right?

1852
02:02:24,840 --> 02:02:26,880
 Actually, they have found this problem.

1853
02:02:31,120 --> 02:02:34,240
 The way that they maximize this question

1854
02:02:34,240 --> 02:02:37,360
 must satisfy this condition.

1855
02:02:37,360 --> 02:02:40,760
 SB is a between class scatter matrix.

1856
02:02:40,760 --> 02:02:46,880
 Times W equals to lambda SWW.

1857
02:02:46,880 --> 02:02:47,380
 OK.

1858
02:02:47,380 --> 02:02:50,320
 Lambda here actually is a constant.

1859
02:02:50,320 --> 02:02:52,160
 Constant.

1860
02:02:52,160 --> 02:02:53,160
 OK.

1861
02:02:53,160 --> 02:02:56,040
 And this W actually lambda should

1862
02:02:56,040 --> 02:02:58,440
 be a positive real constant.

1863
02:02:58,440 --> 02:03:02,280
 At least 0 greater than or equal to 0.

1864
02:03:02,280 --> 02:03:03,240
 OK.

1865
02:03:03,240 --> 02:03:05,920
 And actually, this problem actually

1866
02:03:05,920 --> 02:03:09,880
 is called a generalized eigenvalue problem.

1867
02:03:09,880 --> 02:03:12,559
 So probably you are familiar with the eigenvalue problem.

1868
02:03:12,559 --> 02:03:15,240
 Maybe I show you this slide.

1869
02:03:15,240 --> 02:03:15,960
 OK.

1870
02:03:15,960 --> 02:03:20,840
 So actually, we are very familiar with this for vector,

1871
02:03:20,920 --> 02:03:28,080
 if this x times x just equal to actually lambda times x.

1872
02:03:28,080 --> 02:03:30,200
 So this x is called eigenvector.

1873
02:03:30,200 --> 02:03:34,480
 Lambda is called the eigenvalue of the matrix A. OK.

1874
02:03:34,480 --> 02:03:37,280
 So this problem is the eigenvalue problem.

1875
02:03:37,280 --> 02:03:40,000
 And this question is a bit different, right?

1876
02:03:40,000 --> 02:03:45,920
 So we have SBW equals to lambda SWW.

1877
02:03:45,920 --> 02:03:48,680
 So we have two matrices in this form.

1878
02:03:48,680 --> 02:03:52,800
 Ax equals to lambda Bx.

1879
02:03:52,800 --> 02:03:54,440
 We have two matrices here.

1880
02:03:54,440 --> 02:03:59,600
 And this actually is called a generalized eigenvalue problem.

1881
02:03:59,600 --> 02:04:05,640
 And actually, if the B is a non-singular,

1882
02:04:05,640 --> 02:04:07,600
 then actually we can multiply them

1883
02:04:07,600 --> 02:04:10,720
 in worst matrices of B in the two sides.

1884
02:04:10,720 --> 02:04:16,000
 And then this will become actually an eigenvalue problem.

1885
02:04:16,000 --> 02:04:16,680
 OK.

1886
02:04:16,680 --> 02:04:23,880
 So normally in the topos, we have functions for both.

1887
02:04:23,880 --> 02:04:31,280
 For generalized eigenvalues for the normal eigenvalue problem.

1888
02:04:31,280 --> 02:04:36,240
 So we can always find solutions for this generalized

1889
02:04:36,240 --> 02:04:40,840
 or the normal eigenvalue problem.

1890
02:04:40,840 --> 02:04:44,200
 And so here, if the SW, the within class

1891
02:04:44,200 --> 02:04:47,400
 of the matrix is non-singular, and then we can multiply them

1892
02:04:47,400 --> 02:04:51,240
 in worst in the two sides, then this is just

1893
02:04:51,240 --> 02:04:54,400
 the A write AW equals to lambda W. So this

1894
02:04:54,400 --> 02:04:57,240
 is just a normal eigenvalue problem.

1895
02:04:57,240 --> 02:05:03,160
 Actually, in practice, there may be no need to do this conversion.

1896
02:05:03,160 --> 02:05:05,320
 Convert this generalized eigenvalue

1897
02:05:05,320 --> 02:05:07,280
 into the normal eigenvalue problem.

1898
02:05:07,280 --> 02:05:11,920
 Because we have functions normally in the topos to solve both.

1899
02:05:11,920 --> 02:05:12,840
 OK.

1900
02:05:12,840 --> 02:05:14,120
 So this is no fun.

1901
02:05:14,120 --> 02:05:19,080
 And we see we summarize the solution of W

1902
02:05:19,080 --> 02:05:22,760
 into an omniscient problem to omit the JW.

1903
02:05:22,760 --> 02:05:26,040
 And this solution can be found by solving

1904
02:05:26,040 --> 02:05:28,640
 actually the eigenvalue problem.

1905
02:05:28,640 --> 02:05:30,840
 Egenvalue, right?

1906
02:05:30,840 --> 02:05:33,400
 All generalized eigenvalue problem.

1907
02:05:33,400 --> 02:05:36,760
 In the generalized, we use SB.

1908
02:05:36,760 --> 02:05:42,240
 This A is a B. And so in the normal eigenvalue problem,

1909
02:05:42,280 --> 02:05:45,800
 this is the A. This is a matrix A.

1910
02:05:45,800 --> 02:05:48,000
 In a function normally, eigenvalue, right?

1911
02:05:48,000 --> 02:05:53,000
 For example, in my laboratory, for general eigenvalue,

1912
02:05:53,000 --> 02:05:56,080
 you need to input A, you need to input B.

1913
02:05:56,080 --> 02:05:58,920
 So A is just the SB.

1914
02:05:58,920 --> 02:06:02,400
 B is just SW.

1915
02:06:02,400 --> 02:06:04,679
 Or in the normal eigenvalue problem,

1916
02:06:04,679 --> 02:06:09,120
 you need to enter matrix A. So matrix A is just

1917
02:06:09,120 --> 02:06:12,320
 the inverse of the within class scatter matrix

1918
02:06:12,320 --> 02:06:15,840
 times the between class scatter matrix.

1919
02:06:15,840 --> 02:06:15,960
 OK.

1920
02:06:15,960 --> 02:06:18,760
 So this is a matrix A. So this is a matrix A.

1921
02:06:18,760 --> 02:06:24,400
 This is a matrix B. So this is a matrix A in the normal eigenvalue

1922
02:06:24,400 --> 02:06:25,920
 problem.

1923
02:06:25,920 --> 02:06:26,800
 OK.

1924
02:06:26,800 --> 02:06:29,000
 So we can find the solution right through solving

1925
02:06:29,000 --> 02:06:30,160
 this eigenvalue problem.

1926
02:06:31,160 --> 02:06:31,660
 OK.

1927
02:06:36,840 --> 02:06:39,559
 And as a tree, we introduce another method

1928
02:06:39,559 --> 02:06:42,680
 that we don't need to solve the eigenvalue problem tree.

1929
02:06:42,680 --> 02:06:44,800
 We can also calculate.

1930
02:06:44,800 --> 02:06:48,000
 And so we can calculate it.

1931
02:06:48,000 --> 02:06:52,200
 So this is because a tree can do some mathematical manipulation

1932
02:06:52,200 --> 02:06:56,040
 for SBW, which is the one in the Rahasad, right?

1933
02:06:56,040 --> 02:06:58,120
 In the eigenvalue problem.

1934
02:06:58,120 --> 02:07:00,120
 This is equal to M1, M2.

1935
02:07:00,120 --> 02:07:04,680
 So this is the definition of the between class scatter matrix,

1936
02:07:04,680 --> 02:07:05,200
 right?

1937
02:07:05,200 --> 02:07:09,640
 Based on the two mean vectors, M1 times M1 and M2,

1938
02:07:09,640 --> 02:07:12,240
 M1, M1, M2 transpose.

1939
02:07:12,240 --> 02:07:17,599
 And then actually, so M1 times W, actually this part,

1940
02:07:17,599 --> 02:07:19,720
 actually is just the projection.

1941
02:07:19,720 --> 02:07:24,760
 The projection of the M1, M2, right?

1942
02:07:24,760 --> 02:07:29,480
 M1, M1, M2 along the W. So this is also

1943
02:07:29,839 --> 02:07:32,400
 the difference between the two mean values, right?

1944
02:07:32,400 --> 02:07:34,559
 The difference between M2 mean values.

1945
02:07:34,559 --> 02:07:38,280
 It's M1 minus M2.

1946
02:07:38,280 --> 02:07:41,240
 M1 times M2 times.

1947
02:07:41,240 --> 02:07:44,200
 The two mean values for the samples

1948
02:07:44,200 --> 02:07:47,440
 are the projected onto the W. OK?

1949
02:07:47,440 --> 02:07:49,160
 So we assume this is the rule.

1950
02:07:49,160 --> 02:07:50,120
 This is the rule.

1951
02:07:50,120 --> 02:07:51,000
 OK.

1952
02:07:51,000 --> 02:07:55,919
 And so this rule is just known, the projection, right?

1953
02:07:55,920 --> 02:08:00,680
 The projection of M1, M2 on this W. OK?

1954
02:08:00,680 --> 02:08:04,080
 And then now, actually, we substitute this SBW

1955
02:08:04,080 --> 02:08:08,080
 into the original general eigenvalue problem, right?

1956
02:08:08,080 --> 02:08:11,600
 So these are known as SBW.

1957
02:08:11,600 --> 02:08:17,120
 And so this is the original general eigenvalue problem, right?

1958
02:08:17,120 --> 02:08:19,720
 Now we spread this SBW into such a form.

1959
02:08:19,720 --> 02:08:22,440
 M1 minus M2 times the rule.

1960
02:08:22,440 --> 02:08:23,040
 OK.

1961
02:08:23,040 --> 02:08:28,600
 So this is equal to the raheyser lambda SBW and W. OK.

1962
02:08:28,600 --> 02:08:31,880
 And actually, from here, we know the rule and lambda

1963
02:08:31,880 --> 02:08:34,000
 are just constant, the constant, right?

1964
02:08:34,000 --> 02:08:35,040
 The constant.

1965
02:08:35,040 --> 02:08:41,680
 And so of course, the W values are affected by this rule and W.

1966
02:08:41,680 --> 02:08:43,320
 But actually, when we think about W,

1967
02:08:43,320 --> 02:08:46,880
 actually, our main concern is the direction of W,

1968
02:08:46,880 --> 02:08:48,920
 not the specific values.

1969
02:08:48,920 --> 02:08:49,519
 OK.

1970
02:08:49,519 --> 02:08:52,120
 So if we just ignore this rule and the W,

1971
02:08:52,160 --> 02:08:55,160
 we just let rule 1, lambda also 1, right?

1972
02:08:55,160 --> 02:08:56,280
 We just ignore this.

1973
02:08:56,280 --> 02:08:56,960
 OK.

1974
02:08:56,960 --> 02:09:01,599
 And then, if the SW is non-singular,

1975
02:09:01,599 --> 02:09:06,240
 then we can multiply actually the SW in worst metric

1976
02:09:06,240 --> 02:09:08,680
 in the two sides of the equation, right?

1977
02:09:08,680 --> 02:09:13,880
 Then this SW in worst will become identity metric, right?

1978
02:09:13,880 --> 02:09:17,000
 Then the raheyser just W. Then the left hand side,

1979
02:09:17,000 --> 02:09:21,320
 that will be the SW in worst times M1, M2.

1980
02:09:21,320 --> 02:09:21,880
 OK.

1981
02:09:21,880 --> 02:09:25,040
 Actually, this tells us another way to find the W.

1982
02:09:25,040 --> 02:09:31,400
 We use the within class geometry in worst, then times M1, M2.

1983
02:09:31,400 --> 02:09:32,080
 OK.

1984
02:09:32,080 --> 02:09:34,600
 So this is W. And actually, this W,

1985
02:09:34,600 --> 02:09:40,080
 I think it's easy to understand this W. Why this form?

1986
02:09:40,080 --> 02:09:41,520
 Actually, you remember, we always

1987
02:09:41,520 --> 02:09:48,440
 try to maximize the within class difference, right?

1988
02:09:48,440 --> 02:09:49,520
 Minimize.

1989
02:09:49,520 --> 02:09:52,280
 And then we need to maximize the between class difference,

1990
02:09:52,280 --> 02:09:53,200
 right?

1991
02:09:53,200 --> 02:09:55,920
 And actually, M1, M2 is something really

1992
02:09:55,920 --> 02:09:57,280
 to between class difference.

1993
02:09:57,280 --> 02:10:00,440
 But this, of course, is our original speed, right?

1994
02:10:00,440 --> 02:10:01,720
 M1, M2.

1995
02:10:01,720 --> 02:10:04,280
 This something different between two classes.

1996
02:10:04,280 --> 02:10:05,120
 OK.

1997
02:10:05,120 --> 02:10:08,400
 And then SB in worst.

1998
02:10:08,400 --> 02:10:10,320
 In worst, actually, in the extreme case,

1999
02:10:10,320 --> 02:10:13,960
 if SW is a scalar, this should be divided by SW, right?

2000
02:10:13,960 --> 02:10:14,680
 Divide by SW.

2001
02:10:14,680 --> 02:10:15,720
 Then in worst.

2002
02:10:15,720 --> 02:10:18,360
 This should be smaller, as small as possible.

2003
02:10:18,360 --> 02:10:20,160
 So this is just not within class.

2004
02:10:20,160 --> 02:10:21,559
 Difference should be small.

2005
02:10:21,559 --> 02:10:23,960
 So with SW in worst, right?

2006
02:10:23,960 --> 02:10:26,320
 And then the between class difference should be big.

2007
02:10:26,320 --> 02:10:27,679
 This is between class difference.

2008
02:10:27,679 --> 02:10:30,080
 It's M1, M2.

2009
02:10:30,080 --> 02:10:30,320
 OK.

2010
02:10:30,320 --> 02:10:31,799
 If you understand this point, right,

2011
02:10:31,799 --> 02:10:33,960
 we have repeated this concept.

2012
02:10:33,960 --> 02:10:38,639
 And then it's easy to understand onto this W. It's not SB,

2013
02:10:38,639 --> 02:10:40,320
 not SB, right?

2014
02:10:40,320 --> 02:10:42,040
 Not SB in worst.

2015
02:10:42,040 --> 02:10:43,240
 Why not SB in worst?

2016
02:10:43,240 --> 02:10:45,839
 Because within class, this is in worst.

2017
02:10:45,880 --> 02:10:48,560
 Should be within class.

2018
02:10:48,560 --> 02:10:53,240
 So with W in worst, not SB in worst.

2019
02:10:53,240 --> 02:10:57,080
 Then M1, M2 represent the between class difference.

2020
02:10:57,080 --> 02:11:02,160
 SB, W in worst, represent actually the within class

2021
02:11:02,160 --> 02:11:04,520
 between the difference.

2022
02:11:04,520 --> 02:11:11,120
 So we can find W using this from the within class

2023
02:11:11,120 --> 02:11:16,720
 kind of matrix in worst times M1, M2.

2024
02:11:16,720 --> 02:11:17,000
 OK.

2025
02:11:17,000 --> 02:11:18,519
 So there's another way, right?

2026
02:11:18,519 --> 02:11:19,920
 There's another way.

2027
02:11:19,920 --> 02:11:22,760
 And in practice, of course, actually, if you have some data,

2028
02:11:22,760 --> 02:11:25,360
 we can use the machine learning toolbox.

2029
02:11:25,360 --> 02:11:31,080
 All the, like, my lab, we have a lot of functions.

2030
02:11:31,080 --> 02:11:34,599
 And then we can solve this invariable problem,

2031
02:11:34,599 --> 02:11:36,720
 just using the toolbox.

2032
02:11:36,720 --> 02:11:39,080
 But sometimes in the exam, I give some data,

2033
02:11:39,080 --> 02:11:43,920
 you cannot use actually the computer, right,

2034
02:11:43,920 --> 02:11:45,000
 to solve the problem.

2035
02:11:45,000 --> 02:11:48,000
 But then you can use this formula to find the W, right?

2036
02:11:48,000 --> 02:11:53,840
 And this formula to find the projection W.

2037
02:11:53,840 --> 02:11:55,160
 And OK.

2038
02:11:55,160 --> 02:11:56,840
 So this is the second way.

2039
02:11:56,840 --> 02:12:00,800
 And so in the exam, we could test this part, right?

2040
02:12:00,800 --> 02:12:02,960
 Not the invariable problem.

2041
02:12:02,960 --> 02:12:05,559
 But we can test using this formula.

2042
02:12:05,559 --> 02:12:08,160
 OK.

2043
02:12:08,160 --> 02:12:11,120
 And then, you know, here we need to use the inverse matrix

2044
02:12:11,120 --> 02:12:12,440
 of SW, right?

2045
02:12:12,440 --> 02:12:16,760
 So sometimes SW is singular, or close to singular.

2046
02:12:16,760 --> 02:12:19,240
 That means the determinant of this matrix

2047
02:12:19,240 --> 02:12:22,639
 is very small, close to zero.

2048
02:12:22,639 --> 02:12:23,519
 OK.

2049
02:12:23,519 --> 02:12:28,920
 Maybe, like, it's less than 10 power minus 15 or 17 in my lab.

2050
02:12:28,920 --> 02:12:31,040
 So you value is less than this value.

2051
02:12:31,040 --> 02:12:34,360
 And then you show this matrix is singular.

2052
02:12:34,360 --> 02:12:39,519
 And then the inverse of this matrix does not exist.

2053
02:12:39,519 --> 02:12:41,240
 So how to address this problem?

2054
02:12:41,240 --> 02:12:44,000
 Actually, we can use the so-called singular value,

2055
02:12:44,000 --> 02:12:47,040
 the regularization technique.

2056
02:12:47,040 --> 02:12:50,240
 And actually, the big idea of this

2057
02:12:50,240 --> 02:12:56,440
 is to add one small positive value on the diagonal elements

2058
02:12:56,440 --> 02:12:59,040
 for the SW.

2059
02:12:59,040 --> 02:13:00,839
 So this is a small value, positive number,

2060
02:13:00,840 --> 02:13:04,120
 a very small positive real number, beta.

2061
02:13:04,120 --> 02:13:06,200
 It's a small value.

2062
02:13:06,200 --> 02:13:06,960
 OK.

2063
02:13:06,960 --> 02:13:08,480
 And in practice, actually, if you just

2064
02:13:08,480 --> 02:13:12,320
 want to address the singular value problem,

2065
02:13:12,320 --> 02:13:16,880
 the beta can be set to 10 power minus 5, 10 power minus 10.

2066
02:13:16,880 --> 02:13:20,080
 Very small value, actually, will address this singular value

2067
02:13:20,080 --> 02:13:22,600
 problem.

2068
02:13:22,600 --> 02:13:25,560
 So this R is an identity matrix.

2069
02:13:25,560 --> 02:13:27,480
 Actually, the operation here, you just

2070
02:13:27,480 --> 02:13:32,599
 need to put a small value on the diagonal elements in the SW.

2071
02:13:32,599 --> 02:13:33,679
 OK.

2072
02:13:33,679 --> 02:13:39,320
 Then you can address the singular problem of this SW.

2073
02:13:39,320 --> 02:13:41,559
 And actually, in practice, actually, sometimes,

2074
02:13:41,559 --> 02:13:44,519
 actually, even the SW is not singular.

2075
02:13:44,519 --> 02:13:46,919
 But we still use this regularization.

2076
02:13:46,919 --> 02:13:50,400
 And actually, regularization is a kind of technique

2077
02:13:50,400 --> 02:13:53,440
 in machine learning for improvement

2078
02:13:53,440 --> 02:13:57,040
 of good generalization.

2079
02:13:57,040 --> 02:14:01,160
 Actually, the introduction of this non-regularization technique

2080
02:14:01,160 --> 02:14:04,200
 aims to address the graffiti problem.

2081
02:14:04,200 --> 02:14:06,320
 Or graffiti.

2082
02:14:06,320 --> 02:14:07,200
 OK.

2083
02:14:07,200 --> 02:14:11,560
 And so even the SW is non-singular, for example.

2084
02:14:11,560 --> 02:14:13,440
 Even the SW is non-singular.

2085
02:14:13,440 --> 02:14:15,160
 So you read the beta is 0.

2086
02:14:15,160 --> 02:14:18,120
 So this is the optimal solution for the training data.

2087
02:14:18,120 --> 02:14:20,320
 But maybe not naturally, this is the best solution

2088
02:14:20,320 --> 02:14:21,800
 for testing data.

2089
02:14:21,800 --> 02:14:23,560
 OK.

2090
02:14:23,600 --> 02:14:28,200
 So we can put actually one of the beta here, right?

2091
02:14:28,200 --> 02:14:30,080
 Even SW is non-singular.

2092
02:14:30,080 --> 02:14:34,120
 But by introducing this regularization term here, right?

2093
02:14:34,120 --> 02:14:36,320
 Actually, we can find actually a solution that

2094
02:14:36,320 --> 02:14:38,760
 is suboptimal to training data.

2095
02:14:38,760 --> 02:14:42,600
 But this solution could generate well to testing data.

2096
02:14:42,600 --> 02:14:45,520
 So this is called a regularization.

2097
02:14:45,520 --> 02:14:46,120
 OK.

2098
02:14:46,120 --> 02:14:48,760
 So in practice, even SW is non-singular.

2099
02:14:48,760 --> 02:14:53,280
 Quite often, we introduce this regularization term

2100
02:14:53,280 --> 02:14:57,760
 by adding actually the beta i, right?

2101
02:14:57,760 --> 02:15:05,200
 By adding a small positive number on the diagonal elements

2102
02:15:05,200 --> 02:15:09,480
 within class-catametric SW to improve the generalization

2103
02:15:09,480 --> 02:15:11,920
 capability.

2104
02:15:11,920 --> 02:15:12,520
 OK.

2105
02:15:12,520 --> 02:15:14,040
 So this is SW.

2106
02:15:14,040 --> 02:15:14,800
 The W. OK.

2107
02:15:14,800 --> 02:15:17,040
 We have solved the W, right?

2108
02:15:17,040 --> 02:15:20,519
 And actually, in the discriminant function,

2109
02:15:20,519 --> 02:15:21,880
 we have two parts, right?

2110
02:15:21,880 --> 02:15:23,640
 For the hyperplane, we have two parts.

2111
02:15:23,640 --> 02:15:25,320
 Why is the direction, right?

2112
02:15:25,320 --> 02:15:28,120
 That is determined by W. And that is the location.

2113
02:15:28,120 --> 02:15:30,280
 Location is determined by the W0.

2114
02:15:30,280 --> 02:15:33,040
 So where the W0 should be?

2115
02:15:33,040 --> 02:15:38,240
 So that's actually, we look at a way to find the W0.

2116
02:15:38,240 --> 02:15:41,680
 And actually, the best idea for determining the W0

2117
02:15:41,680 --> 02:15:45,240
 is that we hope actually the middle of the M1, M2,

2118
02:15:45,240 --> 02:15:50,160
 the middle point for each class, we have a mean vector, right?

2119
02:15:50,160 --> 02:15:51,560
 So this is the middle point.

2120
02:15:51,560 --> 02:15:52,060
 OK.

2121
02:15:52,060 --> 02:15:53,760
 And so this is the mean vector, right?

2122
02:15:53,760 --> 02:15:57,960
 The two mean vectors, the centroid for each class.

2123
02:15:57,960 --> 02:16:00,320
 And certainly, we can have a middle point

2124
02:16:00,320 --> 02:16:02,360
 for these two mean vectors.

2125
02:16:02,360 --> 02:16:05,000
 That is M1 plus M2 divided by 2.

2126
02:16:05,000 --> 02:16:06,120
 This is the middle point.

2127
02:16:06,120 --> 02:16:06,620
 OK.

2128
02:16:06,620 --> 02:16:11,980
 We hope this middle point could be on the hyperplane.

2129
02:16:11,980 --> 02:16:13,220
 On the hyperplane.

2130
02:16:13,220 --> 02:16:16,099
 That means that if the middle point here is M1 plus M2

2131
02:16:16,099 --> 02:16:17,500
 divided by 2, right?

2132
02:16:17,500 --> 02:16:20,860
 This is actually the average of these two mean vectors.

2133
02:16:20,860 --> 02:16:23,900
 If we substitute this as x into the W transpose x

2134
02:16:23,900 --> 02:16:26,900
 plus half is 0, this should be 0.

2135
02:16:26,900 --> 02:16:29,820
 Because we hope this point is on the decimal boundary.

2136
02:16:29,820 --> 02:16:32,539
 That means if we substitute this point, this x,

2137
02:16:32,539 --> 02:16:34,900
 into the discriminant function, then the discriminant

2138
02:16:34,900 --> 02:16:37,139
 function should be 0.

2139
02:16:37,139 --> 02:16:37,400
 OK.

2140
02:16:37,400 --> 02:16:41,940
 So based on this principle, and so we, this x, right?

2141
02:16:41,940 --> 02:16:42,820
 W transpose x.

2142
02:16:42,820 --> 02:16:45,299
 This x is just in the middle point of the two

2143
02:16:45,299 --> 02:16:47,580
 two mean vectors.

2144
02:16:47,580 --> 02:16:49,100
 And plus W, this should be 0.

2145
02:16:49,100 --> 02:16:51,940
 This means this point on the decimal boundary.

2146
02:16:51,940 --> 02:16:52,440
 OK.

2147
02:16:52,440 --> 02:16:57,740
 So based on this principle, we can find W. So M1 plus M2

2148
02:16:57,740 --> 02:16:59,180
 divided by 2.

2149
02:16:59,180 --> 02:17:03,340
 The middle point, W transpose.

2150
02:17:03,340 --> 02:17:04,139
 OK.

2151
02:17:04,139 --> 02:17:07,619
 So this is the negative sign, so this

2152
02:17:07,619 --> 02:17:10,260
 is the way to determine the W is 0.

2153
02:17:10,260 --> 02:17:15,939
 The position, the location of the hyperplane.

2154
02:17:15,939 --> 02:17:18,619
 Just now we determined the direction, right?

2155
02:17:18,619 --> 02:17:21,500
 The W, the orientation, right?

2156
02:17:21,500 --> 02:17:25,180
 Actually, the hyperplane is a perpendicular to this W.

2157
02:17:25,180 --> 02:17:28,180
 Once W is determined, of course, the orientation of this

2158
02:17:28,180 --> 02:17:30,699
 decimal boundary is determined.

2159
02:17:30,699 --> 02:17:33,219
 But where it should be, the direction should be, right?

2160
02:17:33,219 --> 02:17:35,699
 The direction should be determined by W is 0.

2161
02:17:35,699 --> 02:17:36,199
 OK.

2162
02:17:36,199 --> 02:17:37,379
 How do we determine W is 0?

2163
02:17:37,379 --> 02:17:41,099
 Based on this principle, that is, the middle point of the two

2164
02:17:41,099 --> 02:17:45,420
 mean vectors should be on the decimal boundary.

2165
02:17:45,420 --> 02:17:45,939
 OK.

2166
02:17:45,939 --> 02:17:51,099
 So based on this principle, we find the W is 0.

2167
02:17:51,099 --> 02:17:52,019
 OK.

2168
02:17:52,019 --> 02:17:53,420
 So the W is 0.

2169
02:17:53,420 --> 02:17:56,859
 And actually, this is W is 0, so we need to actually know

2170
02:17:56,859 --> 02:18:02,099
 that this W is 0, we, this point should be on the decimal

2171
02:18:02,099 --> 02:18:02,939
 boundary, right?

2172
02:18:02,980 --> 02:18:04,580
 This is based on W assumption.

2173
02:18:04,580 --> 02:18:09,580
 That is, the scatter of these two classes are the same or

2174
02:18:09,580 --> 02:18:11,500
 very similar.

2175
02:18:11,500 --> 02:18:12,060
 Or very similar.

2176
02:18:12,060 --> 02:18:13,300
 The scatter, right?

2177
02:18:13,300 --> 02:18:15,940
 The scatter is very similar for the two classes.

2178
02:18:15,940 --> 02:18:18,900
 After present on this, we can find the standard vision, we

2179
02:18:18,900 --> 02:18:20,620
 can find the variance.

2180
02:18:20,620 --> 02:18:23,660
 The variance is very similar.

2181
02:18:23,660 --> 02:18:26,100
 And then, actually, this is the optimal point.

2182
02:18:26,100 --> 02:18:27,300
 It's the same.

2183
02:18:27,300 --> 02:18:29,380
 They are similar or similar.

2184
02:18:29,380 --> 02:18:32,140
 But in practice, this may not be true.

2185
02:18:32,139 --> 02:18:34,859
 And the spread of one class could be big.

2186
02:18:34,859 --> 02:18:40,859
 The spread of actually another class could be small.

2187
02:18:40,859 --> 02:18:41,139
 OK.

2188
02:18:41,139 --> 02:18:44,699
 So the variance or standard vision of the spread or the

2189
02:18:44,699 --> 02:18:48,099
 scatter of the sample in the two classes could be very

2190
02:18:48,099 --> 02:18:49,660
 different.

2191
02:18:49,660 --> 02:18:53,900
 If they are different, and then such a way to determine the

2192
02:18:53,900 --> 02:18:58,500
 W is 0, it's not the optimal way.

2193
02:18:58,500 --> 02:19:01,139
 Then you practice how to solve the problem.

2194
02:19:01,139 --> 02:19:04,099
 First, actually, we can, based on this principle, find the

2195
02:19:04,099 --> 02:19:05,059
 W is 0.

2196
02:19:05,059 --> 02:19:08,059
 So this W is 0, although not optimal, should be close to

2197
02:19:08,059 --> 02:19:09,980
 optimal value.

2198
02:19:09,980 --> 02:19:12,900
 The next step, actually, we need to, through a great

2199
02:19:12,900 --> 02:19:17,699
 search, we can try a few values that are above W0,

2200
02:19:17,699 --> 02:19:19,219
 calculate W0.

2201
02:19:19,219 --> 02:19:22,619
 And also, below this, calculate W0.

2202
02:19:22,619 --> 02:19:24,539
 We try a few values.

2203
02:19:24,539 --> 02:19:30,580
 Then, through a few trials, then we repeat the one that

2204
02:19:30,620 --> 02:19:33,020
 performed the best.

2205
02:19:33,020 --> 02:19:36,700
 So we do an adjustment of this W0 in practice.

2206
02:19:36,700 --> 02:19:41,820
 Because this may, before determining the W0, is based

2207
02:19:41,820 --> 02:19:45,660
 on the assumption that the spread of the two classes are

2208
02:19:45,660 --> 02:19:46,820
 the same.

2209
02:19:46,820 --> 02:19:50,539
 But in practice, if they are different, so this W0 will not

2210
02:19:50,539 --> 02:19:53,740
 lead to the best class result.

2211
02:19:53,740 --> 02:19:57,020
 So in practice, actually, this W0 can give us the rough

2212
02:19:57,020 --> 02:19:58,980
 range, the rough value.

2213
02:19:58,980 --> 02:20:04,140
 Then, along that value, we can do some adjustment.

2214
02:20:04,140 --> 02:20:06,500
 We try a few values of W0.

2215
02:20:06,500 --> 02:20:08,500
 And then, we look at the performance on the training

2216
02:20:08,500 --> 02:20:09,500
 data.

2217
02:20:09,500 --> 02:20:11,660
 Then, we find the one with the best performance.

2218
02:20:16,020 --> 02:20:20,340
 So we try W0.

2219
02:20:20,340 --> 02:20:27,580
 So in the second assignment, and I give you a problem, maybe

2220
02:20:27,580 --> 02:20:32,460
 the same data set, but you need to use this method to

2221
02:20:32,460 --> 02:20:33,940
 classify the sample.

2222
02:20:33,940 --> 02:20:36,740
 Then, to find the W, you need to find the W0.

2223
02:20:36,740 --> 02:20:43,140
 And W0, you can use this way to adjust, to find the best

2224
02:20:43,140 --> 02:20:44,740
 performing W0.

2225
02:20:48,100 --> 02:20:52,340
 So this is the determination of W and W0.

2226
02:20:52,340 --> 02:20:55,980
 So once these are determined, then we can just write out

2227
02:20:55,980 --> 02:21:00,779
 the Gisler function, the Gx, W transpose x plus W0.

2228
02:21:00,779 --> 02:21:04,699
 For any given x, then we can substitute into this Gx.

2229
02:21:04,699 --> 02:21:07,699
 Then, based on the Gx, we can determine the class.

2230
02:21:07,699 --> 02:21:10,060
 Gx greater than 0, class 1.

2231
02:21:10,060 --> 02:21:13,900
 Gx is less than 0, negative, class minus 1.

2232
02:21:13,900 --> 02:21:16,180
 If we equal 0, that means the point is on the

2233
02:21:16,180 --> 02:21:17,260
 descent boundary.

2234
02:21:17,260 --> 02:21:20,060
 We cannot decide.

2235
02:21:20,060 --> 02:21:24,980
 So this is the design of the Fisher-Linus-Dieselman analysis

2236
02:21:24,980 --> 02:21:26,300
 and classify.

2237
02:21:26,300 --> 02:21:30,539
 So here is a summary procedure.

2238
02:21:30,539 --> 02:21:33,420
 Given some train data, in particular, in the Fisher-Linus,

2239
02:21:33,420 --> 02:21:35,820
 we always provide the train data.

2240
02:21:35,820 --> 02:21:37,580
 The data is the label train data.

2241
02:21:37,580 --> 02:21:39,740
 We know all these data belong to class 1.

2242
02:21:39,740 --> 02:21:41,460
 These data belong to class 2.

2243
02:21:41,460 --> 02:21:44,220
 So we know the labels.

2244
02:21:44,220 --> 02:21:48,460
 So for each class, we can calculate the mean vector.

2245
02:21:48,460 --> 02:21:50,020
 So this is the first step, right?

2246
02:21:50,020 --> 02:21:53,140
 Mean vector for class 1, mean vector for class 2.

2247
02:21:53,140 --> 02:21:56,980
 And then, based on the mean vector, we can calculate

2248
02:21:56,980 --> 02:22:02,019
 within class scatter matrix for class 1, S1, for class 2,

2249
02:22:02,019 --> 02:22:03,099
 S2, right?

2250
02:22:03,099 --> 02:22:08,859
 Actually, we need to use the mean vector to determine

2251
02:22:08,859 --> 02:22:10,460
 within class scatter matrix.

2252
02:22:10,460 --> 02:22:13,220
 And then we find the total scatter matrix

2253
02:22:13,220 --> 02:22:14,699
 within class scatter matrix, right?

2254
02:22:14,699 --> 02:22:17,300
 It's SW.

2255
02:22:17,300 --> 02:22:19,060
 Within class, sometimes it's also called

2256
02:22:19,060 --> 02:22:20,619
 actually intra-class, right?

2257
02:22:20,620 --> 02:22:23,100
 Between class, it's also called an inter-class.

2258
02:22:23,100 --> 02:22:24,740
 Between class, inter-class.

2259
02:22:24,740 --> 02:22:28,020
 Within class, it's called actually intra-class.

2260
02:22:28,020 --> 02:22:30,380
 Sometimes you will read paper, often they

2261
02:22:30,380 --> 02:22:31,900
 use another set of concepts, right?

2262
02:22:31,900 --> 02:22:36,660
 Inter, intra, and within, between.

2263
02:22:36,660 --> 02:22:38,460
 So this is SW.

2264
02:22:38,460 --> 02:22:41,460
 And after we determine SW, then we

2265
02:22:41,460 --> 02:22:44,700
 can find the W using the formula, right?

2266
02:22:44,700 --> 02:22:50,340
 In words of SW, times M1 minus M2.

2267
02:22:50,540 --> 02:22:51,660
 OK.

2268
02:22:51,660 --> 02:22:55,340
 M1 is the mean of the class with a label of 1.

2269
02:22:55,340 --> 02:23:01,780
 M2 mean the vector for class of sample with a label of minus 1.

2270
02:23:01,780 --> 02:23:03,060
 Remember, minus 1.

2271
02:23:03,060 --> 02:23:04,740
 M2.

2272
02:23:04,740 --> 02:23:06,420
 So this is KFW0.

2273
02:23:06,420 --> 02:23:10,220
 Then, actually, based on the W0 calculated,

2274
02:23:10,220 --> 02:23:12,620
 you can do a bit of adjustment, right?

2275
02:23:12,620 --> 02:23:13,120
 Adjustment.

2276
02:23:14,120 --> 02:23:16,760
 OK.

2277
02:23:16,760 --> 02:23:20,360
 So then, finally, we can construct this in a G-cell function.

2278
02:23:23,000 --> 02:23:24,000
 OK.

2279
02:23:24,000 --> 02:23:26,840
 So based on the value of GX, right?

2280
02:23:26,840 --> 02:23:28,840
 And GX, actually, we also know, actually,

2281
02:23:28,840 --> 02:23:30,720
 GX divided by the norm of W means

2282
02:23:30,720 --> 02:23:33,000
 the distance between the X to the distance of the boundary,

2283
02:23:33,000 --> 02:23:33,720
 right?

2284
02:23:33,720 --> 02:23:35,760
 So sometimes we do the classification

2285
02:23:35,760 --> 02:23:37,520
 if the value is very small, right?

2286
02:23:37,520 --> 02:23:39,200
 Very small, close to zero.

2287
02:23:39,200 --> 02:23:41,440
 So that's the reason why you adjust W0, right?

2288
02:23:41,440 --> 02:23:43,040
 You adjust a bit.

2289
02:23:43,040 --> 02:23:45,840
 And then, because if the sample along the distance of the boundary,

2290
02:23:45,840 --> 02:23:46,520
 right?

2291
02:23:46,520 --> 02:23:48,680
 Could be classified into a different class

2292
02:23:48,680 --> 02:23:51,600
 if you adjust W0 a bit.

2293
02:23:51,600 --> 02:23:52,320
 OK.

2294
02:23:52,320 --> 02:23:54,680
 For those samples close to the distance of the boundary,

2295
02:23:54,680 --> 02:23:58,680
 that means that GX divided by W norm, this value, R,

2296
02:23:58,680 --> 02:23:59,880
 is small.

2297
02:23:59,880 --> 02:24:02,520
 When you adjust the W0, you can put them

2298
02:24:02,520 --> 02:24:04,880
 into different classes.

2299
02:24:04,880 --> 02:24:05,400
 OK.

2300
02:24:05,400 --> 02:24:09,000
 But for those with a big R, GX divided by the norm,

2301
02:24:09,000 --> 02:24:12,320
 actually, this adjustment of W0 will not affect

2302
02:24:12,320 --> 02:24:16,080
 that classification result.

2303
02:24:16,080 --> 02:24:16,580
 OK.

2304
02:24:16,580 --> 02:24:19,480
 Only for those that is close to the distance of the boundary

2305
02:24:19,480 --> 02:24:26,600
 with a very small R, GX divided by the norm of W. OK.

2306
02:24:26,600 --> 02:24:27,100
 OK.

2307
02:24:27,100 --> 02:24:34,760
 So this is the summary of the procedure, right?

2308
02:24:34,760 --> 02:24:38,720
 For the designing of the Fisher linear adjustment analysis

2309
02:24:38,720 --> 02:24:40,720
 classifier.

2310
02:24:40,720 --> 02:24:43,600
 So that's actually we show an example.

2311
02:24:43,600 --> 02:24:45,240
 So this is an example.

2312
02:24:47,960 --> 02:24:50,360
 The example we already seen is an example, right?

2313
02:24:50,360 --> 02:24:51,880
 So two class.

2314
02:24:51,880 --> 02:24:54,640
 And we want to find a decent boundary.

2315
02:24:54,640 --> 02:24:57,680
 And so of course, actually, even if we

2316
02:24:57,680 --> 02:24:59,200
 don't see that you're around the data,

2317
02:24:59,200 --> 02:25:00,840
 we can still classify, right?

2318
02:25:00,840 --> 02:25:04,000
 But here, we use this example to illustrate.

2319
02:25:04,000 --> 02:25:08,080
 And then, actually, we can have a better understanding.

2320
02:25:08,080 --> 02:25:08,800
 And OK.

2321
02:25:08,800 --> 02:25:13,400
 So for class, sample in class one and class two,

2322
02:25:13,400 --> 02:25:17,240
 and each of them have actually 100 samples.

2323
02:25:17,240 --> 02:25:21,280
 And then we can take the average and the vectors, right?

2324
02:25:21,280 --> 02:25:24,000
 And then the mean vector for the two classes.

2325
02:25:24,000 --> 02:25:26,080
 So these are the mean vectors.

2326
02:25:26,080 --> 02:25:28,400
 Actually, based on the data, actually, I calculate, right?

2327
02:25:28,400 --> 02:25:31,480
 These are mean vector, mean vector.

2328
02:25:31,480 --> 02:25:34,359
 And this is the within class scatter metric SW.

2329
02:25:34,359 --> 02:25:37,199
 Based on the mean vector, we can calculate the scatter metric

2330
02:25:37,199 --> 02:25:39,039
 for each of the two classes.

2331
02:25:39,039 --> 02:25:41,720
 Then summation of the two scatter metrics,

2332
02:25:41,720 --> 02:25:45,600
 we get the SW, total scatter metric.

2333
02:25:45,600 --> 02:25:51,960
 Then based on this formula, we calculate W.

2334
02:25:51,960 --> 02:25:55,359
 And then based on the principle, the middle point of the M1M2

2335
02:25:55,359 --> 02:25:57,240
 should be on the decent boundary, right?

2336
02:25:57,240 --> 02:26:00,880
 We can calculate SW, W zero.

2337
02:26:00,920 --> 02:26:04,519
 So once W and W zero determine, then the decimal function

2338
02:26:04,519 --> 02:26:05,640
 determine.

2339
02:26:05,640 --> 02:26:13,839
 W transpose X plus A. So this is just

2340
02:26:13,839 --> 02:26:20,720
 minus 0.0071X1 minus 0.0153X2 plus the W zero here.

2341
02:26:20,720 --> 02:26:22,279
 So this is the decimal function.

2342
02:26:22,279 --> 02:26:26,320
 For any feature, for any sample, we have a value for feature Y,

2343
02:26:26,320 --> 02:26:27,000
 X1.

2344
02:26:27,000 --> 02:26:28,920
 We have value for feature 2X2.

2345
02:26:28,920 --> 02:26:31,200
 Then we just substitute into the decimal function.

2346
02:26:31,200 --> 02:26:32,640
 We can get a GX.

2347
02:26:32,640 --> 02:26:34,680
 Then based on the GX, the positive or negative,

2348
02:26:34,680 --> 02:26:37,920
 we can decide the class, right?

2349
02:26:37,920 --> 02:26:44,440
 So for this class, actually, we can look at the GX.

2350
02:26:44,440 --> 02:26:49,000
 So based on this, actually, for all the training samples,

2351
02:26:49,000 --> 02:26:52,360
 here we have totally, we have 200 samples.

2352
02:26:52,360 --> 02:26:55,680
 The first 100 sample is from class 1.

2353
02:26:55,680 --> 02:26:58,760
 The second 100 sample from class 2.

2354
02:26:59,360 --> 02:27:03,920
 So from here, we can see from here until 100, right?

2355
02:27:03,920 --> 02:27:06,600
 I believe these horizontal axis here

2356
02:27:06,600 --> 02:27:08,360
 mean the index of the sample.

2357
02:27:08,360 --> 02:27:10,880
 Sample number 1, number 20, and number 200.

2358
02:27:10,880 --> 02:27:13,680
 And the vertical axis is through the GX.

2359
02:27:13,680 --> 02:27:18,080
 And actually, from 1 to 100, we can see the values are most

2360
02:27:18,080 --> 02:27:20,320
 of the value above 0, right?

2361
02:27:20,320 --> 02:27:21,840
 Above 0, that means all the samples

2362
02:27:21,840 --> 02:27:23,720
 will be classified to class 1.

2363
02:27:23,720 --> 02:27:25,440
 So this is correct.

2364
02:27:25,440 --> 02:27:27,080
 But there are some exceptions.

2365
02:27:27,080 --> 02:27:29,600
 Only 1, 2, 3, 4, right?

2366
02:27:29,600 --> 02:27:35,080
 So these four samples, 1, 2, 3, 4, these four samples,

2367
02:27:35,080 --> 02:27:38,400
 actually, have negative GX.

2368
02:27:38,400 --> 02:27:42,120
 So they will be misclassified.

2369
02:27:42,120 --> 02:27:44,360
 They will be classified to class 2

2370
02:27:44,360 --> 02:27:47,280
 because they have negative values.

2371
02:27:47,280 --> 02:27:51,920
 And for the sample in another class, right?

2372
02:27:51,920 --> 02:27:56,400
 From 1 to 200, then you look at the GX.

2373
02:27:56,440 --> 02:27:58,600
 So negative, this is 0, right?

2374
02:27:58,600 --> 02:28:00,440
 So this is line 0.

2375
02:28:00,440 --> 02:28:08,160
 So only 1, 2, 3, 4 samples actually have positive GX.

2376
02:28:08,160 --> 02:28:12,560
 Then they will be classified as class 1 sample.

2377
02:28:12,560 --> 02:28:15,640
 And some of the samples actually are close to boundary, right?

2378
02:28:15,640 --> 02:28:17,440
 You can see this data, right?

2379
02:28:17,440 --> 02:28:18,800
 The GX is very small, right?

2380
02:28:18,800 --> 02:28:20,279
 Close to boundaries.

2381
02:28:20,279 --> 02:28:23,000
 So this sample actually is very small, right?

2382
02:28:23,000 --> 02:28:23,760
 Close to 0.

2383
02:28:23,760 --> 02:28:26,240
 That means this sample is close to boundaries.

2384
02:28:27,240 --> 02:28:31,320
 And then if you change the W0, so this sample may be put

2385
02:28:31,320 --> 02:28:33,520
 into different classes, different classes.

2386
02:28:36,240 --> 02:28:38,520
 So this is the GX.

2387
02:28:38,520 --> 02:28:41,880
 And we look at the data.

2388
02:28:41,880 --> 02:28:45,440
 I also generate a lot of data in this range.

2389
02:28:45,440 --> 02:28:47,880
 For X1 from 1 to 3 to 6, right?

2390
02:28:47,880 --> 02:28:52,119
 For X2 also from 1 to 3 to 6, I generate a lot of data points.

2391
02:28:52,119 --> 02:28:54,960
 For each of the data points, I substitute the GX.

2392
02:28:54,960 --> 02:28:58,560
 And then I perform classification based on the GX value,

2393
02:28:58,560 --> 02:28:59,060
 right?

2394
02:28:59,060 --> 02:29:00,800
 Positive or negative, OK?

2395
02:29:00,800 --> 02:29:07,720
 And then I find that all the data points in this yellow range

2396
02:29:07,720 --> 02:29:10,199
 are classified into class 2.

2397
02:29:10,199 --> 02:29:12,800
 All the samples in the green region

2398
02:29:12,800 --> 02:29:15,679
 are classified to class 1, right?

2399
02:29:15,679 --> 02:29:19,039
 So between the yellow and the green regions,

2400
02:29:19,039 --> 02:29:20,519
 there is a line here, right?

2401
02:29:20,519 --> 02:29:22,800
 So this is the decision boundary.

2402
02:29:22,800 --> 02:29:24,080
 This is the decision boundary.

2403
02:29:24,080 --> 02:29:26,360
 The decision boundary is a straight line, right?

2404
02:29:26,360 --> 02:29:29,960
 And certainly, actually, we know that some of the points

2405
02:29:29,960 --> 02:29:32,360
 are actually 1, 2, 3, 4, right?

2406
02:29:32,360 --> 02:29:34,680
 So these are class 1 samples.

2407
02:29:34,680 --> 02:29:36,880
 But they are now in the right-hand side.

2408
02:29:36,880 --> 02:29:40,680
 They are in this negative side of this decision boundary.

2409
02:29:40,680 --> 02:29:42,600
 So they will be misclassified.

2410
02:29:42,600 --> 02:29:46,400
 So these samples most likely are outliers.

2411
02:29:46,400 --> 02:29:48,800
 OK, so they are misclassified.

2412
02:29:48,800 --> 02:29:53,080
 And in this side, we know that a few black sample,

2413
02:29:53,120 --> 02:29:54,240
 black sample samples, right?

2414
02:29:54,240 --> 02:29:56,920
 1, 2, 3, 4.

2415
02:29:56,920 --> 02:30:01,200
 So these four samples certainly will actually

2416
02:30:01,200 --> 02:30:02,520
 misclassify.

2417
02:30:02,520 --> 02:30:04,560
 They are sample of class 2, but they are

2418
02:30:04,560 --> 02:30:06,960
 classified into class 1.

2419
02:30:06,960 --> 02:30:11,480
 OK, so also along the decision boundary,

2420
02:30:11,480 --> 02:30:12,600
 there are some samples, right?

2421
02:30:12,600 --> 02:30:14,760
 You can see these are them, right?

2422
02:30:14,760 --> 02:30:16,960
 Some of the samples along this side.

2423
02:30:16,960 --> 02:30:21,280
 They could be easily misclassified.

2424
02:30:21,280 --> 02:30:24,400
 If you change the W0 a bit, so you change the position,

2425
02:30:24,400 --> 02:30:27,560
 you move above a bit, then some of the samples,

2426
02:30:27,560 --> 02:30:33,120
 you know, the black sample will be classified in the class 2.

2427
02:30:33,120 --> 02:30:36,400
 If you move the decision boundary a bit above, right?

2428
02:30:36,400 --> 02:30:40,600
 If you move a bit below, right, some of the red samples

2429
02:30:40,600 --> 02:30:43,600
 will be actually put in the negative side

2430
02:30:43,600 --> 02:30:45,880
 of the decision boundary.

2431
02:30:45,880 --> 02:30:48,120
 So this is determined by W0.

2432
02:30:48,120 --> 02:30:50,440
 The W0 is important, right?

2433
02:30:50,440 --> 02:30:53,320
 For those samples, close to boundary.

2434
02:30:53,320 --> 02:30:57,080
 OK, close boundary, the W0 is very important.

2435
02:30:57,080 --> 02:30:59,400
 For those samples, actually, normally,

2436
02:30:59,400 --> 02:31:04,440
 I think we won't make them pretty at the morale, right?

2437
02:31:04,440 --> 02:31:07,040
 Because they are far away from the decision boundary.

2438
02:31:07,040 --> 02:31:11,480
 The Gx divided by W0 could be a very big wedding.

2439
02:31:14,400 --> 02:31:17,320
 OK, so this is the decision boundary.

2440
02:31:17,320 --> 02:31:19,560
 And this is the class 2 result.

2441
02:31:19,560 --> 02:31:22,480
 And based on the W, right?

2442
02:31:22,480 --> 02:31:24,560
 And also W0.

2443
02:31:24,560 --> 02:31:26,240
 So there's a tree.

2444
02:31:26,240 --> 02:31:30,080
 Now, we use another way to find the tree, the W.

2445
02:31:30,080 --> 02:31:34,080
 So we try to solve the generalized eigenvalue problem,

2446
02:31:34,080 --> 02:31:37,279
 or just the normal eigenvalue problem, right?

2447
02:31:37,279 --> 02:31:43,039
 So in this question, even SW within class

2448
02:31:43,039 --> 02:31:45,160
 got a metric is nonsingular.

2449
02:31:45,160 --> 02:31:49,320
 And then we can actually multiply it in worse metrics.

2450
02:31:50,080 --> 02:31:51,640
 So the two sides of the equation, right?

2451
02:31:51,640 --> 02:31:53,600
 The generalized eigenvalue problem,

2452
02:31:53,600 --> 02:31:55,960
 then this become a normal eigenvalue problem.

2453
02:31:55,960 --> 02:32:02,000
 So this SW in worse SB is just the A, matrix A.

2454
02:32:02,000 --> 02:32:05,760
 And in my laboratory, you can just use this EIG

2455
02:32:05,760 --> 02:32:08,840
 to find the eigenvalues.

2456
02:32:08,840 --> 02:32:13,920
 And you just need to input the matrix A.

2457
02:32:13,920 --> 02:32:19,240
 Then you can find the eigenvalue and also the eigenvectors.

2458
02:32:19,800 --> 02:32:23,119
 So this is the matrix A. So this matrix A,

2459
02:32:23,119 --> 02:32:28,640
 just the inverse of SW, INV, times SB.

2460
02:32:28,640 --> 02:32:31,640
 So this is the matrix A. The eigenvalue,

2461
02:32:31,640 --> 02:32:34,600
 you find the V and the D. So this is the V and the D.

2462
02:32:34,600 --> 02:32:37,880
 And D is a diagonal matrix.

2463
02:32:37,880 --> 02:32:39,920
 Actually, the diagonal elements actually

2464
02:32:39,920 --> 02:32:43,880
 are the eigenvalues.

2465
02:32:43,880 --> 02:32:47,560
 The first eigenvalue is 0.0001, close to 0, right?

2466
02:32:47,560 --> 02:32:50,960
 The second eigenvalue is 0.058.

2467
02:32:50,960 --> 02:32:53,160
 And correspond to the two eigenvalues,

2468
02:32:53,160 --> 02:32:57,240
 we have two eigenvectors, two W's.

2469
02:32:57,240 --> 02:32:58,439
 So correspond to this.

2470
02:32:58,439 --> 02:33:06,400
 We have the first one, minus 0.858, 0.151517.

2471
02:33:06,400 --> 02:33:08,480
 So this is the vector, V1.

2472
02:33:08,480 --> 02:33:10,640
 So this can be W1.

2473
02:33:10,640 --> 02:33:12,119
 This is W1.

2474
02:33:12,119 --> 02:33:15,320
 Correspond to the eigenvalue 0.000, close to 0,

2475
02:33:15,480 --> 02:33:21,840
 and then correspond to this eigenvalue, 0.058.

2476
02:33:21,840 --> 02:33:28,400
 We have this eigenvector, minus 0.418, minus 0.9076.

2477
02:33:28,400 --> 02:33:31,320
 Then which vector we should use?

2478
02:33:31,320 --> 02:33:34,279
 I treat the here, the two eigenvectors, right?

2479
02:33:34,279 --> 02:33:36,160
 Which vector we should use?

2480
02:33:36,160 --> 02:33:39,160
 Actually, we should use a vector corresponding

2481
02:33:39,160 --> 02:33:43,039
 to the larger eigenvalues.

2482
02:33:43,040 --> 02:33:43,800
 So that's a treat.

2483
02:33:52,200 --> 02:33:53,520
 OK, so that's a treat.

2484
02:33:53,520 --> 02:33:59,320
 We look at we test each of the two eigenvectors.

2485
02:33:59,320 --> 02:34:02,120
 We look at if this is actually the first eigenvector,

2486
02:34:02,120 --> 02:34:06,440
 the first column in the way is used at W. So what is the effect?

2487
02:34:06,440 --> 02:34:08,720
 Then we look at what is the effect

2488
02:34:08,720 --> 02:34:11,760
 if we use the second column as W.

2489
02:34:11,760 --> 02:34:13,440
 What is the eigenvector?

2490
02:34:13,440 --> 02:34:15,560
 OK, correspond to the eigenvalue.

2491
02:34:19,080 --> 02:34:23,360
 So this is the projection of all the samples on the W,

2492
02:34:23,360 --> 02:34:26,160
 which is the first column of the way,

2493
02:34:26,160 --> 02:34:29,560
 corresponding to the eigenvalue, very small eigenvalue,

2494
02:34:29,560 --> 02:34:31,600
 close to 0.

2495
02:34:31,600 --> 02:34:35,560
 So again, the first 100 samples are the projection

2496
02:34:35,560 --> 02:34:38,000
 of the projection, right?

2497
02:34:39,000 --> 02:34:43,960
 Of the class 1 samples.

2498
02:34:43,960 --> 02:34:46,560
 And actually, we know many of them actually

2499
02:34:46,560 --> 02:34:48,360
 have a value, zero value.

2500
02:34:48,360 --> 02:34:52,360
 And many of them below value, no, GX, right?

2501
02:34:52,360 --> 02:34:55,280
 So this is actually from 1 to 100.

2502
02:34:55,280 --> 02:34:56,720
 Some of them have positive values.

2503
02:34:56,720 --> 02:34:59,040
 Some of them negative values.

2504
02:34:59,040 --> 02:35:03,400
 And therefore, some in class 2, that is from 101 to 200.

2505
02:35:03,400 --> 02:35:06,080
 And many of them have positive values.

2506
02:35:06,080 --> 02:35:09,200
 Of course, some of them have a negative GX.

2507
02:35:09,200 --> 02:35:11,360
 So from here, you cannot differentiate.

2508
02:35:11,360 --> 02:35:16,120
 If you compare with the previous, you can see, right?

2509
02:35:16,120 --> 02:35:17,400
 So this part is big.

2510
02:35:17,400 --> 02:35:18,720
 This part is small, right?

2511
02:35:18,720 --> 02:35:21,480
 So you can see the difference after projection.

2512
02:35:21,480 --> 02:35:23,760
 That's the output of the Gismar function.

2513
02:35:23,760 --> 02:35:25,880
 But here, we have this.

2514
02:35:29,160 --> 02:35:31,520
 Actually, what is the mean value for the samples

2515
02:35:31,520 --> 02:35:32,760
 after projection GX?

2516
02:35:32,760 --> 02:35:34,320
 Around here, right?

2517
02:35:34,320 --> 02:35:35,400
 Around 0.

2518
02:35:35,400 --> 02:35:40,000
 The mean value of another class sample, also around 0.

2519
02:35:40,000 --> 02:35:42,000
 So different, almost 0.

2520
02:35:42,000 --> 02:35:45,920
 In other words, this W, actually after projection,

2521
02:35:45,920 --> 02:35:48,720
 cannot separate the sample in the two classes.

2522
02:35:48,720 --> 02:35:52,039
 So certainly, this is not a good value.

2523
02:35:52,039 --> 02:35:54,240
 Necessarily, look at the second W, right?

2524
02:35:54,240 --> 02:35:58,440
 The second column corresponding to the larger or greater

2525
02:35:58,440 --> 02:36:02,160
 between the eigenvalue.

2526
02:36:02,160 --> 02:36:04,680
 So this is the result.

2527
02:36:04,680 --> 02:36:13,360
 From the first sample to 200, and the GX, most of them,

2528
02:36:13,360 --> 02:36:15,240
 actually agree that 0, right?

2529
02:36:15,240 --> 02:36:16,240
 Positive.

2530
02:36:16,240 --> 02:36:20,680
 Only 1, 2, 3, 4 actually have negative values.

2531
02:36:20,680 --> 02:36:26,280
 And so from 1 to 200, only actually 1, 2, 3, 4

2532
02:36:26,280 --> 02:36:27,480
 have positive values.

2533
02:36:27,480 --> 02:36:29,120
 All other have negative values.

2534
02:36:29,120 --> 02:36:32,120
 So certainly, we can easily separate them, right?

2535
02:36:32,400 --> 02:36:38,640
 So from this tree, we can see we should use the eigenvector

2536
02:36:38,640 --> 02:36:42,520
 corresponding to the larger eigenvalue.

2537
02:36:42,520 --> 02:36:44,240
 But if we have a few, right?

2538
02:36:44,240 --> 02:36:47,760
 And the first one with the corresponding to the largest

2539
02:36:47,760 --> 02:36:50,240
 eigenvalue is the best one.

2540
02:36:50,240 --> 02:36:52,040
 The best one.

2541
02:36:52,040 --> 02:36:57,720
 And maybe sometime the tree, maybe like in this one,

2542
02:36:57,720 --> 02:37:00,080
 we have 1.

2543
02:37:00,080 --> 02:37:03,960
 But in some scenario, maybe we have more than 1.

2544
02:37:03,960 --> 02:37:07,680
 Then we should, based on the value, eigenvalue, right?

2545
02:37:07,680 --> 02:37:11,080
 We should select the one with the best, the largest value,

2546
02:37:11,080 --> 02:37:15,920
 then the one with the second back, the second largest value.

2547
02:37:15,920 --> 02:37:18,280
 Then we look at the corresponding eigenvalue test.

2548
02:37:21,560 --> 02:37:26,320
 And so here, this is a question result based on the new W.

2549
02:37:26,320 --> 02:37:29,720
 And actually, if we compare the two Ws,

2550
02:37:30,480 --> 02:37:35,320
 this is the W based on the eigenvalue problem.

2551
02:37:35,320 --> 02:37:39,599
 So the eigenvalue problem, we get this W minus 0.4198,

2552
02:37:39,599 --> 02:37:42,240
 minus 0.9076.

2553
02:37:42,240 --> 02:37:44,840
 And previously, we have found a very different W.

2554
02:37:59,720 --> 02:38:12,119
 OK.

2555
02:38:12,119 --> 02:38:14,880
 And actually, we have found a very different W.

2556
02:38:14,880 --> 02:38:18,320
 But actually, these two W, although they appear very

2557
02:38:18,320 --> 02:38:22,199
 different, but actually, they are the same.

2558
02:38:22,199 --> 02:38:25,240
 And actually, so this W, when we look at the W,

2559
02:38:25,240 --> 02:38:27,480
 the direction is the most important thing, right?

2560
02:38:27,480 --> 02:38:29,560
 Not the specific values.

2561
02:38:29,560 --> 02:38:31,960
 And how about the direction?

2562
02:38:31,960 --> 02:38:33,600
 For this vector, actually, this is

2563
02:38:33,600 --> 02:38:38,039
 the one we found previously by solving the SW in words,

2564
02:38:38,039 --> 02:38:39,880
 times M1 minus M2, right?

2565
02:38:39,880 --> 02:38:43,199
 We find this is the W. So 0.0071.

2566
02:38:43,199 --> 02:38:48,199
 But if we get the reach of these two, 0.4625, OK?

2567
02:38:48,199 --> 02:38:49,880
 And now we have a new one, right?

2568
02:38:49,880 --> 02:38:53,560
 We have a new one by solving the eigenvalue problem.

2569
02:38:53,560 --> 02:38:55,119
 This is the one.

2570
02:38:55,119 --> 02:38:57,760
 Very different from the first W.

2571
02:38:57,760 --> 02:38:59,480
 But if we get the reach of the two values,

2572
02:38:59,480 --> 02:39:02,200
 the two elements in the vector, right?

2573
02:39:02,200 --> 02:39:04,040
 This W1 means the first value.

2574
02:39:04,040 --> 02:39:06,080
 W2 means the second value.

2575
02:39:06,080 --> 02:39:08,000
 Also, 0.4625.

2576
02:39:08,000 --> 02:39:10,520
 Actually, it didn't mean that the two vector have

2577
02:39:10,520 --> 02:39:13,520
 the same direction.

2578
02:39:13,520 --> 02:39:16,880
 Although they appear, or they seem very different,

2579
02:39:16,880 --> 02:39:19,040
 but actually, they are the same.

2580
02:39:19,040 --> 02:39:19,600
 OK.

2581
02:39:19,600 --> 02:39:22,480
 In other words, actually, we use the two different methods

2582
02:39:22,480 --> 02:39:24,840
 by solving the eigenvalue problem,

2583
02:39:24,840 --> 02:39:27,960
 or use the formula to calculate directly, right?

2584
02:39:27,960 --> 02:39:32,080
 We actually got the same result, the same W.

2585
02:39:32,080 --> 02:39:36,160
 Although they appear different, but they are actually the same.

2586
02:39:36,160 --> 02:39:38,720
 Before W, we are concerned with the direction,

2587
02:39:38,720 --> 02:39:42,760
 not the specific value of W. OK.

2588
02:39:49,480 --> 02:39:50,160
 OK.

2589
02:39:50,160 --> 02:39:53,720
 So next, actually, I think this is the generalization.

2590
02:39:53,720 --> 02:39:56,000
 Just now, we talked about the two-class classical problem,

2591
02:39:56,000 --> 02:39:56,500
 right?

2592
02:39:56,500 --> 02:39:58,760
 But in many real problems, actually,

2593
02:39:58,760 --> 02:40:01,279
 we have multi-class classical problem,

2594
02:40:01,279 --> 02:40:03,760
 more than two classes.

2595
02:40:03,760 --> 02:40:05,960
 And for example, when you're setting analysis,

2596
02:40:05,960 --> 02:40:08,640
 we can have positive setting, but we can have a negative setting.

2597
02:40:08,640 --> 02:40:10,439
 We have a neutral.

2598
02:40:10,439 --> 02:40:12,679
 So it's a three-class classical problem.

2599
02:40:12,679 --> 02:40:15,240
 And in the categorization of news,

2600
02:40:15,240 --> 02:40:19,279
 we can have categories of sports news.

2601
02:40:19,279 --> 02:40:21,640
 We can have a category of education news.

2602
02:40:21,640 --> 02:40:23,680
 We can have a category of technology and science

2603
02:40:23,680 --> 02:40:24,800
 of technology.

2604
02:40:24,800 --> 02:40:27,880
 We can have a category of entertainment.

2605
02:40:27,880 --> 02:40:31,199
 We can have multiple classes.

2606
02:40:31,199 --> 02:40:41,080
 So certainly, we can generalize the two-class solution

2607
02:40:41,080 --> 02:40:44,560
 to the multi-class classical problem.

2608
02:40:44,560 --> 02:40:51,199
 So this is just the multiple-discriminate analysis.

2609
02:40:51,200 --> 02:40:53,760
 And actually, just now, when we talk about the classification,

2610
02:40:53,760 --> 02:40:56,400
 for two-class classical problem, we just

2611
02:40:56,400 --> 02:41:00,000
 have one-discriminate function, GX.

2612
02:41:00,000 --> 02:41:04,560
 But for a C-class, or you have three-class, four-class,

2613
02:41:04,560 --> 02:41:10,240
 common-discriminate function, we should have a C minus 1.

2614
02:41:10,240 --> 02:41:11,760
 So my C equal to 2, right?

2615
02:41:11,760 --> 02:41:13,400
 With C minus 1, just 1.

2616
02:41:13,400 --> 02:41:15,640
 So we have a 1-discriminate function.

2617
02:41:15,640 --> 02:41:19,440
 We have 1W or W0.

2618
02:41:19,440 --> 02:41:27,560
 So for C-class, we should have a C minus 1, actually W.

2619
02:41:27,560 --> 02:41:34,680
 And W minus 1, no, W1, W2, and your WC minus 1, right?

2620
02:41:34,680 --> 02:41:38,760
 Each of them corresponds to one-discriminate function.

2621
02:41:38,760 --> 02:41:41,040
 And also, of course, for each W1, W2,

2622
02:41:41,040 --> 02:41:44,120
 we have a corresponding W0, right?

2623
02:41:44,120 --> 02:41:48,280
 So actually, in the G-discriminate analysis,

2624
02:41:48,280 --> 02:41:52,520
 actually, we use here as a classification problem, right?

2625
02:41:52,520 --> 02:41:53,720
 Classified.

2626
02:41:53,720 --> 02:41:57,800
 And actually, linear G-discriminate analysis

2627
02:41:57,800 --> 02:42:03,240
 often used as a terminology reduction, actually, in method.

2628
02:42:03,240 --> 02:42:07,920
 Because actually, we predict the data right from 2D to 2W.

2629
02:42:07,920 --> 02:42:12,800
 So the data reputation is GX, actually, so one dimension.

2630
02:42:12,800 --> 02:42:15,000
 Orally, it could be a multi-dimension.

2631
02:42:15,000 --> 02:42:17,240
 According to the example, it's two-dimension, right?

2632
02:42:17,240 --> 02:42:21,600
 Actually, the dimensionality could be arbitrary.

2633
02:42:21,600 --> 02:42:24,560
 Could be two, could be three, could be $100,000.

2634
02:42:24,560 --> 02:42:27,840
 But the other projection by the GX,

2635
02:42:27,840 --> 02:42:30,320
 the representation of the data is one dimension.

2636
02:42:30,320 --> 02:42:32,760
 That is the GX.

2637
02:42:32,760 --> 02:42:35,520
 So the data from high-dimensional space,

2638
02:42:35,520 --> 02:42:39,160
 d-dimensional, to 1D.

2639
02:42:39,160 --> 02:42:43,039
 So this technology is also actually kind of a dimension

2640
02:42:43,039 --> 02:42:46,320
 reduction technique.

2641
02:42:46,360 --> 02:42:49,080
 So from multiple D to 1D.

2642
02:42:49,080 --> 02:42:51,720
 Actually, this is only for two-class classification problems.

2643
02:42:51,720 --> 02:42:56,039
 From two, from arbitrary D into 1D, right?

2644
02:42:56,039 --> 02:42:59,440
 But for actually, the C-class classification problem,

2645
02:42:59,440 --> 02:43:03,440
 we can design a C minus 1 distribution function.

2646
02:43:03,440 --> 02:43:04,880
 And then the projection of the data

2647
02:43:04,880 --> 02:43:10,320
 is from arbitrary D dimension to C minus 1 dimension.

2648
02:43:10,320 --> 02:43:12,400
 C minus 1.

2649
02:43:12,440 --> 02:43:16,359
 So if we consider this is a dimension reduction,

2650
02:43:16,359 --> 02:43:22,880
 then this D, this C, should be much less than D.

2651
02:43:22,880 --> 02:43:26,279
 Because after the projection, the dimensionality C minus 1,

2652
02:43:26,279 --> 02:43:27,840
 we see the dimensionality reduction.

2653
02:43:27,840 --> 02:43:33,600
 That means the D should be greater than C minus 1, right?

2654
02:43:33,600 --> 02:43:37,640
 So this is actually a digital analysis,

2655
02:43:37,640 --> 02:43:39,199
 not just for classification, because it's

2656
02:43:39,200 --> 02:43:43,640
 often used for dimensionality reduction.

2657
02:43:43,640 --> 02:43:48,600
 And I think the details of multiple digital analysis

2658
02:43:48,600 --> 02:43:52,280
 I think will be explained in the next week.

2659
02:43:52,280 --> 02:43:54,240
 So today, we just stop here.

2660
02:43:54,240 --> 02:43:58,360
 And I think it's very important to understand the concept.

2661
02:43:58,360 --> 02:44:01,320
 So not just remember the formula.

2662
02:44:01,320 --> 02:44:04,480
 That's of course actually meaning to remember that formula,

2663
02:44:04,480 --> 02:44:04,980
 right?

2664
02:44:04,980 --> 02:44:08,240
 But most importantly, we understand the concept.

2665
02:44:08,280 --> 02:44:13,039
 So how machine learning problem should be solved, right?

2666
02:44:13,039 --> 02:44:17,960
 And how to interpret a good data

2667
02:44:17,960 --> 02:44:21,320
 value for linear classifier.

2668
02:44:21,320 --> 02:44:22,960
 OK, so I think that's all for today.

2669
02:44:22,960 --> 02:44:25,280
 Thank you.

2670
02:44:25,280 --> 02:44:25,880
 OK, thank you.

2671
02:44:25,880 --> 02:44:26,380
 Thank you.

2672
02:46:08,240 --> 02:46:09,240
 Thank you.

2673
02:46:38,240 --> 02:46:40,240
 Thank you.

2674
02:47:08,240 --> 02:47:10,240
 Thank you.

2675
02:47:38,240 --> 02:47:40,240
 Thank you.

2676
02:48:08,240 --> 02:48:10,240
 Thank you.

2677
02:48:38,240 --> 02:48:39,240
 Thank you.

2678
02:49:08,240 --> 02:49:10,240
 Thank you.

2679
02:49:38,240 --> 02:49:40,240
 Thank you.

2680
02:50:08,240 --> 02:50:09,240
 Thank you.

2681
02:50:38,240 --> 02:50:40,240
 Thank you.

2682
02:51:08,240 --> 02:51:10,240
 Thank you.

2683
02:51:38,240 --> 02:51:40,240
 Thank you.

2684
02:52:08,240 --> 02:52:10,240
 Thank you.

2685
02:52:38,240 --> 02:52:40,240
 Thank you.

2686
02:53:08,240 --> 02:53:10,240
 Thank you.

2687
02:53:38,240 --> 02:53:40,240
 Thank you.

2688
02:54:08,240 --> 02:54:11,240
 Thank you.

2689
02:54:38,240 --> 02:54:41,240
 Thank you.

2690
02:55:08,240 --> 02:55:11,240
 Thank you.

2691
02:55:38,240 --> 02:55:41,240
 Thank you.

2692
02:56:08,240 --> 02:56:11,240
 Thank you.

2693
02:56:38,240 --> 02:56:40,240
 Thank you.

2694
02:57:08,240 --> 02:57:11,240
 Thank you.

2695
02:57:38,240 --> 02:57:41,240
 Thank you.

2696
02:58:08,240 --> 02:58:10,240
 Thank you.

2697
02:58:38,240 --> 02:58:40,240
 Thank you.

2698
02:59:08,240 --> 02:59:11,240
 Thank you.

2699
02:59:38,240 --> 02:59:40,240
 Thank you.

