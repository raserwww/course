1
00:01:30,000 --> 00:01:32,000
 .

2
00:04:30,080 --> 00:04:32,080
 Because everyone begins to look for 10.

3
00:04:32,080 --> 00:04:34,080
 OK.

4
00:04:52,080 --> 00:04:55,080
 So today, actually, we are first thing is a lecture,

5
00:04:55,080 --> 00:04:58,080
 actually, about the evaluation of the clustering.

6
00:04:58,080 --> 00:05:00,080
 And then, actually, we have a reviewer.

7
00:05:00,080 --> 00:05:05,080
 We will review what we have learned in the main content

8
00:05:05,080 --> 00:05:08,080
 running this machine learning part.

9
00:05:08,080 --> 00:05:10,080
 And finally, I think in the last one hour,

10
00:05:10,080 --> 00:05:12,080
 actually, we have a Q&A session.

11
00:05:12,080 --> 00:05:13,080
 OK.

12
00:05:13,080 --> 00:05:15,080
 We use Zoom to type your question.

13
00:05:15,080 --> 00:05:17,080
 OK.

14
00:05:17,080 --> 00:05:18,080
 OK.

15
00:05:18,080 --> 00:05:20,080
 I think when we started the super hard learning,

16
00:05:20,080 --> 00:05:22,080
 we have started different kind of classifier.

17
00:05:22,080 --> 00:05:26,080
 And then, of course, actually, this classification,

18
00:05:26,080 --> 00:05:29,080
 classifier are learned from the label training data.

19
00:05:29,080 --> 00:05:30,080
 Right?

20
00:05:30,080 --> 00:05:32,080
 So after we have learned the model,

21
00:05:32,080 --> 00:05:35,080
 and, of course, actually, we need to evaluate the performance

22
00:05:35,080 --> 00:05:36,080
 of the model.

23
00:05:36,080 --> 00:05:37,080
 OK.

24
00:05:37,080 --> 00:05:40,080
 So normally, we just need to use, actually,

25
00:05:40,080 --> 00:05:42,080
 the testing data.

26
00:05:42,080 --> 00:05:43,080
 Right?

27
00:05:43,080 --> 00:05:45,080
 We can use the learning model to predict

28
00:05:45,080 --> 00:05:49,080
 the class label for testing data.

29
00:05:49,080 --> 00:05:50,080
 OK.

30
00:05:50,080 --> 00:05:53,080
 And then we compare the prediction with the ground truth

31
00:05:53,080 --> 00:05:55,080
 of the testing data.

32
00:05:55,080 --> 00:05:57,080
 And then, based on this, actually,

33
00:05:57,080 --> 00:06:00,080
 we can have a different matrix to evaluate

34
00:06:00,080 --> 00:06:06,080
 the performance of the classifier, such as error rate, accuracy,

35
00:06:06,080 --> 00:06:10,080
 recall, precision, or the combination recall, precision,

36
00:06:10,080 --> 00:06:12,080
 that is the F score.

37
00:06:12,080 --> 00:06:13,080
 OK.

38
00:06:13,080 --> 00:06:15,080
 So we can, I think, in general, the evolution

39
00:06:15,080 --> 00:06:17,080
 of the super hard learning is simple.

40
00:06:17,080 --> 00:06:18,080
 Right?

41
00:06:18,080 --> 00:06:19,080
 Because we have a ground truth.

42
00:06:19,080 --> 00:06:23,080
 And we can compare the prediction model with a ground truth.

43
00:06:23,080 --> 00:06:24,080
 OK.

44
00:06:24,080 --> 00:06:27,080
 So for unsupervised learning, we know

45
00:06:27,080 --> 00:06:31,080
 classroom or unsupervised learning is based on un-labeled data.

46
00:06:31,080 --> 00:06:33,080
 We don't have a ground truth.

47
00:06:33,080 --> 00:06:34,080
 OK.

48
00:06:34,080 --> 00:06:37,080
 Then how to evaluate the performance of unsupervised learning,

49
00:06:37,080 --> 00:06:40,080
 in particular, that class.

50
00:06:40,080 --> 00:06:43,080
 And I read the evaluation for unsupervised learning.

51
00:06:43,080 --> 00:06:46,080
 I think it's much more challenging than the evaluation

52
00:06:46,080 --> 00:06:48,080
 of super hard learning, simply because we

53
00:06:48,080 --> 00:06:50,080
 don't have the ground truth.

54
00:06:50,080 --> 00:06:51,080
 OK.

55
00:06:51,080 --> 00:06:52,080
 You can imagine, right?

56
00:06:52,080 --> 00:06:56,080
 And the data is a high domino space in the high domino.

57
00:06:56,080 --> 00:06:58,080
 You cannot visualize the data distribution.

58
00:06:58,080 --> 00:07:01,080
 And you don't have a ground truth.

59
00:07:01,080 --> 00:07:06,080
 So I think it's really challenging to evaluate the performance.

60
00:07:06,080 --> 00:07:07,080
 OK.

61
00:07:07,080 --> 00:07:12,080
 But basically, the evaluation of the classroom

62
00:07:12,080 --> 00:07:18,080
 is based on the principle of the classroom.

63
00:07:18,080 --> 00:07:23,080
 And you remember what is called a classroom.

64
00:07:23,080 --> 00:07:25,080
 A classroom is just a group of data, right?

65
00:07:25,080 --> 00:07:26,080
 Group of data.

66
00:07:26,080 --> 00:07:30,080
 We want to find the natural groups underlying the data.

67
00:07:30,080 --> 00:07:33,080
 So classroom is a natural grouping.

68
00:07:33,080 --> 00:07:36,080
 And when we talk about this natural group concept,

69
00:07:36,080 --> 00:07:38,080
 we have two aspects, you remember, right?

70
00:07:38,080 --> 00:07:42,080
 One is that within the cluster, actually,

71
00:07:42,080 --> 00:07:46,080
 the samples show strong similarities.

72
00:07:46,080 --> 00:07:50,080
 So this is one aspect of the natural grouping.

73
00:07:50,080 --> 00:07:53,080
 Another aspect is that between clusters,

74
00:07:53,080 --> 00:08:00,080
 there should be strong dissimilarities or separations

75
00:08:00,080 --> 00:08:02,080
 between clusters.

76
00:08:02,080 --> 00:08:03,080
 OK.

77
00:08:03,080 --> 00:08:06,080
 So these are the two aspects of the natural grouping.

78
00:08:06,080 --> 00:08:09,080
 And actually, in the evaluation of the classroom,

79
00:08:09,080 --> 00:08:12,080
 actually, we will follow these two.

80
00:08:12,080 --> 00:08:14,080
 We will actually, necessarily, we

81
00:08:14,080 --> 00:08:17,080
 will have different metrics or index

82
00:08:17,080 --> 00:08:19,080
 to evaluate the performance of the classroom.

83
00:08:19,080 --> 00:08:21,080
 But the basic idea, I think, still

84
00:08:21,080 --> 00:08:25,080
 follows these two aspects about the natural grouping

85
00:08:25,080 --> 00:08:27,080
 of the classroom.

86
00:08:27,080 --> 00:08:33,079
 Within cluster similarity, between cluster dissimilarities.

87
00:08:33,079 --> 00:08:36,079
 But just knowing the issue last thing,

88
00:08:36,079 --> 00:08:39,079
 how to measure the within cluster similarity,

89
00:08:39,079 --> 00:08:42,079
 how to measure the between cluster dissimilarity.

90
00:08:42,080 --> 00:08:45,080
 And we can have different methods to evaluate.

91
00:08:45,080 --> 00:08:47,080
 And based on these different ideas,

92
00:08:47,080 --> 00:08:52,080
 then we can have different metrics or index

93
00:08:52,080 --> 00:08:54,080
 for the classroom evaluation.

94
00:08:54,080 --> 00:08:55,080
 OK.

95
00:08:58,080 --> 00:08:59,080
 OK.

96
00:08:59,080 --> 00:09:03,080
 So here, we assume, actually, totally, we have n samples.

97
00:09:03,080 --> 00:09:07,080
 Now, in super learning, we have the training data, validation

98
00:09:07,080 --> 00:09:09,080
 data, or testing data, right?

99
00:09:09,080 --> 00:09:11,080
 But for unsupervised learning, we don't have this concept.

100
00:09:11,080 --> 00:09:12,080
 We just have a data.

101
00:09:12,080 --> 00:09:14,080
 You can call it the training data or what?

102
00:09:14,080 --> 00:09:15,080
 OK.

103
00:09:15,080 --> 00:09:16,080
 We just have the data.

104
00:09:16,080 --> 00:09:17,080
 We don't have testing.

105
00:09:17,080 --> 00:09:19,080
 We don't have any validations.

106
00:09:19,080 --> 00:09:20,080
 OK.

107
00:09:20,080 --> 00:09:21,080
 We just have the data.

108
00:09:21,080 --> 00:09:22,080
 OK.

109
00:09:22,080 --> 00:09:25,080
 As you may see, there are n samples, x1, x2, and xn.

110
00:09:25,080 --> 00:09:30,080
 And so these samples have been partitioned into,

111
00:09:30,080 --> 00:09:38,080
 or assigned to n, actually, or to k sub, this is the subset.

112
00:09:38,080 --> 00:09:40,080
 d1, d2, dk.

113
00:09:40,080 --> 00:09:43,080
 And each of these subsets correspond to one cluster.

114
00:09:43,080 --> 00:09:45,080
 One cluster is just a group, right?

115
00:09:45,080 --> 00:09:48,080
 It corresponds to one group or one cluster.

116
00:09:48,080 --> 00:09:51,080
 And here, actually, we see they are disjoint, right?

117
00:09:51,080 --> 00:09:54,080
 Disjoint, that means, actually, between clusters,

118
00:09:54,080 --> 00:09:57,080
 there are no relapses.

119
00:09:57,080 --> 00:09:58,080
 OK.

120
00:09:58,080 --> 00:10:00,080
 Of course, they're not the all-fourth classroom, actually.

121
00:10:00,080 --> 00:10:04,080
 In some classroom methods, one method is called C-means,

122
00:10:04,080 --> 00:10:07,080
 fuzzy C-means classroom, actually.

123
00:10:07,080 --> 00:10:11,080
 And in that method, actually, each sample has a certain probability

124
00:10:11,080 --> 00:10:14,080
 belonging to one of the clusters.

125
00:10:14,080 --> 00:10:15,080
 OK.

126
00:10:15,080 --> 00:10:19,080
 So, but actually, we did not talk about that, actually,

127
00:10:19,080 --> 00:10:21,080
 and introduce that method.

128
00:10:21,080 --> 00:10:24,080
 So in all the methods introduced in this course, actually,

129
00:10:24,080 --> 00:10:30,080
 in that, actually, the older known clusters, actually,

130
00:10:30,080 --> 00:10:31,080
 are disjoint.

131
00:10:31,080 --> 00:10:32,080
 OK.

132
00:10:32,080 --> 00:10:36,080
 So we have k1, d1, d2, dk, right?

133
00:10:36,080 --> 00:10:39,080
 And each of these d correspond to one cluster.

134
00:10:39,080 --> 00:10:42,080
 So totally, we have k clusters or k groups.

135
00:10:42,080 --> 00:10:43,080
 OK.

136
00:10:43,080 --> 00:10:45,080
 And also, actually, we know for each of the groups,

137
00:10:45,080 --> 00:10:49,080
 we have the number of samples, n1, n2, and nk.

138
00:10:49,080 --> 00:10:50,080
 OK.

139
00:10:54,080 --> 00:10:55,080
 OK.

140
00:10:55,080 --> 00:10:57,080
 So these are the following, actually,

141
00:10:57,080 --> 00:11:02,080
 the metrics or the index we are going to use for the evaluation,

142
00:11:02,080 --> 00:11:03,080
 actually.

143
00:11:03,080 --> 00:11:06,080
 So I think these have different meanings, like different tree.

144
00:11:06,080 --> 00:11:10,080
 But actually, the older tree tried to actually measure the two

145
00:11:10,080 --> 00:11:14,080
 aspects of the cluster, actually, that natural group,

146
00:11:14,080 --> 00:11:18,080
 but within cluster similarity and between cluster,

147
00:11:18,080 --> 00:11:22,080
 actually, no, disimilarities, between cluster, right?

148
00:11:22,080 --> 00:11:24,080
 We don't have the cluster cluster,

149
00:11:24,080 --> 00:11:27,080
 but within cluster, between clusters.

150
00:11:27,080 --> 00:11:28,080
 OK.

151
00:11:33,080 --> 00:11:35,080
 Actually, yesterday, actually, I sent you an email, right,

152
00:11:35,080 --> 00:11:41,080
 about the teaching material of this part, actually.

153
00:11:41,080 --> 00:11:43,080
 I probably, I think, I made this a change, actually.

154
00:11:43,080 --> 00:11:46,080
 Last semester, probably, they copied the material from a

155
00:11:46,080 --> 00:11:51,080
 very old, actually, like last, I think last year, not last

156
00:11:51,080 --> 00:11:53,080
 semester, last year, actually, photo.

157
00:11:53,080 --> 00:11:54,080
 OK.

158
00:11:54,080 --> 00:11:57,080
 And that's why, actually, I emailed you about the change,

159
00:11:57,080 --> 00:11:58,080
 right?

160
00:11:58,080 --> 00:12:02,080
 We need to update of the lecture notes.

161
00:12:02,080 --> 00:12:03,080
 OK.

162
00:12:03,080 --> 00:12:06,080
 And so you need to use a new lecture notes.

163
00:12:06,080 --> 00:12:07,080
 OK.

164
00:12:11,080 --> 00:12:12,080
 OK.

165
00:12:12,080 --> 00:12:14,080
 And the first method we are going to introduce is called,

166
00:12:14,080 --> 00:12:17,080
 actually, silhouette coefficient.

167
00:12:17,080 --> 00:12:18,080
 OK.

168
00:12:18,080 --> 00:12:22,080
 And in this method, so basically, actually,

169
00:12:22,080 --> 00:12:24,080
 it measures these two things, actually,

170
00:12:24,080 --> 00:12:26,080
 one is called a coherent.

171
00:12:26,080 --> 00:12:30,080
 A coherent, here, actually, just how close a sample is

172
00:12:30,080 --> 00:12:34,080
 to other samples in own cluster.

173
00:12:34,080 --> 00:12:39,080
 So this, basically, this coherent is kind of metric

174
00:12:39,080 --> 00:12:43,080
 for the within cluster similarity.

175
00:12:43,080 --> 00:12:44,080
 How close, right?

176
00:12:44,080 --> 00:12:45,080
 How close?

177
00:12:45,080 --> 00:12:48,080
 Actually, because, actually, we know each sample is

178
00:12:48,080 --> 00:12:50,080
 represented by a vector.

179
00:12:50,080 --> 00:12:53,080
 And in the official space, we know each vector is just a

180
00:12:53,080 --> 00:12:55,080
 point, right, a data point.

181
00:12:55,080 --> 00:12:56,080
 OK.

182
00:12:56,080 --> 00:12:58,080
 So the distance, actually, between, actually,

183
00:12:58,080 --> 00:13:02,080
 the samples show how, actually, similar to samples are,

184
00:13:02,080 --> 00:13:03,080
 right?

185
00:13:03,080 --> 00:13:08,080
 So if they are close, how close it is to other samples

186
00:13:08,080 --> 00:13:10,080
 within the same cluster.

187
00:13:10,080 --> 00:13:16,080
 That means how similar, actually, the sample is,

188
00:13:16,080 --> 00:13:20,080
 how simple is similar to other samples within the same cluster.

189
00:13:20,080 --> 00:13:22,080
 So here, this, actually, silhouette coefficient,

190
00:13:22,080 --> 00:13:24,080
 they call this a coherent.

191
00:13:24,080 --> 00:13:26,080
 But the coherent, here, basically,

192
00:13:26,080 --> 00:13:29,080
 is about the within cluster similarity.

193
00:13:29,080 --> 00:13:30,080
 OK.

194
00:13:30,080 --> 00:13:34,080
 So this is one, actually, aspect.

195
00:13:34,080 --> 00:13:37,080
 Then another aspect, actually, is the separation.

196
00:13:37,080 --> 00:13:42,080
 So how far away, how far a sample is from sample

197
00:13:42,080 --> 00:13:45,080
 in other clusters.

198
00:13:45,080 --> 00:13:46,080
 OK.

199
00:13:46,080 --> 00:13:51,080
 So one sample to sample in other clusters.

200
00:13:51,080 --> 00:13:54,080
 So this, actually, is called a separation.

201
00:13:54,080 --> 00:13:55,080
 Separation.

202
00:13:55,080 --> 00:13:56,080
 How far, right?

203
00:13:56,080 --> 00:13:58,080
 How far is, how about the feature space?

204
00:13:58,080 --> 00:13:59,080
 OK.

205
00:13:59,080 --> 00:14:01,080
 And then the geometric view, right?

206
00:14:01,080 --> 00:14:03,080
 Actually, the distance, how far away they are,

207
00:14:03,080 --> 00:14:07,080
 how close, how similar the samples are.

208
00:14:07,080 --> 00:14:08,080
 OK.

209
00:14:08,080 --> 00:14:10,080
 So these, actually, the two know in the silhouette

210
00:14:10,080 --> 00:14:13,080
 coefficient, and they try to evaluate these two aspects,

211
00:14:13,080 --> 00:14:15,080
 coherent and separation.

212
00:14:15,080 --> 00:14:18,080
 Basically, they are trying to evaluate the within cluster

213
00:14:18,080 --> 00:14:24,080
 similarity and the between cluster dissimilarities.

214
00:14:24,080 --> 00:14:25,080
 OK.

215
00:14:25,080 --> 00:14:29,080
 And so how the tree is here in the silhouette coefficient

216
00:14:29,080 --> 00:14:30,080
 method.

217
00:14:30,080 --> 00:14:33,080
 Each sample, for each sample, now we can calculate

218
00:14:33,080 --> 00:14:36,080
 the so-called silhouette coefficient.

219
00:14:36,080 --> 00:14:37,080
 OK.

220
00:14:37,080 --> 00:14:40,080
 So this is the definition of the silhouette coefficient.

221
00:14:40,080 --> 00:14:41,080
 OK.

222
00:14:41,080 --> 00:14:45,080
 So actually, here, actually, it is a ratio, right?

223
00:14:45,080 --> 00:14:49,080
 Bi minus ai divided by maximum ai bi.

224
00:14:49,080 --> 00:14:50,080
 OK.

225
00:14:50,080 --> 00:14:51,080
 So what is ai?

226
00:14:51,080 --> 00:14:52,080
 What is bi?

227
00:14:52,080 --> 00:14:56,080
 And actually, if I see, if I just say, for the silhouette

228
00:14:56,080 --> 00:14:58,080
 coefficient, the larger the better.

229
00:14:58,080 --> 00:14:59,080
 The larger the better.

230
00:14:59,080 --> 00:15:00,080
 OK.

231
00:15:00,080 --> 00:15:02,080
 So do we think, what is bi?

232
00:15:02,080 --> 00:15:03,080
 If we can get, what is bi?

233
00:15:03,080 --> 00:15:06,080
 What is ai?

234
00:15:06,080 --> 00:15:07,080
 The larger the better.

235
00:15:07,080 --> 00:15:10,080
 Normally, we see, for the separation, right, the larger

236
00:15:10,080 --> 00:15:11,080
 the better.

237
00:15:11,080 --> 00:15:15,080
 And so for the coherent, for that one, the smaller the better.

238
00:15:15,080 --> 00:15:18,080
 I mean, if you look at the distance, right, the distance,

239
00:15:18,080 --> 00:15:20,080
 the larger the better.

240
00:15:20,080 --> 00:15:23,080
 So that means, actually, the bi here, basically, is about

241
00:15:23,080 --> 00:15:29,080
 the distance from one sample to other sample in other

242
00:15:29,080 --> 00:15:32,080
 clusters.

243
00:15:32,080 --> 00:15:33,080
 OK.

244
00:15:33,080 --> 00:15:38,080
 And ai basically should be the distance from the sample to

245
00:15:38,080 --> 00:15:41,080
 the other samples within the cluster.

246
00:15:41,080 --> 00:15:45,080
 So this difference, the larger the better.

247
00:15:45,080 --> 00:15:48,080
 So this is the definition of ai and bi.

248
00:15:48,080 --> 00:15:49,080
 OK.

249
00:15:49,080 --> 00:15:51,080
 So ai, we look at ai.

250
00:15:51,080 --> 00:15:57,080
 So for the sample, for sample ai, sample xi, and we can

251
00:15:57,080 --> 00:16:01,080
 take the distance to all other samples within the same cluster.

252
00:16:01,080 --> 00:16:04,080
 So the cluster here is p, right, cluster p, because actually

253
00:16:04,080 --> 00:16:10,080
 the sample j xi, it's a, both belong to the cluster p, right,

254
00:16:10,080 --> 00:16:11,080
 dp.

255
00:16:11,080 --> 00:16:12,080
 OK.

256
00:16:12,080 --> 00:16:14,080
 dp is the sample in cluster p.

257
00:16:14,080 --> 00:16:15,080
 OK.

258
00:16:15,080 --> 00:16:18,080
 So this actually, for each of the sample, for sample xi,

259
00:16:18,080 --> 00:16:23,080
 we calculate the distance to all other samples within the same

260
00:16:23,080 --> 00:16:27,080
 cluster, within the same cluster, right.

261
00:16:27,080 --> 00:16:31,080
 Then we take the average, the take the average.

262
00:16:31,080 --> 00:16:32,080
 OK.

263
00:16:32,080 --> 00:16:35,080
 Totally, and then we have an MP sample, right, we have an MP

264
00:16:35,080 --> 00:16:38,080
 sample for this cluster p.

265
00:16:38,080 --> 00:16:39,080
 OK.

266
00:16:39,080 --> 00:16:43,080
 So totally, here we calculate MP minus one distance.

267
00:16:43,080 --> 00:16:44,080
 OK.

268
00:16:44,080 --> 00:16:47,080
 So this is the average distance from sample xi to other

269
00:16:47,080 --> 00:16:49,080
 samples within the same cluster.

270
00:16:49,080 --> 00:16:52,080
 So this is the ai, the ai.

271
00:16:52,080 --> 00:16:53,080
 OK.

272
00:16:53,080 --> 00:16:59,080
 Ai is the, you know, average of the distance, OK, from one

273
00:16:59,080 --> 00:17:04,079
 sample xi to all other samples within the same cluster.

274
00:17:04,079 --> 00:17:05,079
 So this is the ai.

275
00:17:05,079 --> 00:17:06,079
 OK.

276
00:17:06,079 --> 00:17:09,079
 So we assume all the samples, right, other samples within the

277
00:17:09,079 --> 00:17:10,079
 same cluster.

278
00:17:10,079 --> 00:17:11,079
 OK.

279
00:17:11,079 --> 00:17:16,079
 And dij is just the distance between xi and the xj.

280
00:17:17,079 --> 00:17:19,079
 And then we look at what is bi?

281
00:17:21,079 --> 00:17:23,079
 Bi, look at this bi, right.

282
00:17:23,079 --> 00:17:24,079
 OK.

283
00:17:24,079 --> 00:17:27,079
 And actually, you know, so this is the sample, right.

284
00:17:27,079 --> 00:17:30,080
 So bi actually, just now we talked about, you know, bi should

285
00:17:30,080 --> 00:17:34,080
 reflect actually the separation, right, the separation.

286
00:17:34,080 --> 00:17:39,080
 The sample, distance from the sample xi to other samples, to

287
00:17:39,080 --> 00:17:41,080
 sample in other clusters.

288
00:17:41,080 --> 00:17:42,080
 OK.

289
00:17:42,080 --> 00:17:48,080
 So here, now we have our favorite sample, actually xj,

290
00:17:48,080 --> 00:17:55,080
 xi, right, xj, and the, q, p not equal to q.

291
00:17:55,080 --> 00:17:58,080
 And then we have to all samples not belonging to, you know,

292
00:17:58,080 --> 00:18:00,080
 to the cluster p.

293
00:18:00,080 --> 00:18:01,080
 OK.

294
00:18:01,080 --> 00:18:02,080
 Then we calculate the distance.

295
00:18:02,080 --> 00:18:06,080
 Then here is a minimum distance, minimum distance.

296
00:18:06,080 --> 00:18:07,080
 OK.

297
00:18:07,080 --> 00:18:11,080
 So how to, you know, understand this distance.

298
00:18:11,080 --> 00:18:15,080
 If you remember, we recall actually the hierarchical

299
00:18:15,080 --> 00:18:20,080
 cluster, and you remember, you know, we know in the

300
00:18:20,080 --> 00:18:24,080
 agglomerative algorithm, we start from the bottom, right,

301
00:18:24,080 --> 00:18:26,080
 the bottom arm method.

302
00:18:26,080 --> 00:18:28,080
 We gradually merge the clusters.

303
00:18:28,080 --> 00:18:29,080
 OK.

304
00:18:29,080 --> 00:18:32,080
 So normally, when we calculate the similarity between two

305
00:18:32,080 --> 00:18:35,080
 clusters, for example, right, between two clusters.

306
00:18:35,080 --> 00:18:36,080
 OK.

307
00:18:36,080 --> 00:18:39,080
 And how to evaluate?

308
00:18:39,080 --> 00:18:42,080
 For each of the clusters, we have some sample, right,

309
00:18:42,080 --> 00:18:43,080
 some sample.

310
00:18:43,080 --> 00:18:47,080
 For example, for one sample, you know, we have so many

311
00:18:47,080 --> 00:18:50,080
 distance, so many samples in other clusters, right.

312
00:18:50,080 --> 00:18:53,080
 So which distance should be used as a separation between

313
00:18:53,080 --> 00:18:57,080
 one sample to others?

314
00:18:57,080 --> 00:19:00,080
 Actually, you remember, we have a different linkage, right.

315
00:19:00,080 --> 00:19:03,080
 Complete linkage, fetids, you know, actually, you know,

316
00:19:03,080 --> 00:19:06,080
 the single linkage, every linkage.

317
00:19:06,080 --> 00:19:07,080
 OK.

318
00:19:07,080 --> 00:19:11,080
 Every linkage, you remember, just, you know, like,

319
00:19:11,080 --> 00:19:14,080
 every distance, right, and then complete linkage,

320
00:19:14,080 --> 00:19:17,080
 like the fetids samples, and then single linkage,

321
00:19:17,080 --> 00:19:19,080
 the, you know, the nearest samples, right.

322
00:19:19,080 --> 00:19:21,080
 So actually, we have different linkage.

323
00:19:21,080 --> 00:19:24,080
 So all the known are reasonable, right, just different ways

324
00:19:24,080 --> 00:19:27,080
 to evaluate the distance, similarity or distance between

325
00:19:27,080 --> 00:19:30,080
 two clusters, and each cluster has a multiple sample.

326
00:19:30,080 --> 00:19:33,080
 So here, now, this is a specific idea, right here,

327
00:19:33,080 --> 00:19:34,080
 similar idea.

328
00:19:34,080 --> 00:19:37,080
 So for a sample XI, we calculate distance for all other

329
00:19:37,080 --> 00:19:39,080
 samples in other clusters.

330
00:19:39,080 --> 00:19:42,080
 So which distance should be used as the separation,

331
00:19:42,080 --> 00:19:46,080
 should be used as distance between this sample to other

332
00:19:46,080 --> 00:19:49,080
 clusters?

333
00:19:49,080 --> 00:19:53,080
 So here, they use the minimum distance.

334
00:19:53,080 --> 00:19:57,080
 This bi is the minimum distance.

335
00:19:57,080 --> 00:19:59,080
 Actually, this minimum distance could be,

336
00:19:59,080 --> 00:20:02,080
 could be something like, you know, this is a worst-case

337
00:20:02,080 --> 00:20:06,080
 estimation for the similarity between one sample and

338
00:20:06,080 --> 00:20:09,080
 sample, and the other sample in other cluster,

339
00:20:09,080 --> 00:20:11,080
 worst-case estimation.

340
00:20:11,080 --> 00:20:14,080
 Because these are minimum value, right, minimum value.

341
00:20:14,080 --> 00:20:17,080
 Some of the samples are very far away from this,

342
00:20:17,080 --> 00:20:19,080
 very far away from this sample, right.

343
00:20:19,080 --> 00:20:21,080
 But some of the samples, actually, they use a

344
00:20:21,080 --> 00:20:25,080
 closest sample distance as the, you know,

345
00:20:25,080 --> 00:20:28,080
 the value, actually, for the separation.

346
00:20:28,080 --> 00:20:31,080
 So this is the minimum, right, makes minimum

347
00:20:31,080 --> 00:20:33,080
 difference between the sample and the sample.

348
00:20:33,080 --> 00:20:36,080
 So this is the worst-case, worst-case,

349
00:20:36,080 --> 00:20:38,080
 actually, estimation.

350
00:20:38,080 --> 00:20:41,080
 So this is the best idea for the bi.

351
00:20:41,080 --> 00:20:42,080
 Okay.

352
00:20:42,080 --> 00:20:45,080
 So we use the minimum distance.

353
00:20:45,080 --> 00:20:46,080
 Okay.

354
00:20:46,080 --> 00:20:49,080
 Worst-case.

355
00:20:49,080 --> 00:20:50,080
 Okay.

356
00:20:50,080 --> 00:20:52,080
 I think this is actually, you know, like,

357
00:20:52,080 --> 00:20:53,080
 very reasonable, right.

358
00:20:53,080 --> 00:20:55,080
 Even in the super-linear, if you remember,

359
00:20:55,080 --> 00:20:58,080
 actually, you know, we have like, you know,

360
00:20:58,080 --> 00:21:01,080
 we try to maximize actually the margin of separation.

361
00:21:01,080 --> 00:21:04,080
 So what is defined as a margin of separation?

362
00:21:04,080 --> 00:21:07,080
 Actually, it is defined as distance between

363
00:21:07,080 --> 00:21:11,080
 two closest samples in the two classes, right.

364
00:21:11,080 --> 00:21:15,080
 So this distance is used as distance, you know,

365
00:21:15,080 --> 00:21:17,080
 between the two classes, right.

366
00:21:17,080 --> 00:21:19,080
 But some, actually, in others, like,

367
00:21:19,080 --> 00:21:22,080
 like in the, in the Fisher-Linian distance,

368
00:21:22,080 --> 00:21:24,080
 Fisher-Linian distance analysis,

369
00:21:24,080 --> 00:21:26,080
 when we try to, you know, evaluate the distance

370
00:21:26,080 --> 00:21:30,080
 between two classes, we use the distance

371
00:21:30,080 --> 00:21:33,080
 between the two mean vectors, two mean vectors, right.

372
00:21:33,080 --> 00:21:35,080
 So that's in between the mean vectors.

373
00:21:35,080 --> 00:21:37,080
 Normally, the distance is greater than the distance

374
00:21:37,080 --> 00:21:39,080
 between two nearest samples.

375
00:21:39,080 --> 00:21:40,080
 Okay.

376
00:21:40,080 --> 00:21:42,080
 So here are just the different ways

377
00:21:42,080 --> 00:21:44,080
 to evaluate the distance.

378
00:21:44,080 --> 00:21:45,080
 Okay.

379
00:21:45,080 --> 00:21:47,080
 So different considerations.

380
00:21:47,080 --> 00:21:48,080
 Okay.

381
00:21:48,080 --> 00:21:51,080
 All are actually, are, are, are right, right.

382
00:21:51,080 --> 00:21:55,080
 All are correct in the method to evaluate the performance.

383
00:21:55,080 --> 00:21:56,080
 Okay.

384
00:21:56,080 --> 00:21:58,080
 So here to evaluate the separation.

385
00:21:58,080 --> 00:21:59,080
 Okay.

386
00:21:59,080 --> 00:22:02,080
 But here they use a worst case to represent,

387
00:22:02,080 --> 00:22:05,080
 actually, the distance between one sample

388
00:22:05,080 --> 00:22:08,080
 with a sample in other classes,

389
00:22:08,080 --> 00:22:11,080
 actually, as a measure of the separation.

390
00:22:11,080 --> 00:22:12,080
 Okay.

391
00:22:12,080 --> 00:22:16,080
 For coherent, they use the, actually, the,

392
00:22:16,080 --> 00:22:18,080
 they use the, you know, the average.

393
00:22:18,080 --> 00:22:21,080
 And for the separation, they use a worst case.

394
00:22:21,080 --> 00:22:22,080
 Okay.

395
00:22:22,080 --> 00:22:24,080
 That's nearest, actually.

396
00:22:24,080 --> 00:22:26,080
 The distance between the nearest sample

397
00:22:26,080 --> 00:22:28,080
 in other classes.

398
00:22:28,080 --> 00:22:29,080
 Okay.

399
00:22:29,080 --> 00:22:30,080
 So we get a Bi.

400
00:22:30,080 --> 00:22:33,080
 And then we combine this Bi and Ai,

401
00:22:33,080 --> 00:22:36,080
 actually, you know, to form the so-called

402
00:22:36,080 --> 00:22:38,080
 S-celloid coefficient.

403
00:22:38,080 --> 00:22:40,080
 So Bi minus Ai.

404
00:22:40,080 --> 00:22:41,080
 Okay.

405
00:22:41,080 --> 00:22:43,080
 Then divided by the maximum of the two.

406
00:22:43,080 --> 00:22:46,080
 So again, this is a very conservative estimation

407
00:22:46,080 --> 00:22:48,080
 of the celloid coefficient, right.

408
00:22:48,080 --> 00:22:52,080
 Actually, Bi minus Ai, then divided by the maximum value.

409
00:22:52,080 --> 00:22:55,080
 Very conservative estimation, right.

410
00:22:55,080 --> 00:22:57,080
 Very conservative.

411
00:22:57,080 --> 00:22:59,080
 They don't use Bi, they don't use Ai,

412
00:22:59,080 --> 00:23:01,080
 they use the maximum of these two.

413
00:23:01,080 --> 00:23:03,080
 Then you murdered the maximum.

414
00:23:03,080 --> 00:23:05,080
 That means that the value should be,

415
00:23:05,080 --> 00:23:07,080
 tend to become smaller, right.

416
00:23:07,080 --> 00:23:10,080
 So again, this is kind of conservative estimation.

417
00:23:10,080 --> 00:23:13,080
 Actually, for the, actually, for the two, right,

418
00:23:13,080 --> 00:23:14,080
 combined two.

419
00:23:14,080 --> 00:23:15,080
 Okay.

420
00:23:15,080 --> 00:23:20,080
 So Bi minus Ai, divided by the maximum Ai and Bi.

421
00:23:21,080 --> 00:23:22,080
 Okay.

422
00:23:22,080 --> 00:23:24,080
 So later we will see, actually,

423
00:23:24,080 --> 00:23:27,080
 in other total method for the evaluation of the classroom,

424
00:23:27,080 --> 00:23:29,080
 then they adopt different thinking,

425
00:23:29,080 --> 00:23:33,080
 different logic, okay, to, to measure similarities

426
00:23:33,080 --> 00:23:37,080
 within cluster or dissimilarity between clusters.

427
00:23:37,080 --> 00:23:39,080
 But here in the celloid coefficient,

428
00:23:39,080 --> 00:23:40,080
 they use this.

429
00:23:40,080 --> 00:23:43,080
 And actually, I think in the research, right,

430
00:23:43,080 --> 00:23:46,080
 you never, actually, I talk about the celloid coefficient.

431
00:23:46,080 --> 00:23:48,080
 We always, we can find some variants.

432
00:23:49,080 --> 00:23:51,080
 I know from this.

433
00:23:51,080 --> 00:23:54,080
 Some actually, even though they, the distance from one cluster

434
00:23:54,080 --> 00:23:56,080
 to another, some to other cluster,

435
00:23:56,080 --> 00:23:58,080
 they can also use the every distance.

436
00:23:58,080 --> 00:24:01,080
 There are some variants, right, they can use distance, right.

437
00:24:01,080 --> 00:24:03,080
 But here we introduce this method.

438
00:24:03,080 --> 00:24:04,080
 Okay.

439
00:24:04,080 --> 00:24:05,080
 Some they use the average.

440
00:24:05,080 --> 00:24:06,080
 Okay.

441
00:24:06,080 --> 00:24:08,080
 They don't use the worst case.

442
00:24:08,080 --> 00:24:09,080
 Okay.

443
00:24:09,080 --> 00:24:11,080
 So, okay.

444
00:24:11,080 --> 00:24:12,080
 So this is the celloid coefficient.

445
00:24:12,080 --> 00:24:14,080
 For every training sample, for, not training,

446
00:24:14,080 --> 00:24:17,080
 for every sample, for every sample,

447
00:24:17,080 --> 00:24:20,080
 actually we can calculate celloid coefficient.

448
00:24:20,080 --> 00:24:21,080
 Okay.

449
00:24:21,080 --> 00:24:23,080
 So finally, actually we can take the average

450
00:24:23,080 --> 00:24:25,080
 of the celloid coefficient.

451
00:24:29,080 --> 00:24:30,080
 Take the average.

452
00:24:30,080 --> 00:24:32,080
 Submission for, actually all samples,

453
00:24:32,080 --> 00:24:36,080
 because each sample we can calculate celloid coefficient, right.

454
00:24:36,080 --> 00:24:39,080
 And then we, after we calculate celloid coefficient

455
00:24:39,080 --> 00:24:42,080
 for every sample, then we take the average.

456
00:24:42,080 --> 00:24:46,080
 So these are, you know, we'll be used actually, you know,

457
00:24:46,080 --> 00:24:51,080
 to indicate actually the goodness of the cluster result.

458
00:24:53,080 --> 00:24:58,080
 And the value of actually celloid coefficient

459
00:24:58,080 --> 00:25:00,080
 is in the range from minus one to one.

460
00:25:00,080 --> 00:25:02,080
 Could be negative.

461
00:25:02,080 --> 00:25:03,080
 Could be negative, right.

462
00:25:03,080 --> 00:25:08,080
 For very bad actually cluster result,

463
00:25:08,080 --> 00:25:11,080
 maybe the BI is less than AI, right.

464
00:25:11,080 --> 00:25:14,080
 BI is less than AI, that's possible, right.

465
00:25:14,080 --> 00:25:17,080
 So the NBI money could be, you know,

466
00:25:17,080 --> 00:25:20,080
 could be, you know, negative value, right.

467
00:25:24,080 --> 00:25:27,080
 So it is actually the celloid coefficient.

468
00:25:27,080 --> 00:25:31,080
 And so normally the higher celloid coefficient

469
00:25:31,080 --> 00:25:35,080
 and the indicate that the samples are well-clustered.

470
00:25:35,080 --> 00:25:38,080
 So the higher, the better, right.

471
00:25:38,080 --> 00:25:40,080
 The higher the better.

472
00:25:41,080 --> 00:25:42,080
 Okay.

473
00:25:42,080 --> 00:25:44,080
 So this is the celloid coefficient.

474
00:25:44,080 --> 00:25:47,080
 You need to understand, right, this is the meaning, right.

475
00:25:47,080 --> 00:25:50,080
 For each training sample, we can calculate celloid coefficient.

476
00:25:50,080 --> 00:25:54,080
 And this celloid coefficient evaluates the cluster performance

477
00:25:54,080 --> 00:25:58,080
 based on two aspects, co-coherent and the separation.

478
00:25:58,080 --> 00:26:01,080
 Coherent is, you know, for a sample,

479
00:26:01,080 --> 00:26:04,080
 is just every distance to all samples within the cluster, right.

480
00:26:04,080 --> 00:26:06,080
 So that is AI.

481
00:26:06,080 --> 00:26:10,080
 And then BI, actually we use the worst case, right.

482
00:26:10,080 --> 00:26:16,080
 So the distance to the nearest samples in other clusters.

483
00:26:16,080 --> 00:26:17,080
 Okay.

484
00:26:17,080 --> 00:26:19,080
 So finally we combine the two, right.

485
00:26:19,080 --> 00:26:22,080
 So BI minus AI, okay.

486
00:26:22,080 --> 00:26:25,080
 Because we hope that the known BI, the larger the better.

487
00:26:25,080 --> 00:26:27,080
 AI, the smaller the better, right.

488
00:26:27,080 --> 00:26:30,080
 So BI minus AI, you know, AI minus BI, right.

489
00:26:30,080 --> 00:26:33,080
 So BI minus AI, okay.

490
00:26:33,080 --> 00:26:36,080
 Sometimes we hope when one is the larger the better,

491
00:26:36,080 --> 00:26:38,080
 another smaller the better, right.

492
00:26:38,080 --> 00:26:41,080
 Then we use the ratio, sometimes we use the ratio, right.

493
00:26:41,080 --> 00:26:45,080
 In the, for example, in the future linear distance analysis,

494
00:26:45,080 --> 00:26:52,080
 we hope that within class similarity, right.

495
00:26:52,080 --> 00:26:56,080
 To the distance represent the smaller the better.

496
00:26:56,080 --> 00:26:59,080
 And between class distance, the larger the better.

497
00:26:59,080 --> 00:27:01,080
 Then we use the ratio, right.

498
00:27:01,080 --> 00:27:03,080
 So we have to finish at the ratio, okay.

499
00:27:03,080 --> 00:27:06,080
 But here they use the difference between the two.

500
00:27:06,080 --> 00:27:08,080
 BI, the larger the better.

501
00:27:08,080 --> 00:27:10,080
 AI, the smaller the better, right.

502
00:27:10,080 --> 00:27:11,080
 BI minus AI.

503
00:27:11,080 --> 00:27:14,080
 Sometimes we use the division, we use the ratio.

504
00:27:14,080 --> 00:27:16,080
 Okay, so different ways, okay.

505
00:27:16,080 --> 00:27:17,080
 So different ways.

506
00:27:17,080 --> 00:27:19,080
 All right, okay.

507
00:27:24,080 --> 00:27:28,080
 Okay, so here actually we have a little one example.

508
00:27:28,080 --> 00:27:32,080
 And certainly, you know, from the distribution of the data,

509
00:27:32,080 --> 00:27:35,080
 we can see that there are three clusters, right.

510
00:27:35,080 --> 00:27:39,080
 We can assign the sample to three clusters.

511
00:27:39,080 --> 00:27:42,080
 Actually, indeed, actually, when I create the data,

512
00:27:42,080 --> 00:27:44,080
 actually I use the three Gaussian functions.

513
00:27:44,080 --> 00:27:48,080
 And actually to create the data points.

514
00:27:48,080 --> 00:27:51,080
 And so certainly there are three clusters, okay.

515
00:27:51,080 --> 00:27:53,080
 So that's actually, of course, in practice,

516
00:27:53,080 --> 00:27:57,080
 normally we don't know, right, how many classes

517
00:27:57,080 --> 00:27:59,080
 are underlying the data, right.

518
00:27:59,080 --> 00:28:02,080
 Even if, for example, you use the k-means clustering,

519
00:28:02,080 --> 00:28:04,080
 and we don't know the k-value, right.

520
00:28:04,080 --> 00:28:06,080
 And previously we have now used this,

521
00:28:06,080 --> 00:28:11,080
 now able method, able method to decide the cluster, right.

522
00:28:11,080 --> 00:28:14,080
 Now suitable number of clusters.

523
00:28:14,080 --> 00:28:17,080
 And so here actually, now we can also use

524
00:28:17,080 --> 00:28:21,080
 the silhouette coefficient to determine

525
00:28:21,080 --> 00:28:23,080
 the suitable number of clusters.

526
00:28:23,080 --> 00:28:25,080
 Okay, for example, if we don't know, right.

527
00:28:25,080 --> 00:28:29,080
 Okay, then you can set the number of clusters to two.

528
00:28:29,080 --> 00:28:32,080
 To two, then you can perform a clustering,

529
00:28:32,080 --> 00:28:34,080
 you know, k-means clustering, for example.

530
00:28:34,080 --> 00:28:37,080
 And then you can finally use the silhouette coefficient

531
00:28:37,080 --> 00:28:38,080
 to evaluate the performance.

532
00:28:38,080 --> 00:28:41,080
 Then you can calculate the h-e-value.

533
00:28:41,080 --> 00:28:46,080
 Corresponding to the scenario when k equals two, right.

534
00:28:46,080 --> 00:28:48,080
 Then you can set the k to three.

535
00:28:48,080 --> 00:28:50,080
 Then you again perform k-means clustering.

536
00:28:50,080 --> 00:28:52,080
 You can get the result.

537
00:28:52,080 --> 00:28:54,080
 And then again you can use the silhouette coefficient

538
00:28:54,080 --> 00:28:56,080
 to evaluate the performance.

539
00:28:56,080 --> 00:29:00,080
 Okay, then you set it in the k to four to five.

540
00:29:00,080 --> 00:29:02,080
 Okay, so this is the result.

541
00:29:02,080 --> 00:29:04,080
 This is the result.

542
00:29:04,080 --> 00:29:07,080
 And the first actually, the horizontal axis

543
00:29:07,080 --> 00:29:10,080
 actually are the number of clusters.

544
00:29:10,080 --> 00:29:15,080
 And the vertical axis actually shows the silhouette coefficient.

545
00:29:15,080 --> 00:29:18,080
 So you see actually when the number of clusters

546
00:29:18,080 --> 00:29:21,080
 is set to three, set to three, right.

547
00:29:21,080 --> 00:29:23,080
 And then this h-e-value,

548
00:29:23,080 --> 00:29:26,080
 silhouette coefficient value is the maximum.

549
00:29:26,080 --> 00:29:28,080
 So that is the peak value here, right.

550
00:29:28,080 --> 00:29:30,080
 The peak value.

551
00:29:30,080 --> 00:29:33,080
 Okay, so the three is the better.

552
00:29:33,080 --> 00:29:38,080
 And you can use the WCSS, right.

553
00:29:38,080 --> 00:29:40,080
 Within cluster sum of squares.

554
00:29:40,080 --> 00:29:43,080
 Okay, and actually the more cluster we use, right.

555
00:29:43,080 --> 00:29:45,080
 The more cluster we use.

556
00:29:45,080 --> 00:29:50,080
 And then the rule of the WCSS, right.

557
00:29:50,080 --> 00:29:52,080
 It's always monotonically decreasing.

558
00:29:52,080 --> 00:29:54,080
 And so we use the, actually, you know,

559
00:29:54,080 --> 00:29:59,080
 the Able method to determine the suitable number of clusters.

560
00:29:59,080 --> 00:30:02,080
 But if we use the silhouette coefficient

561
00:30:02,080 --> 00:30:04,080
 to determine the suitable number of clusters,

562
00:30:04,080 --> 00:30:08,080
 and then actually we can use this peak value, right.

563
00:30:08,080 --> 00:30:10,080
 We can use the peak value.

564
00:30:10,080 --> 00:30:15,080
 The peak value is a trip when the number of clusters is three.

565
00:30:15,080 --> 00:30:18,080
 Okay, then, or probably you can just actually

566
00:30:18,080 --> 00:30:23,080
 three is the suitable number of clusters.

567
00:30:23,080 --> 00:30:26,080
 Okay, so this is the silhouette coefficient.

568
00:30:26,080 --> 00:30:28,080
 Okay, I hope actually you understand

569
00:30:28,080 --> 00:30:30,080
 actually the meaning of the silhouette coefficient.

570
00:30:30,080 --> 00:30:31,080
 You interpret.

571
00:30:31,080 --> 00:30:33,080
 Not just a formula, right.

572
00:30:33,080 --> 00:30:35,080
 I never ask you to remember the formula.

573
00:30:35,080 --> 00:30:38,080
 But actually you need to, actually, you understand this, right.

574
00:30:38,080 --> 00:30:39,080
 You understand.

575
00:30:39,080 --> 00:30:43,080
 Okay.

576
00:30:43,080 --> 00:30:45,080
 Okay, so next I just call that down index,

577
00:30:45,080 --> 00:30:49,080
 down index, and down index actually

578
00:30:49,080 --> 00:30:53,080
 quantifies the compactness and the variance of cluster.

579
00:30:53,080 --> 00:30:56,080
 Compactness, compact, right.

580
00:30:56,080 --> 00:30:58,080
 It's still actually then talked about

581
00:30:58,080 --> 00:31:00,080
 the within cluster similarity, compactness.

582
00:31:00,080 --> 00:31:03,080
 But it's just a different word, right.

583
00:31:03,080 --> 00:31:04,080
 The wording different.

584
00:31:04,080 --> 00:31:08,080
 But actually what they want to actually evaluate the same thing.

585
00:31:08,080 --> 00:31:12,080
 Okay, compactness is just within cluster, right.

586
00:31:12,080 --> 00:31:15,080
 Within cluster similarities.

587
00:31:15,080 --> 00:31:17,080
 And then they use the, you know,

588
00:31:17,080 --> 00:31:19,080
 and the variance of the cluster.

589
00:31:19,080 --> 00:31:21,080
 Variance, here they mean between clusters.

590
00:31:21,080 --> 00:31:25,080
 Okay, then let's look at how they evaluate these actually

591
00:31:25,080 --> 00:31:27,080
 compactness.

592
00:31:27,080 --> 00:31:30,080
 How they evaluate these actually, you know,

593
00:31:30,080 --> 00:31:31,080
 variance.

594
00:31:31,080 --> 00:31:33,080
 Okay.

595
00:31:33,080 --> 00:31:36,080
 So here they define, you know,

596
00:31:36,080 --> 00:31:38,080
 data P.

597
00:31:38,080 --> 00:31:39,080
 Okay.

598
00:31:39,080 --> 00:31:43,080
 And so actually a cluster is considered compact.

599
00:31:43,080 --> 00:31:46,080
 Compact normally the spread, the dispersion is small, right.

600
00:31:46,080 --> 00:31:48,080
 It's small, compact.

601
00:31:48,080 --> 00:31:52,080
 If spread very big, actually they're not actually compact.

602
00:31:52,080 --> 00:31:54,080
 Small, right.

603
00:31:54,080 --> 00:31:55,080
 Very tight.

604
00:31:55,080 --> 00:31:56,080
 Okay.

605
00:31:56,080 --> 00:31:57,080
 Compact.

606
00:31:57,080 --> 00:31:59,080
 And a cluster can see the compact.

607
00:31:59,080 --> 00:32:03,080
 If there is small variance between sample of the cluster.

608
00:32:03,080 --> 00:32:05,080
 Small variance, right.

609
00:32:05,080 --> 00:32:06,080
 Small variance.

610
00:32:06,080 --> 00:32:08,080
 Within the cluster.

611
00:32:08,080 --> 00:32:10,080
 Remember within the cluster.

612
00:32:10,080 --> 00:32:11,080
 Okay.

613
00:32:11,080 --> 00:32:14,080
 So how to define this, okay.

614
00:32:14,080 --> 00:32:17,080
 So here actually, you know, we have two samples,

615
00:32:17,080 --> 00:32:19,080
 after two samples, right.

616
00:32:19,080 --> 00:32:21,080
 Actually, you know, for all the samples,

617
00:32:21,080 --> 00:32:24,080
 and within actually the same cluster P,

618
00:32:24,080 --> 00:32:27,080
 we can have many pairs, right.

619
00:32:27,080 --> 00:32:29,080
 Every sample to other samples, right.

620
00:32:29,080 --> 00:32:31,080
 We can form many pairs.

621
00:32:31,080 --> 00:32:32,080
 Okay.

622
00:32:32,080 --> 00:32:36,080
 So here, you know, there are one pair of the data.

623
00:32:37,080 --> 00:32:39,080
 The sample within the same cluster.

624
00:32:39,080 --> 00:32:40,080
 Okay.

625
00:32:40,080 --> 00:32:42,080
 So X, R, X, J.

626
00:32:42,080 --> 00:32:44,080
 And of course actually we can calculate the distance,

627
00:32:44,080 --> 00:32:45,080
 pair-wise distance.

628
00:32:45,080 --> 00:32:46,080
 Okay.

629
00:32:46,080 --> 00:32:50,080
 And then actually, so what distance could be used,

630
00:32:50,080 --> 00:32:58,080
 actually, you know, to represent the companions of the cluster.

631
00:32:58,080 --> 00:32:59,080
 Okay.

632
00:32:59,080 --> 00:33:01,080
 I think very naturally, I think before,

633
00:33:01,080 --> 00:33:03,080
 we see probably for each cluster,

634
00:33:03,080 --> 00:33:05,080
 you know, we can have a centroid, right.

635
00:33:05,080 --> 00:33:07,080
 And then we can calculate the distance to the centroid

636
00:33:07,080 --> 00:33:10,080
 to determine the companions.

637
00:33:10,080 --> 00:33:12,080
 Yes, actually some of the matter later,

638
00:33:12,080 --> 00:33:15,080
 we will see you adopt these ideas.

639
00:33:15,080 --> 00:33:19,080
 But here, they use the distance between two fuzzies,

640
00:33:19,080 --> 00:33:22,080
 the fuzzies, you know, samples.

641
00:33:22,080 --> 00:33:24,080
 This distance, actually, as a measure

642
00:33:24,080 --> 00:33:26,080
 for the compactness of the cluster.

643
00:33:26,080 --> 00:33:28,080
 So again, this is the worst case, right.

644
00:33:28,080 --> 00:33:31,080
 Worst case estimation for the companions.

645
00:33:32,080 --> 00:33:34,080
 Because actually some of the samples could be very close, right.

646
00:33:34,080 --> 00:33:36,080
 Some samples could be very far.

647
00:33:36,080 --> 00:33:38,080
 Actually, they use the fuzzies distance.

648
00:33:38,080 --> 00:33:41,080
 The fuzzies, the distance between two fuzzies samples

649
00:33:41,080 --> 00:33:44,080
 actually represent the compactness of the cluster.

650
00:33:44,080 --> 00:33:46,080
 This is a delta P.

651
00:33:46,080 --> 00:33:48,080
 So this is also a worst case, right.

652
00:33:48,080 --> 00:33:51,080
 Worst case estimation about the compactness of the cluster.

653
00:33:51,080 --> 00:33:52,080
 They don't use the Irish.

654
00:33:52,080 --> 00:33:57,080
 They don't use the smooth, the best one, right.

655
00:33:57,080 --> 00:34:00,080
 So these are no pessimistic, pessimistic estimation.

656
00:34:01,080 --> 00:34:03,080
 No optimal, optimistic, right.

657
00:34:03,080 --> 00:34:05,080
 Pessimistic estimation.

658
00:34:05,080 --> 00:34:07,080
 Very conservative estimation.

659
00:34:07,080 --> 00:34:11,080
 Worst case estimation about the compactness of the cluster.

660
00:34:13,080 --> 00:34:15,080
 Maximum, right, maximum.

661
00:34:15,080 --> 00:34:16,080
 Okay.

662
00:34:16,080 --> 00:34:18,080
 And so these are delta P, right.

663
00:34:18,080 --> 00:34:20,080
 So these are actually delta P is used,

664
00:34:20,080 --> 00:34:23,080
 is a measure for the compactness of the cluster.

665
00:34:23,080 --> 00:34:26,080
 So for every cluster, we can have this,

666
00:34:26,080 --> 00:34:28,080
 this measure, right.

667
00:34:28,080 --> 00:34:30,080
 We can carry the pair of distance

668
00:34:30,080 --> 00:34:32,080
 for all sample within cluster.

669
00:34:32,080 --> 00:34:34,080
 Then we find the maximum weight.

670
00:34:34,080 --> 00:34:35,080
 Actually, that is just a distance between

671
00:34:35,080 --> 00:34:38,080
 two fuzzies samples within the cluster.

672
00:34:38,080 --> 00:34:39,080
 Okay.

673
00:34:45,080 --> 00:34:48,080
 So then, next up, we look at the separation, right.

674
00:34:48,080 --> 00:34:50,080
 Actually, the separation.

675
00:34:50,080 --> 00:34:53,080
 And the two clusters can be considered as well separated

676
00:34:53,080 --> 00:34:56,080
 if the cluster are far apart, right.

677
00:34:56,080 --> 00:34:58,080
 Of course, we already have this concept, right.

678
00:34:58,080 --> 00:35:00,080
 In super high learning, we see,

679
00:35:00,080 --> 00:35:02,080
 well, under the concept of class, right.

680
00:35:02,080 --> 00:35:05,080
 If the sample in two classes are far away, right.

681
00:35:05,080 --> 00:35:07,080
 They are well separated.

682
00:35:07,080 --> 00:35:09,080
 Then the classification is easy.

683
00:35:09,080 --> 00:35:12,080
 So well separated, they are far away from each other.

684
00:35:12,080 --> 00:35:15,080
 So here, actually, we can also calculate the distance, right.

685
00:35:15,080 --> 00:35:18,080
 So here, actually, we have two clusters,

686
00:35:18,080 --> 00:35:21,080
 P and Q, right, P and Q.

687
00:35:21,080 --> 00:35:22,080
 P and Q.

688
00:35:22,080 --> 00:35:24,080
 And of course, actually, for all samples

689
00:35:24,080 --> 00:35:27,080
 from one cluster to all sample in another cluster,

690
00:35:27,080 --> 00:35:30,080
 we can calculate the pairwise distance, right.

691
00:35:30,080 --> 00:35:32,080
 Pairwise distance.

692
00:35:32,080 --> 00:35:35,080
 So there are so many pairs, right.

693
00:35:35,080 --> 00:35:37,080
 So there are so many values.

694
00:35:37,080 --> 00:35:39,080
 So which value is used to represent, actually,

695
00:35:39,080 --> 00:35:44,080
 the separation between the two clusters.

696
00:35:44,080 --> 00:35:46,080
 We use the minimum.

697
00:35:46,080 --> 00:35:48,080
 Worst case.

698
00:35:48,080 --> 00:35:49,080
 Worst case.

699
00:35:49,080 --> 00:35:52,080
 If you compare this with the hierarchical cluster, right.

700
00:35:52,080 --> 00:35:55,080
 We compare, we calculate the distance between two clusters.

701
00:35:55,080 --> 00:35:57,080
 Each cluster has multiple samples.

702
00:35:57,080 --> 00:36:00,080
 How to calculate the distance between two clusters.

703
00:36:00,080 --> 00:36:02,080
 So these correspond to the single linkage.

704
00:36:02,080 --> 00:36:04,080
 Single linkage, right.

705
00:36:04,080 --> 00:36:08,080
 The sample, the closed samples in the two clusters, right.

706
00:36:08,080 --> 00:36:09,080
 Closed.

707
00:36:09,080 --> 00:36:12,080
 That is single linkage, right.

708
00:36:12,080 --> 00:36:15,080
 Now we are talking about separation, the larger the better.

709
00:36:15,080 --> 00:36:18,080
 But now they take the, with the smallest value, right.

710
00:36:18,080 --> 00:36:20,080
 So we see this is a worst case.

711
00:36:20,080 --> 00:36:21,080
 Worst case.

712
00:36:21,080 --> 00:36:23,080
 When we talk about the compactness, right.

713
00:36:23,080 --> 00:36:24,080
 The smaller the better.

714
00:36:24,080 --> 00:36:25,080
 But they take the maximum value.

715
00:36:25,080 --> 00:36:27,080
 So I see this is a worst case.

716
00:36:27,080 --> 00:36:28,080
 Right.

717
00:36:28,080 --> 00:36:30,080
 So for the, you know, within cluster compactness,

718
00:36:30,080 --> 00:36:32,080
 they use the worst case.

719
00:36:32,080 --> 00:36:34,080
 And for the separation between cluster,

720
00:36:34,080 --> 00:36:37,080
 they again, they use the worst case.

721
00:36:37,080 --> 00:36:38,080
 Right.

722
00:36:38,080 --> 00:36:39,080
 Worst case, right.

723
00:36:39,080 --> 00:36:40,080
 Because the larger the better.

724
00:36:40,080 --> 00:36:42,080
 But they use the smaller, the smallest value,

725
00:36:42,080 --> 00:36:45,080
 the minimum value.

726
00:36:45,080 --> 00:36:46,080
 Okay.

727
00:36:46,080 --> 00:36:49,080
 So these are not correspond to the, like, a single linkage, right.

728
00:36:49,080 --> 00:36:51,080
 They are not distant between two clusters.

729
00:36:51,080 --> 00:36:54,080
 There are so many samples in the two clusters.

730
00:36:54,080 --> 00:36:57,080
 So actually, so here are closed samples.

731
00:36:57,080 --> 00:37:00,080
 These are correspond to a single linkage.

732
00:37:00,080 --> 00:37:01,080
 Right.

733
00:37:01,080 --> 00:37:02,080
 Single linkage.

734
00:37:02,080 --> 00:37:03,080
 Okay.

735
00:37:03,080 --> 00:37:07,080
 And so these are DEPQ.

736
00:37:07,080 --> 00:37:11,080
 And these are the separation, the distance between two clusters.

737
00:37:11,080 --> 00:37:12,080
 Okay.

738
00:37:12,080 --> 00:37:14,080
 So now we have these two values, right.

739
00:37:14,080 --> 00:37:15,080
 Two values.

740
00:37:15,080 --> 00:37:16,080
 Okay.

741
00:37:17,080 --> 00:37:19,080
 We need to combine, right.

742
00:37:19,080 --> 00:37:21,080
 Combine all these, actually.

743
00:37:21,080 --> 00:37:23,080
 DEP, actually, you know, we assume,

744
00:37:23,080 --> 00:37:24,080
 we have multiple clusters, right.

745
00:37:24,080 --> 00:37:26,080
 You know, normally, you know, many, right.

746
00:37:26,080 --> 00:37:30,080
 So pairwise, as you know, we can calculate the D,

747
00:37:30,080 --> 00:37:32,080
 the separation, DEPQ, right.

748
00:37:32,080 --> 00:37:35,080
 And the larger the better.

749
00:37:35,080 --> 00:37:36,080
 Right.

750
00:37:36,080 --> 00:37:38,080
 But now we use, actually, you know, single linkage.

751
00:37:38,080 --> 00:37:40,080
 We take the minimum value.

752
00:37:40,080 --> 00:37:43,080
 These are worst estimation between two clusters.

753
00:37:43,080 --> 00:37:46,080
 But finally, when we combine all of them together, right,

754
00:37:46,080 --> 00:37:48,080
 we again, we use a minimum value.

755
00:37:48,080 --> 00:37:50,080
 Again, this is another round of, you know,

756
00:37:50,080 --> 00:37:52,080
 worst case estimation.

757
00:37:52,080 --> 00:37:54,080
 Worst case.

758
00:37:54,080 --> 00:37:55,080
 Okay.

759
00:37:55,080 --> 00:37:58,080
 Some of the clusters maybe are well separated, right.

760
00:37:58,080 --> 00:38:00,080
 But some of the clusters are not well separated.

761
00:38:00,080 --> 00:38:04,080
 But they use the worst case.

762
00:38:04,080 --> 00:38:07,080
 So it's a minimum, minimum.

763
00:38:07,080 --> 00:38:10,080
 And when we talk about, you know, for each cluster,

764
00:38:10,080 --> 00:38:12,080
 you know, actually, you know, based on the distance, right,

765
00:38:12,080 --> 00:38:16,080
 we use the worst case estimation to estimate, you know,

766
00:38:16,080 --> 00:38:18,080
 the company of each cluster.

767
00:38:18,080 --> 00:38:20,080
 But finally, when we combine all of them, right,

768
00:38:20,080 --> 00:38:22,080
 we use the worst case.

769
00:38:22,080 --> 00:38:24,080
 Maximum.

770
00:38:24,080 --> 00:38:26,080
 Maximum.

771
00:38:26,080 --> 00:38:28,080
 Maximum data P, right, among all data P.

772
00:38:28,080 --> 00:38:31,080
 Because these values should be, you know, smaller the better, right.

773
00:38:31,080 --> 00:38:33,080
 They use the largest value.

774
00:38:33,080 --> 00:38:36,080
 I mean, these are worst case.

775
00:38:36,080 --> 00:38:37,080
 Okay.

776
00:38:37,080 --> 00:38:40,080
 So finally, combine these between class separation

777
00:38:40,080 --> 00:38:42,080
 and also the within class, actually,

778
00:38:42,080 --> 00:38:44,080
 cluster, no compactness, right.

779
00:38:44,080 --> 00:38:48,080
 They finally calculate one single, actually, you know,

780
00:38:48,080 --> 00:38:54,080
 value, right, that the, the, the dine index, D i.

781
00:38:54,080 --> 00:38:56,080
 Okay.

782
00:38:56,080 --> 00:39:01,080
 So all worst case estimation, right, worst case.

783
00:39:01,080 --> 00:39:03,080
 So a few rounder.

784
00:39:03,080 --> 00:39:06,080
 Within, for each cluster, they use the worst case.

785
00:39:06,080 --> 00:39:07,080
 Okay.

786
00:39:07,080 --> 00:39:09,080
 Between class and worst case.

787
00:39:09,080 --> 00:39:11,080
 So they combine together, right,

788
00:39:11,080 --> 00:39:14,080
 for the between class separation, you use the worst case.

789
00:39:14,080 --> 00:39:19,080
 For the no, within class, within class compactness, right,

790
00:39:19,080 --> 00:39:21,080
 because each one has their own, right.

791
00:39:21,080 --> 00:39:24,080
 Some of them may be very compact, but some are not.

792
00:39:24,080 --> 00:39:26,080
 But they use the worst case.

793
00:39:26,080 --> 00:39:27,080
 Okay.

794
00:39:27,080 --> 00:39:30,080
 So finally, actually, they calculate the d i.

795
00:39:30,080 --> 00:39:33,080
 Actually, the higher the dine index, the better, right,

796
00:39:33,080 --> 00:39:36,080
 because the separation, separation over, you know,

797
00:39:36,080 --> 00:39:38,080
 the compactness.

798
00:39:38,080 --> 00:39:41,080
 So sometimes, you can see, we use the ratio, right,

799
00:39:41,080 --> 00:39:42,080
 we use the ratio.

800
00:39:42,080 --> 00:39:45,080
 Sometimes we use one value minus another value, right.

801
00:39:45,080 --> 00:39:47,080
 Actually, in the, in the, in the,

802
00:39:47,080 --> 00:39:50,080
 I see it as a coefficient with a separation minus,

803
00:39:50,080 --> 00:39:53,080
 actually, the cohesion, right.

804
00:39:53,080 --> 00:39:55,080
 So the larger the better.

805
00:39:55,080 --> 00:39:58,080
 But now they use, you know, the separation over the,

806
00:39:58,080 --> 00:40:01,080
 over them, the ratio of over the cohesion,

807
00:40:01,080 --> 00:40:03,080
 something like that, compactness, right.

808
00:40:03,080 --> 00:40:07,080
 So, so, so they know the larger the better.

809
00:40:07,080 --> 00:40:08,080
 Okay.

810
00:40:08,080 --> 00:40:10,080
 Sometimes we use the ratio, sometimes we want to subtract

811
00:40:10,080 --> 00:40:11,080
 another bit.

812
00:40:11,080 --> 00:40:12,080
 Okay.

813
00:40:12,080 --> 00:40:14,080
 So I think in research, sometimes we all use these two,

814
00:40:14,080 --> 00:40:16,080
 two different ways, right.

815
00:40:16,080 --> 00:40:18,080
 If we want to emphasize, we want to look something,

816
00:40:18,080 --> 00:40:20,080
 one variable, the larger the better,

817
00:40:20,080 --> 00:40:21,080
 another smaller the better.

818
00:40:21,080 --> 00:40:23,080
 Sometimes we combine them, right.

819
00:40:23,080 --> 00:40:25,080
 Actually, using the ratio.

820
00:40:25,080 --> 00:40:26,080
 Okay.

821
00:40:26,080 --> 00:40:28,080
 And sometimes we use one, subtract another one.

822
00:40:28,080 --> 00:40:29,080
 Okay.

823
00:40:31,080 --> 00:40:32,080
 Okay.

824
00:40:32,080 --> 00:40:34,080
 So this is the dine index,

825
00:40:35,080 --> 00:40:37,080
 the larger the better.

826
00:40:37,080 --> 00:40:38,080
 Okay.

827
00:40:38,080 --> 00:40:43,080
 And so this actually, actually shows, actually,

828
00:40:43,080 --> 00:40:45,080
 you know, the example, right, the example,

829
00:40:45,080 --> 00:40:49,080
 and we have three clusters, and we use the K-means

830
00:40:49,080 --> 00:40:53,080
 cluster to, to cluster, to group the data.

831
00:40:53,080 --> 00:40:56,080
 And we set the K to different value,

832
00:40:56,080 --> 00:40:58,080
 to two, to three, to four, to five.

833
00:40:58,080 --> 00:41:00,080
 And finally, actually, you know,

834
00:41:00,080 --> 00:41:03,080
 get the corresponding dine index, right.

835
00:41:03,080 --> 00:41:05,080
 Finally, actually, we plot this.

836
00:41:05,080 --> 00:41:09,080
 And you can see, actually, when the cluster is number,

837
00:41:09,080 --> 00:41:12,080
 set to three, right, then the, the,

838
00:41:12,080 --> 00:41:15,080
 the dine index is the maximum, the p value.

839
00:41:15,080 --> 00:41:16,080
 Okay.

840
00:41:16,080 --> 00:41:19,080
 So again, from here, actually, we can use this dine index

841
00:41:19,080 --> 00:41:22,080
 to determine a suitable number of care, right,

842
00:41:22,080 --> 00:41:24,080
 if we use the K-means cluster.

843
00:41:27,080 --> 00:41:28,080
 Okay.

844
00:41:28,080 --> 00:41:30,080
 And not just for K-means cluster,

845
00:41:30,080 --> 00:41:33,080
 but even for, for others, right, actually, like,

846
00:41:36,080 --> 00:41:39,080
 I remember, I think we have the, like,

847
00:41:39,080 --> 00:41:43,080
 another one, it's a distribution-based method, right,

848
00:41:43,080 --> 00:41:44,080
 distribution-based method.

849
00:41:44,080 --> 00:41:46,080
 We also need to determine a number, right.

850
00:41:46,080 --> 00:41:48,080
 I showed you a number of, you know,

851
00:41:48,080 --> 00:41:49,080
 in a Gaussian-Misterial model.

852
00:41:49,080 --> 00:41:51,080
 We need to determine how many Gaussian components

853
00:41:51,080 --> 00:41:54,080
 need to be included in this Gaussian-Misterial model.

854
00:41:54,080 --> 00:41:56,080
 So this is also a hyper parameter.

855
00:41:56,080 --> 00:41:58,080
 So from here, this is not,

856
00:41:58,080 --> 00:42:00,080
 this is not, the number is also not the same

857
00:42:00,080 --> 00:42:02,080
 as the number of clusters, right.

858
00:42:02,080 --> 00:42:04,080
 So we can also use this, actually,

859
00:42:04,080 --> 00:42:07,080
 on the dine index to determine

860
00:42:07,080 --> 00:42:10,080
 the suitable number of clusters.

861
00:42:11,080 --> 00:42:12,080
 Okay.

862
00:42:12,080 --> 00:42:14,080
 So that would be the peak value.

863
00:42:19,080 --> 00:42:22,080
 Next, that's why it's called Davis-Boulding,

864
00:42:22,080 --> 00:42:25,080
 actually, index, actually, the dB index.

865
00:42:25,080 --> 00:42:26,080
 Okay.

866
00:42:26,080 --> 00:42:29,080
 So again, here, you can just see different ideas, right.

867
00:42:29,080 --> 00:42:31,080
 Here again, there are things they need to know,

868
00:42:31,080 --> 00:42:34,080
 emphasize the natural group in the 2S-A, okay.

869
00:42:34,080 --> 00:42:37,080
 Within class similarity, between class difference

870
00:42:37,080 --> 00:42:39,080
 or separation.

871
00:42:39,080 --> 00:42:41,080
 But just, you know, there are different ways

872
00:42:41,080 --> 00:42:43,080
 to evaluate the within class similarity

873
00:42:43,080 --> 00:42:45,080
 and also different ways to measure the

874
00:42:45,080 --> 00:42:49,080
 between class, actually, you know, separations, okay.

875
00:42:50,080 --> 00:42:51,080
 Okay.

876
00:42:55,080 --> 00:42:57,080
 So here, they have the idea,

877
00:42:57,080 --> 00:43:00,080
 actually, this is a dB-Dine index.

878
00:43:00,080 --> 00:43:03,080
 Evaluate the goodness of a cluster

879
00:43:03,080 --> 00:43:07,080
 result based on the within class dispersion.

880
00:43:07,080 --> 00:43:09,080
 Spread.

881
00:43:09,080 --> 00:43:12,080
 Like compactness, right, this is the same.

882
00:43:12,080 --> 00:43:13,080
 Okay.

883
00:43:13,080 --> 00:43:16,080
 See, actually, then and also the between class separation.

884
00:43:16,080 --> 00:43:18,080
 So that, again, the two parts,

885
00:43:18,080 --> 00:43:20,080
 the two aspects of the natural group here,

886
00:43:20,080 --> 00:43:23,080
 within class similarity, between class difference

887
00:43:23,080 --> 00:43:26,080
 or separation, okay.

888
00:43:26,080 --> 00:43:30,080
 But how to measure the within class dispersion, right.

889
00:43:30,080 --> 00:43:34,080
 Actually, previously, we can use the average distance.

890
00:43:34,080 --> 00:43:38,080
 Oh, actually, now we can use the worst case, right.

891
00:43:38,080 --> 00:43:43,080
 Here, in this method, how the method is.

892
00:43:43,080 --> 00:43:46,080
 And actually, for this, in this method,

893
00:43:46,080 --> 00:43:48,080
 within class similarity, right,

894
00:43:48,080 --> 00:43:51,080
 actually, they use the distance to the centroid.

895
00:43:51,080 --> 00:43:53,080
 For all the samples, you know,

896
00:43:53,080 --> 00:43:56,080
 within the class DI, actually,

897
00:43:56,080 --> 00:43:59,080
 they can calculate the centroid, right.

898
00:43:59,080 --> 00:44:02,080
 These are just the mean vector, the average,

899
00:44:02,080 --> 00:44:03,080
 the mean vector.

900
00:44:03,080 --> 00:44:07,080
 This is also called the centroid or class centers.

901
00:44:07,080 --> 00:44:09,080
 And then from, for each of the samples,

902
00:44:09,080 --> 00:44:11,080
 we calculate the distance.

903
00:44:13,080 --> 00:44:15,080
 We calculate the distance, right,

904
00:44:15,080 --> 00:44:18,080
 to the centroid, okay.

905
00:44:18,080 --> 00:44:20,080
 So finally, we take the, you know,

906
00:44:20,080 --> 00:44:22,080
 this is the square distance, right.

907
00:44:22,080 --> 00:44:25,080
 And then, summation for all the samples,

908
00:44:25,080 --> 00:44:28,080
 each of the samples, we have one distance to the centroid.

909
00:44:28,080 --> 00:44:31,080
 For all the samples, and I, okay,

910
00:44:31,080 --> 00:44:34,080
 then we have, you know, the average square distance.

911
00:44:34,080 --> 00:44:36,080
 Then we get square roots.

912
00:44:36,080 --> 00:44:38,080
 So this is the dispersion, okay.

913
00:44:38,080 --> 00:44:42,080
 So within class, actually, you know, the dispersion.

914
00:44:42,080 --> 00:44:46,080
 So this is used, you know, as a evaluation, right,

915
00:44:46,080 --> 00:44:48,080
 for the compactness, actually,

916
00:44:48,080 --> 00:44:51,080
 all the within class similarity.

917
00:44:51,080 --> 00:44:53,080
 So they have a centroid,

918
00:44:53,080 --> 00:44:56,080
 and then they take, look at the distance to the centroid.

919
00:44:59,080 --> 00:45:01,080
 Okay, so this is one aspect

920
00:45:01,080 --> 00:45:03,080
 for the within class similarity.

921
00:45:03,080 --> 00:45:05,080
 So for this data, right,

922
00:45:05,080 --> 00:45:07,080
 actually, the smaller the better, right.

923
00:45:07,080 --> 00:45:09,080
 You can imagine that there is centroid.

924
00:45:09,080 --> 00:45:11,080
 If all the samples are very close to the centroid,

925
00:45:11,080 --> 00:45:13,080
 then the distance is very small, right.

926
00:45:13,080 --> 00:45:15,080
 Then the tree, this is data R, right.

927
00:45:15,080 --> 00:45:18,080
 Here they call dispersion also, very small.

928
00:45:18,080 --> 00:45:23,080
 Okay, if the spread of the cluster, right, is big,

929
00:45:23,080 --> 00:45:25,080
 then the distance is big.

930
00:45:25,080 --> 00:45:27,080
 Okay.

931
00:45:33,080 --> 00:45:35,080
 Okay, so this is actually within cluster.

932
00:45:35,080 --> 00:45:38,080
 So next, we should look at the between cluster, right.

933
00:45:38,080 --> 00:45:40,080
 So this is between cluster, this similarity,

934
00:45:40,080 --> 00:45:42,080
 this similarity or separation, okay.

935
00:45:42,080 --> 00:45:48,080
 And so then the similarity between two clusters, okay,

936
00:45:48,080 --> 00:45:52,080
 is defined as this, the similarity between two.

937
00:45:52,080 --> 00:45:55,080
 Actually, before that, we see the separation

938
00:45:55,080 --> 00:45:57,080
 between two clusters, the R, D, J,

939
00:45:57,080 --> 00:46:01,080
 are defined as the distance between two centroid.

940
00:46:01,080 --> 00:46:03,080
 Okay, these are very natural, right.

941
00:46:03,080 --> 00:46:07,080
 We look at the, in the official linear dish analysis,

942
00:46:07,080 --> 00:46:10,080
 we look at the distance between two clusters, right.

943
00:46:10,080 --> 00:46:13,080
 Actually, we, between two clusters,

944
00:46:13,080 --> 00:46:16,080
 we use the distance between two centroid.

945
00:46:16,080 --> 00:46:18,080
 Here they use this concept, right.

946
00:46:18,080 --> 00:46:22,080
 That is the between cluster,

947
00:46:22,080 --> 00:46:27,080
 between cluster separation actually is measured

948
00:46:27,080 --> 00:46:29,080
 by the distance between the two centroid

949
00:46:29,080 --> 00:46:31,080
 of the two clusters.

950
00:46:31,080 --> 00:46:34,080
 So this is using the D, I, J to denote actually

951
00:46:34,080 --> 00:46:39,080
 the distance between the two centroid, M, I and M, J.

952
00:46:39,080 --> 00:46:42,080
 Okay, so now, this is a separation.

953
00:46:42,080 --> 00:46:45,080
 Next, we need to combine the separation and dispersion, right,

954
00:46:45,080 --> 00:46:48,080
 into one single metric.

955
00:46:48,080 --> 00:46:54,080
 So that is data I, data J, D, I, D, J.

956
00:46:54,080 --> 00:46:57,080
 If you look at this, just like in the inverse of the

957
00:46:57,080 --> 00:46:58,080
 facial ratio, right.

958
00:46:58,080 --> 00:47:01,080
 Facial ratio, we look at the distance between two cluster,

959
00:47:02,080 --> 00:47:04,080
 these are the classes, right.

960
00:47:04,080 --> 00:47:08,080
 So that's D, I, J, divided by the sigma I or sigma J.

961
00:47:08,080 --> 00:47:10,080
 All we use is the distance squared,

962
00:47:10,080 --> 00:47:13,080
 divided by the sigma I squared, sigma J squared, right.

963
00:47:13,080 --> 00:47:15,080
 So this is similar.

964
00:47:15,080 --> 00:47:18,080
 This case, but for the, for the, for the,

965
00:47:18,080 --> 00:47:19,080
 for the facial ratio, right.

966
00:47:19,080 --> 00:47:23,080
 So now they use this kind of similarity.

967
00:47:23,080 --> 00:47:27,080
 That I, that J, D, I, D, J.

968
00:47:27,080 --> 00:47:29,080
 But you can use it in inverse, right.

969
00:47:29,080 --> 00:47:30,080
 They just use it in this.

970
00:47:30,080 --> 00:47:33,080
 But for us, if we want to evaluate,

971
00:47:33,080 --> 00:47:35,080
 actually we can use it in inverse of this, right.

972
00:47:35,080 --> 00:47:37,080
 Like D, I, J, divided by data I,

973
00:47:37,080 --> 00:47:40,080
 that J, just like facial ratio, the larger the better.

974
00:47:40,080 --> 00:47:44,080
 But here for this S, I, J, here the smaller the better, right.

975
00:47:44,080 --> 00:47:46,080
 Because the compactness, right,

976
00:47:46,080 --> 00:47:48,080
 the D, I, that J should be small.

977
00:47:48,080 --> 00:47:51,080
 And the big, you know, between cluster separation,

978
00:47:51,080 --> 00:47:54,080
 the D, I, J should be the larger the better, right.

979
00:47:54,080 --> 00:47:58,080
 So the ratio should be the smaller the better.

980
00:47:59,080 --> 00:48:01,080
 So this is defined as a similarity

981
00:48:01,080 --> 00:48:05,080
 between between two, between two clusters.

982
00:48:05,080 --> 00:48:06,080
 Okay.

983
00:48:06,080 --> 00:48:08,080
 Similarity between two clusters.

984
00:48:08,080 --> 00:48:09,080
 Okay.

985
00:48:09,080 --> 00:48:11,080
 So these are clusters, S, I, S, J, right.

986
00:48:11,080 --> 00:48:14,080
 So now actually we, of course we have many clusters, cluster, right.

987
00:48:14,080 --> 00:48:16,080
 Actually we need to calculate a pair-wise,

988
00:48:16,080 --> 00:48:18,080
 you know, similarities,

989
00:48:18,080 --> 00:48:20,080
 based on this.

990
00:48:20,080 --> 00:48:21,080
 Okay.

991
00:48:21,080 --> 00:48:24,080
 And finally, we take the maximum.

992
00:48:24,080 --> 00:48:27,080
 S, I, I, J, right.

993
00:48:27,080 --> 00:48:29,080
 The smaller the better.

994
00:48:29,080 --> 00:48:31,080
 But they take the maximum.

995
00:48:31,080 --> 00:48:34,080
 So it's also a worst case estimation.

996
00:48:34,080 --> 00:48:36,080
 S, I, J, right.

997
00:48:36,080 --> 00:48:39,080
 Pair-wise, some could be big, some could be small, right.

998
00:48:39,080 --> 00:48:41,080
 And the smaller the better.

999
00:48:41,080 --> 00:48:43,080
 But they use the biggest value.

1000
00:48:43,080 --> 00:48:46,080
 The worst case, maximum.

1001
00:48:46,080 --> 00:48:47,080
 Okay.

1002
00:48:47,080 --> 00:48:53,080
 So the similarity of the most similar cluster to cluster D, I, right.

1003
00:48:53,080 --> 00:48:56,080
 So cluster D, I, for you to know,

1004
00:48:56,080 --> 00:49:00,080
 kind of have a know, calculate the S, I, one S, I,

1005
00:49:00,080 --> 00:49:03,080
 not all other cluster for class S, I, right.

1006
00:49:03,080 --> 00:49:07,080
 But the, some could be known, actually this value could be, you know,

1007
00:49:07,080 --> 00:49:11,080
 big, some are small, but they use the big, maximum value.

1008
00:49:11,080 --> 00:49:16,080
 So this maximum value indicates actually the similarity of class S, I,

1009
00:49:16,080 --> 00:49:18,080
 to other clusters.

1010
00:49:18,080 --> 00:49:20,080
 So this S, I.

1011
00:49:20,080 --> 00:49:22,080
 Worst case.

1012
00:49:22,080 --> 00:49:23,080
 Okay.

1013
00:49:23,080 --> 00:49:25,080
 So this is just for S, I.

1014
00:49:25,080 --> 00:49:28,080
 Then for each of the cluster, we can calculate one, right.

1015
00:49:28,080 --> 00:49:31,080
 We can calculate one similarity to other clusters.

1016
00:49:31,080 --> 00:49:32,080
 Okay.

1017
00:49:32,080 --> 00:49:35,080
 So finally, we need to actually take the average.

1018
00:49:35,080 --> 00:49:41,080
 So finally the last step to take the average.

1019
00:49:41,080 --> 00:49:47,080
 S, I, J is a class I to class J, right, to all other clusters.

1020
00:49:47,080 --> 00:49:52,080
 So finally we need to have one value, right, as a measure of the similarity

1021
00:49:52,080 --> 00:49:55,080
 of one cluster with others.

1022
00:49:55,080 --> 00:49:58,080
 So this is S, I, the maximum, the worst case.

1023
00:49:58,080 --> 00:49:59,080
 Okay.

1024
00:49:59,080 --> 00:50:03,080
 So finally, we take the average.

1025
00:50:03,080 --> 00:50:09,080
 This is the DB, actually index.

1026
00:50:09,080 --> 00:50:12,080
 And from here, we know that DB, for the DB index,

1027
00:50:12,080 --> 00:50:15,080
 actually the smaller the better, smaller the better.

1028
00:50:15,080 --> 00:50:18,080
 Actually, you know, for S, actually, you can also modify this, right.

1029
00:50:18,080 --> 00:50:19,080
 You can modify this.

1030
00:50:19,080 --> 00:50:23,080
 For example, for S, I, J, I can use the inverse.

1031
00:50:23,080 --> 00:50:25,080
 D, I, J divided by sigma, I, M plus sigma, J, right.

1032
00:50:25,080 --> 00:50:28,080
 Just like fish and fish, the larger the better, right.

1033
00:50:28,080 --> 00:50:31,080
 And then later, actually, I also take the worst case.

1034
00:50:31,080 --> 00:50:34,080
 S, I, J, here I take the minimum S, I, J.

1035
00:50:34,080 --> 00:50:36,080
 And finally, take the average, right.

1036
00:50:36,080 --> 00:50:38,080
 You can also choose the inverse.

1037
00:50:38,080 --> 00:50:42,080
 But in the original index, they use this.

1038
00:50:43,080 --> 00:50:46,080
 Okay, the smaller the better.

1039
00:50:46,080 --> 00:50:47,080
 Okay.

1040
00:50:47,080 --> 00:50:55,080
 Then we have this, actually, the DB index underline,

1041
00:50:55,080 --> 00:50:59,080
 as you correspond to different key, key values.

1042
00:50:59,080 --> 00:51:00,080
 Okay.

1043
00:51:00,080 --> 00:51:01,080
 So a number of clusters.

1044
00:51:01,080 --> 00:51:04,080
 Set to two, to two, three, to four, to five, actually,

1045
00:51:04,080 --> 00:51:08,080
 you can see when this set to three, the value is a minimum.

1046
00:51:08,080 --> 00:51:09,080
 It's a minimum.

1047
00:51:09,080 --> 00:51:10,080
 Okay.

1048
00:51:10,080 --> 00:51:14,080
 Again, this DB index can be used, actually, you know,

1049
00:51:14,080 --> 00:51:18,080
 to determine the suitable number of care, right.

1050
00:51:18,080 --> 00:51:19,080
 Okay.

1051
00:51:22,080 --> 00:51:23,080
 Okay.

1052
00:51:23,080 --> 00:51:29,080
 So this is the DB index.

1053
00:51:29,080 --> 00:51:32,080
 So let's look at another index,

1054
00:51:32,080 --> 00:51:36,080
 Kalinsky-Harabas index, okay, the C-H-H index.

1055
00:51:36,080 --> 00:51:38,080
 Again, the two concepts.

1056
00:51:38,080 --> 00:51:39,080
 Okay.

1057
00:51:39,080 --> 00:51:41,080
 You just look at how they evaluate these two.

1058
00:51:41,080 --> 00:51:42,080
 Okay.

1059
00:51:42,080 --> 00:51:45,080
 Then, cohesion, right, separation, the two, right.

1060
00:51:45,080 --> 00:51:46,080
 Okay.

1061
00:51:46,080 --> 00:51:49,080
 So next we look at how they measure the cohesion.

1062
00:51:49,080 --> 00:51:56,080
 Actually, for each of the clusters, we have a same choice, right.

1063
00:51:56,080 --> 00:51:57,080
 We have a same choice.

1064
00:51:57,080 --> 00:52:01,080
 And then, actually, the summation for all samples, right,

1065
00:52:01,080 --> 00:52:03,080
 the distance.

1066
00:52:03,080 --> 00:52:04,080
 This is the same choice.

1067
00:52:04,080 --> 00:52:06,080
 So this is, similarly to the, you know,

1068
00:52:06,080 --> 00:52:08,080
 the dispersion measurement, right, dispersion.

1069
00:52:08,080 --> 00:52:09,080
 Okay.

1070
00:52:09,080 --> 00:52:11,080
 And the dispersion just actually is,

1071
00:52:11,080 --> 00:52:14,080
 here is a square distance.

1072
00:52:14,080 --> 00:52:16,080
 And also, you know, dispersion, you know,

1073
00:52:16,080 --> 00:52:19,080
 is, you know, the average distance, right, average.

1074
00:52:19,080 --> 00:52:22,080
 But here, they just, the summation of all the distance

1075
00:52:22,080 --> 00:52:25,080
 to the same choice, square distance.

1076
00:52:25,080 --> 00:52:27,080
 And this is the CI.

1077
00:52:27,080 --> 00:52:32,080
 And this CI indicates the cohesion of sample within the cluster.

1078
00:52:32,080 --> 00:52:35,080
 This value, the smaller the better, right.

1079
00:52:35,080 --> 00:52:38,080
 Because, you know, the similarity in versatility

1080
00:52:38,080 --> 00:52:40,080
 proportion to the distance, right.

1081
00:52:40,080 --> 00:52:44,080
 So the smaller the distance, the more similar, right.

1082
00:52:44,080 --> 00:52:48,080
 So here, actually, for the CI, coherent,

1083
00:52:48,080 --> 00:52:50,080
 it's smaller the better.

1084
00:52:50,080 --> 00:52:51,080
 Okay.

1085
00:52:51,080 --> 00:52:53,080
 So these are, for the coherent, right,

1086
00:52:53,080 --> 00:52:57,080
 the within cluster, they use this metric to evaluate,

1087
00:52:57,080 --> 00:52:58,080
 to measure.

1088
00:52:58,080 --> 00:52:59,080
 Okay.

1089
00:52:59,080 --> 00:53:00,080
 We have seen others, right.

1090
00:53:00,080 --> 00:53:02,080
 So this is also one way.

1091
00:53:03,080 --> 00:53:06,080
 And then, how about the between cluster?

1092
00:53:06,080 --> 00:53:07,080
 Between cluster.

1093
00:53:07,080 --> 00:53:11,080
 Between cluster, here, they don't do the pairwise.

1094
00:53:11,080 --> 00:53:14,080
 They don't do the pairwise similarities or distance.

1095
00:53:14,080 --> 00:53:15,080
 They don't use this.

1096
00:53:15,080 --> 00:53:16,080
 Okay.

1097
00:53:16,080 --> 00:53:19,080
 Here, they define a global, global, you know,

1098
00:53:19,080 --> 00:53:20,080
 centroid.

1099
00:53:20,080 --> 00:53:22,080
 For all samples, right, for all samples,

1100
00:53:22,080 --> 00:53:25,080
 we, we, we carry a global centroid.

1101
00:53:25,080 --> 00:53:28,080
 And then, from, from each of the cluster center

1102
00:53:28,080 --> 00:53:30,080
 and to the global centroid,

1103
00:53:30,080 --> 00:53:32,080
 we can have a square distance.

1104
00:53:32,080 --> 00:53:34,080
 So this is different SI.

1105
00:53:34,080 --> 00:53:35,080
 SI.

1106
00:53:35,080 --> 00:53:37,080
 And then, you should remember,

1107
00:53:37,080 --> 00:53:39,080
 to either multiple, you know,

1108
00:53:39,080 --> 00:53:41,080
 feature linear disk analysis,

1109
00:53:41,080 --> 00:53:43,080
 when we calculate the, you know,

1110
00:53:43,080 --> 00:53:49,080
 between class, between class distance, right.

1111
00:53:49,080 --> 00:53:52,080
 And then, we don't use the pairwise.

1112
00:53:52,080 --> 00:53:56,080
 Actually, we also calculate the same global centroid,

1113
00:53:56,080 --> 00:53:58,080
 global mean vector.

1114
00:53:58,080 --> 00:54:00,080
 And then, from each of the class,

1115
00:54:00,080 --> 00:54:02,080
 mean or centroid or mean vector,

1116
00:54:02,080 --> 00:54:06,080
 we can calculate the distance to this global centroid.

1117
00:54:06,080 --> 00:54:09,080
 So here, the base, the, the similar idea.

1118
00:54:09,080 --> 00:54:11,080
 Here, just, we don't have the concept of a cluster.

1119
00:54:11,080 --> 00:54:13,080
 We have the concept of a cluster.

1120
00:54:13,080 --> 00:54:14,080
 Okay.

1121
00:54:14,080 --> 00:54:17,080
 So this is the, for each of the cluster,

1122
00:54:17,080 --> 00:54:20,080
 from the centroid to the global centroid,

1123
00:54:20,080 --> 00:54:23,080
 we can calculate a square distance, SI.

1124
00:54:23,080 --> 00:54:25,080
 Of course, for this SI,

1125
00:54:25,080 --> 00:54:28,080
 the larger the better, the larger the better.

1126
00:54:28,080 --> 00:54:31,080
 And for this SI, the smaller the better, right.

1127
00:54:31,080 --> 00:54:33,080
 Finally, we need to combine the two, right,

1128
00:54:33,080 --> 00:54:34,080
 combine the two.

1129
00:54:34,080 --> 00:54:36,080
 Okay.

1130
00:54:36,080 --> 00:54:38,080
 So this is a global centroid.

1131
00:54:38,080 --> 00:54:40,080
 This is M.

1132
00:54:40,080 --> 00:54:43,080
 And finally, we need to combine the two.

1133
00:54:43,080 --> 00:54:46,080
 And combine the two here, actually,

1134
00:54:46,080 --> 00:54:49,080
 in the LSI calculation, right,

1135
00:54:49,080 --> 00:54:52,080
 we just have a few values, few values, right.

1136
00:54:52,080 --> 00:54:54,080
 But here, we have many values, right.

1137
00:54:54,080 --> 00:54:57,080
 So we have a number of the sample to the,

1138
00:54:57,080 --> 00:55:00,080
 to the, to the, to the centroid, we have one value, right.

1139
00:55:00,080 --> 00:55:04,080
 So, you know, to balance the size of the numerator and denominator, right.

1140
00:55:04,080 --> 00:55:07,080
 So here, they multiply the number of samples.

1141
00:55:07,080 --> 00:55:13,080
 And I, SI, then divided by the summation of CI.

1142
00:55:13,080 --> 00:55:15,080
 So this is a CAH index.

1143
00:55:15,080 --> 00:55:16,080
 Okay.

1144
00:55:16,080 --> 00:55:20,080
 So SI is a between class separation, the larger the better.

1145
00:55:20,080 --> 00:55:23,080
 And CI, the cohesion within class similarity, right.

1146
00:55:23,080 --> 00:55:25,080
 The smaller the better, the larger the better,

1147
00:55:25,080 --> 00:55:27,080
 the distance smaller the better, right.

1148
00:55:27,080 --> 00:55:31,080
 So this will show the larger the better CAH.

1149
00:55:31,080 --> 00:55:32,080
 Okay.

1150
00:55:32,080 --> 00:55:37,080
 So for this CAH, and the, and the high value of CAH index

1151
00:55:37,080 --> 00:55:40,080
 means that class are dense and well separated.

1152
00:55:40,080 --> 00:55:42,080
 So here, that means the larger the better.

1153
00:55:42,080 --> 00:55:43,080
 Okay.

1154
00:55:43,080 --> 00:55:48,080
 And so, again, we use this index to evaluate the performance

1155
00:55:48,080 --> 00:55:51,080
 of K-means classroom for the previous example, right.

1156
00:55:51,080 --> 00:55:56,080
 When the K is set to different values, to two, to three, to four, to five,

1157
00:55:56,080 --> 00:55:59,080
 then we can see the corresponding value, right.

1158
00:55:59,080 --> 00:56:02,080
 So here, you can see a tree, for this tree, you know, a CI index,

1159
00:56:02,080 --> 00:56:04,080
 the value is very big, right.

1160
00:56:04,080 --> 00:56:08,080
 Not like others in the range from zero to one or from minus one to one, right.

1161
00:56:08,080 --> 00:56:10,080
 But here, very big.

1162
00:56:10,080 --> 00:56:13,080
 This depends on the data, how many data you have, right.

1163
00:56:13,080 --> 00:56:16,080
 The larger the number of data, because you times the N i, right,

1164
00:56:16,080 --> 00:56:19,080
 in the numerator part, this value could be very big.

1165
00:56:19,080 --> 00:56:23,080
 If you have more samples, then this value could be even bigger.

1166
00:56:23,080 --> 00:56:29,080
 But no matter the scale of the data, right, the values,

1167
00:56:29,080 --> 00:56:32,080
 we can always find the peak value, right, the peak value.

1168
00:56:32,080 --> 00:56:33,080
 Okay.

1169
00:56:33,080 --> 00:56:38,080
 So we just use it based on the peak value to determine the suit of the number of K.

1170
00:56:38,080 --> 00:56:44,080
 Okay.

1171
00:56:44,080 --> 00:56:47,080
 So this is the CI index.

1172
00:56:47,080 --> 00:56:50,080
 So so far, we have introduced four, right, four methods, right.

1173
00:56:50,080 --> 00:56:54,080
 Four methods, four metrics or index, actually,

1174
00:56:54,080 --> 00:56:57,080
 to evaluate the performance of the cluster, okay.

1175
00:56:57,080 --> 00:57:01,080
 And all of these four shows the peak values, right, peak values.

1176
00:57:01,080 --> 00:57:02,080
 Okay.

1177
00:57:02,080 --> 00:57:05,080
 And so this is different from WCSS, right.

1178
00:57:05,080 --> 00:57:11,080
 WCSS actually, with the increase, actually, of the class numbers,

1179
00:57:11,080 --> 00:57:14,080
 actually, the WCSS decreases.

1180
00:57:14,080 --> 00:57:15,080
 Okay.

1181
00:57:15,080 --> 00:57:17,080
 So we don't have peak values.

1182
00:57:17,080 --> 00:57:21,080
 But for these four metrics, actually, or index, we have peak values.

1183
00:57:21,080 --> 00:57:26,080
 And then I think it's easier to determine the suit of the number of, you know,

1184
00:57:26,080 --> 00:57:28,080
 class and underlying the data.

1185
00:57:28,080 --> 00:57:34,080
 If we use these four metrics, right, okay.

1186
00:57:34,080 --> 00:57:39,080
 The last one, actually, is to, the last one is called the run index.

1187
00:57:39,080 --> 00:57:44,080
 Actually, this method is to compare the, you know, results of the data.

1188
00:57:44,080 --> 00:57:49,080
 You know, result of two class and algorithm, compared to two class and algorithm.

1189
00:57:49,080 --> 00:57:50,080
 Okay.

1190
00:57:50,080 --> 00:57:55,080
 And so they're not for evaluation of one class and algorithm.

1191
00:57:55,080 --> 00:57:56,080
 This is different.

1192
00:57:56,080 --> 00:57:57,080
 They just compare, right.

1193
00:57:57,080 --> 00:58:02,080
 How close the result of two class and algorithms.

1194
00:58:02,080 --> 00:58:03,080
 Okay.

1195
00:58:03,080 --> 00:58:06,080
 How close the two class and class, class and result, right.

1196
00:58:06,080 --> 00:58:07,080
 Okay.

1197
00:58:07,080 --> 00:58:10,080
 And so here, this is the formula for run index.

1198
00:58:10,080 --> 00:58:17,080
 And as, indeed, as the number of pairs of sample assigned to the same cluster.

1199
00:58:17,080 --> 00:58:18,080
 Okay.

1200
00:58:18,080 --> 00:58:21,080
 So the, there are many pairs of the data, right.

1201
00:58:21,080 --> 00:58:22,080
 Numerous pairs.

1202
00:58:22,080 --> 00:58:23,080
 Not numerous.

1203
00:58:23,080 --> 00:58:26,080
 There are so many pairs, the number of pairs of data is this.

1204
00:58:26,080 --> 00:58:28,080
 This is not common at all.

1205
00:58:28,080 --> 00:58:30,080
 From n, we select two.

1206
00:58:30,080 --> 00:58:32,080
 From n, we select two.

1207
00:58:32,080 --> 00:58:33,080
 Okay.

1208
00:58:33,080 --> 00:58:35,080
 So these are numbers, huge, right.

1209
00:58:35,080 --> 00:58:36,080
 Okay.

1210
00:58:36,080 --> 00:58:39,080
 So, so actually, these are total number pairs.

1211
00:58:39,080 --> 00:58:45,080
 And these are number of pairs, the data assigned to the same cluster, across the same, the

1212
00:58:45,080 --> 00:58:46,080
 two algorithm.

1213
00:58:46,080 --> 00:58:47,080
 Okay.

1214
00:58:47,080 --> 00:58:48,080
 For all, how many pairs?

1215
00:58:48,080 --> 00:58:49,080
 There are so many pairs, right.

1216
00:58:49,080 --> 00:58:53,080
 How many pairs, the two algorithms, send them to the same cluster.

1217
00:58:53,080 --> 00:58:56,080
 Send them to the same cluster.

1218
00:58:56,080 --> 00:58:59,080
 So these, the number of pairs is n, n.

1219
00:58:59,080 --> 00:59:05,080
 So, indeed, here, the number of pairs of sample assigned to two different clusters.

1220
00:59:05,080 --> 00:59:07,080
 How many pairs, right.

1221
00:59:07,080 --> 00:59:12,080
 So these pairs for one algorithm, for this data, one assigned this, no data to one cluster.

1222
00:59:12,080 --> 00:59:14,080
 Then another set to a different cluster, right.

1223
00:59:14,080 --> 00:59:16,080
 So these are number of pairs.

1224
00:59:16,080 --> 00:59:20,080
 Then divided by total number of pairs here.

1225
00:59:20,080 --> 00:59:23,080
 Select n from two, common factorial, right.

1226
00:59:23,080 --> 00:59:30,080
 So this will be n factorial divided by n factorial, two factorial, m factorial, two factorial.

1227
00:59:30,080 --> 00:59:31,080
 Okay.

1228
00:59:31,080 --> 00:59:37,080
 So these are used to compare to actually, you know, a class on an algorithm.

1229
00:59:37,080 --> 00:59:41,080
 Whether that results in a similar or a very different.

1230
00:59:41,080 --> 00:59:42,080
 Okay.

1231
00:59:42,080 --> 00:59:44,080
 So I think this is not commonly used.

1232
00:59:44,080 --> 00:59:49,080
 But actually, the first format are commonly used in the evaluation for a classroom.

1233
00:59:49,080 --> 00:59:50,080
 Okay.

1234
00:59:50,080 --> 00:59:55,080
 This is just like, you know, previously we have, you know, error rate, we have, you know, accuracy,

1235
00:59:56,080 --> 01:00:01,080
 we have a precision, we have different metrics to evaluate the performance.

1236
01:00:01,080 --> 01:00:02,080
 Okay.

1237
01:00:06,080 --> 01:00:08,080
 Okay, I think, yeah, okay, that's all.

1238
01:00:08,080 --> 01:00:09,080
 Yeah.

1239
01:00:09,080 --> 01:00:13,080
 So this is the evaluation of the classroom.

1240
01:00:13,080 --> 01:00:16,080
 I think this part is very important, right.

1241
01:00:16,080 --> 01:00:18,080
 We learn, but what is the learning result?

1242
01:00:18,080 --> 01:00:20,080
 The learning result is good or not, right.

1243
01:00:20,080 --> 01:00:22,080
 So you study this course.

1244
01:00:22,080 --> 01:00:24,080
 So what is your learning outcome?

1245
01:00:24,080 --> 01:00:30,080
 So finally, when you, you will be tested, right, through the quizzes, through the exams.

1246
01:00:30,080 --> 01:00:31,080
 Okay.

1247
01:00:31,080 --> 01:00:37,080
 So this quiz exam, just like this kind of metrics, right, to evaluate the performance of the linear.

1248
01:00:37,080 --> 01:00:43,080
 So linear is one aspect, but the evaluation of the linear result is also a very important aspect.

1249
01:00:43,080 --> 01:00:44,080
 Right.

1250
01:00:44,080 --> 01:00:45,080
 So, okay.

1251
01:00:45,080 --> 01:00:48,080
 So that's actually, we have a ten-minute break.

1252
01:00:48,080 --> 01:00:51,080
 So after a break, we have a revision, right.

1253
01:00:51,080 --> 01:00:54,080
 And to summarize the main points in this course.

1254
01:00:54,080 --> 01:00:55,080
 Okay.

1255
01:01:21,080 --> 01:01:22,080
 Okay.

1256
01:01:51,080 --> 01:01:53,080
 Okay.

1257
01:02:21,080 --> 01:02:23,080
 Okay.

1258
01:02:51,080 --> 01:02:53,080
 Okay.

1259
01:03:21,080 --> 01:03:23,080
 Okay.

1260
01:03:51,080 --> 01:03:53,080
 Okay.

1261
01:04:21,080 --> 01:04:23,080
 Okay.

1262
01:04:51,080 --> 01:04:53,080
 Okay.

1263
01:05:21,080 --> 01:05:23,080
 Okay.

1264
01:05:51,080 --> 01:05:53,080
 Okay.

1265
01:06:21,080 --> 01:06:23,080
 Okay.

1266
01:06:51,080 --> 01:06:53,080
 Okay.

1267
01:07:21,080 --> 01:07:23,080
 Okay.

1268
01:07:51,080 --> 01:07:53,080
 Okay.

1269
01:08:21,080 --> 01:08:23,080
 Okay.

1270
01:08:51,080 --> 01:08:53,080
 Okay.

1271
01:09:21,080 --> 01:09:23,080
 Okay.

1272
01:09:51,080 --> 01:09:53,080
 Okay.

1273
01:09:53,080 --> 01:09:55,080
 Okay.

1274
01:09:55,080 --> 01:09:57,080
 Okay.

1275
01:09:57,080 --> 01:09:59,080
 Okay.

1276
01:09:59,080 --> 01:10:01,080
 Okay.

1277
01:10:01,080 --> 01:10:03,080
 Okay.

1278
01:10:03,080 --> 01:10:09,080
 In the first part of the course, actually, we talked about data preparation for machine learning, right.

1279
01:10:09,080 --> 01:10:15,080
 And in this part, the first thing we know, we have studied different types of data.

1280
01:10:15,080 --> 01:10:18,080
 And basically, the data can be categorized into two categories, right.

1281
01:10:18,080 --> 01:10:21,080
 One is continuous data.

1282
01:10:21,080 --> 01:10:23,080
 Another is discrete data.

1283
01:10:23,080 --> 01:10:26,080
 And the discrete also called categorical data.

1284
01:10:26,080 --> 01:10:32,080
 And under this category, categorical data, we have two types.

1285
01:10:32,080 --> 01:10:37,080
 One is the, you know, the ordinal data.

1286
01:10:37,080 --> 01:10:38,080
 Okay.

1287
01:10:38,080 --> 01:10:40,080
 So we just have a few values, right.

1288
01:10:40,080 --> 01:10:48,080
 And actually, the distance between the ordinal data can show the difference.

1289
01:10:48,080 --> 01:10:51,080
 How big the difference is.

1290
01:10:51,080 --> 01:10:54,080
 And then another type of nominal data.

1291
01:10:54,080 --> 01:11:02,080
 Nominal data, actually, then the difference between the two values cannot represent how different the two values are, right.

1292
01:11:02,080 --> 01:11:04,080
 They're just, they're different.

1293
01:11:04,080 --> 01:11:06,080
 Just show they are different.

1294
01:11:06,080 --> 01:11:08,080
 Okay.

1295
01:11:08,080 --> 01:11:18,080
 And so the challenge of, you know, mixed data that contains both continuous and discrete, in particular the nominal data.

1296
01:11:18,080 --> 01:11:22,080
 That, you know, the distance metric normally cannot be used.

1297
01:11:22,080 --> 01:11:24,080
 I cannot say it cannot be used.

1298
01:11:24,080 --> 01:11:29,080
 Actually, distance, there are some limitations if we use the distance metric.

1299
01:11:29,080 --> 01:11:30,080
 Okay.

1300
01:11:30,080 --> 01:11:33,080
 And actually, many classified use the distance metric, right.

1301
01:11:33,080 --> 01:11:41,080
 And even in the, you know, just now when we talk about the evaluation of a classroom, we'll also use a distance concept.

1302
01:11:41,080 --> 01:11:42,080
 Okay.

1303
01:11:42,080 --> 01:11:49,080
 But if the data contains nominal features, then the distance is not very meaningful.

1304
01:11:49,080 --> 01:11:58,080
 But we still can use, but just actually, when you will use the distance metric and maybe the performance is not the best.

1305
01:11:58,080 --> 01:11:59,080
 Not optimal performance.

1306
01:11:59,080 --> 01:12:01,080
 But we can still use that.

1307
01:12:01,080 --> 01:12:02,080
 Okay.

1308
01:12:02,080 --> 01:12:09,080
 And in particular, if there are not a number of features, but only a few number of features that are nominal.

1309
01:12:09,080 --> 01:12:10,080
 Okay.

1310
01:12:10,080 --> 01:12:18,080
 In that tree, and this scenario, and then the influence of the nominal feature is not very big.

1311
01:12:18,080 --> 01:12:19,080
 Okay.

1312
01:12:19,080 --> 01:12:20,080
 So we can still use that.

1313
01:12:20,080 --> 01:12:23,080
 But maybe just the performance is not optimal.

1314
01:12:23,080 --> 01:12:24,080
 Okay.

1315
01:12:24,080 --> 01:12:26,080
 So these are the data types.

1316
01:12:26,080 --> 01:12:27,080
 Okay.

1317
01:12:27,080 --> 01:12:28,080
 So discrete, continuous.

1318
01:12:28,080 --> 01:12:29,080
 Okay.

1319
01:12:29,080 --> 01:12:30,080
 Or categorical, discrete.

1320
01:12:30,080 --> 01:12:31,080
 Okay.

1321
01:12:31,080 --> 01:12:34,080
 And we have nominal, we have an ordinal, right.

1322
01:12:34,080 --> 01:12:35,080
 Okay.

1323
01:12:35,080 --> 01:12:39,080
 Another note is that, in the tree, quite often we can have mission values, right.

1324
01:12:39,080 --> 01:12:46,080
 In the sample, we can, some of the values, some of the features for some of the samples, we don't have a value there.

1325
01:12:46,080 --> 01:12:49,080
 So it's called mission value problem.

1326
01:12:49,080 --> 01:12:50,080
 Okay.

1327
01:12:50,080 --> 01:12:53,080
 So before we apply machine learning algorithm to the data, right.

1328
01:12:53,080 --> 01:12:57,080
 So we must actually handle this mission value problem.

1329
01:12:57,080 --> 01:12:58,080
 Okay.

1330
01:12:58,080 --> 01:12:59,080
 We must feel a value.

1331
01:12:59,080 --> 01:13:02,080
 We must feel a value.

1332
01:13:02,080 --> 01:13:06,080
 And actually, of course, the tree to handle the mission value problem.

1333
01:13:06,080 --> 01:13:13,080
 And first of all, the simply method, I need to remove the sample that have mission values.

1334
01:13:13,080 --> 01:13:14,080
 Okay.

1335
01:13:14,080 --> 01:13:16,080
 This is a simplest way, right.

1336
01:13:16,080 --> 01:13:23,080
 But actually, only if the number of samples is small, the number of samples with mission values is small.

1337
01:13:23,080 --> 01:13:30,080
 And then the elimination of these samples, we are not having a significant influence on the final learning result.

1338
01:13:30,080 --> 01:13:35,080
 And then we can just eliminate these samples from the training data, right.

1339
01:13:35,080 --> 01:13:36,080
 Training data set.

1340
01:13:36,080 --> 01:13:37,080
 Okay.

1341
01:13:37,080 --> 01:13:41,080
 If the initial, you know, the training sample is just small, right.

1342
01:13:41,080 --> 01:13:43,080
 For example, we just have hundreds of samples.

1343
01:13:43,080 --> 01:13:47,080
 But actually, like 10, 20 samples are with the mission values.

1344
01:13:47,080 --> 01:13:53,080
 So in such a scenario, we cannot actually just discard these, you know, mission samples with mission values.

1345
01:13:53,080 --> 01:13:58,080
 We should use other methods to handle this mission value problem, right.

1346
01:13:58,080 --> 01:14:02,080
 And for example, we can use the so-called impute method, right.

1347
01:14:02,080 --> 01:14:12,080
 And actually, we can use other samples to estimate the mission values, such as the mean values, the median values, right.

1348
01:14:12,080 --> 01:14:15,080
 To replace the mission values.

1349
01:14:15,080 --> 01:14:20,080
 And if the data is discrete, we don't have median, we don't have a mean value.

1350
01:14:20,080 --> 01:14:25,080
 And then problem, like nominal data, we don't have a mean, median value, we don't have mean value.

1351
01:14:25,080 --> 01:14:26,080
 Okay.

1352
01:14:26,080 --> 01:14:30,080
 Of course, you can carry mean or median value, but this value is a meaningless.

1353
01:14:30,080 --> 01:14:32,080
 But then we can use the mode.

1354
01:14:32,080 --> 01:14:36,080
 The mode actually, the value that moves frequently occur.

1355
01:14:36,080 --> 01:14:37,080
 Actually, that is called mode.

1356
01:14:37,080 --> 01:14:41,080
 We can use the mode to replace actually the mission values.

1357
01:14:41,080 --> 01:14:46,080
 So these are the ways to handle the mission value problem.

1358
01:14:46,080 --> 01:14:49,080
 And then this problem, you know, auto-lias.

1359
01:14:49,080 --> 01:14:57,080
 Auto-lias, that means samples have some, you know, extreme small or extreme large values, right.

1360
01:14:57,080 --> 01:15:03,080
 So these are called, these samples with this kind of data is called actually auto-lias.

1361
01:15:03,080 --> 01:15:09,080
 Of course, if the auto-lias number is small, right, very few samples are auto-lias.

1362
01:15:09,080 --> 01:15:13,080
 Then, of course, you can remove them from the trinium data set.

1363
01:15:13,080 --> 01:15:14,080
 Okay.

1364
01:15:14,080 --> 01:15:24,080
 And if the initial trinium data set is small, right, and we should not actually, you know, just eliminate these auto-lias.

1365
01:15:24,080 --> 01:15:33,080
 And for the very few extreme values, right, we should try to replace this using the median values or mean values, right.

1366
01:15:33,080 --> 01:15:36,080
 Or even the mode to replace.

1367
01:15:36,080 --> 01:15:43,080
 And you can also use other methods to estimate actually like a camping, right.

1368
01:15:43,080 --> 01:15:50,080
 And most of the data is in the range, for example, from zero to one, right, I assume.

1369
01:15:50,080 --> 01:15:54,080
 And then for some of the value in our feature, it could be like 10, 20, right.

1370
01:15:54,080 --> 01:15:56,080
 A dream, you know, camping method.

1371
01:15:56,080 --> 01:15:57,080
 Okay, we see.

1372
01:15:57,080 --> 01:16:03,080
 If greater than one, then we will let the value just set one to that value, right.

1373
01:16:03,080 --> 01:16:04,080
 Okay.

1374
01:16:04,080 --> 01:16:09,080
 So, we have different ways to handle this mission value and the auto-lias problem.

1375
01:16:09,080 --> 01:16:13,080
 Then another issue is the standardization and normalization.

1376
01:16:13,080 --> 01:16:17,080
 Sometimes the features actually have different scales, right.

1377
01:16:17,080 --> 01:16:21,080
 For example, if you measure the body weight, normally it's KG, right.

1378
01:16:21,080 --> 01:16:24,080
 50, 60, 70, 80 KG, right.

1379
01:16:24,080 --> 01:16:32,080
 And if you measure the height, right, maybe 1.7, 1.8, 1.6, something like that, right.

1380
01:16:32,080 --> 01:16:34,080
 The scale are very different, right.

1381
01:16:34,080 --> 01:16:39,080
 So, you put all the features in the same metric, like this metric.

1382
01:16:39,080 --> 01:16:43,080
 As in some of the features with very big scale, we are dominant, right.

1383
01:16:43,080 --> 01:16:50,080
 So, in such a scenario, we need to standardize the normal data, okay.

1384
01:16:50,080 --> 01:16:56,080
 So, all the features are in the similar range.

1385
01:16:56,080 --> 01:16:57,080
 Okay.

1386
01:16:57,080 --> 01:17:05,080
 Standardization is to convert the data X to X prime.

1387
01:17:05,080 --> 01:17:08,080
 And so that the X prime could have a zero mean.

1388
01:17:08,080 --> 01:17:10,080
 Every value is zero.

1389
01:17:10,080 --> 01:17:13,080
 Some could be positive, could be negative, right.

1390
01:17:13,080 --> 01:17:16,080
 But the value is the mean zero.

1391
01:17:16,080 --> 01:17:18,080
 We shift the data, right.

1392
01:17:18,080 --> 01:17:25,080
 And then we also scale the data by dividing the data by actually the sigma, the standardization.

1393
01:17:25,080 --> 01:17:30,080
 So, after this standardization, the data is zero mean and standardization of one.

1394
01:17:30,080 --> 01:17:33,080
 The sigma will become one, okay.

1395
01:17:33,080 --> 01:17:38,080
 It becomes one and the mean will be zero for the X prime.

1396
01:17:38,080 --> 01:17:40,080
 So, this is called standardization.

1397
01:17:40,080 --> 01:17:44,080
 So, another is normalization to normalize the data to a certain range.

1398
01:17:44,080 --> 01:17:49,080
 For example, from zero to one, from minus one to one, so you can decide the range.

1399
01:17:49,080 --> 01:17:50,080
 Okay.

1400
01:17:50,080 --> 01:17:54,080
 So, this is the so-called treat normalization.

1401
01:17:54,080 --> 01:17:55,080
 Okay.

1402
01:17:55,080 --> 01:17:58,080
 But sometimes they don't know the difference in the two concepts.

1403
01:17:58,080 --> 01:18:01,080
 They all call it normalization or standardization.

1404
01:18:01,080 --> 01:18:08,080
 But we know that they are just trying to actually make all the data to a similar range, right.

1405
01:18:08,080 --> 01:18:09,080
 Similar range.

1406
01:18:09,080 --> 01:18:10,080
 Okay.

1407
01:18:10,080 --> 01:18:14,080
 So, this is the first part of the data preparation for machine learning.

1408
01:18:14,080 --> 01:18:16,080
 And then we have a link over this.

1409
01:18:16,080 --> 01:18:18,080
 Okay.

1410
01:18:18,080 --> 01:18:23,080
 And then after that, actually, we started the, you know, the business classifier, right,

1411
01:18:23,080 --> 01:18:29,080
 which is the statistical approach for decision making.

1412
01:18:29,080 --> 01:18:36,080
 Decision making here is just to decide the class of the sample, right, class one, class two, class three.

1413
01:18:36,080 --> 01:18:38,080
 This is the decision making in classification.

1414
01:18:38,080 --> 01:18:39,080
 Okay.

1415
01:18:39,080 --> 01:18:43,080
 So, decision making is based on the probabilities.

1416
01:18:43,080 --> 01:18:46,080
 Actually, it is based on the posterior probabilities.

1417
01:18:46,080 --> 01:18:47,080
 Okay.

1418
01:18:47,080 --> 01:18:50,080
 So, the probability of omega j given x.

1419
01:18:50,080 --> 01:18:53,080
 So, this is actually the posterior probability.

1420
01:18:53,080 --> 01:18:56,080
 Pursuant probability, right, given x.

1421
01:18:56,080 --> 01:18:57,080
 Okay.

1422
01:18:57,080 --> 01:19:04,080
 So, this is actually, you know, equal to the, you know, the new px omega j times p omega j.

1423
01:19:04,080 --> 01:19:07,080
 p omega j is called actually the prior probabilities.

1424
01:19:07,080 --> 01:19:13,080
 So, we can easily estimate the prior probability from the percentage or proportion of the, you

1425
01:19:13,080 --> 01:19:15,080
 know, sample of each class.

1426
01:19:15,080 --> 01:19:16,080
 Okay.

1427
01:19:16,080 --> 01:19:23,080
 So, this is 20% beyond class one, 20%, 30% beyond class two, 30% beyond class three.

1428
01:19:23,080 --> 01:19:26,080
 Or then we can have the prior probability, right.

1429
01:19:26,080 --> 01:19:29,080
 Zero point two, zero point three, zero point five.

1430
01:19:29,080 --> 01:19:31,080
 This is called prior probability.

1431
01:19:31,080 --> 01:19:36,080
 Can be easily estimated from the percentage of the sample in each class, right.

1432
01:19:36,080 --> 01:19:38,080
 And so, from the training data.

1433
01:19:38,080 --> 01:19:39,080
 Okay.

1434
01:19:39,080 --> 01:19:43,080
 So, the key issue actually here is the px given omega j.

1435
01:19:43,080 --> 01:19:48,080
 This is called actually class-candinal probability density function, px omega j.

1436
01:19:48,080 --> 01:19:49,080
 Okay.

1437
01:19:49,080 --> 01:19:53,080
 So, actually normally, you know, we assume the data for normal distribution, right.

1438
01:19:53,080 --> 01:20:00,080
 Then we can specifically form for this px omega j, the class-candinal probability density

1439
01:20:00,080 --> 01:20:01,080
 function.

1440
01:20:01,080 --> 01:20:02,080
 Okay.

1441
01:20:02,080 --> 01:20:07,080
 And in this code, we assume, you know, it's this px, the data for normal distribution,

1442
01:20:07,080 --> 01:20:09,080
 multivariate normal distribution.

1443
01:20:09,080 --> 01:20:10,080
 Okay.

1444
01:20:10,080 --> 01:20:15,080
 Our focus is just to estimate the parameters in this Gaussian function, right.

1445
01:20:15,080 --> 01:20:23,080
 In the Gaussian function, multivariate Gaussian distribution function, we have two parameters.

1446
01:20:23,080 --> 01:20:26,080
 One is the current matrix and that is the mean vector.

1447
01:20:26,080 --> 01:20:27,080
 Okay.

1448
01:20:27,080 --> 01:20:28,080
 Okay.

1449
01:20:28,080 --> 01:20:37,080
 So, this is the, okay.

1450
01:20:37,080 --> 01:20:41,080
 So, this is the, yeah.

1451
01:20:41,080 --> 01:20:46,080
 So, we see there are two parameters, one mean vector and the current matrix.

1452
01:20:46,080 --> 01:20:47,080
 Current matrix.

1453
01:20:47,080 --> 01:20:48,080
 Okay.

1454
01:20:48,080 --> 01:20:52,080
 And also, actually, we know the decision rule, right.

1455
01:20:52,080 --> 01:20:55,080
 And actually, we're based on the probability, right.

1456
01:20:55,080 --> 01:21:00,080
 Actually, after we calculate the posterior probability, then we look at which probability

1457
01:21:00,080 --> 01:21:03,080
 has the maximum value, right, is the maximum.

1458
01:21:03,080 --> 01:21:07,080
 And then we will start to assemble to the corresponding class.

1459
01:21:07,080 --> 01:21:08,080
 Okay.

1460
01:21:08,080 --> 01:21:12,080
 So, and the, often we can use a variant of this.

1461
01:21:12,080 --> 01:21:16,080
 We don't need to calculate the posterior probability.

1462
01:21:16,080 --> 01:21:23,080
 Instead, we can use a joint probability of the omega, the class, and the x joint probability,

1463
01:21:23,080 --> 01:21:26,080
 which is just the product of these two, right.

1464
01:21:26,080 --> 01:21:30,080
 The probability and the class-candinal probability density function.

1465
01:21:31,080 --> 01:21:36,080
 Because the Px is a common factor, right, for all classes.

1466
01:21:36,080 --> 01:21:39,080
 We don't need to calculate that part.

1467
01:21:39,080 --> 01:21:40,080
 Okay.

1468
01:21:40,080 --> 01:21:48,080
 So, and so I think, one, one extension here is the NIO base, right.

1469
01:21:48,080 --> 01:21:54,080
 NIO base actually assume the features or attributes are independent in between.

1470
01:21:54,080 --> 01:21:56,080
 Then we can simplify.

1471
01:21:56,080 --> 01:21:57,080
 We can simplify.

1472
01:21:57,080 --> 01:21:58,080
 Okay.

1473
01:21:58,080 --> 01:22:02,080
 So, this is a multivariate, multivariate, actually, in the normal distribution.

1474
01:22:02,080 --> 01:22:08,080
 Actually, this multivariate normal distribution can be actually decomposed into the multiplication

1475
01:22:08,080 --> 01:22:17,080
 of multiple, multiple, right, and just, multiple, actually, you know, it's a univariate question,

1476
01:22:17,080 --> 01:22:21,080
 the normal distribution function, univariate, right.

1477
01:22:21,080 --> 01:22:29,080
 So, for each of the univariate function, and we have this, right, this is mean, right.

1478
01:22:29,080 --> 01:22:33,080
 Subtract the mean divided by the, you know, the C squared, right.

1479
01:22:33,080 --> 01:22:34,080
 Yeah.

1480
01:22:34,080 --> 01:22:35,080
 The two C squared.

1481
01:22:35,080 --> 01:22:40,080
 So, this is the, actually, the univariate normal distribution function.

1482
01:22:40,080 --> 01:22:44,080
 So, we assume the independence between the features.

1483
01:22:44,080 --> 01:22:53,080
 Then we simplify, actually, the multivariate normal, normal distribution, right, to the

1484
01:22:53,080 --> 01:22:57,080
 multiple, actually, univariate normal distributions.

1485
01:22:57,080 --> 01:23:01,080
 So, this simplified this process.

1486
01:23:01,080 --> 01:23:02,080
 Okay.

1487
01:23:02,080 --> 01:23:09,080
 And so, this is actually the, the mu ij, so j here means, you know, class of omega j, right,

1488
01:23:09,080 --> 01:23:10,080
 class of omega j.

1489
01:23:10,080 --> 01:23:15,080
 So, this i, i here means which feature, okay, feature number one, feature, along feature

1490
01:23:15,080 --> 01:23:18,080
 number two, along feature number three, right.

1491
01:23:18,080 --> 01:23:23,080
 So, this is actually, this is for sample in class j, and along the feature i, this is

1492
01:23:23,080 --> 01:23:24,080
 the mean value.

1493
01:23:24,080 --> 01:23:29,080
 This is along the feature j, i for  in class rj.

1494
01:23:29,080 --> 01:23:31,080
 So, this is the standard.

1495
01:23:31,080 --> 01:23:32,080
 Standard.

1496
01:23:32,080 --> 01:23:33,080
 Okay.

1497
01:23:33,080 --> 01:23:39,080
 So, this is the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,

1498
01:23:40,080 --> 01:23:44,080
 the, the, the, the, the, the, the, the, the.

1499
01:23:44,080 --> 01:23:45,080
 Okay.

1500
01:23:45,080 --> 01:23:51,080
 So, here, I, I, I gave an example to further, I chose, actually, if you have, you know,

1501
01:23:51,080 --> 01:23:53,080
 like a mean vector, right.

1502
01:23:53,080 --> 01:23:55,080
 If you have no, this is actually a quaring metric and actually, the estimation of the

1503
01:23:55,080 --> 01:24:00,080
 quaring metric and the, the mean vector is based on the so called the maximum likelihood

1504
01:24:00,080 --> 01:24:06,080
 measure, right, with the, maximum likelihood, to estimate the parameters of the multivariate

1505
01:24:06,080 --> 01:24:07,080
 normal functions, okay.

1506
01:24:07,080 --> 01:24:08,120
 function.

1507
01:24:08,120 --> 01:24:11,160
 So we estimate the mean value and the current matrix.

1508
01:24:11,160 --> 01:24:14,840
 So current matrix is estimated based on this.

1509
01:24:14,840 --> 01:24:17,360
 And so first, we have the mean value.

1510
01:24:17,360 --> 01:24:20,400
 Mean value here is just average.

1511
01:24:20,400 --> 01:24:22,519
 So we, based on the mean value, then

1512
01:24:22,519 --> 01:24:25,960
 we can calculate current matrix.

1513
01:24:25,960 --> 01:24:28,640
 If a current matrix is given, so this

1514
01:24:28,640 --> 01:24:33,280
 is the diagonal elements on the main diagonal.

1515
01:24:33,280 --> 01:24:34,760
 So this is the four class.

1516
01:24:34,760 --> 01:24:35,920
 Because here is the sigma 1.

1517
01:24:35,920 --> 01:24:37,400
 This is four class 1.

1518
01:24:37,400 --> 01:24:38,400
 Four class 1.

1519
01:24:38,400 --> 01:24:43,840
 So this is the sigma along the feature 1.

1520
01:24:43,840 --> 01:24:46,680
 So this is the standard region along the feature 2.

1521
01:24:46,680 --> 01:24:50,640
 So both are actually for a sample in class 1.

1522
01:24:50,640 --> 01:24:53,480
 So if a given such a current matrix,

1523
01:24:53,480 --> 01:24:57,840
 and this is for class 2, sigma 2.

1524
01:24:57,840 --> 01:25:01,960
 And so this is along feature 2, actually the standard region.

1525
01:25:01,960 --> 01:25:05,880
 So this is along the feature 1, the standard region, 1.188.

1526
01:25:06,920 --> 01:25:12,600
 So given the current matrix, you need to know the mean.

1527
01:25:12,600 --> 01:25:14,920
 What do you mean by know the mean?

1528
01:25:14,920 --> 01:25:17,520
 But this is on the main diagonal.

1529
01:25:17,520 --> 01:25:20,560
 So this is class 1 along feature 1.

1530
01:25:20,560 --> 01:25:21,920
 Sigma 1, 1.

1531
01:25:21,920 --> 01:25:23,280
 Sigma 2, 1.

1532
01:25:23,280 --> 01:25:23,780
 OK.

1533
01:25:26,880 --> 01:25:27,920
 This is the mean.

1534
01:25:27,920 --> 01:25:30,160
 The mean is just all the samples in the same class.

1535
01:25:30,160 --> 01:25:33,600
 We take the average, the mean vector, the centroid.

1536
01:25:33,600 --> 01:25:36,680
 So this is actually for class 1.

1537
01:25:36,680 --> 01:25:39,760
 For class 1, then this mu, the first element,

1538
01:25:39,760 --> 01:25:41,880
 is along the feature number 1.

1539
01:25:41,880 --> 01:25:44,680
 And this is the second value along the feature number 2.

1540
01:25:44,680 --> 01:25:47,880
 So this is mu 1, 1, mu 2, 1.

1541
01:25:47,880 --> 01:25:50,080
 So similar for class 2 samples.

1542
01:25:50,080 --> 01:25:53,280
 So this is along feature number 1, along feature number 2.

1543
01:25:53,280 --> 01:25:56,040
 We have the mean values.

1544
01:25:56,040 --> 01:26:02,280
 So given current matrix and also the mean vector,

1545
01:26:02,280 --> 01:26:07,679
 which will be able to design the basic design rule.

1546
01:26:07,679 --> 01:26:10,120
 And we assume the multi-variable normal distribution

1547
01:26:10,120 --> 01:26:11,200
 function.

1548
01:26:11,200 --> 01:26:14,639
 And also we can assume the dependence between the features.

1549
01:26:14,639 --> 01:26:17,519
 Then we can use the naive base method

1550
01:26:17,519 --> 01:26:19,920
 to classify the samples.

1551
01:26:19,920 --> 01:26:21,240
 OK.

1552
01:26:21,240 --> 01:26:27,240
 So this is the statistical approach

1553
01:26:27,240 --> 01:26:29,559
 for pattern classification.

1554
01:26:29,560 --> 01:26:31,040
 Then the next part, there's the feature

1555
01:26:31,040 --> 01:26:33,360
 of linear dismal analysis.

1556
01:26:33,360 --> 01:26:37,360
 So the objective for feature linear dismal analysis,

1557
01:26:37,360 --> 01:26:39,880
 and later we see the linear support-widen machine,

1558
01:26:39,880 --> 01:26:41,880
 are both linear classifies.

1559
01:26:41,880 --> 01:26:42,520
 Linear classifies.

1560
01:26:42,520 --> 01:26:48,440
 Actually, the feature linear, the base design rule,

1561
01:26:48,440 --> 01:26:50,000
 is not a linear classifier.

1562
01:26:50,000 --> 01:26:52,480
 And then we see the design boundary is not a line.

1563
01:26:52,480 --> 01:26:54,080
 It's a curve.

1564
01:26:54,080 --> 01:26:56,520
 But for the feature linear dismal analysis

1565
01:26:56,520 --> 01:26:59,000
 and the linear support-widen machines,

1566
01:26:59,440 --> 01:27:02,400
 the dismal boundary is a straight line or hyper plane.

1567
01:27:02,400 --> 01:27:03,880
 So they are linear classifier.

1568
01:27:03,880 --> 01:27:07,440
 For all linear classifiers, they have the same form.

1569
01:27:07,440 --> 01:27:11,080
 Js equal to w transpose x plus w is 0.

1570
01:27:11,080 --> 01:27:13,920
 So w determines the projection direction.

1571
01:27:13,920 --> 01:27:17,960
 And also we determine the dismal boundary direction.

1572
01:27:17,960 --> 01:27:20,800
 And w is 0 determines the bias term.

1573
01:27:20,800 --> 01:27:25,880
 W determines the position of these dismal boundaries.

1574
01:27:25,880 --> 01:27:27,880
 So in the feature linear dismal analysis,

1575
01:27:27,880 --> 01:27:30,200
 we try to find such a w.

1576
01:27:30,200 --> 01:27:34,240
 And so that after projection of all sum on this w,

1577
01:27:34,240 --> 01:27:39,120
 and we could have the maximum between class difference,

1578
01:27:39,120 --> 01:27:40,600
 between class separation.

1579
01:27:40,600 --> 01:27:45,920
 We could have the minimum within class similarity.

1580
01:27:45,920 --> 01:27:48,720
 And within class minimum similarity,

1581
01:27:48,720 --> 01:27:54,480
 within class dissimilarity.

1582
01:27:55,480 --> 01:27:58,320
 So this is a big idea.

1583
01:27:58,320 --> 01:28:01,360
 We want to have the maximum between class separation

1584
01:28:01,360 --> 01:28:07,959
 and the minimum within class separate difference.

1585
01:28:07,959 --> 01:28:12,320
 Then in the feature linear dismal analysis,

1586
01:28:12,320 --> 01:28:17,839
 they try to find such a w so that this ratio makes more.

1587
01:28:17,839 --> 01:28:21,839
 So one is a lot better, another is smaller the better.

1588
01:28:21,840 --> 01:28:23,840
 So here they use the ratio.

1589
01:28:23,840 --> 01:28:25,840
 The ratio, the numerator, the larger the better.

1590
01:28:25,840 --> 01:28:27,840
 The denominator, the smaller the better.

1591
01:28:27,840 --> 01:28:30,840
 The ratio of course, the larger the better.

1592
01:28:30,840 --> 01:28:34,840
 So they find that this is a big idea of the feature linear

1593
01:28:34,840 --> 01:28:36,840
 dismal analysis.

1594
01:28:36,840 --> 01:28:38,840
 I think previously when we talk about each method,

1595
01:28:38,840 --> 01:28:41,840
 I think probably you cannot connect them, cannot link them.

1596
01:28:41,840 --> 01:28:43,840
 But now you have learned all the methods.

1597
01:28:43,840 --> 01:28:45,840
 You have learned linear class.

1598
01:28:45,840 --> 01:28:47,840
 You have learned the class, super linear.

1599
01:28:47,840 --> 01:28:49,840
 You have learned unsupertional.

1600
01:28:50,840 --> 01:28:52,840
 You have learned unsupertional class.

1601
01:28:52,840 --> 01:28:54,840
 You can also compare the similarities.

1602
01:28:54,840 --> 01:28:57,840
 And in particular, when we talk about the between class

1603
01:28:57,840 --> 01:29:01,840
 or between class separation, and within class,

1604
01:29:01,840 --> 01:29:05,840
 or within class difference or variance,

1605
01:29:05,840 --> 01:29:07,840
 you can link them.

1606
01:29:10,840 --> 01:29:14,840
 So here we have two very important metrics.

1607
01:29:14,840 --> 01:29:17,840
 One is called between class scatter.

1608
01:29:18,840 --> 01:29:20,840
 So this is the between class.

1609
01:29:20,840 --> 01:29:25,840
 We have two main vectors, two main vectors.

1610
01:29:25,840 --> 01:29:28,840
 And then, so what is how to calculate the scatter,

1611
01:29:28,840 --> 01:29:31,840
 the sb, b means between class scatter, right?

1612
01:29:31,840 --> 01:29:33,840
 Between class scatter metrics.

1613
01:29:33,840 --> 01:29:35,840
 You know how to calculate, right?

1614
01:29:35,840 --> 01:29:39,840
 And also for each class, we can have a within class scatter

1615
01:29:39,840 --> 01:29:41,840
 metrics.

1616
01:29:41,840 --> 01:29:45,840
 For all samples, look at it based on the centroid, right?

1617
01:29:46,840 --> 01:29:51,840
 And then we can calculate the within class scatter.

1618
01:29:51,840 --> 01:29:54,840
 For each of the two classes, we have a within class scatter.

1619
01:29:54,840 --> 01:29:57,840
 So the summation, so we have sw, w here,

1620
01:29:57,840 --> 01:29:59,840
 I mean within class, right?

1621
01:29:59,840 --> 01:30:04,840
 So within class scatter metrics, b is between class scatter metrics.

1622
01:30:04,840 --> 01:30:05,840
 OK.

1623
01:30:05,840 --> 01:30:10,840
 Of course, the way to find w, and we can use a few ways.

1624
01:30:10,840 --> 01:30:17,840
 In the class, we talk about the general value problem, right?

1625
01:30:17,840 --> 01:30:19,840
 By solving general value problem,

1626
01:30:19,840 --> 01:30:24,840
 or solving the normal value problem, we can find w.

1627
01:30:24,840 --> 01:30:28,840
 And also we have a finder, they introduce another way, right?

1628
01:30:28,840 --> 01:30:29,840
 Another way.

1629
01:30:29,840 --> 01:30:33,840
 That is, sw inverse m1 and m2.

1630
01:30:33,840 --> 01:30:36,840
 Actually, these are easy to understand, right?

1631
01:30:36,840 --> 01:30:40,840
 Because we want to know m1 and m2, right?

1632
01:30:40,840 --> 01:30:43,840
 Something represented between class, right?

1633
01:30:43,840 --> 01:30:47,840
 m1 minus m2, OK, between class.

1634
01:30:47,840 --> 01:30:49,840
 Sw is within class.

1635
01:30:49,840 --> 01:30:54,840
 So again, for calculation of the w, we use a two-part of information.

1636
01:30:54,840 --> 01:30:55,840
 Between class.

1637
01:30:55,840 --> 01:30:59,840
 So here between class, we use m1 minus m2.

1638
01:30:59,840 --> 01:31:02,840
 Within class, we use sw to represent.

1639
01:31:02,840 --> 01:31:05,840
 So in the calculation of the w, we still use a two-part of information.

1640
01:31:05,840 --> 01:31:08,840
 Between class, within class, OK?

1641
01:31:08,840 --> 01:31:12,840
 And within class, sw should be smaller than better, right?

1642
01:31:12,840 --> 01:31:14,840
 Within normally compact, right?

1643
01:31:14,840 --> 01:31:15,840
 So we use an inverse.

1644
01:31:15,840 --> 01:31:17,840
 Inverse, something like a divider, right?

1645
01:31:17,840 --> 01:31:20,840
 But just because it's a metric, we cannot divide the metric.

1646
01:31:20,840 --> 01:31:22,840
 We still use an inverse.

1647
01:31:22,840 --> 01:31:24,840
 So I think so m1 and the larger the better.

1648
01:31:24,840 --> 01:31:27,840
 Sw inverse is smaller the better, right?

1649
01:31:27,840 --> 01:31:28,840
 So we use this part.

1650
01:31:28,840 --> 01:31:30,840
 OK, the compactness, right?

1651
01:31:30,840 --> 01:31:33,840
 Sw, the compactness, OK?

1652
01:31:33,840 --> 01:31:37,840
 So I think for this formula, I think it's easy to understand, right?

1653
01:31:37,840 --> 01:31:40,840
 m1 minus m1, no, it's not SB, right?

1654
01:31:40,840 --> 01:31:42,840
 If you use SB, inverse, that's wrong.

1655
01:31:42,840 --> 01:31:46,840
 m1, m2 represent between class.

1656
01:31:46,840 --> 01:31:50,840
 Then you should not use another metric just for between class, right?

1657
01:31:50,840 --> 01:31:55,840
 So here we should use a metric representing within class.

1658
01:31:55,840 --> 01:31:59,840
 OK, so that is sw.

1659
01:31:59,840 --> 01:32:02,840
 OK, and then of course this is for the determination of w.

1660
01:32:02,840 --> 01:32:06,840
 The next part is determined the w0, the bias term.

1661
01:32:06,840 --> 01:32:14,840
 So the basic idea is that we hope that the middle point of the two centroid is on the dissonant boundary.

1662
01:32:14,840 --> 01:32:17,840
 On the dissonant boundary means that the gx should be 0, right?

1663
01:32:17,840 --> 01:32:22,840
 So that means that w transpose x plus w0 equals x equal to the middle of the two.

1664
01:32:22,840 --> 01:32:25,840
 That is m1 plus m2 divided by 2, right?

1665
01:32:25,840 --> 01:32:30,840
 Then plus w0 should be equal to 0, should be 0, right?

1666
01:32:31,840 --> 01:32:35,840
 Then based on that, actually we can calculate this w0 value.

1667
01:32:35,840 --> 01:32:38,840
 So this is the way to determine the bias term.

1668
01:32:38,840 --> 01:32:40,840
 Of course, here we have one assumption.

1669
01:32:40,840 --> 01:32:47,840
 So that is the spread of the two classes are the same or very similar, the spread.

1670
01:32:47,840 --> 01:32:51,840
 But if one class spreads very big, another class spreads small,

1671
01:32:51,840 --> 01:32:57,840
 and then this value, w0, actually is not the best value.

1672
01:32:57,840 --> 01:33:03,840
 If you only give a rough value, then along this value, we can plus, we can minus, right?

1673
01:33:03,840 --> 01:33:06,840
 We can do, you know, try a few values to find the best ones.

1674
01:33:06,840 --> 01:33:15,840
 Because this is a formula, this way is assumed the two classes have the same spread.

1675
01:33:15,840 --> 01:33:17,840
 If another variable, another small, right?

1676
01:33:17,840 --> 01:33:19,840
 If you think of the middle point, right?

1677
01:33:19,840 --> 01:33:25,840
 So actually the middle point, actually is not on the dissonant boundary.

1678
01:33:25,840 --> 01:33:30,840
 If you're on the dissonant boundary, then you will have a lot of class-fying areas.

1679
01:33:30,840 --> 01:33:34,840
 So usually this will give us a very rough guide.

1680
01:33:34,840 --> 01:33:40,840
 Then based on this value, we can increase or we can decrease, right?

1681
01:33:40,840 --> 01:33:42,840
 Then we can find the optimal value.

1682
01:33:42,840 --> 01:33:45,840
 We can try a few w0, based on this w0, right?

1683
01:33:45,840 --> 01:33:49,840
 We can try a few values to find the best one.

1684
01:33:49,840 --> 01:33:54,840
 Okay, so this is the, I know, the Fisher linear gymnastics.

1685
01:33:54,840 --> 01:33:56,840
 So I think why I introduce this matter?

1686
01:33:56,840 --> 01:34:00,840
 I think this concept is really good, really good, right?

1687
01:34:00,840 --> 01:34:04,840
 And for example, sometimes, for example, the whole fair neural network,

1688
01:34:04,840 --> 01:34:07,840
 when I talk about neural networks, I mean another cost.

1689
01:34:07,840 --> 01:34:13,840
 This whole fair neural network is very old, some don't think, but I think the idea is very good.

1690
01:34:13,840 --> 01:34:16,840
 So for years, I always talk about that.

1691
01:34:16,840 --> 01:34:19,840
 But indeed, this year, they won the Nobel Prize, right?

1692
01:34:19,840 --> 01:34:22,840
 I think, because this idea is really good.

1693
01:34:22,840 --> 01:34:25,840
 So here, I also think for the Fisher linear gymnastics analysis,

1694
01:34:25,840 --> 01:34:27,840
 I think the idea is very good.

1695
01:34:27,840 --> 01:34:30,840
 So we should know, right, the between class, within class,

1696
01:34:30,840 --> 01:34:33,840
 between class, within class, okay?

1697
01:34:33,840 --> 01:34:37,840
 In many different methods, even super, unsupervised, we all use this concept.

1698
01:34:37,840 --> 01:34:41,840
 Okay, so here I think this idea is really good.

1699
01:34:41,840 --> 01:34:44,840
 And then another thing we, okay, so here I want to also

1700
01:34:44,840 --> 01:34:48,840
 talk about relationship with the career metric and the scatter metric.

1701
01:34:48,840 --> 01:34:51,840
 Okay, so we have scatter metric, right?

1702
01:34:51,840 --> 01:34:53,840
 We also have a career metric.

1703
01:34:53,840 --> 01:34:55,840
 Actually, there are a relationship between the two.

1704
01:34:55,840 --> 01:35:00,840
 Okay, and actually, we see the career metric, actually,

1705
01:35:00,840 --> 01:35:03,840
 we know we divided by a number of samples.

1706
01:35:03,840 --> 01:35:07,840
 In the scatter metric, we don't divide by the number of samples, right?

1707
01:35:07,840 --> 01:35:11,840
 So this is a relationship between, this SI is a scatter metric,

1708
01:35:11,840 --> 01:35:15,840
 within class scatter metric for class one, right?

1709
01:35:15,840 --> 01:35:20,840
 So this sigma one is a career metric for class one.

1710
01:35:20,840 --> 01:35:25,840
 Okay, here n one over n one, right?

1711
01:35:25,840 --> 01:35:27,840
 Actually, this is actually estimated, it's called,

1712
01:35:27,840 --> 01:35:30,840
 actually, maximum likelihood estimation,

1713
01:35:30,840 --> 01:35:34,840
 maximum likelihood estimation, n one, n one, right?

1714
01:35:34,840 --> 01:35:37,840
 But some, actually, you know, both actually this,

1715
01:35:37,840 --> 01:35:40,840
 u is a one over n one minus one.

1716
01:35:40,840 --> 01:35:43,840
 n one minus one is called the third degree of freedom.

1717
01:35:43,840 --> 01:35:49,840
 So that estimate is called unbiased estimate of the career metric, unbiased.

1718
01:35:49,840 --> 01:35:52,840
 So this is called the maximum likelihood estimate.

1719
01:35:52,840 --> 01:35:56,840
 But if n is big, right, one over n minus one or one over n,

1720
01:35:56,840 --> 01:35:59,840
 that doesn't make much difference, right?

1721
01:35:59,840 --> 01:36:02,840
 In particular, I think for a large value of n.

1722
01:36:02,840 --> 01:36:07,840
 And in most of the, in all the tourbos, like in my lab,

1723
01:36:07,840 --> 01:36:09,840
 when we talk about career metric,

1724
01:36:09,840 --> 01:36:13,840
 what they gave actually is unbiased estimate.

1725
01:36:13,840 --> 01:36:18,840
 So they divided by n minus one, n one minus one.

1726
01:36:18,840 --> 01:36:23,840
 Okay, so this is just the relationship between the career metric and the scatter metric, right?

1727
01:36:23,840 --> 01:36:28,840
 And actually in the official linear determinacy,

1728
01:36:28,840 --> 01:36:30,840
 we use the scatter metric.

1729
01:36:30,840 --> 01:36:34,840
 And in the previous tree, we used actually,

1730
01:36:34,840 --> 01:36:39,840
 we used in the Gaussian, which is more,

1731
01:36:39,840 --> 01:36:44,840
 in the Gaussian basis tree, you know, the basic decision rule, right?

1732
01:36:44,840 --> 01:36:48,840
 So in the previous tree, for the multi-weather Gaussian function,

1733
01:36:48,840 --> 01:36:50,840
 we use the career metric.

1734
01:36:50,840 --> 01:36:52,840
 So this shows the relationship.

1735
01:36:52,840 --> 01:36:55,840
 So don't confuse that too, okay?

1736
01:36:55,840 --> 01:37:00,840
 Okay, so the next method actually,

1737
01:37:00,840 --> 01:37:03,840
 we have started to call linear support wind machines, okay?

1738
01:37:03,840 --> 01:37:07,840
 And so the basic idea is for,

1739
01:37:07,840 --> 01:37:10,840
 again, this is a standard form for linear classifier, right?

1740
01:37:10,840 --> 01:37:12,840
 So they just use different ones.

1741
01:37:12,840 --> 01:37:15,840
 No w, b, b represented bias, okay?

1742
01:37:15,840 --> 01:37:20,840
 And so the basic idea here, they try to find a w

1743
01:37:20,840 --> 01:37:23,840
 so that the margin of separation makes mine, right?

1744
01:37:23,840 --> 01:37:26,840
 So this is the basic idea of the support wind machine,

1745
01:37:26,840 --> 01:37:29,840
 is to maximize the margin of separation.

1746
01:37:29,840 --> 01:37:31,840
 So what is the margin of the two classes?

1747
01:37:31,840 --> 01:37:35,840
 Actually, the distance between the two closed samples, right?

1748
01:37:35,840 --> 01:37:37,840
 No, no, actually the boundary,

1749
01:37:37,840 --> 01:37:39,840
 no, no, no, you direct the distance, right?

1750
01:37:39,840 --> 01:37:43,840
 So they use another hyperplane by part of the snare sample,

1751
01:37:43,840 --> 01:37:45,840
 then they use this distance, right?

1752
01:37:45,840 --> 01:37:48,840
 So this is called a distance, right?

1753
01:37:48,840 --> 01:37:52,840
 And between, so this is the distance,

1754
01:37:52,840 --> 01:37:54,840
 this is the margin of separation.

1755
01:37:54,840 --> 01:37:58,840
 So we want to find w so that the margin of separation is maximized.

1756
01:37:58,840 --> 01:38:04,840
 Under the condition that all samples could be classified correctly, right?

1757
01:38:04,840 --> 01:38:07,840
 So linear support wind machine can be summarized

1758
01:38:07,840 --> 01:38:09,840
 as a constant or a musician problem.

1759
01:38:09,840 --> 01:38:12,840
 So we first actually did a constant,

1760
01:38:12,840 --> 01:38:17,840
 and this linear constant, every sample should be classified correctly.

1761
01:38:17,840 --> 01:38:24,840
 And this condition, we minimize actually the square norm of the w,

1762
01:38:24,840 --> 01:38:29,840
 which is inversely proportional to the, you know, the margin of separation, right?

1763
01:38:29,840 --> 01:38:31,840
 The margin of separation maximized.

1764
01:38:31,840 --> 01:38:39,840
 So here is a, actually, minimize the square norm of the w.

1765
01:38:39,840 --> 01:38:43,840
 So this is a, you know, for the separable case.

1766
01:38:43,840 --> 01:38:46,840
 And for the non-separable case, actually,

1767
01:38:46,840 --> 01:38:49,840
 we introduce a select variable, right?

1768
01:38:49,840 --> 01:38:50,840
 Select variable.

1769
01:38:50,840 --> 01:38:55,840
 And so in such a scenario, we call the margin soft, margin soft.

1770
01:38:55,840 --> 01:38:56,840
 Okay.

1771
01:38:56,840 --> 01:39:00,840
 And so here in the loss function, we introduce one extra term.

1772
01:39:00,840 --> 01:39:01,840
 Okay.

1773
01:39:01,840 --> 01:39:03,840
 So this is a seed, a hyperparameter, right?

1774
01:39:03,840 --> 01:39:10,840
 So we have this, actually, here is a quasi-i, the select variable.

1775
01:39:10,840 --> 01:39:13,840
 So this is a non-separable case.

1776
01:39:13,840 --> 01:39:20,840
 So in all cases, actually, you know, we can find the w, right?

1777
01:39:20,840 --> 01:39:22,840
 And actually, the very important, you know,

1778
01:39:22,840 --> 01:39:28,840
 carrier 6 of these smaller machines, then the w, this is no present w, actually,

1779
01:39:28,840 --> 01:39:32,840
 is a linear summation of the support vectors.

1780
01:39:32,840 --> 01:39:34,840
 It's a w-summiner support vector.

1781
01:39:34,840 --> 01:39:37,840
 Because here we have support vector xi, right?

1782
01:39:37,840 --> 01:39:39,840
 Summation of the extra.

1783
01:39:39,840 --> 01:39:42,840
 By each of the support vectors, we have a corresponding w.

1784
01:39:42,840 --> 01:39:44,840
 And this w contains two parts.

1785
01:39:44,840 --> 01:39:47,840
 One is the class label of this class support vector.

1786
01:39:47,840 --> 01:39:52,840
 Another is the Lagrange multiplied alpha, alpha, right?

1787
01:39:52,840 --> 01:39:57,840
 So alpha, corresponding alpha times the class label.

1788
01:39:57,840 --> 01:40:01,840
 So this is a weightage for this support vector.

1789
01:40:01,840 --> 01:40:04,840
 And then each support vector has a corresponding weight, right?

1790
01:40:04,840 --> 01:40:09,840
 So finally, we get a summation of all the support vectors.

1791
01:40:09,840 --> 01:40:11,840
 So we get this w.

1792
01:40:11,840 --> 01:40:16,840
 So this w is a linear summation of the support vectors.

1793
01:40:16,840 --> 01:40:23,840
 We know most of the samples will have a corresponding alpha with a zero value.

1794
01:40:23,840 --> 01:40:28,840
 So normally, a small number of samples will have a non-zero alpha.

1795
01:40:28,840 --> 01:40:31,840
 So this sample are called support vectors.

1796
01:40:31,840 --> 01:40:36,840
 We just use this part of sample to determine this w.

1797
01:40:36,840 --> 01:40:38,840
 To determine this w, right?

1798
01:40:38,840 --> 01:40:39,840
 OK.

1799
01:40:39,840 --> 01:40:45,840
 So w is determined by the linear summation of the support vectors.

1800
01:40:45,840 --> 01:40:48,840
 Summation.

1801
01:40:48,840 --> 01:40:49,840
 Summation.

1802
01:40:49,840 --> 01:40:50,840
 Get a sum of them, right?

1803
01:40:50,840 --> 01:40:52,840
 Summation.

1804
01:40:52,840 --> 01:40:54,840
 So this is not the real summation.

1805
01:40:54,840 --> 01:40:58,840
 Each support vector has a corresponding weight.

1806
01:40:58,840 --> 01:41:07,840
 This weight is just a product of the class label with a greater alpha, the corresponding alpha value.

1807
01:41:07,840 --> 01:41:08,840
 OK.

1808
01:41:08,840 --> 01:41:12,840
 So this is the calculation of this w.

1809
01:41:12,840 --> 01:41:16,840
 For the bias term, I think bias term calculation is easy, right?

1810
01:41:16,840 --> 01:41:20,840
 Because if we substitute the support vector into the non-discipline function,

1811
01:41:20,840 --> 01:41:24,840
 the value should be equal to 1 or minus 5, equal to class label, right?

1812
01:41:24,840 --> 01:41:29,840
 So for each of the support vectors, we can calculate a bias term, right?

1813
01:41:29,840 --> 01:41:33,840
 So because of some calculation, rounding areas or whatever,

1814
01:41:33,840 --> 01:41:37,840
 so you may use a different support vector.

1815
01:41:37,840 --> 01:41:42,840
 Maybe we can get a slightly different trade bias term, but b.

1816
01:41:42,840 --> 01:41:43,840
 OK.

1817
01:41:43,840 --> 01:41:48,840
 So normally, we can use each of the samples to calculate b, right?

1818
01:41:48,840 --> 01:41:51,840
 And then finally, we take the average.

1819
01:41:51,840 --> 01:41:57,840
 So we substitute this w, and not any support vector, into the discriminant function.

1820
01:41:57,840 --> 01:42:01,840
 W transpose x plus b equals 1 or minus 1.

1821
01:42:01,840 --> 01:42:04,840
 Equal to di, right? di.

1822
01:42:04,840 --> 01:42:05,840
 So you can get.

1823
01:42:05,840 --> 01:42:09,840
 So this is the way to calculate b, the bias term.

1824
01:42:09,840 --> 01:42:10,840
 OK.

1825
01:42:10,840 --> 01:42:11,840
 So this is for the separable case.

1826
01:42:11,840 --> 01:42:16,840
 In the non-separable case, when we introduce the quasi-i, right?

1827
01:42:17,840 --> 01:42:22,840
 So this is the zeta i, which is the slight variable.

1828
01:42:22,840 --> 01:42:27,840
 And therefore, some of the, I think for the calculation of the w, the same.

1829
01:42:27,840 --> 01:42:30,840
 We still use all the support vectors, right?

1830
01:42:30,840 --> 01:42:34,840
 But for the calculation of the b, we don't use all support vectors.

1831
01:42:34,840 --> 01:42:37,840
 We only use some of the support vectors, right?

1832
01:42:37,840 --> 01:42:39,840
 We support vectors.

1833
01:42:39,840 --> 01:42:40,840
 That is the alpha.

1834
01:42:40,840 --> 01:42:42,840
 Of course, the alpha should be greater than 0.

1835
01:42:42,840 --> 01:42:45,840
 Otherwise, this should not be a support vector, right?

1836
01:42:45,840 --> 01:42:51,840
 And we use only those part of support vectors with the corresponding alpha, less than the

1837
01:42:51,840 --> 01:42:55,840
 c, which is the c, not in the loss function, right?

1838
01:42:55,840 --> 01:43:00,840
 And we, for the slight variable zeta i, and we have a weight c.

1839
01:43:00,840 --> 01:43:03,840
 So this is the c value.

1840
01:43:03,840 --> 01:43:04,840
 OK.

1841
01:43:04,840 --> 01:43:10,840
 So in this range, I think only for those part of the support vector, and we can use to calculate

1842
01:43:10,840 --> 01:43:13,840
 the bias term.

1843
01:43:13,840 --> 01:43:17,840
 So this is the support vector.

1844
01:43:17,840 --> 01:43:21,840
 So for support vector, I think first you need to understand the basic idea, right?

1845
01:43:21,840 --> 01:43:23,840
 So what's the basic idea?

1846
01:43:23,840 --> 01:43:29,840
 The maximization of the model separation, and then they summarize the support vector machine

1847
01:43:29,840 --> 01:43:32,840
 into a constant omission problem, right?

1848
01:43:32,840 --> 01:43:36,840
 For linear separable case, for nonlinear separable case, OK?

1849
01:43:36,840 --> 01:43:39,840
 So this is called a primal problem, right?

1850
01:43:39,840 --> 01:43:44,840
 And also, we have the concept of the dual problem, right?

1851
01:43:44,840 --> 01:43:52,840
 And we just have to solve a quadratic programming problem to find alpha values.

1852
01:43:52,840 --> 01:43:53,840
 OK.

1853
01:43:53,840 --> 01:43:56,840
 The loss function is only a function of the alpha.

1854
01:43:56,840 --> 01:43:57,840
 OK.

1855
01:43:57,840 --> 01:43:59,840
 So this is a dual problem.

1856
01:43:59,840 --> 01:44:00,840
 OK.

1857
01:44:00,840 --> 01:44:01,840
 So we find alpha values.

1858
01:44:01,840 --> 01:44:06,840
 Once we know the alpha values, and then actually we know the corresponding support vector,

1859
01:44:06,840 --> 01:44:10,840
 then we can calculate this w and this b.

1860
01:44:10,840 --> 01:44:11,840
 OK.

1861
01:44:11,840 --> 01:44:14,840
 So we need to know this, OK?

1862
01:44:14,840 --> 01:44:15,840
 OK.

1863
01:44:15,840 --> 01:44:18,840
 The next part, I think, is a classification tree.

1864
01:44:18,840 --> 01:44:23,840
 And so here, I think the answer to this question, right?

1865
01:44:23,840 --> 01:44:26,840
 You know, to build a tree, right?

1866
01:44:26,840 --> 01:44:28,840
 So these are some key points.

1867
01:44:28,840 --> 01:44:35,840
 And actually, normally to determine actually which variable to use and to ask what question

1868
01:44:35,840 --> 01:44:36,840
 in a node, right?

1869
01:44:36,840 --> 01:44:41,840
 Actually, we're based on the drop of the impurity.

1870
01:44:41,840 --> 01:44:42,840
 OK.

1871
01:44:42,840 --> 01:44:44,840
 So we have two methods for impurity measures.

1872
01:44:44,840 --> 01:44:49,840
 One is the entropy impurity, another is the gene impurity.

1873
01:44:49,840 --> 01:44:52,840
 So the total impurity, right?

1874
01:44:52,840 --> 01:45:02,840
 So after we divide the data using this attribute, and ask this question about this attribute,

1875
01:45:02,840 --> 01:45:06,840
 we can have the maximum reduction of the impurity.

1876
01:45:06,840 --> 01:45:12,840
 And then we can select that variable and ask that question to divide the data.

1877
01:45:12,840 --> 01:45:13,840
 OK.

1878
01:45:13,840 --> 01:45:19,840
 And also in this part of the tree, we have studied the random forest, right?

1879
01:45:19,840 --> 01:45:24,840
 And this also, I think, now is very popular, very popular.

1880
01:45:24,840 --> 01:45:32,840
 So actually, in the random forest, actually, now we have multiple trees, right?

1881
01:45:32,840 --> 01:45:40,840
 And each of the trees is built on part of the data, a random selection from the original data, right?

1882
01:45:40,840 --> 01:45:45,840
 So this random data tree is the so-called, based on the boost-raping technique.

1883
01:45:45,840 --> 01:45:49,840
 It is kind of a sampling technique, sample with replacement.

1884
01:45:49,840 --> 01:45:52,840
 You collect this sample, then you put it back, right?

1885
01:45:52,840 --> 01:45:57,840
 Then you have another chance to get this sample be selected.

1886
01:45:57,840 --> 01:45:58,840
 OK.

1887
01:45:58,840 --> 01:46:00,840
 Then another is aggregating.

1888
01:46:00,840 --> 01:46:02,840
 Aggregating is the final stage.

1889
01:46:02,840 --> 01:46:06,840
 When you, from each of the trees, we have a decision, decision, right?

1890
01:46:06,840 --> 01:46:08,840
 We have a classification result.

1891
01:46:08,840 --> 01:46:10,840
 So finally, we need to combine all the results.

1892
01:46:10,840 --> 01:46:13,840
 So this aggregating, we normally use a majority of voting.

1893
01:46:13,840 --> 01:46:14,840
 OK.

1894
01:46:14,840 --> 01:46:18,840
 So boost-raping means, actually, the kind of sampling technique.

1895
01:46:18,840 --> 01:46:22,840
 And we sample the data, then we build a tree.

1896
01:46:22,840 --> 01:46:23,840
 OK.

1897
01:46:23,840 --> 01:46:25,840
 Like this.

1898
01:46:25,840 --> 01:46:28,840
 Training data, we sample the data, right?

1899
01:46:28,840 --> 01:46:35,840
 Randomly select a certain number of data to get subset number one, randomly select another set, right?

1900
01:46:35,840 --> 01:46:37,840
 So total, we have n set.

1901
01:46:37,840 --> 01:46:41,840
 Then based on each of the set, we can build a classification tree.

1902
01:46:41,840 --> 01:46:43,840
 And how to build classification tree?

1903
01:46:43,840 --> 01:46:45,840
 Just the one we have, the method we have studied, right?

1904
01:46:45,840 --> 01:46:49,840
 And actually, for the classification tree, we have introduced how to build a tree, right?

1905
01:46:49,840 --> 01:46:54,840
 So here, based on each of the subset, we should build a tree.

1906
01:46:54,840 --> 01:47:02,840
 But of course, in the random forest, each of the trees use different features or attributes.

1907
01:47:02,840 --> 01:47:03,840
 OK.

1908
01:47:03,840 --> 01:47:06,840
 So after you get these samples, actually, not all features are used.

1909
01:47:06,840 --> 01:47:11,840
 You randomly select some of the features to build a tree.

1910
01:47:11,840 --> 01:47:12,840
 OK.

1911
01:47:12,840 --> 01:47:15,840
 So here, you can see all the data are different.

1912
01:47:15,840 --> 01:47:19,840
 The features used are different from one tree to another tree.

1913
01:47:19,840 --> 01:47:20,840
 OK.

1914
01:47:20,840 --> 01:47:25,840
 So finally, we combine the result, right, from all trees.

1915
01:47:25,840 --> 01:47:27,840
 And then there's regression.

1916
01:47:27,840 --> 01:47:34,840
 I think we also talk about regression, and read regression, and also the multi-regression.

1917
01:47:34,840 --> 01:47:38,840
 The only release square estimate for the parameter estimation.

1918
01:47:38,840 --> 01:47:39,840
 OK.

1919
01:47:40,840 --> 01:47:46,840
 So classifier performance evaluation.

1920
01:47:46,840 --> 01:47:51,840
 And for the method that we talked about, for you, right, hold out method, repeat hold out.

1921
01:47:51,840 --> 01:47:56,840
 And care food cross validation, repeat the care food cross validation.

1922
01:47:56,840 --> 01:47:58,840
 And also the leave out.

1923
01:47:58,840 --> 01:48:00,840
 It should be clear about each of these methods, right?

1924
01:48:00,840 --> 01:48:02,840
 Hold out, randomly select, right?

1925
01:48:02,840 --> 01:48:06,840
 And then we have repeated hold out.

1926
01:48:06,840 --> 01:48:07,840
 Care food cross validation.

1927
01:48:07,840 --> 01:48:09,840
 Divide the data into care food.

1928
01:48:09,840 --> 01:48:14,840
 Care food is used, actually, once as a testing data, right?

1929
01:48:14,840 --> 01:48:16,840
 You repeat these current care times.

1930
01:48:16,840 --> 01:48:19,840
 So that is a care food cross validation.

1931
01:48:19,840 --> 01:48:20,840
 Leave out.

1932
01:48:20,840 --> 01:48:22,840
 All of them for the metrics, right?

1933
01:48:22,840 --> 01:48:30,840
 Accuracy, error rate, selectivity, specificity, precision recall, and F score.

1934
01:48:30,840 --> 01:48:31,840
 OK.

1935
01:48:31,840 --> 01:48:32,840
 So, yeah.

1936
01:48:33,840 --> 01:48:34,840
 OK.

1937
01:48:34,840 --> 01:48:35,840
 So, yeah.

1938
01:48:35,840 --> 01:48:37,840
 This is one example.

1939
01:48:37,840 --> 01:48:39,840
 And the test is playing.

1940
01:48:39,840 --> 01:48:48,840
 I think to enhance your understanding about this, actually, like precision recall and F score.

1941
01:48:48,840 --> 01:48:53,840
 I didn't even notice you can have a look at that.

1942
01:48:53,840 --> 01:48:54,840
 OK.

1943
01:48:54,840 --> 01:48:57,840
 So that's actually about feature subsets selection, right?

1944
01:48:57,840 --> 01:49:05,840
 And actually, in this part, actually, we have some very important concept, like cursor of the minority.

1945
01:49:05,840 --> 01:49:09,840
 Cursor of the minority have two issues.

1946
01:49:09,840 --> 01:49:13,840
 Actually, there are two issues related to the cursor of the minority.

1947
01:49:13,840 --> 01:49:19,840
 One is actually in the hard terminal space, there is a concentration, different concentration.

1948
01:49:19,840 --> 01:49:25,840
 So another actually is, I think, the sparsity of the data, right?

1949
01:49:25,840 --> 01:49:32,840
 You can imagine in a hard terminal space, the data could be very spars.

1950
01:49:32,840 --> 01:49:36,840
 So this is actually, and also actually, the picking phenomena, right?

1951
01:49:36,840 --> 01:49:39,840
 Just not in the quiz, we have the picking phenomena, right?

1952
01:49:39,840 --> 01:49:43,840
 So this picking phenomena can guide us on the data collection, right?

1953
01:49:43,840 --> 01:49:45,840
 And also on feature selection.

1954
01:49:45,840 --> 01:49:49,840
 So we should try to collect as much as possible of the data.

1955
01:49:49,840 --> 01:49:54,840
 And if the data is limited, then we should not select a limited number of features, right?

1956
01:49:54,840 --> 01:50:01,840
 So this will give us some guidance on the features, on the selection and the data collection.

1957
01:50:01,840 --> 01:50:07,840
 And also in the video feature evaluation selection, and also the feature subsets selection.

1958
01:50:07,840 --> 01:50:12,840
 We need to have an answer on all these concepts, OK?

1959
01:50:12,840 --> 01:50:16,840
 And also in a feature subsets selection algorithm, we have two components.

1960
01:50:16,840 --> 01:50:21,840
 One is a set algorithm, another is a evaluation criteria, OK?

1961
01:50:21,840 --> 01:50:29,840
 And actually the set algorithm is to generate a candidate feature subset, OK?

1962
01:50:29,840 --> 01:50:35,840
 And then the evaluation criteria is to evaluate the goodness of each of the generated candidate features subset.

1963
01:50:35,840 --> 01:50:40,840
 And then finally, we'll find the best performing subset, OK?

1964
01:50:40,840 --> 01:50:43,840
 So this is the two components.

1965
01:50:43,840 --> 01:50:50,840
 And so based on the evaluation criteria, actually all the matter can be classified into two categories.

1966
01:50:50,840 --> 01:50:52,840
 One is called field of matter.

1967
01:50:52,840 --> 01:50:55,840
 So we don't use actually the pent-clustering result.

1968
01:50:55,840 --> 01:51:04,840
 We use other metrics like the Mahanobhi distance or other separability measures, like based on the within class,

1969
01:51:04,840 --> 01:51:09,840
 keta matrix, between class, keta matrix, we can have some separability measures.

1970
01:51:09,840 --> 01:51:13,840
 So based on separability measure or based on Mahanobhi distance, right?

1971
01:51:13,840 --> 01:51:17,840
 We can actually evaluate the performance, the goodness of feature subset.

1972
01:51:17,840 --> 01:51:25,840
 We can also actually, based on the classification result to evaluate the performance of the subset.

1973
01:51:25,840 --> 01:51:28,840
 So that is called wrapper method.

1974
01:51:28,840 --> 01:51:33,840
 OK, so it depends on actually the evaluation criteria that are classified.

1975
01:51:33,840 --> 01:51:37,840
 All the matter can be classified into wrapper or field of matter.

1976
01:51:37,840 --> 01:51:44,840
 You're based on the set algorithm, then all the matter can be classified into optimal, suboptimal.

1977
01:51:44,840 --> 01:51:49,840
 For suboptimal, then we have forward search selection by one nation.

1978
01:51:49,840 --> 01:51:53,840
 Forward selection, that means that we start from empty set.

1979
01:51:53,840 --> 01:51:57,840
 Every iteration, we select one feature.

1980
01:51:57,840 --> 01:52:05,840
 Then we gradually build up the features of set until actually the solving criteria is satisfied.

1981
01:52:05,840 --> 01:52:12,840
 The solving criteria could be a certain number of features or could be the improvement of the criteria.

1982
01:52:12,840 --> 01:52:16,840
 The trivial after the addition of one extra feature.

1983
01:52:16,840 --> 01:52:21,840
 Then the back-rolling nation is to remove the features one by one.

1984
01:52:21,840 --> 01:52:30,840
 So we start from the full feature set, then we remove one at each step until the solving criteria is satisfied.

1985
01:52:30,840 --> 01:52:36,840
 Then the classroom, the four types of classroom algorithm, right?

1986
01:52:36,840 --> 01:52:44,840
 Central-based method, distribution-based method, hierarchical classroom, and also the denser-based classroom.

1987
01:52:44,840 --> 01:52:48,840
 For each of the methods, we introduce one.

1988
01:52:48,840 --> 01:52:55,840
 K-means, agglomerative, hierarchical classroom, DB scan, Gaussian-Mister model.

1989
01:52:55,840 --> 01:53:04,840
 I think for the evaluation, I don't give the formula, but this part we just talked about in the last session.

1990
01:53:04,840 --> 01:53:15,840
 I don't list all the equations here, but you need to understand each of these evaluation metrics.

1991
01:53:15,840 --> 01:53:17,840
 Silhouette coefficient.

1992
01:53:17,840 --> 01:53:22,840
 If you use the old nodes, you need to use the updated nodes.

1993
01:53:22,840 --> 01:53:28,840
 Because in the updated nodes, I introduced the updated silhouette coefficient part.

1994
01:53:28,840 --> 01:53:32,840
 But the one in the old feature, actually, old nodes are not wrong.

1995
01:53:32,840 --> 01:53:35,840
 But actually, they are just a variant, right?

1996
01:53:35,840 --> 01:53:39,840
 But most commonly, we use this one, you introduce this today.

1997
01:53:39,840 --> 01:53:41,840
 Silhouette coefficient.

1998
01:53:41,840 --> 01:53:47,840
 Dine index, a DB index, series chartry index, this different index, right?

1999
01:53:47,840 --> 01:53:52,840
 And the advantage of this index is that we have a peak value, right?

2000
01:53:52,840 --> 01:53:54,840
 We have peak values.

2001
01:53:54,840 --> 01:54:03,840
 And then we can easily determine the number of clusters underlying the data.

2002
01:54:03,840 --> 01:54:04,840
 Peak value.

2003
01:54:04,840 --> 01:54:06,840
 We look at the peak, right?

2004
01:54:06,840 --> 01:54:08,840
 Okay.

2005
01:54:08,840 --> 01:54:12,840
 Okay, I think that's revision.

2006
01:54:12,840 --> 01:54:20,840
 The main, the key point, the main content of the machine learning part.

2007
01:54:20,840 --> 01:54:22,840
 Okay.

2008
01:54:22,840 --> 01:54:24,840
 So that's actually, we have a 10-minute break.

2009
01:54:24,840 --> 01:54:27,840
 So after break, actually, we do zoom.

2010
01:54:27,840 --> 01:54:30,840
 If you bring laptop, then probably you can log into the zoom.

2011
01:54:30,840 --> 01:54:33,840
 Using the link I sent to you in the announcement, right?

2012
01:54:33,840 --> 01:54:38,840
 And then you can use the chat function to type your questions.

2013
01:54:38,840 --> 01:54:42,840
 Okay, then I shoot a question on the board and then I'll answer your questions.

2014
01:54:42,840 --> 01:54:44,840
 Okay.

2015
01:54:52,840 --> 01:54:54,840
 Okay.

2016
01:55:22,840 --> 01:55:48,840
 I think the, I just reviewed the machine learning part, right?

2017
01:55:48,840 --> 01:55:50,840
 I treated general rhythm.

2018
01:55:50,840 --> 01:55:52,840
 One question, exam question.

2019
01:55:52,840 --> 01:55:54,840
 One question from the genetic algorithm part.

2020
01:55:54,840 --> 01:55:57,840
 Three questions from the machine learning part in the final exam.

2021
01:55:57,840 --> 01:55:59,840
 Okay.

2022
01:56:20,840 --> 01:56:22,840
 Okay.

2023
01:56:50,840 --> 01:56:52,840
 Okay.

2024
01:57:20,840 --> 01:57:22,840
 Okay.

2025
01:57:50,840 --> 01:57:52,840
 Okay.

2026
01:58:20,840 --> 01:58:22,840
 Okay.

2027
01:58:50,840 --> 01:58:52,840
 Okay.

2028
01:59:20,840 --> 01:59:22,840
 Okay.

2029
01:59:50,840 --> 01:59:52,840
 Okay.

2030
02:00:20,840 --> 02:00:22,840
 Okay.

2031
02:00:50,840 --> 02:00:52,840
 Okay.

2032
02:01:20,840 --> 02:01:22,840
 Okay.

2033
02:01:50,840 --> 02:01:52,840
 Okay.

2034
02:02:20,840 --> 02:02:22,840
 Okay.

2035
02:02:50,840 --> 02:02:52,840
 Okay.

2036
02:03:20,840 --> 02:03:22,840
 Okay.

2037
02:03:50,840 --> 02:03:52,840
 Okay.

2038
02:04:20,840 --> 02:04:22,840
 Okay.

2039
02:04:50,840 --> 02:04:52,840
 Okay.

2040
02:05:20,840 --> 02:05:22,840
 Okay.

2041
02:05:22,840 --> 02:05:24,840
 I think I ask a question.

2042
02:05:24,840 --> 02:05:26,840
 Right?

2043
02:05:26,840 --> 02:05:46,840
 Can I ask for another question from the preparation group?

2044
02:05:46,840 --> 02:05:49,840
 Any more questions?

2045
02:05:49,840 --> 02:05:54,840
 I think you are in the Monday morning class.

2046
02:05:54,840 --> 02:05:57,840
 Actually, a lot of questions actually answer the whole hour,

2047
02:05:57,840 --> 02:05:59,840
 actually, for the questions.

2048
02:05:59,840 --> 02:06:01,840
 I think you can also ask questions, right?

2049
02:06:01,840 --> 02:06:12,840
 Yeah, I think you better bring the calculator.

2050
02:06:12,840 --> 02:06:17,840
 In the exam, you cannot use the handphone, right?

2051
02:06:17,840 --> 02:06:22,840
 You cannot bring the handphone to the class room,

2052
02:06:22,840 --> 02:06:28,840
 to the examination hall, but you need to bring the calculator.

2053
02:06:28,840 --> 02:06:40,840
 Yeah.

2054
02:06:40,840 --> 02:06:41,840
 So, more questions.

2055
02:06:41,840 --> 02:06:44,840
 Are there any more questions?

2056
02:06:44,840 --> 02:06:54,840
 Any kind of questions that you can ask?

2057
02:06:54,840 --> 02:07:01,840
 Yeah.

2058
02:07:01,840 --> 02:07:04,840
 I think, yeah, if you understand these,

2059
02:07:04,840 --> 02:07:07,840
 actually, then you can write it out, right?

2060
02:07:07,840 --> 02:07:09,840
 Yeah, I think like, which way?

2061
02:07:09,840 --> 02:07:13,840
 Like, covariance matrix, or like, I think the, you know,

2062
02:07:13,840 --> 02:07:15,840
 the within class can measure between class, right?

2063
02:07:15,840 --> 02:07:17,840
 If you understand the meaning,

2064
02:07:17,840 --> 02:07:29,840
 I think it's easy to write out them.

2065
02:07:29,840 --> 02:07:40,840
 So, I don't know.

2066
02:07:40,840 --> 02:07:41,840
 Classroom method, right?

2067
02:07:41,840 --> 02:07:43,840
 Actually, normally in the exam,

2068
02:07:43,840 --> 02:07:46,840
 we should expect a standard answer, right?

2069
02:07:46,840 --> 02:07:50,840
 So, we know the question will be very specific.

2070
02:07:50,840 --> 02:07:52,840
 If you ask you to use what method, like,

2071
02:07:52,840 --> 02:07:55,840
 what distance, or whatever, which is specific.

2072
02:07:55,840 --> 02:08:03,840
 Yeah.

2073
02:08:03,840 --> 02:08:04,840
 Come on, questions, right?

2074
02:08:04,840 --> 02:08:15,840
 You can ask questions.

2075
02:08:15,840 --> 02:08:28,840
 No questions?

2076
02:08:28,840 --> 02:08:29,840
 So, you ask.

2077
02:08:29,840 --> 02:08:31,840
 Don't ask others to ask, right?

2078
02:08:31,840 --> 02:08:40,840
 You ask questions.

2079
02:08:40,840 --> 02:08:43,840
 In the past years, actually, I think a lot of questions.

2080
02:08:43,840 --> 02:08:46,840
 This year, actually, the morning, morning class,

2081
02:08:46,840 --> 02:08:53,840
 lot of questions.

2082
02:08:53,840 --> 02:08:55,840
 So, you ask questions, then clarify.

2083
02:08:55,840 --> 02:08:58,840
 Then everyone should benefit.

2084
02:08:58,840 --> 02:09:20,840
 So, that means you make a contribution, right?

2085
02:09:20,840 --> 02:09:23,840
 So, if you don't actually, you know, have a computer,

2086
02:09:23,840 --> 02:09:26,840
 I think you ask questions directly, right?

2087
02:09:27,840 --> 02:09:31,840
 So, anyway, any form, right?

2088
02:09:31,840 --> 02:09:34,840
 Anyway, you can ask questions.

2089
02:09:34,840 --> 02:09:55,840
 Any questions?

2090
02:09:55,840 --> 02:10:08,840
 I think NTU, NTU, I think the 40 marks is past 40.

2091
02:10:08,840 --> 02:10:10,840
 But the result here is different from in China.

2092
02:10:10,840 --> 02:10:13,840
 Now, you can see all 80 marks, 90 marks, right?

2093
02:10:13,840 --> 02:10:15,840
 So, here, almost impossible.

2094
02:10:15,840 --> 02:10:21,840
 So, I think this year is like 75, the A minus, Gp is 4.5.

2095
02:10:22,840 --> 02:10:27,840
 80 is 8, 8 is 5 upon 5.

2096
02:10:27,840 --> 02:10:29,840
 So, you can imagine, right?

2097
02:10:29,840 --> 02:10:32,840
 So, actually, you get 80 marks, it's very hard.

2098
02:10:32,840 --> 02:10:34,840
 75 is also hard.

2099
02:10:34,840 --> 02:10:37,840
 So, 40 marks is a past.

2100
02:10:37,840 --> 02:10:40,840
 40, 40, yeah.

2101
02:10:41,840 --> 02:10:51,840
 But if you just pass, you cannot have a degree, right?

2102
02:10:51,840 --> 02:10:56,840
 Actually, if you get a degree, you need to be about 2.5, right?

2103
02:10:56,840 --> 02:11:00,840
 2.5, maybe, I think you need to get 50 something, right?

2104
02:11:11,840 --> 02:11:14,840
 Yeah, basically, I think it's a learning part, I think.

2105
02:11:14,840 --> 02:11:16,840
 Should be, let's go.

2106
02:11:16,840 --> 02:11:20,840
 So, which part I miss, or I think that's a little bit of a moment.

2107
02:11:20,840 --> 02:11:22,840
 I think that's an important point, right?

2108
02:11:22,840 --> 02:11:24,840
 We need to master in this course.

2109
02:11:24,840 --> 02:11:28,840
 But maybe some part, I don't know where I miss or not.

2110
02:11:32,840 --> 02:11:35,840
 Yeah, yeah, that's right, that's right, yeah.

2111
02:11:35,840 --> 02:11:38,840
 So, if you get familiar with all the countertits today,

2112
02:11:38,840 --> 02:11:41,840
 at least you can pass a lot, right?

2113
02:11:41,840 --> 02:11:43,840
 Not a problem to pass, yeah.

2114
02:11:52,840 --> 02:11:55,840
 Questions? No questions?

2115
02:11:58,840 --> 02:12:00,840
 Any questions?

2116
02:12:09,840 --> 02:12:11,840
 Yeah, maybe I think you recorded it, right?

2117
02:12:11,840 --> 02:12:14,840
 Because on the web, so maybe you can look at that.

2118
02:12:14,840 --> 02:12:18,840
 Yeah, normally that is lecture recording, right?

2119
02:12:26,840 --> 02:12:28,840
 I'm not very sure, actually.

2120
02:12:28,840 --> 02:12:29,840
 Sure, I think.

2121
02:12:29,840 --> 02:12:31,840
 I have no idea, actually.

2122
02:12:31,840 --> 02:12:34,840
 Can you see the yesterday recording?

2123
02:12:34,840 --> 02:12:36,840
 You cannot see.

2124
02:12:39,840 --> 02:12:44,840
 Maybe a check whether I can share this part.

2125
02:12:44,840 --> 02:12:46,840
 You ask questions, right?

2126
02:12:46,840 --> 02:12:48,840
 You ask questions, then you, Benny, I think,

2127
02:12:48,840 --> 02:12:50,840
 or other benefits.

2128
02:12:50,840 --> 02:12:53,840
 You ask questions, benefits just from those groups, right?

2129
02:12:53,840 --> 02:12:55,840
 If you ask questions, then I've uploaded today's video,

2130
02:12:55,840 --> 02:12:58,840
 then other groups also benefit from your question, right?

2131
02:12:58,840 --> 02:13:01,840
 Then you ask questions, try to think about which you don't understand,

2132
02:13:01,840 --> 02:13:04,840
 or you want me to clarify.

2133
02:13:08,840 --> 02:13:10,840
 Yeah.

2134
02:13:21,840 --> 02:13:23,840
 Yeah, I think then, yeah, I think,

2135
02:13:23,840 --> 02:13:27,840
 I think today, I did a revision at least some main point, right?

2136
02:13:27,840 --> 02:13:30,840
 So at least I think this part needs to understand a lot, yeah.

2137
02:13:31,840 --> 02:13:33,840
 Yeah.

2138
02:13:37,840 --> 02:13:39,840
 So today's revision, at least the important point,

2139
02:13:39,840 --> 02:13:41,840
 this, that's, yeah.

2140
02:13:41,840 --> 02:13:43,840
 Yeah.

2141
02:13:56,840 --> 02:13:59,840
 Okay, no questions?

2142
02:13:59,840 --> 02:14:01,840
 Yeah.

2143
02:14:08,840 --> 02:14:10,840
 Sorry.

2144
02:14:16,840 --> 02:14:19,840
 Yeah, I think we need to have calculated,

2145
02:14:19,840 --> 02:14:21,840
 calculate, calculate, right?

2146
02:14:21,840 --> 02:14:23,840
 And to calculate, like,

2147
02:14:23,840 --> 02:14:25,840
 what kind of instruction?

2148
02:14:35,840 --> 02:14:39,840
 Normally in the exam, you can bring nothing.

2149
02:14:39,840 --> 02:14:41,840
 You cannot bring anything to the,

2150
02:14:41,840 --> 02:14:45,840
 to the, yeah, to the examination hall.

2151
02:14:48,840 --> 02:14:50,840
 Yeah, they have found an announcement.

2152
02:14:50,840 --> 02:14:52,840
 Normally, you know, actually normally, you know,

2153
02:14:52,840 --> 02:14:55,840
 in the very big examination hall, you can bring the bag,

2154
02:14:55,840 --> 02:14:57,840
 but you can, you need to put a bag in one side,

2155
02:14:57,840 --> 02:15:00,840
 in one corner of the examination hall.

2156
02:15:00,840 --> 02:15:02,840
 You cannot bring anything, any paper.

2157
02:15:02,840 --> 02:15:04,840
 I think last year, I think in the examination hall,

2158
02:15:04,840 --> 02:15:08,840
 two students, I think, yeah, I think,

2159
02:15:08,840 --> 02:15:11,840
 what I called about the, in the major data,

2160
02:15:11,840 --> 02:15:14,840
 you bring, I just remember a paper,

2161
02:15:14,840 --> 02:15:19,840
 and you write some formula on the paper, so, yeah.

2162
02:15:19,840 --> 02:15:21,840
 So you cannot bring anything.

2163
02:15:21,840 --> 02:15:23,840
 Here, the examination is very, very straight.

2164
02:15:23,840 --> 02:15:25,840
 Now, if you, you know, do that,

2165
02:15:25,840 --> 02:15:27,840
 they can see that it's cheating, actually.

2166
02:15:27,840 --> 02:15:29,840
 Why see a research, yeah.

2167
02:15:31,840 --> 02:15:33,840
 So, yeah, you can already bring the computer.

2168
02:15:33,840 --> 02:15:36,840
 Maybe then, no, no, bring, bring the calculator.

2169
02:15:36,840 --> 02:15:39,840
 Calculator, you also need to be certified by, you know,

2170
02:15:39,840 --> 02:15:42,840
 NTU, then, set the office, yeah.

2171
02:15:44,840 --> 02:15:46,840
 And so, you cannot bring the handphone.

2172
02:15:46,840 --> 02:15:47,840
 You cannot use the handphone,

2173
02:15:47,840 --> 02:15:49,840
 or kind of bring any book, reference book,

2174
02:15:49,840 --> 02:15:51,840
 or notes, anything, anything, yeah.

2175
02:15:57,840 --> 02:16:00,840
 In the exam, they will provide the so-called answer book,

2176
02:16:00,840 --> 02:16:02,840
 actually, just like a notebook, actually,

2177
02:16:02,840 --> 02:16:05,840
 many pages, actually, everyone have this.

2178
02:16:05,840 --> 02:16:09,840
 You cannot write your name on the, on the, on the,

2179
02:16:09,840 --> 02:16:10,840
 on the answer book.

2180
02:16:10,840 --> 02:16:14,840
 You can only write your metric calculation number, okay.

2181
02:16:14,840 --> 02:16:18,840
 And also, you need to bring your metric calculation card, okay.

2182
02:16:18,840 --> 02:16:22,840
 You, you, during the examination, actually,

2183
02:16:22,840 --> 02:16:24,840
 no, I really don't know, the emulator,

2184
02:16:24,840 --> 02:16:26,840
 I will also be there as an emulator.

2185
02:16:26,840 --> 02:16:29,840
 I will check, actually, based on the sequence, you know,

2186
02:16:29,840 --> 02:16:33,840
 compare the name, your metric calculation card, actually,

2187
02:16:33,840 --> 02:16:36,840
 to make sure you sit on the right seat,

2188
02:16:36,840 --> 02:16:39,840
 and the seat number, right, you try the right seat,

2189
02:16:39,840 --> 02:16:43,840
 and also, you know, you check, actually,

2190
02:16:43,840 --> 02:16:47,840
 if you actually miss your metric calculation card,

2191
02:16:47,840 --> 02:16:50,840
 probably you need to bring your passport to prove,

2192
02:16:50,840 --> 02:16:52,840
 you know, you, you, yourself, right.

2193
02:16:52,840 --> 02:16:57,840
 Yeah, so, I think in the, in the,

2194
02:16:57,840 --> 02:17:00,840
 in the, during the exam, actually,

2195
02:17:00,840 --> 02:17:03,840
 because this is the first time we've been come to NTU, right.

2196
02:17:03,840 --> 02:17:07,840
 So, probably, you know, you can come to the,

2197
02:17:07,840 --> 02:17:10,840
 you can enter the examination hall,

2198
02:17:10,840 --> 02:17:13,840
 10 minutes before the start of the exam, okay.

2199
02:17:13,840 --> 02:17:15,840
 Then you cannot write down anything,

2200
02:17:15,840 --> 02:17:17,840
 you can just, actually, then they will give instructions,

2201
02:17:17,840 --> 02:17:20,840
 I went to start, I went to stop,

2202
02:17:20,840 --> 02:17:23,840
 after they say stop writing, you cannot write.

2203
02:17:23,840 --> 02:17:26,840
 Here, it's very strict, okay, very strict.

2204
02:17:26,840 --> 02:17:28,840
 You cannot, you cannot actually,

2205
02:17:28,840 --> 02:17:30,840
 because during the quiz, I see some,

2206
02:17:30,840 --> 02:17:33,840
 soon I come in, they write, they write down something, right,

2207
02:17:33,840 --> 02:17:36,840
 they start to write, and then they emulator the student,

2208
02:17:36,840 --> 02:17:39,840
 actually, you know, get the papers,

2209
02:17:39,840 --> 02:17:42,840
 and then give them a new one, don't let them write,

2210
02:17:42,840 --> 02:17:47,840
 but to start writing, until we are not, we start, okay.

2211
02:17:47,840 --> 02:17:49,840
 Yeah.

2212
02:17:54,840 --> 02:17:56,840
 And we don't have,

2213
02:17:56,840 --> 02:17:59,840
 don't have a, we don't have a,

2214
02:17:59,840 --> 02:18:03,840
 kind of a solution, but roughly, you know the answer, right.

2215
02:18:03,840 --> 02:18:06,840
 You know the answer, right, yeah.

2216
02:18:06,840 --> 02:18:08,840
 But actually, something like, if you,

2217
02:18:08,840 --> 02:18:11,840
 if sometimes, if you don't actually, like,

2218
02:18:11,840 --> 02:18:13,840
 I see, or train a neural network,

2219
02:18:13,840 --> 02:18:16,840
 or talk about something, right, just something like,

2220
02:18:16,840 --> 02:18:20,840
 so can you introduce yourself, if I ask a question, right.

2221
02:18:20,840 --> 02:18:23,840
 Of course you can see, my name, you can just see,

2222
02:18:23,840 --> 02:18:28,840
 I'm called actually, like, David, for example, right.

2223
02:18:28,840 --> 02:18:30,840
 This is also an introduction,

2224
02:18:30,840 --> 02:18:33,840
 but you can also give a more complete introduction,

2225
02:18:33,840 --> 02:18:36,840
 or I call David, I'm David, I'm from, you know,

2226
02:18:36,840 --> 02:18:40,840
 scrub, shubo, NTU, and I'm involved in, you know,

2227
02:18:40,840 --> 02:18:43,840
 in which program, you know, it's a more complete introduction,

2228
02:18:43,840 --> 02:18:47,840
 right. So if you, you know, what kind of answer you expect,

2229
02:18:47,840 --> 02:18:51,840
 right, so you ask me, right, how to answer that question,

2230
02:18:51,840 --> 02:18:55,840
 so, I mean, what kind of answer you expect,

2231
02:18:55,840 --> 02:18:58,840
 that should be the answer I expect from you,

2232
02:18:58,840 --> 02:19:02,840
 right, it should be as complete as possible, yeah.

2233
02:19:03,840 --> 02:19:14,840
 Okay, that's my question, good.

2234
02:19:14,840 --> 02:19:24,840
 Okay, let me ask the question.

2235
02:19:24,840 --> 02:19:28,840
 Okay, I think the problem, I think, actually, you know,

2236
02:19:28,840 --> 02:19:32,840
 we know, you know, the basic idea of the support machine

2237
02:19:32,840 --> 02:19:37,840
 is to, you know, to find that W, right, to find the projection

2238
02:19:37,840 --> 02:19:40,840
 and the boundary, and so that, actually,

2239
02:19:40,840 --> 02:19:43,840
 the margin of separation makes a mark.

2240
02:19:43,840 --> 02:19:49,840
 Actually, we know margin of separation is two over the norm of W,

2241
02:19:49,840 --> 02:19:51,840
 right, norm of W, actually.

2242
02:19:51,840 --> 02:19:53,840
 Then, actually, the optimization problem, actually,

2243
02:19:53,840 --> 02:19:58,840
 is that we try to optimize, actually, the minimized.

2244
02:19:58,840 --> 02:20:02,840
 Now, maximize the norm, the margin of separation,

2245
02:20:02,840 --> 02:20:08,840
 but in practice, we minimize the square of the norm, right, of W, okay.

2246
02:20:08,840 --> 02:20:14,840
 Subject to a certain constant, that means all samples are, you know,

2247
02:20:14,840 --> 02:20:16,840
 classified correctly.

2248
02:20:16,840 --> 02:20:19,840
 So, support machine is summarizing to,

2249
02:20:19,840 --> 02:20:23,840
 or converting to a constant optimization problem, okay.

2250
02:20:23,840 --> 02:20:26,840
 So, to solve this constant optimization problem,

2251
02:20:26,840 --> 02:20:29,840
 actually, we need to use the Lagrange Multiply method.

2252
02:20:29,840 --> 02:20:32,840
 If you recall in your year one calculus, right,

2253
02:20:32,840 --> 02:20:36,840
 actually, we learn how to solve a constant optimization problem.

2254
02:20:36,840 --> 02:20:38,840
 You learn Lagrange method.

2255
02:20:38,840 --> 02:20:41,840
 But just, actually, in our previous work, actually,

2256
02:20:41,840 --> 02:20:43,840
 we just tried one variable, two variables,

2257
02:20:43,840 --> 02:20:48,840
 but here, the number of variables, you know, the dimension are very high.

2258
02:20:48,840 --> 02:20:51,840
 Okay, so, no, no, the constant,

2259
02:20:51,840 --> 02:20:54,840
 in previous, we maybe just had one constant,

2260
02:20:54,840 --> 02:20:56,840
 but here, we have a lot number of constant,

2261
02:20:56,840 --> 02:21:00,840
 because each sample needs to be correctly classified.

2262
02:21:00,840 --> 02:21:03,840
 So, this should be a constant, okay.

2263
02:21:03,840 --> 02:21:05,840
 So, this, actually, we have lost function,

2264
02:21:05,840 --> 02:21:07,840
 and then combine the two parts, right.

2265
02:21:07,840 --> 02:21:12,840
 Why is Lagrange multiplier, okay, Lagrange multiplier, okay.

2266
02:21:12,840 --> 02:21:15,840
 Another part, actually, is the, you know, the original, you know,

2267
02:21:15,840 --> 02:21:20,840
 lost function, the square of the norm of the W, okay.

2268
02:21:20,840 --> 02:21:22,840
 So, this is a parameter problem.

2269
02:21:22,840 --> 02:21:26,840
 And the dual problem, actually, through the relationship, right,

2270
02:21:26,840 --> 02:21:31,840
 and also the KTT, you know, like, you know, the optimization theory.

2271
02:21:31,840 --> 02:21:33,840
 And finally, this is no lost function,

2272
02:21:33,840 --> 02:21:36,840
 is converting to another, you know, lost function

2273
02:21:36,840 --> 02:21:40,840
 that contains the Lagrange multiplier only.

2274
02:21:40,840 --> 02:21:43,840
 Previously, we have a few, you know, parameters, right.

2275
02:21:43,840 --> 02:21:45,840
 Why is W zero or B?

2276
02:21:45,840 --> 02:21:47,840
 We have a W parameter.

2277
02:21:47,840 --> 02:21:50,840
 We have multipliers, the alphas, okay.

2278
02:21:50,840 --> 02:21:52,840
 So, this is the parameter problem, right.

2279
02:21:52,840 --> 02:21:55,840
 But after, actually, the conversion in the dual problem,

2280
02:21:55,840 --> 02:21:58,840
 we just have one top of parameters.

2281
02:21:58,840 --> 02:22:00,840
 So, that is just Lagrange multiplier.

2282
02:22:00,840 --> 02:22:02,840
 So, that's the dual problem.

2283
02:22:02,840 --> 02:22:05,840
 So, we solve this, you know, the dual problem to get alphas.

2284
02:22:05,840 --> 02:22:08,840
 And then from alphas, actually, we can get the W, right.

2285
02:22:08,840 --> 02:22:14,840
 The W is just the weight summation of the, of the, the, the support vectors.

2286
02:22:14,840 --> 02:22:18,840
 And the support vectors are those with the non-zero alphas, right.

2287
02:22:18,840 --> 02:22:23,840
 So, with the weight summation of the W, the support vectors,

2288
02:22:23,840 --> 02:22:28,840
 the weight of the support vector is just the class label

2289
02:22:28,840 --> 02:22:31,840
 multiplied, Lagrange multiplied alpha i.

2290
02:22:31,840 --> 02:22:33,840
 So, this is the weight summation of W.

2291
02:22:33,840 --> 02:22:35,840
 So, this is a dual problem.

2292
02:22:35,840 --> 02:22:39,840
 It's just a lost function that contains, actually, the alpha i

2293
02:22:39,840 --> 02:22:41,840
 as a coefficient only.

2294
02:22:41,840 --> 02:22:43,840
 Okay.

2295
02:22:43,840 --> 02:22:45,840
 So, these are the difference between the two, right.

2296
02:22:45,840 --> 02:22:47,840
 Yeah.

2297
02:22:47,840 --> 02:22:49,840
 So, this is the difference between the two, right.

2298
02:22:49,840 --> 02:22:54,840
 Because actually, the dual problem is equivalent to the, the primal problem.

2299
02:22:54,840 --> 02:22:56,840
 Equivalent.

2300
02:22:56,840 --> 02:22:58,840
 Okay.

2301
02:22:58,840 --> 02:23:01,840
 Because we, we discovered the relationship between the W's and the alphas, right.

2302
02:23:01,840 --> 02:23:04,840
 So, all the W's are replaced by alphas.

2303
02:23:04,840 --> 02:23:08,840
 And finally, we get one, actually, the lost function contains,

2304
02:23:08,840 --> 02:23:10,840
 contains alpha only.

2305
02:23:10,840 --> 02:23:13,840
 So, that is a dual problem.

2306
02:23:13,840 --> 02:23:15,840
 Okay.

2307
02:23:15,840 --> 02:23:20,840
 Please.

2308
02:23:20,840 --> 02:23:22,840
 Any kind of question, right.

2309
02:23:22,840 --> 02:23:24,840
 This is probably the last chance, right.

2310
02:23:24,840 --> 02:23:27,840
 We, we made, actually, after that, I will see you in the examination hall, right.

2311
02:23:27,840 --> 02:23:32,840
 On the 29th of November, right.

2312
02:23:32,840 --> 02:23:34,840
 So, you have a question, actually, you can ask in the class.

2313
02:23:34,840 --> 02:23:40,840
 Any question?

2314
02:23:40,840 --> 02:23:42,840
 Yeah.

2315
02:23:42,840 --> 02:23:44,840
 No, no need, no need.

2316
02:23:44,840 --> 02:23:51,840
 Actually, I'm, I'm a book, right, into, right, actually, but you kind of calculate in the back of the paper.

2317
02:23:51,840 --> 02:23:53,840
 We just, when we mark, we just look at that.

2318
02:23:53,840 --> 02:23:55,840
 Yeah.

2319
02:23:55,840 --> 02:23:57,840
 Not in the back, yeah.

2320
02:23:57,840 --> 02:23:59,840
 Yeah.

2321
02:23:59,840 --> 02:24:04,840
 If you kind of bring any paper, you can bring a few pens, right.

2322
02:24:04,840 --> 02:24:10,840
 A few pens, the calculators, the water bottles, maybe, you know, maybe a coat, actually,

2323
02:24:10,840 --> 02:24:13,840
 because in the examination hall, it could be a coat.

2324
02:24:13,840 --> 02:24:19,840
 Okay.

2325
02:24:19,840 --> 02:24:34,840
 Yeah.

2326
02:24:34,840 --> 02:24:36,840
 For that part, I don't know, actually, yeah.

2327
02:24:36,840 --> 02:24:37,840
 But actually, I don't know.

2328
02:24:37,840 --> 02:25:06,840
 I'm going to bring one question from the general argument part, three questions from the machine learning part.

2329
02:25:06,840 --> 02:25:12,840
 The answer, actually, I don't know if it's wrong, maybe it's just not complete.

2330
02:25:12,840 --> 02:25:15,840
 I think I don't know what is your mark.

2331
02:25:15,840 --> 02:25:23,840
 But when you calculate the mark, you cannot compare the mindset, you know, I need to be 1890 something, that's not China, right.

2332
02:25:23,840 --> 02:25:25,840
 The mark normally in that range.

2333
02:25:25,840 --> 02:25:31,840
 So here, you know, like, 18 marks is a, you know, GPA is a full file, right.

2334
02:25:31,840 --> 02:25:34,840
 You can see that the standard here, right, is different.

2335
02:25:34,840 --> 02:25:36,840
 You don't see, oh, I just get 75, right.

2336
02:25:36,840 --> 02:25:43,840
 75, like, you know, those like in China, 90 marks like that, because the GPA is full with five, right.

2337
02:25:43,840 --> 02:25:49,840
 75 is already, you know, 4.8, right, 4.5, very good.

2338
02:25:49,840 --> 02:25:53,840
 If 18 marks, the GPA is five, a full mark.

2339
02:25:53,840 --> 02:25:58,840
 A plus 85, A plus is also five GPA.

2340
02:25:58,840 --> 02:26:01,840
 So you need to consider that, actually, not just actually.

2341
02:26:01,840 --> 02:26:11,840
 So I see if you get a mark, actually, you see like 70, 80, and this is because normally, you know, the standard here, you know, we don't give that kind of mark like about 90,

2342
02:26:11,840 --> 02:26:20,840
 unless your, your really answer is perfect, right, because open answer questions, you know, we don't have this kind of perfect answers.

2343
02:26:20,840 --> 02:26:26,840
 Okay, but when you answer the question, I think if you, the mark is not satisfactory, I don't see your answer is wrong.

2344
02:26:26,840 --> 02:26:31,840
 Normally, in most cases, actually, your answer is not complete.

2345
02:26:31,840 --> 02:26:47,840
 Okay, and for example, if you like, you know, the nine, the, you know, multi-variable normal distribution, and then we use the nine bits, right, when you compare the two, actually, you can compare from different aspects, right, like, you know, the computational efficiency,

2346
02:26:47,840 --> 02:26:51,840
 and also for the, for the, you know, primary estimation point of view, right.

2347
02:26:51,840 --> 02:26:58,840
 Actually, if you use the multi-variable normal distribution, how many parameters you can estimate in the current matrix?

2348
02:26:58,840 --> 02:27:06,840
 Then if you use the Gaussian, you know, the nine bits, right, and then how many parameters you can estimate?

2349
02:27:06,840 --> 02:27:14,840
 Even a small number of samples, so how, you know, which should be more accurate, right, you can't expect all these, many aspects.

2350
02:27:14,840 --> 02:27:20,840
 So if you don't get a very high mark, most likely your answer is not complete, not wrong, actually.

2351
02:27:20,840 --> 02:27:24,840
 Yeah, so this kind of question normally is a open answer.

2352
02:27:24,840 --> 02:27:31,840
 Okay, so sometimes you, or train a classifier, train that, you, you need to give a classifier.

2353
02:27:31,840 --> 02:27:40,840
 For example, in the linear distribution analysis, right, in the assignment, I think it's a feature, in the, you know, in the, in the, in the basic decision rule.

2354
02:27:40,840 --> 02:27:52,840
 Some just give, actually, the, you know, the, the mean vectors give the, the current matrix, but you don't give the form of the, the decision, you know, the, the decision rules, right.

2355
02:27:52,840 --> 02:27:56,840
 So that's actually, you know, I mean, actually, you are not wrong, but your answer is not complete.

2356
02:27:56,840 --> 02:27:58,840
 You just give part of the question, right.

2357
02:27:58,840 --> 02:28:04,840
 You are not actually really trained at a, at a discrimination, kind of, you know, decision rule, right.

2358
02:28:04,840 --> 02:28:12,840
 Because actually you just give the mean vector, you don't know, actually you need to put this into a, into a function function.

2359
02:28:12,840 --> 02:28:14,840
 I will assume you don't know.

2360
02:28:14,840 --> 02:28:18,840
 Okay, so this kind of answer is not complete.

2361
02:28:18,840 --> 02:28:20,840
 So we will get some marks.

2362
02:28:20,840 --> 02:28:28,840
 So I think the example, whatever, when I ask a question, you should elaborate, try to give as complete as possible.

2363
02:28:28,840 --> 02:28:31,840
 Just like I asked you, you know, please introduce yourself.

2364
02:28:31,840 --> 02:28:35,840
 Oh, I'm joined. This is a very simple answer, right? It's wrong? No wrong.

2365
02:28:35,840 --> 02:28:46,840
 But even the other way, I'm joined from China with my degree from which university and they are now in NTU, I mean, we're all in which program, right.

2366
02:28:46,840 --> 02:28:51,840
 Then this answer is more complete. Which answer you prefer when you ask a student to introduce yourself?

2367
02:28:51,840 --> 02:28:56,840
 You prefer the complete answer, right. Yeah.

2368
02:28:56,840 --> 02:29:01,840
 So that's the way when you answer a question in the exam, in the assignment, you quit.

2369
02:29:01,840 --> 02:29:08,840
 You should, you, more complete, that means you answer them more about this problem.

2370
02:29:08,840 --> 02:29:20,840
 Okay.

2371
02:29:20,840 --> 02:29:29,840
 And maybe I don't know how to answer your question.

2372
02:29:29,840 --> 02:29:37,840
 And I think in the, in the use of geometry, every year, you know, we need to show some difference, right, on the question.

2373
02:29:37,840 --> 02:29:42,840
 Because the school always check whether this year's different question, the same as last year's or whatever.

2374
02:29:42,840 --> 02:29:44,840
 We cannot have the same question.

2375
02:29:44,840 --> 02:29:49,840
 So last year, we did not test, but I don't know me in this year, we were not test.

2376
02:29:49,840 --> 02:30:02,840
 But sometimes actually, indeed, actually, maybe not easy to, if something you need to know, to use a computer, right, then we cannot test you, right, in the exam.

2377
02:30:02,840 --> 02:30:17,840
 Okay. So, yeah, I think you don't, I think at least the last year, you know, the past year exam papers can give you a basic idea on the format of the exam questions, right.

2378
02:30:17,840 --> 02:30:22,840
 But don't say, in the past, we have this question, this year, we have the same question, no.

2379
02:30:22,840 --> 02:30:31,840
 Actually, we will try to set different questions for different years.

2380
02:30:31,840 --> 02:30:38,840
 But just for some of the questions you can imagine, right, it's easy for us to set an assignment because we can use a computer.

2381
02:30:38,840 --> 02:30:42,840
 But in the exam, maybe not easy, right, yeah.

2382
02:30:54,840 --> 02:30:55,840
 Questions?

2383
02:31:08,840 --> 02:31:20,840
 Sorry.

2384
02:31:20,840 --> 02:31:23,840
 This course is related to the learning model.

2385
02:31:23,840 --> 02:31:24,840
 What?

2386
02:31:24,840 --> 02:31:25,840
 Can you?

2387
02:31:25,840 --> 02:31:27,840
 This course is related to learning model.

2388
02:31:27,840 --> 02:31:29,840
 Do you think it's in the sense?

2389
02:31:29,840 --> 02:31:34,840
 During the market, we give the marks.

2390
02:31:34,840 --> 02:31:36,840
 But finally, based on the mark, they give you ABCD.

2391
02:31:36,840 --> 02:31:40,840
 You just get ABCD that is like the letter graded.

2392
02:31:40,840 --> 02:31:45,840
 You just get this result, letter graded.

2393
02:31:46,840 --> 02:31:52,840
 I think for all the courses, you finally get A or B or C.

2394
02:31:52,840 --> 02:32:04,840
 Yes, actually, we, yeah, then, yeah, of course, actually,

2395
02:32:04,840 --> 02:32:06,840
 initially you get a mark.

2396
02:32:06,840 --> 02:32:11,840
 And you know, 1929 or 1885, this kind of mark.

2397
02:32:11,840 --> 02:32:15,840
 But finally, based on the mark, we get a grade.

2398
02:32:30,840 --> 02:32:31,840
 Question?

2399
02:32:33,840 --> 02:32:34,840
 Any questions?

2400
02:32:42,840 --> 02:32:44,840
 Optimal.

2401
02:32:46,840 --> 02:32:49,840
 Optimal and suboptimal, right?

2402
02:32:49,840 --> 02:32:55,840
 In the, okay, her question is about, for the future sub-sacrination,

2403
02:32:55,840 --> 02:32:59,840
 the optimal method is all the carbonatorals.

2404
02:32:59,840 --> 02:33:05,840
 We have no features, and the current features are X1, X2, Xn, right?

2405
02:33:05,840 --> 02:33:08,840
 So how to find the optimal features?

2406
02:33:08,840 --> 02:33:12,840
 I think based on the set, set of algorithm, right?

2407
02:33:12,840 --> 02:33:15,840
 Based on the set of algorithm, we have the resultive search.

2408
02:33:15,840 --> 02:33:18,840
 That means all the combination, all the possible combinations.

2409
02:33:18,840 --> 02:33:21,840
 So that is an optimal method.

2410
02:33:21,840 --> 02:33:25,840
 And the sequential based method, sequential forward search,

2411
02:33:25,840 --> 02:33:30,840
 or sequential by-willing nation are the suboptimal method.

2412
02:33:30,840 --> 02:33:32,840
 Because actually, we select one first.

2413
02:33:32,840 --> 02:33:34,840
 Then once this one is selected, right?

2414
02:33:34,840 --> 02:33:36,840
 Normally, we don't remove that.

2415
02:33:36,840 --> 02:33:39,840
 Then we select the second one based on the first one.

2416
02:33:39,840 --> 02:33:42,840
 And we select that one based on the first two.

2417
02:33:42,840 --> 02:33:44,840
 So this kind of a suboptimal method.

2418
02:33:44,840 --> 02:33:47,840
 This is called stepwise, stepwise optimal.

2419
02:33:47,840 --> 02:33:49,840
 It's not global optimal.

2420
02:33:49,840 --> 02:33:54,840
 Global optimal, we need to use the resultive search.

2421
02:33:54,840 --> 02:33:56,840
 All the possible combinations.

2422
02:33:56,840 --> 02:33:59,840
 Normally, this is all possible combinations.

2423
02:33:59,840 --> 02:34:01,840
 This number is huge, right?

2424
02:34:01,840 --> 02:34:05,840
 Then you need to evaluate a huge number of features subset.

2425
02:34:05,840 --> 02:34:08,840
 So this normally is a computational variable.

2426
02:34:08,840 --> 02:34:11,840
 It's passive.

2427
02:34:11,840 --> 02:34:16,840
 So this method is possible only the initial feature subset.

2428
02:34:16,840 --> 02:34:21,840
 The feature set, the full set, only have a small number of features

2429
02:34:21,840 --> 02:34:23,840
 that we can use.

2430
02:34:23,840 --> 02:34:25,840
 Yesterday, actually, something asked us, actually,

2431
02:34:25,840 --> 02:34:27,840
 when we should use a back-willing nation,

2432
02:34:27,840 --> 02:34:30,840
 when we should use a forward selection, right?

2433
02:34:30,840 --> 02:34:33,840
 And actually, these depend on application.

2434
02:34:33,840 --> 02:34:36,840
 For example, initially, you have a huge number of features, right?

2435
02:34:36,840 --> 02:34:38,840
 But normally, in the final,

2436
02:34:38,840 --> 02:34:41,840
 we just use a small number of features, right?

2437
02:34:41,840 --> 02:34:44,840
 So that means that we best use a forward search.

2438
02:34:44,840 --> 02:34:46,840
 Because forward search, for example, initially,

2439
02:34:46,840 --> 02:34:48,840
 we have 1,000 features.

2440
02:34:48,840 --> 02:34:51,840
 Maybe finally, we just need to use 20.

2441
02:34:51,840 --> 02:34:53,840
 Even if we use a forward search, right?

2442
02:34:53,840 --> 02:34:56,840
 We just need to repeat all the first one, the second one,

2443
02:34:56,840 --> 02:34:59,840
 the other one with 20.

2444
02:34:59,840 --> 02:35:01,840
 So 20 steps, 20 iterations.

2445
02:35:01,840 --> 02:35:04,840
 We can find the feature set.

2446
02:35:04,840 --> 02:35:06,840
 But if you use a back-willing nation,

2447
02:35:06,840 --> 02:35:10,840
 you need to go through 980 steps, right?

2448
02:35:10,840 --> 02:35:12,840
 From the full search, you remove one.

2449
02:35:12,840 --> 02:35:14,840
 Then you remove the second one.

2450
02:35:14,840 --> 02:35:16,840
 You remove the 98 features.

2451
02:35:16,840 --> 02:35:19,840
 Then you get 20 features remaining.

2452
02:35:19,840 --> 02:35:22,840
 So this is a different application.

2453
02:35:22,840 --> 02:35:27,840
 So how many features will be finally set in the feature subset?

2454
02:35:28,840 --> 02:35:30,840
 But even the number of features,

2455
02:35:30,840 --> 02:35:33,840
 the initial feature set is small, not very big, right?

2456
02:35:33,840 --> 02:35:36,840
 So then you can use other methods.

2457
02:35:36,840 --> 02:35:39,840
 Forward search or back-willing nation.

2458
02:35:39,840 --> 02:35:41,840
 And sometimes, actually, we combine the two.

2459
02:35:41,840 --> 02:35:45,840
 So we select the few, and then we remove some of them.

2460
02:35:45,840 --> 02:35:48,840
 And then, again, for example, we select five,

2461
02:35:48,840 --> 02:35:50,840
 and then we remove two.

2462
02:35:50,840 --> 02:35:51,840
 We get three, right?

2463
02:35:51,840 --> 02:35:54,840
 When we select another five, then we remove two.

2464
02:35:54,840 --> 02:35:57,840
 It's kind of a combination.

2465
02:35:57,840 --> 02:36:01,840
 The forward search and the back-willing nation are combined.

2466
02:36:01,840 --> 02:36:03,840
 So there are different kinds of ways.

2467
02:36:03,840 --> 02:36:05,840
 At least, yeah, search, or go in there.

2468
02:36:15,840 --> 02:36:17,840
 Question, yes, D?

2469
02:36:24,840 --> 02:36:26,840
 No more questions?

2470
02:36:38,840 --> 02:36:40,840
 Sorry.

2471
02:36:54,840 --> 02:36:59,840
 So, I think that's the difference.

2472
02:36:59,840 --> 02:37:04,840
 So I know that there are many

2473
02:37:04,840 --> 02:37:06,840
 teachers that teach the science.

2474
02:37:06,840 --> 02:37:08,840
 I think that's the case.

2475
02:37:08,840 --> 02:37:13,840
 So maybe there's a standard.

2476
02:37:13,840 --> 02:37:16,840
 I think I gave the same standard.

2477
02:37:16,840 --> 02:37:18,840
 But, of course, in the marketing process,

2478
02:37:18,840 --> 02:37:20,840
 actually, they could have their own judgment law.

2479
02:37:20,840 --> 02:37:24,840
 Now, they could be exactly the same law.

2480
02:37:24,840 --> 02:37:27,840
 So I think for a class with 700 students,

2481
02:37:27,840 --> 02:37:30,840
 it's hardly actually for one person to mark.

2482
02:37:30,840 --> 02:37:32,840
 Oh, actually, it's very hard.

2483
02:37:32,840 --> 02:37:34,840
 But in the exam, actually, each marker will mark

2484
02:37:34,840 --> 02:37:36,840
 one single question.

2485
02:37:36,840 --> 02:37:39,840
 So for example, actually, maybe one marker

2486
02:37:39,840 --> 02:37:41,840
 will mark one question.

2487
02:37:41,840 --> 02:37:42,840
 So for all, yeah.

2488
02:37:42,840 --> 02:37:45,840
 But different questions, maybe slightly different standard.

2489
02:37:45,840 --> 02:37:47,840
 Then there's reasonable.

2490
02:37:47,840 --> 02:37:49,840
 And even today, you mark that.

2491
02:37:49,840 --> 02:37:51,840
 Tomorrow, maybe after a few days.

2492
02:37:51,840 --> 02:37:54,840
 I think the standard may be a bit different.

2493
02:37:54,840 --> 02:37:58,840
 But actually, basically, you cannot have a profile of that.

2494
02:37:58,840 --> 02:38:02,840
 Even in China, I go, actually, today, tomorrow,

2495
02:38:02,840 --> 02:38:04,840
 different days, the marketing standard, maybe

2496
02:38:04,840 --> 02:38:06,840
 will change a bit.

2497
02:38:06,840 --> 02:38:08,840
 But actually, the variance should be smaller.

2498
02:38:09,840 --> 02:38:16,840
 The TE, actually, I normally, we have this.

2499
02:38:16,840 --> 02:38:18,840
 I tell them the marketing standard.

2500
02:38:18,840 --> 02:38:21,840
 But of course, actually, even for me, right, actually,

2501
02:38:21,840 --> 02:38:25,840
 you know, from day to day, right, could be changed a bit.

2502
02:38:25,840 --> 02:38:27,840
 I think therefore, TE is the same.

2503
02:38:27,840 --> 02:38:32,840
 But they all have a reason, like, no, no, normally, yeah.

2504
02:38:32,840 --> 02:38:34,840
 Tomorrow, you get that mark.

2505
02:38:39,840 --> 02:38:42,840
 Okay.

2506
02:38:44,840 --> 02:38:46,840
 Any more questions?

2507
02:38:53,840 --> 02:38:54,840
 Okay, even no questions.

2508
02:38:54,840 --> 02:38:55,840
 I think that's all, yeah.

2509
02:38:55,840 --> 02:38:57,840
 So thank you very much.

2510
02:38:57,840 --> 02:39:00,840
 I think this year, I think I'm very happy to teach you.

2511
02:39:00,840 --> 02:39:04,840
 I think that attendance anomaly, I think this class is good.

2512
02:39:04,840 --> 02:39:07,840
 So of course, some of you, I think it's all your choice, right?

2513
02:39:08,840 --> 02:39:10,840
 Someone to attend a physical lecture,

2514
02:39:10,840 --> 02:39:13,840
 and then have some interaction with the professors

2515
02:39:13,840 --> 02:39:15,840
 or with fellow students.

2516
02:39:15,840 --> 02:39:16,840
 I think it's good.

2517
02:39:16,840 --> 02:39:19,840
 But some prefer, because of the career,

2518
02:39:19,840 --> 02:39:21,840
 some are used to kind of online lecture.

2519
02:39:21,840 --> 02:39:23,840
 They prefer to watch a tree.

2520
02:39:23,840 --> 02:39:28,840
 Maybe they prefer to, like, when you are studying here,

2521
02:39:28,840 --> 02:39:31,840
 listen to a lecture, they prefer to watch more way

2522
02:39:31,840 --> 02:39:32,840
 or whatever, right?

2523
02:39:32,840 --> 02:39:35,840
 Then they prefer to use other time to study this course.

2524
02:39:35,840 --> 02:39:39,840
 So that's the, you have your own freedom, right, to do that.

2525
02:39:39,840 --> 02:39:43,840
 So, but actually, I see the attendance of this class is good.

2526
02:39:43,840 --> 02:39:45,840
 So I feel very happy to teach.

2527
02:39:45,840 --> 02:39:47,840
 And of course, actually, for this year,

2528
02:39:47,840 --> 02:39:49,840
 this is quite a big challenge for you, right?

2529
02:39:49,840 --> 02:39:52,840
 Also for the lectures, actually, for the professors,

2530
02:39:52,840 --> 02:39:54,840
 because we have a very big class.

2531
02:39:54,840 --> 02:39:56,840
 And we have more questions, actually.

2532
02:39:56,840 --> 02:39:59,840
 Also, we have all three other prayers,

2533
02:39:59,840 --> 02:40:01,840
 in particular for the Monday class, actually.

2534
02:40:01,840 --> 02:40:03,840
 Actually, I'm informed, I'm well-informed

2535
02:40:03,840 --> 02:40:05,840
 to have the Monday class, actually.

2536
02:40:05,840 --> 02:40:07,840
 Let's see.

2537
02:40:07,840 --> 02:40:09,840
 Because in Singapore, I remember Friday, the public holiday,

2538
02:40:09,840 --> 02:40:11,840
 Saturday, Sunday, right?

2539
02:40:11,840 --> 02:40:16,840
 So we're informed the Monday-Monday class from 8.30 only on Saturday.

2540
02:40:16,840 --> 02:40:18,840
 So it's quite a big challenge.

2541
02:40:18,840 --> 02:40:21,840
 For me, actually, now on Monday-Monday, 8.30, right?

2542
02:40:21,840 --> 02:40:24,840
 I need to be here by 8.00 or 9.00.

2543
02:40:24,840 --> 02:40:27,840
 I need to get up at 6.20.

2544
02:40:27,840 --> 02:40:31,840
 Then breakfast, and then I leave my home at 10.00

2545
02:40:31,840 --> 02:40:38,840
 before 7.20 to make sure I will be in class room by 8.30.

2546
02:40:38,840 --> 02:40:40,840
 It's a big challenge.

2547
02:40:40,840 --> 02:40:42,840
 I think for you, the same, right?

2548
02:40:42,840 --> 02:40:46,840
 I need to come to the classroom for the Monday class

2549
02:40:46,840 --> 02:40:48,840
 very early.

2550
02:40:48,840 --> 02:40:53,840
 But anyway, I think we have gone through this process

2551
02:40:53,840 --> 02:40:57,840
 from your quiz and also the assignment,

2552
02:40:57,840 --> 02:41:00,840
 I think, I've done very well.

2553
02:41:00,840 --> 02:41:03,840
 And today we have the revision, right?

2554
02:41:03,840 --> 02:41:06,840
 You can prepare your exam based on the main points.

2555
02:41:06,840 --> 02:41:08,840
 Actually, you cannot see, based on this,

2556
02:41:08,840 --> 02:41:11,840
 you can get 100 marks, or you cannot never get that, right?

2557
02:41:11,840 --> 02:41:13,840
 But at least the main points are reviewed,

2558
02:41:13,840 --> 02:41:15,840
 actually, in the revision slides.

2559
02:41:15,840 --> 02:41:19,840
 So we can hope that the revision slides can help you

2560
02:41:19,840 --> 02:41:22,840
 to prepare the final exam.

2561
02:41:22,840 --> 02:41:26,840
 Just now, actually, some of you asked questions about the exam, right?

2562
02:41:26,840 --> 02:41:28,840
 Actually, I think that's good.

2563
02:41:28,840 --> 02:41:30,840
 Actually, because this is the first time, right,

2564
02:41:30,840 --> 02:41:33,840
 you take the exam in NTU and you need to know the rules

2565
02:41:33,840 --> 02:41:35,840
 and regulations in NTU.

2566
02:41:35,840 --> 02:41:37,840
 And it's very strict.

2567
02:41:37,840 --> 02:41:39,840
 You cannot bring anything, right?

2568
02:41:39,840 --> 02:41:43,840
 The computer, the handphone, the papers,

2569
02:41:43,840 --> 02:41:46,840
 other material, like reference material,

2570
02:41:46,840 --> 02:41:49,840
 to the examination hall, okay?

2571
02:41:49,840 --> 02:41:52,840
 So, and you can start writing.

2572
02:41:52,840 --> 02:41:54,840
 Only when you are allowed to start writing,

2573
02:41:54,840 --> 02:41:57,840
 you need to stop writing when the announcement, okay?

2574
02:41:57,840 --> 02:42:00,840
 You cannot continue writing.

2575
02:42:00,840 --> 02:42:02,840
 Sometimes I do see some students,

2576
02:42:02,840 --> 02:42:06,840
 but maybe that's okay in your university,

2577
02:42:06,840 --> 02:42:08,840
 and I know where you started your university, right?

2578
02:42:08,840 --> 02:42:11,840
 But now in NTU, I think this is very strict.

2579
02:42:11,840 --> 02:42:14,840
 I should stop writing. You need to stop writing.

2580
02:42:14,840 --> 02:42:18,840
 Okay, so these are the points you need to note in the exam.

2581
02:42:18,840 --> 02:42:21,840
 Okay, so I really appreciate it.

2582
02:42:21,840 --> 02:42:23,840
 Thank you very much. Thank you.

2583
02:42:23,840 --> 02:42:25,840
 Thank you.

2584
02:42:53,840 --> 02:42:56,840
 Thank you.

2585
02:43:23,840 --> 02:43:26,840
 Thank you.

2586
02:43:53,840 --> 02:43:56,840
 Thank you.

2587
02:44:23,840 --> 02:44:26,840
 Thank you.

2588
02:44:53,840 --> 02:44:56,840
 Thank you.

2589
02:45:23,840 --> 02:45:25,840
 Thank you.

2590
02:45:53,840 --> 02:45:56,840
 Thank you.

2591
02:46:23,840 --> 02:46:25,840
 Thank you.

2592
02:46:53,840 --> 02:46:55,840
 Thank you.

2593
02:47:23,840 --> 02:47:25,840
 Thank you.

2594
02:47:53,840 --> 02:47:55,840
 Thank you.

2595
02:48:23,840 --> 02:48:25,840
 Thank you.

2596
02:48:53,840 --> 02:48:55,840
 Thank you.

2597
02:49:23,840 --> 02:49:25,840
 Thank you.

2598
02:49:53,840 --> 02:49:55,840
 Thank you.

2599
02:50:23,840 --> 02:50:25,840
 Thank you.

2600
02:50:53,840 --> 02:50:55,840
 Thank you.

2601
02:51:23,840 --> 02:51:25,840
 Thank you.

2602
02:51:53,840 --> 02:51:55,840
 Thank you.

2603
02:52:23,840 --> 02:52:25,840
 Thank you.

2604
02:52:53,840 --> 02:52:55,840
 Thank you.

2605
02:53:23,840 --> 02:53:25,840
 Thank you.

2606
02:53:53,840 --> 02:53:55,840
 Thank you.

2607
02:54:23,840 --> 02:54:25,840
 Thank you.

2608
02:54:53,840 --> 02:54:55,840
 Thank you.

2609
02:55:23,840 --> 02:55:25,840
 Thank you.

2610
02:55:53,840 --> 02:55:55,840
 Thank you.

2611
02:56:23,840 --> 02:56:25,840
 Thank you.

2612
02:56:53,840 --> 02:56:55,840
 Thank you.

2613
02:57:23,840 --> 02:57:25,840
 Thank you.

2614
02:57:53,840 --> 02:57:55,840
 Thank you.

2615
02:58:23,840 --> 02:58:25,840
 Thank you.

2616
02:58:53,840 --> 02:58:55,840
 Thank you.

2617
02:59:23,840 --> 02:59:25,840
 Thank you.

2618
02:59:53,840 --> 02:59:55,840
 Thank you.

