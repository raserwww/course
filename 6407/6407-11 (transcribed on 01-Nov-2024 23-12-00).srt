1
00:03:30,000 --> 00:03:32,000
 .

2
00:04:30,000 --> 00:04:50,000
 .

3
00:04:51,000 --> 00:04:55,000
 So in the last week, we started regression problem,

4
00:04:55,000 --> 00:05:01,000
 regression, and so basically we started the linear regression.

5
00:05:01,000 --> 00:05:05,000
 And of course, actually in the linear regression,

6
00:05:05,000 --> 00:05:09,000
 the key issue is how to estimate the parameters.

7
00:05:09,000 --> 00:05:14,000
 And actually we started one method that is L squared,

8
00:05:14,000 --> 00:05:17,000
 parameter estimation, B squared.

9
00:05:17,000 --> 00:05:22,000
 We tried to find the parameters and estimation

10
00:05:22,000 --> 00:05:27,000
 so that the squared area is minimized.

11
00:05:27,000 --> 00:05:32,000
 So this is the least squared parameter estimation.

12
00:05:32,000 --> 00:05:37,000
 And so besides this, actually the normal regression,

13
00:05:37,000 --> 00:05:41,000
 and we also started the rigid regression.

14
00:05:41,000 --> 00:05:47,000
 So we put actually one additional term in the loss function.

15
00:05:47,000 --> 00:05:52,000
 In the regular least squared estimation,

16
00:05:52,000 --> 00:05:56,000
 the loss function is just a squared area.

17
00:05:56,000 --> 00:06:00,000
 This area is just the difference between actually the

18
00:06:00,000 --> 00:06:04,000
 target value and the prediction from the model.

19
00:06:04,000 --> 00:06:06,000
 So this is called an area.

20
00:06:06,000 --> 00:06:11,000
 So this squared area is used as the loss function.

21
00:06:11,000 --> 00:06:16,000
 And then in the rigid regression,

22
00:06:16,000 --> 00:06:19,000
 we put one additional term in the loss function.

23
00:06:19,000 --> 00:06:25,000
 And that is just a penalty on the size of the parameters.

24
00:06:25,000 --> 00:06:31,000
 So we put L2 norm of the parameter vector.

25
00:06:31,000 --> 00:06:33,000
 So this is a rigid regression.

26
00:06:33,000 --> 00:06:37,000
 And this rigid regression penalized the size of the parameter.

27
00:06:37,000 --> 00:06:42,000
 And actually it shrinks the parameter towards zeros.

28
00:06:42,000 --> 00:06:48,000
 Because actually the zero squared, the L2 norm, is the minimum.

29
00:06:48,000 --> 00:06:52,000
 But actually we cannot just omit this one criteria.

30
00:06:52,000 --> 00:06:58,000
 We also can see the minimization of the prediction area.

31
00:06:58,000 --> 00:07:02,000
 So the loss function should be the balance of the two.

32
00:07:02,000 --> 00:07:09,000
 So these two functions, actually one of the term is weighted.

33
00:07:09,000 --> 00:07:19,000
 That is we put a lambda and before actually the L2 norm of the parameter vector.

34
00:07:19,000 --> 00:07:24,000
 And then the last one is the loss regression.

35
00:07:24,000 --> 00:07:29,000
 In the loss regression, the penalization of the parameter size

36
00:07:29,000 --> 00:07:34,000
 is in the form of the L1 norm.

37
00:07:34,000 --> 00:07:40,000
 And although actually the loss regression and the regression

38
00:07:40,000 --> 00:07:46,000
 has a very subtle difference, but actually the result is quite different.

39
00:07:46,000 --> 00:07:49,000
 And to push further, actually,

40
00:07:49,000 --> 00:08:00,000
 we put a straight requirement on the value of the parameters.

41
00:08:00,000 --> 00:08:09,000
 That means the shrinks have a stronger shrinkage towards zeros.

42
00:08:09,000 --> 00:08:15,000
 So in this loss regression, not only we estimate the parameter,

43
00:08:15,000 --> 00:08:18,000
 but we also select the variables.

44
00:08:18,000 --> 00:08:23,000
 Because many of the parameters will shrink to zero.

45
00:08:23,000 --> 00:08:25,000
 So this is the loss.

46
00:08:25,000 --> 00:08:28,000
 And after we have obtained a model,

47
00:08:28,000 --> 00:08:31,000
 of course we need to evaluate the performance of the model.

48
00:08:31,000 --> 00:08:36,000
 So in the last week, we also started a few metrics

49
00:08:36,000 --> 00:08:41,000
 that can be used to evaluate the performance of a regression model.

50
00:08:41,000 --> 00:08:47,000
 So the first one is the mean squared error.

51
00:08:47,000 --> 00:08:57,000
 And also the root mean squared error.

52
00:08:57,000 --> 00:09:01,000
 And also we treat the mean absolute error.

53
00:09:01,000 --> 00:09:05,000
 And finally, we also started the r squared and adjust r squared.

54
00:09:05,000 --> 00:09:08,000
 So this is for the performance evaluation.

55
00:09:08,000 --> 00:09:14,000
 And today, we are going to study, actually, look at the model validation.

56
00:09:14,000 --> 00:09:17,000
 Whether a model obtains a valid model or not.

57
00:09:17,000 --> 00:09:19,000
 So this is called a model validation.

58
00:09:19,000 --> 00:09:23,000
 And in this model validation, we need to check, actually,

59
00:09:23,000 --> 00:09:26,000
 the assumptions of linear regression.

60
00:09:26,000 --> 00:09:29,000
 What assumption we have made.

61
00:09:29,000 --> 00:09:35,000
 And actually, we have made a few assumptions for the linear regression.

62
00:09:35,000 --> 00:09:39,000
 So first, actually, we assume that there is a linear regression

63
00:09:39,000 --> 00:09:43,000
 between the target variable, which is also called a dependent variable,

64
00:09:43,000 --> 00:09:47,000
 and the independent variables.

65
00:09:47,000 --> 00:09:49,000
 We assume that there is a linear regression.

66
00:09:49,000 --> 00:09:51,000
 So that we use a linear regression.

67
00:09:51,000 --> 00:09:58,000
 If the regression shape is nonlinear, but we assume there's a linear regression.

68
00:09:58,000 --> 00:10:04,000
 But then, of course, this assumption is not correct.

69
00:10:05,000 --> 00:10:10,000
 So here, actually, we will try to check whether or not

70
00:10:10,000 --> 00:10:16,000
 this assumption is violated or it is true.

71
00:10:16,000 --> 00:10:18,000
 So this is a linear regression shape.

72
00:10:18,000 --> 00:10:20,000
 So this is one assumption.

73
00:10:20,000 --> 00:10:24,000
 When we use linear regression, we assume linear.

74
00:10:24,000 --> 00:10:27,000
 And then, actually, the next assumption is that, actually,

75
00:10:27,000 --> 00:10:31,000
 the residual should be normally distributed.

76
00:10:31,000 --> 00:10:33,000
 And the residual, also, the error, right?

77
00:10:33,000 --> 00:10:37,000
 Actually, remember, we have a noise, or error.

78
00:10:37,000 --> 00:10:43,000
 And this error, called residual, actually is a normal distributed.

79
00:10:43,000 --> 00:10:45,000
 It's a normal function.

80
00:10:45,000 --> 00:10:46,000
 It's a Gaussian function.

81
00:10:46,000 --> 00:10:47,000
 And the noise is Gaussian.

82
00:10:47,000 --> 00:10:49,000
 OK, so this is the assumption.

83
00:10:49,000 --> 00:10:52,000
 And, of course, for the Gaussian noise, actually,

84
00:10:52,000 --> 00:10:56,000
 we also assume that this is a Gaussian noise,

85
00:10:56,000 --> 00:10:59,000
 has a zero mean.

86
00:10:59,000 --> 00:11:00,000
 Zero mean.

87
00:11:00,000 --> 00:11:05,000
 OK, so the mean of the residual should be zero or close to zero.

88
00:11:05,000 --> 00:11:07,000
 So this assumption we have made.

89
00:11:07,000 --> 00:11:10,000
 And so if you want to validate a model,

90
00:11:10,000 --> 00:11:13,000
 we need to check all of these assumptions.

91
00:11:15,000 --> 00:11:21,000
 That's a fourth one, actually, the all-independent variable

92
00:11:21,000 --> 00:11:24,000
 to be multi-vary normal.

93
00:11:24,000 --> 00:11:28,000
 All-independent variables need to be normal.

94
00:11:28,000 --> 00:11:31,000
 OK, so these are also assumptions.

95
00:11:31,000 --> 00:11:33,000
 Of course, actually, in practice,

96
00:11:33,000 --> 00:11:36,000
 you don't necessarily know the independent variables,

97
00:11:36,000 --> 00:11:38,000
 actually, are normal.

98
00:11:38,000 --> 00:11:42,000
 If not normal, or not multi-vary normal,

99
00:11:42,000 --> 00:11:50,000
 then probably we need to use, like, we need to modify the regression method,

100
00:11:50,000 --> 00:11:54,000
 rather than the regular square error.

101
00:11:54,000 --> 00:11:57,000
 Maybe you can think about other regression methods.

102
00:11:57,000 --> 00:12:01,000
 So this is the fourth assumption, OK,

103
00:12:01,000 --> 00:12:06,000
 all-independent variables, multi-vary normal.

104
00:12:06,000 --> 00:12:11,000
 And so then let's try the homoesthetic state.

105
00:12:11,000 --> 00:12:15,000
 The mean of the tree is the same virus.

106
00:12:15,000 --> 00:12:17,000
 This means the same virus.

107
00:12:17,000 --> 00:12:20,000
 And the same virus, that means, actually,

108
00:12:20,000 --> 00:12:25,000
 the virus should be actually the same

109
00:12:25,000 --> 00:12:28,000
 as all the range of the input.

110
00:12:30,000 --> 00:12:35,000
 OK, so this is actually an assumption.

111
00:12:35,000 --> 00:12:41,000
 The residual is the same across all values of the independent variables,

112
00:12:41,000 --> 00:12:43,000
 the virus.

113
00:12:43,000 --> 00:12:46,000
 OK.

114
00:12:46,000 --> 00:12:49,000
 And that's, OK, then the tree, the opposite to this,

115
00:12:49,000 --> 00:12:54,000
 is the heteroscedasticity.

116
00:12:54,000 --> 00:12:56,000
 That means the virus is different.

117
00:12:56,000 --> 00:13:00,000
 OK, then in other words, maybe the virus, actually,

118
00:13:00,000 --> 00:13:05,000
 of the residual, right, actually is greater or bigger

119
00:13:05,000 --> 00:13:10,000
 for larger values of the input.

120
00:13:10,000 --> 00:13:15,000
 OK, so this is also a test that we need to check

121
00:13:15,000 --> 00:13:19,000
 whether this condition, homoesthetic state, is satisfied.

122
00:13:19,000 --> 00:13:23,000
 And this is actually a diagram shows this scenario.

123
00:13:23,000 --> 00:13:27,000
 So the two figures here, the left figure shows

124
00:13:27,000 --> 00:13:29,000
 homoesthetic state.

125
00:13:29,000 --> 00:13:32,000
 And actually, the horizontal axis actually shows

126
00:13:32,000 --> 00:13:36,000
 the value of the independent variable input.

127
00:13:36,000 --> 00:13:41,000
 And so the vertical shows the area, right?

128
00:13:41,000 --> 00:13:45,000
 Actually, you can see the range, right?

129
00:13:45,000 --> 00:13:49,000
 Actually, this is actually the same in low

130
00:13:49,000 --> 00:13:53,000
 and also in the high values of these independent variables.

131
00:13:53,000 --> 00:13:56,000
 These are homoesthetic states, the same virus.

132
00:13:56,000 --> 00:13:59,000
 But you look at this right hand side, right?

133
00:13:59,000 --> 00:14:03,000
 So you can see, actually, for larger value of the input,

134
00:14:03,000 --> 00:14:07,000
 actually, the virus has a bigger range, right?

135
00:14:07,000 --> 00:14:08,000
 It has a bigger range.

136
00:14:08,000 --> 00:14:11,000
 So this means a bigger, actually, virus.

137
00:14:11,000 --> 00:14:12,000
 OK.

138
00:14:12,000 --> 00:14:16,000
 But for small values of the input, the noise,

139
00:14:16,000 --> 00:14:20,000
 actually, in a range, in a smaller range.

140
00:14:20,000 --> 00:14:21,000
 OK.

141
00:14:21,000 --> 00:14:24,000
 So this case, actually, is actually

142
00:14:24,000 --> 00:14:29,000
 heteroscedasticity, the virus.

143
00:14:29,000 --> 00:14:34,000
 It's not the same for, actually, low values

144
00:14:34,000 --> 00:14:38,000
 and the higher values of the input.

145
00:14:38,000 --> 00:14:39,000
 OK.

146
00:14:39,000 --> 00:14:42,000
 So this is the, and so this is actually

147
00:14:42,000 --> 00:14:45,000
 a satisfactory model.

148
00:14:45,000 --> 00:14:46,000
 OK.

149
00:14:46,000 --> 00:14:48,000
 Then this is an unsatisfactory model.

150
00:14:48,000 --> 00:14:49,000
 OK.

151
00:14:49,000 --> 00:14:51,000
 So these are the assumptions.

152
00:14:51,000 --> 00:14:52,000
 OK.

153
00:14:52,000 --> 00:14:56,000
 Then we need to check each of these assumptions

154
00:14:56,000 --> 00:14:58,000
 so after we have obtained a model.

155
00:14:58,000 --> 00:14:59,000
 OK.

156
00:14:59,000 --> 00:15:02,000
 So these are called model validation.

157
00:15:02,000 --> 00:15:07,000
 And then we first look at, actually, the linearity,

158
00:15:07,000 --> 00:15:09,000
 the linear relationship.

159
00:15:09,000 --> 00:15:13,000
 And the linearity can be checked based on, actually,

160
00:15:13,000 --> 00:15:15,000
 the scatter plot.

161
00:15:15,000 --> 00:15:16,000
 OK.

162
00:15:16,000 --> 00:15:20,000
 And of the actual value versus, or against,

163
00:15:20,000 --> 00:15:22,000
 the predicted value.

164
00:15:22,000 --> 00:15:24,000
 The predicted value here all means,

165
00:15:24,000 --> 00:15:26,000
 really, the target variable.

166
00:15:26,000 --> 00:15:27,000
 Right?

167
00:15:27,000 --> 00:15:29,000
 For the target variable, we have an actual value.

168
00:15:29,000 --> 00:15:33,000
 And for the predicted value, and then we can have,

169
00:15:33,000 --> 00:15:36,000
 you know, this is a scatter plot.

170
00:15:36,000 --> 00:15:40,000
 You remember, in the second week, right?

171
00:15:40,000 --> 00:15:43,000
 In the first week, for machine learning part,

172
00:15:43,000 --> 00:15:46,000
 we studied the exploration or data preparation

173
00:15:46,000 --> 00:15:47,000
 for machine learning.

174
00:15:47,000 --> 00:15:52,000
 And actually, in the data exploration part,

175
00:15:52,000 --> 00:15:56,000
 actually, one exploration is a scatter plot.

176
00:15:56,000 --> 00:15:58,000
 And the scatter plot actually shows the relationship

177
00:15:58,000 --> 00:16:00,000
 between two variables, right?

178
00:16:00,000 --> 00:16:02,000
 So here the two variables, actually,

179
00:16:02,000 --> 00:16:04,000
 one is the actual value, the target value.

180
00:16:04,000 --> 00:16:07,000
 Another is the predicted value.

181
00:16:07,000 --> 00:16:08,000
 So these two.

182
00:16:08,000 --> 00:16:12,000
 So if these two actually show the linear relationship,

183
00:16:12,000 --> 00:16:17,000
 just like all the data points are along a street line.

184
00:16:17,000 --> 00:16:18,000
 OK.

185
00:16:18,000 --> 00:16:19,000
 Along a street line.

186
00:16:19,000 --> 00:16:23,000
 And then this means that the linear relationship is satisfied.

187
00:16:23,000 --> 00:16:27,000
 Otherwise, actually, the predicted value and the actual value

188
00:16:27,000 --> 00:16:29,000
 are not on a street line.

189
00:16:29,000 --> 00:16:30,000
 OK.

190
00:16:30,000 --> 00:16:32,000
 All the data points are not actually,

191
00:16:32,000 --> 00:16:35,000
 are not actually on a street line.

192
00:16:35,000 --> 00:16:37,000
 So if the relationship is nonlinear,

193
00:16:37,000 --> 00:16:41,000
 but you use a linear model to feed the data, right?

194
00:16:41,000 --> 00:16:47,000
 And then, actually, you cannot get a scatter plot like this.

195
00:16:50,000 --> 00:16:51,000
 OK.

196
00:16:51,000 --> 00:16:57,000
 So here, for this example, and we show, actually,

197
00:16:57,000 --> 00:17:00,000
 indeed, actually, all the data points in this scatter plot

198
00:17:00,000 --> 00:17:03,000
 actually is along a line.

199
00:17:03,000 --> 00:17:08,000
 So this means this linear relationship is true.

200
00:17:08,000 --> 00:17:09,000
 OK.

201
00:17:09,000 --> 00:17:11,000
 So this is our first assumption, right?

202
00:17:11,000 --> 00:17:14,000
 And then the second assumption, actually,

203
00:17:14,000 --> 00:17:17,000
 is noise Gaussian.

204
00:17:17,000 --> 00:17:20,000
 Normal noise, disputed noise, right?

205
00:17:20,000 --> 00:17:21,000
 OK.

206
00:17:21,000 --> 00:17:24,000
 And so here, actually, how to explore,

207
00:17:24,000 --> 00:17:31,000
 how to visualize, how to check whether, actually,

208
00:17:31,000 --> 00:17:35,000
 noise is normal or data is normally disputed.

209
00:17:35,000 --> 00:17:39,000
 Actually, we can use the histogram.

210
00:17:39,000 --> 00:17:42,000
 Actually, the signal pattern on the data preparation method,

211
00:17:42,000 --> 00:17:46,000
 we also talk about this histogram, right?

212
00:17:46,000 --> 00:17:49,000
 So the data, actually, normally, is first, actually,

213
00:17:49,000 --> 00:17:53,000
 the data is sorted in ascending order or descending order, right?

214
00:17:53,000 --> 00:17:57,000
 And then we divide the whole range of the data into two parts,

215
00:17:57,000 --> 00:18:00,000
 into many parts, many beams.

216
00:18:00,000 --> 00:18:01,000
 OK.

217
00:18:01,000 --> 00:18:04,000
 The default value normally is 10 into 10 beams, right?

218
00:18:04,000 --> 00:18:10,000
 And for example, the value range is from minus 5 to positive 5,

219
00:18:10,000 --> 00:18:12,000
 for example, right?

220
00:18:12,000 --> 00:18:15,000
 And then from minus 5, we can divide this, you know,

221
00:18:15,000 --> 00:18:21,000
 from minus 5 to positive 5 into, actually, 10 beams.

222
00:18:21,000 --> 00:18:22,000
 OK.

223
00:18:22,000 --> 00:18:29,000
 Then we count how many samples and how many values are in each beam.

224
00:18:29,000 --> 00:18:30,000
 OK.

225
00:18:30,000 --> 00:18:32,000
 The number of values in each beam, right?

226
00:18:32,000 --> 00:18:35,000
 So then, based on this, we can plot, actually,

227
00:18:35,000 --> 00:18:38,000
 the histogram just like this, right?

228
00:18:38,000 --> 00:18:40,000
 So the horizontal shows that the beams, right?

229
00:18:40,000 --> 00:18:45,000
 The 10 beams and the vertical shows the number of values in each beam.

230
00:18:45,000 --> 00:18:46,000
 OK.

231
00:18:46,000 --> 00:18:49,000
 And so normally, if you look at this, actually,

232
00:18:49,000 --> 00:18:51,000
 it's roughly a bell curve, right?

233
00:18:51,000 --> 00:18:52,000
 Bell curve.

234
00:18:52,000 --> 00:18:55,000
 We know the normal division is a bell curve, right?

235
00:18:55,000 --> 00:18:58,000
 So from the histogram, so we see, oh, roughly,

236
00:18:58,000 --> 00:19:03,000
 I think this residual here is normal.

237
00:19:03,000 --> 00:19:04,000
 It's normal.

238
00:19:04,000 --> 00:19:06,000
 Of course, actually, whether it's normal,

239
00:19:06,000 --> 00:19:10,000
 we can use another technique that is a QQ plot.

240
00:19:10,000 --> 00:19:11,000
 OK.

241
00:19:11,000 --> 00:19:14,000
 So this roughly is normal.

242
00:19:14,000 --> 00:19:18,000
 And then this is a second, actually, assumption, right?

243
00:19:18,000 --> 00:19:20,000
 And then the assumption is that, actually,

244
00:19:20,000 --> 00:19:23,000
 the residual should have a zero mean.

245
00:19:23,000 --> 00:19:24,000
 OK.

246
00:19:24,000 --> 00:19:27,000
 And then, previously, last week, we have, actually,

247
00:19:27,000 --> 00:19:28,000
 one example, right?

248
00:19:28,000 --> 00:19:31,000
 Remember, we have an example for the parameter estimation

249
00:19:31,000 --> 00:19:34,000
 using the least square method, right?

250
00:19:34,000 --> 00:19:38,000
 And actually, this is the example, actually,

251
00:19:38,000 --> 00:19:41,000
 we have the residual, which is the difference between,

252
00:19:41,000 --> 00:19:46,000
 actually, the target value and the predicted value.

253
00:19:46,000 --> 00:19:47,000
 OK.

254
00:19:47,000 --> 00:19:49,000
 And then we calculate the mean.

255
00:19:49,000 --> 00:19:53,000
 The mean, actually, is equal to 10 power minus 17, right?

256
00:19:53,000 --> 00:19:56,000
 So this is very close to zero.

257
00:19:56,000 --> 00:19:57,000
 OK.

258
00:19:57,000 --> 00:20:01,000
 Then this, that means that the assumption is satisfied.

259
00:20:01,000 --> 00:20:02,000
 OK.

260
00:20:02,000 --> 00:20:07,000
 So it's roughly normal and with zero mean.

261
00:20:07,000 --> 00:20:08,000
 OK.

262
00:20:08,000 --> 00:20:10,000
 And then the fourth one, actually, we need to check,

263
00:20:10,000 --> 00:20:14,000
 actually, whether, actually, the independent variables,

264
00:20:14,000 --> 00:20:16,000
 actually, are normal, right?

265
00:20:16,000 --> 00:20:18,000
 Independent variables are normal.

266
00:20:18,000 --> 00:20:21,000
 And to check this, actually, we can use another technique

267
00:20:21,000 --> 00:20:25,000
 which is called, actually, now the QQ plot.

268
00:20:25,000 --> 00:20:26,000
 OK.

269
00:20:26,000 --> 00:20:29,000
 The minimum Q is the quantal.

270
00:20:29,000 --> 00:20:30,000
 OK.

271
00:20:30,000 --> 00:20:34,000
 So given a data, right, you can, again, you can sort the data,

272
00:20:34,000 --> 00:20:36,000
 you can order the data in descending order,

273
00:20:36,000 --> 00:20:38,000
 or ascending order, right?

274
00:20:38,000 --> 00:20:42,000
 Then you can find the quantals, the quantal value, right?

275
00:20:42,000 --> 00:20:43,000
 Quantal.

276
00:20:43,000 --> 00:20:44,000
 OK.

277
00:20:44,000 --> 00:20:47,000
 So also, actually, you can use a standard normal,

278
00:20:47,000 --> 00:20:50,000
 standard normal, standard normal.

279
00:20:50,000 --> 00:20:51,000
 OK.

280
00:20:51,000 --> 00:20:54,000
 Then you can also look at the quantal.

281
00:20:54,000 --> 00:20:55,000
 OK.

282
00:20:55,000 --> 00:20:59,000
 And then you can actually draw, actually, you know,

283
00:20:59,000 --> 00:21:01,000
 the points or the value, right?

284
00:21:01,000 --> 00:21:06,000
 So if this value is, actually, normally, the round street line,

285
00:21:06,000 --> 00:21:10,000
 then we see, oh, this is, actually, you know, it's normal,

286
00:21:10,000 --> 00:21:11,000
 approximately normal.

287
00:21:11,000 --> 00:21:12,000
 OK.

288
00:21:12,000 --> 00:21:14,000
 So for this example, we can see this range, right?

289
00:21:14,000 --> 00:21:19,000
 Normally, I think this is, actually, the quantal of the input

290
00:21:19,000 --> 00:21:23,000
 sample and the standard, actually, the normal quantals,

291
00:21:23,000 --> 00:21:27,000
 basically, it's on a street line, right?

292
00:21:27,000 --> 00:21:29,000
 It's a street line.

293
00:21:29,000 --> 00:21:30,000
 OK.

294
00:21:30,000 --> 00:21:32,000
 So if on a street line, then we, actually, we see

295
00:21:32,000 --> 00:21:35,000
 this condition is also satisfied, right?

296
00:21:35,000 --> 00:21:37,000
 Satisfied.

297
00:21:37,000 --> 00:21:38,000
 OK.

298
00:21:38,000 --> 00:21:41,000
 So it's a, this is a fourth condition, right?

299
00:21:41,000 --> 00:21:45,000
 The input variables are actually roughly, you know,

300
00:21:45,000 --> 00:21:46,000
 normal.

301
00:21:46,000 --> 00:21:47,000
 OK.

302
00:21:48,000 --> 00:21:57,000
 And then the next one, actually, is the next, actually,

303
00:21:57,000 --> 00:22:01,000
 we need to check the one that whom I said,

304
00:22:01,000 --> 00:22:02,000
 taste, taste, taste, right?

305
00:22:02,000 --> 00:22:04,000
 Whom I say taste, taste, taste, taste, taste.

306
00:22:04,000 --> 00:22:08,000
 And then we look at whether the virus of the residual,

307
00:22:08,000 --> 00:22:12,000
 actually, increased with the increase of the,

308
00:22:12,000 --> 00:22:15,000
 of the values, right?

309
00:22:16,000 --> 00:22:19,000
 And so here we can check this.

310
00:22:19,000 --> 00:22:20,000
 OK.

311
00:22:20,000 --> 00:22:24,000
 So, traditionally, the predicted target variable is a

312
00:22:24,000 --> 00:22:28,000
 horizontal axis and the vertical shows the residual,

313
00:22:28,000 --> 00:22:30,000
 the areas, the areas.

314
00:22:30,000 --> 00:22:31,000
 OK.

315
00:22:31,000 --> 00:22:34,000
 And actually, we, we, actually, the area range, you know,

316
00:22:34,000 --> 00:22:37,000
 in the, in the low value of the predicted variable,

317
00:22:37,000 --> 00:22:38,000
 the target value, right?

318
00:22:38,000 --> 00:22:41,000
 Around minus four, minus six, right?

319
00:22:41,000 --> 00:22:43,000
 So the range is from here to here, right?

320
00:22:43,000 --> 00:22:47,000
 And then for the bigger values of the target variable,

321
00:22:47,000 --> 00:22:50,000
 and then the variant, actually, the variant is similar,

322
00:22:50,000 --> 00:22:51,000
 similar, right?

323
00:22:51,000 --> 00:22:55,000
 It's not actually like a, like a funnel, right?

324
00:22:55,000 --> 00:23:00,000
 As we've seen in the previous examples, not like this.

325
00:23:00,000 --> 00:23:03,000
 In the low values of the predicted variable,

326
00:23:03,000 --> 00:23:07,000
 the target variable, the variant is small, right?

327
00:23:07,000 --> 00:23:10,000
 And then in the larger values, actually,

328
00:23:10,000 --> 00:23:12,000
 the variant is big, bigger.

329
00:23:12,000 --> 00:23:13,000
 We don't have this.

330
00:23:13,000 --> 00:23:14,000
 OK.

331
00:23:14,000 --> 00:23:16,000
 Basically, you like this, actually,

332
00:23:16,000 --> 00:23:19,000
 the whom I sketched a statistic scenario.

333
00:23:19,000 --> 00:23:20,000
 OK.

334
00:23:20,000 --> 00:23:24,000
 So from this example, we can see, oh, the model,

335
00:23:24,000 --> 00:23:28,000
 basically, actually, you know, this is valid because all of

336
00:23:28,000 --> 00:23:32,000
 the five assumptions actually are true, or roughly, right?

337
00:23:32,000 --> 00:23:33,000
 True.

338
00:23:34,000 --> 00:23:35,000
 OK.

339
00:23:35,000 --> 00:23:38,000
 So this is a model validation.

340
00:23:38,000 --> 00:23:39,000
 OK.

341
00:23:43,000 --> 00:23:44,000
 OK.

342
00:23:44,000 --> 00:23:49,000
 So, that's actually, we look at the nonlinear regression

343
00:23:49,000 --> 00:23:52,000
 or nonlinear regression problem.

344
00:23:52,000 --> 00:23:55,000
 And so far, actually, we have talked about the linear regression,

345
00:23:55,000 --> 00:23:56,000
 right?

346
00:23:56,000 --> 00:23:59,000
 A linear regression tree, we assume the linear relationship

347
00:23:59,000 --> 00:24:02,000
 between the target variable and the independent variables.

348
00:24:02,000 --> 00:24:03,000
 OK.

349
00:24:03,000 --> 00:24:06,000
 But sometimes, actually, the relationship between the input

350
00:24:06,000 --> 00:24:08,000
 variable, the target variable, input variable,

351
00:24:08,000 --> 00:24:11,000
 the independent variables, actually, are nonlinear.

352
00:24:12,000 --> 00:24:15,000
 So this diagram just shows nonlinear relationship.

353
00:24:15,000 --> 00:24:16,000
 OK.

354
00:24:16,000 --> 00:24:19,000
 And the horizontal axis is an independent variable,

355
00:24:19,000 --> 00:24:22,000
 and the vertical axis shows a dependent variable,

356
00:24:22,000 --> 00:24:24,000
 or the target variable.

357
00:24:24,000 --> 00:24:27,000
 And I believe you can see the data basically shows

358
00:24:27,000 --> 00:24:31,000
 a nonlinear relationship, nonlinear relationship.

359
00:24:31,000 --> 00:24:32,000
 OK.

360
00:24:32,000 --> 00:24:38,000
 And so the nonlinear is just a curve, right?

361
00:24:38,000 --> 00:24:42,000
 A linear relationship, normally, it's a straight line.

362
00:24:42,000 --> 00:24:44,000
 You know, you have the minus beta,

363
00:24:44,000 --> 00:24:46,000
 it's a harbor plane, right?

364
00:24:46,000 --> 00:24:52,000
 But actually, for the nonlinear, this is a curve.

365
00:24:52,000 --> 00:24:53,000
 OK.

366
00:24:53,000 --> 00:24:55,000
 So here you can see, indeed, this is a curve, right?

367
00:24:55,000 --> 00:24:56,000
 This is a curve.

368
00:24:59,000 --> 00:25:01,000
 This is also a curve.

369
00:25:01,000 --> 00:25:05,000
 So this means that the relationship between the target

370
00:25:05,000 --> 00:25:09,000
 variable and the internal variable, actually,

371
00:25:09,000 --> 00:25:11,000
 is nonlinear.

372
00:25:11,000 --> 00:25:14,000
 Nonlinear, certainly, we should use a nonlinear function, right?

373
00:25:14,000 --> 00:25:18,000
 And if you still use a linear regression model to feed the data,

374
00:25:18,000 --> 00:25:24,000
 certainly, we cannot get a valid model.

375
00:25:24,000 --> 00:25:26,000
 And based on the invalid model, of course,

376
00:25:26,000 --> 00:25:31,000
 the prediction could have a very big error, right?

377
00:25:31,000 --> 00:25:33,000
 Could have a very big error.

378
00:25:33,000 --> 00:25:34,000
 OK.

379
00:25:35,000 --> 00:25:40,000
 Then actually, we'll say, what kind of model we can use?

380
00:25:40,000 --> 00:25:41,000
 OK.

381
00:25:41,000 --> 00:25:45,000
 So the relationship, the way between x and theta,

382
00:25:45,000 --> 00:25:50,000
 so here is a nonlinear relationship, nonlinear.

383
00:25:50,000 --> 00:25:51,000
 OK.

384
00:25:51,000 --> 00:25:53,000
 And so what kind of nonlinear?

385
00:25:53,000 --> 00:25:55,000
 There are many types of nonlinear functions, right?

386
00:25:55,000 --> 00:25:59,000
 So I think, my recommended use of nonlinear models

387
00:25:59,000 --> 00:26:01,000
 include the polynomial model.

388
00:26:02,000 --> 00:26:09,000
 And actually, y equals theta 0, theta y, x, theta 2, x squared.

389
00:26:09,000 --> 00:26:13,000
 So here, actually, the y, in the regression model,

390
00:26:13,000 --> 00:26:18,000
 we have x squared term, and we have the s cubed term.

391
00:26:18,000 --> 00:26:21,000
 So this means that it's a nonlinear relationship.

392
00:26:21,000 --> 00:26:22,000
 Nonlinear relationship.

393
00:26:22,000 --> 00:26:23,000
 OK.

394
00:26:23,000 --> 00:26:26,000
 But the good thing for this polynomial is that actually,

395
00:26:26,000 --> 00:26:30,000
 the y is nonlinear in x, x, x, right?

396
00:26:30,000 --> 00:26:31,000
 In x.

397
00:26:31,000 --> 00:26:34,000
 But it is linear in the parameter.

398
00:26:34,000 --> 00:26:36,000
 In the linear in the parameter.

399
00:26:36,000 --> 00:26:37,000
 OK.

400
00:26:37,000 --> 00:26:40,000
 So this kind of model is called linear in the parameter.

401
00:26:40,000 --> 00:26:41,000
 Linear in the parameter.

402
00:26:41,000 --> 00:26:47,000
 But the nonlinear actually in the independent variables.

403
00:26:47,000 --> 00:26:50,000
 So basically, this is a nonlinear model.

404
00:26:50,000 --> 00:26:54,000
 But actually, we can use the linear model estimation method

405
00:26:54,000 --> 00:26:59,000
 to estimate the parameters, theta 0, theta 1, actually, theta 3.

406
00:26:59,000 --> 00:27:03,000
 You just can see that x to x squared is another new variable.

407
00:27:03,000 --> 00:27:06,000
 S cubed is another new variable.

408
00:27:06,000 --> 00:27:07,000
 OK.

409
00:27:07,000 --> 00:27:09,000
 Then this is just like a linear equation, right?

410
00:27:09,000 --> 00:27:10,000
 Linear equation.

411
00:27:10,000 --> 00:27:12,000
 But actually, this equation shows a nonlinear relationship

412
00:27:12,000 --> 00:27:16,000
 between the time variable y and the independent variable x.

413
00:27:16,000 --> 00:27:22,000
 But the y is linear in the parameters.

414
00:27:23,000 --> 00:27:27,000
 Then we can use the linear regression method to estimate

415
00:27:27,000 --> 00:27:33,000
 the values for theta 0, theta 1, theta 2, and theta 3 here.

416
00:27:33,000 --> 00:27:34,000
 OK.

417
00:27:34,000 --> 00:27:37,000
 So this is the polynomial model.

418
00:27:37,000 --> 00:27:40,000
 And then another model is called a rational model.

419
00:27:40,000 --> 00:27:41,000
 Rational here.

420
00:27:41,000 --> 00:27:46,000
 Rational means that it is a ratio of two polynomials.

421
00:27:46,000 --> 00:27:47,000
 So like this.

422
00:27:48,000 --> 00:27:55,000
 Theta 0, theta 1, theta 2, x cubed squared divided by theta 3

423
00:27:55,000 --> 00:28:00,000
 plus theta 4x and theta 5x cubed squared.

424
00:28:00,000 --> 00:28:01,000
 OK.

425
00:28:01,000 --> 00:28:02,000
 So just an example.

426
00:28:02,000 --> 00:28:04,000
 The highest order here is the two.

427
00:28:04,000 --> 00:28:09,000
 So this is called a rational model.

428
00:28:09,000 --> 00:28:10,000
 OK.

429
00:28:10,000 --> 00:28:14,000
 Of course, it's really compared with a polynomial model.

430
00:28:14,000 --> 00:28:17,000
 And I think the rational model parameter estimation

431
00:28:17,000 --> 00:28:24,000
 is challenging because the y no longer has a linear relationship

432
00:28:24,000 --> 00:28:26,000
 with the parameters.

433
00:28:26,000 --> 00:28:29,000
 Then we cannot use the linear regression parameter estimation

434
00:28:29,000 --> 00:28:32,000
 method to estimate the parameters.

435
00:28:32,000 --> 00:28:34,000
 And it is a rational model.

436
00:28:34,000 --> 00:28:37,000
 So this is a challenge.

437
00:28:37,000 --> 00:28:39,000
 OK.

438
00:28:39,000 --> 00:28:40,000
 OK.

439
00:28:40,000 --> 00:28:43,000
 And also, if we look at the polynomial model, right?

440
00:28:43,000 --> 00:28:45,000
 So here we just have a 1x.

441
00:28:45,000 --> 00:28:47,000
 We can have x squared.

442
00:28:47,000 --> 00:28:49,000
 We can have a cube.

443
00:28:49,000 --> 00:28:52,000
 This is just one independent variable.

444
00:28:52,000 --> 00:28:57,000
 But you can imagine if the model has many or have a few, right?

445
00:28:57,000 --> 00:29:00,000
 Or many independent variables.

446
00:29:00,000 --> 00:29:04,000
 x1, x2, x3, x4, and xn.

447
00:29:04,000 --> 00:29:05,000
 OK.

448
00:29:05,000 --> 00:29:06,000
 Then actually the model.

449
00:29:06,000 --> 00:29:12,000
 So what actually nonlinear terms should be included in the model?

450
00:29:13,000 --> 00:29:16,000
 So here we just have our xs squared, x3, right?

451
00:29:16,000 --> 00:29:20,000
 So this is easy if we have just a single variable.

452
00:29:20,000 --> 00:29:25,000
 But now we just consider that we have two variable, x1, x2.

453
00:29:25,000 --> 00:29:30,000
 Of course, we can still have x1 squared, x2 squared, x2, x2,

454
00:29:30,000 --> 00:29:32,000
 x2 cubed, right?

455
00:29:32,000 --> 00:29:40,000
 But also we should have some terms like x1, multiply, x2.

456
00:29:40,000 --> 00:29:41,000
 Right?

457
00:29:41,000 --> 00:29:43,000
 So we can have a combination.

458
00:29:43,000 --> 00:29:48,000
 But if you think there are many variables here, x1, x2, x3,

459
00:29:48,000 --> 00:29:51,000
 maybe until x10, for example, right?

460
00:29:51,000 --> 00:29:54,000
 And then what kind of combinations?

461
00:29:54,000 --> 00:29:56,000
 x1, x2 could be a combination.

462
00:29:56,000 --> 00:29:57,000
 x1 times x2.

463
00:29:57,000 --> 00:29:58,000
 OK.

464
00:29:58,000 --> 00:30:00,000
 x1 times x3.

465
00:30:00,000 --> 00:30:02,000
 x1 times x4.

466
00:30:02,000 --> 00:30:04,000
 x1 times x10.

467
00:30:04,000 --> 00:30:05,000
 OK.

468
00:30:05,000 --> 00:30:06,000
 This kind of combination, right?

469
00:30:06,000 --> 00:30:10,000
 We could also have x1 times x2 squared.

470
00:30:10,000 --> 00:30:12,000
 So we could have a combination, right?

471
00:30:12,000 --> 00:30:16,000
 So then we also, we also bring some challenges.

472
00:30:16,000 --> 00:30:17,000
 OK.

473
00:30:17,000 --> 00:30:19,000
 You have many variables.

474
00:30:19,000 --> 00:30:25,000
 And when you think about all the combinatorials of these variables,

475
00:30:25,000 --> 00:30:27,000
 all the combinations, OK?

476
00:30:27,000 --> 00:30:37,000
 And then this polynomial model could have a huge number of terms.

477
00:30:37,000 --> 00:30:40,000
 So the terms actually is not useful.

478
00:30:40,000 --> 00:30:42,000
 And then in the linear regression model,

479
00:30:42,000 --> 00:30:45,000
 in this polynomial model modeling,

480
00:30:45,000 --> 00:30:49,000
 and a large portion of the, our efforts actually

481
00:30:49,000 --> 00:30:56,000
 will be put on the selection of the terms in the polynomial model.

482
00:30:56,000 --> 00:30:58,000
 OK.

483
00:30:58,000 --> 00:31:02,000
 And again, this program is even more challenging, right?

484
00:31:02,000 --> 00:31:08,000
 Because of the, you know, the rational model.

485
00:31:08,000 --> 00:31:09,000
 OK.

486
00:31:09,000 --> 00:31:13,000
 And so because of these limitations of the polynomial model

487
00:31:13,000 --> 00:31:17,000
 and the rational model, and for the hard terminal data, right?

488
00:31:17,000 --> 00:31:19,000
 We have many independent variables.

489
00:31:19,000 --> 00:31:24,000
 So, so in recent year that we, I think, if the model is really

490
00:31:24,000 --> 00:31:28,000
 for the prediction, not for explanation, not for explain,

491
00:31:28,000 --> 00:31:29,000
 right?

492
00:31:29,000 --> 00:31:31,000
 Just for prediction.

493
00:31:31,000 --> 00:31:37,000
 So, you know, we can use the, you know, the neural networks,

494
00:31:37,000 --> 00:31:39,000
 the neural networks.

495
00:31:39,000 --> 00:31:40,000
 OK.

496
00:31:40,000 --> 00:31:43,000
 And because in the neural networks, actually, the operation normally,

497
00:31:43,000 --> 00:31:49,000
 at the first layer normally is a linear combination of the input,

498
00:31:49,000 --> 00:31:50,000
 linear combination.

499
00:31:50,000 --> 00:31:55,000
 And then we perform nonlinear operation on this linear operation,

500
00:31:55,000 --> 00:31:57,000
 on these linear combinations.

501
00:31:57,000 --> 00:31:58,000
 OK.

502
00:31:58,000 --> 00:31:59,000
 Yeah.

503
00:31:59,000 --> 00:32:01,000
 So, so this is an issue.

504
00:32:01,000 --> 00:32:04,000
 Like I think about the combinations of the different x, y, x,

505
00:32:04,000 --> 00:32:09,000
 two, this problem is no longer a problem because we don't

506
00:32:09,000 --> 00:32:10,000
 consider this.

507
00:32:10,000 --> 00:32:17,000
 At each neuron, we just consider the linear combination of the output

508
00:32:17,000 --> 00:32:20,000
 from previous layers in the neural network.

509
00:32:20,000 --> 00:32:21,000
 OK.

510
00:32:21,000 --> 00:32:22,000
 If you know that a bit, OK.

511
00:32:22,000 --> 00:32:26,000
 So, but we, so in this course, we don't go into deep about the

512
00:32:26,000 --> 00:32:27,000
 linear regression.

513
00:32:27,000 --> 00:32:28,000
 OK.

514
00:32:28,000 --> 00:32:31,000
 But if you have interest, certainly you can always find the material

515
00:32:31,000 --> 00:32:34,000
 on the internet, right, on the nonlinear regression.

516
00:32:34,000 --> 00:32:35,000
 OK.

517
00:32:35,000 --> 00:32:38,000
 So, in this course, when we touch, in the machine learning,

518
00:32:38,000 --> 00:32:41,000
 we call the, normally, I think about the classification.

519
00:32:41,000 --> 00:32:45,000
 So, by the way, we touch a bit about the regression, but we only,

520
00:32:45,000 --> 00:32:49,000
 actually, you know, limit our study on the linear regression.

521
00:32:49,000 --> 00:32:50,000
 OK.

522
00:32:50,000 --> 00:32:53,000
 So, we don't study the nonlinear regression.

523
00:32:53,000 --> 00:32:54,000
 OK.

524
00:32:55,000 --> 00:33:01,000
 So, that's all for the regression.

525
00:33:01,000 --> 00:33:09,000
 So, that's actually what we will think about the, let's find another topic.

526
00:33:09,000 --> 00:33:14,000
 And that is feature selection.

527
00:33:20,000 --> 00:33:21,000
 OK.

528
00:33:21,000 --> 00:33:39,000
 So, let's look at the, OK.

529
00:33:39,000 --> 00:33:44,000
 So, next, we look at the feature selection for the minority reduction

530
00:33:44,000 --> 00:33:45,000
 and classifications.

531
00:33:45,000 --> 00:33:46,000
 OK.

532
00:33:46,000 --> 00:33:50,000
 And that, actually, is also a very important topic in pen classification

533
00:33:50,000 --> 00:33:51,000
 in machine learning.

534
00:33:51,000 --> 00:33:52,000
 OK.

535
00:33:52,000 --> 00:33:56,000
 And quite often, right, quite often, actually, when you are given a problem,

536
00:33:56,000 --> 00:34:01,000
 actually, of course, now, actually, if you are doing machine learning

537
00:34:01,000 --> 00:34:04,000
 problem, quite often, you look at some of the data set,

538
00:34:04,000 --> 00:34:07,000
 probably available data set, right.

539
00:34:07,000 --> 00:34:12,000
 And then you just use this data set to practice, you know, to learn, right.

540
00:34:12,000 --> 00:34:15,000
 But in practice, actually, normally, you don't have a data set, right.

541
00:34:15,000 --> 00:34:18,000
 When you are given problem, we need to collect our own data.

542
00:34:18,000 --> 00:34:19,000
 OK.

543
00:34:19,000 --> 00:34:21,000
 So, we start from scratch, right.

544
00:34:21,000 --> 00:34:23,000
 First step is to collect the data.

545
00:34:23,000 --> 00:34:24,000
 OK.

546
00:34:24,000 --> 00:34:27,000
 When you collect the data, actually, of course, we know our target.

547
00:34:27,000 --> 00:34:30,000
 And, but, actually, we have no idea.

548
00:34:30,000 --> 00:34:34,000
 So, actually, what information, actually, what variables, actually,

549
00:34:34,000 --> 00:34:38,000
 or what features are needed, OK, you know, to predict, you know,

550
00:34:38,000 --> 00:34:40,000
 to classify the samples.

551
00:34:40,000 --> 00:34:41,000
 OK.

552
00:34:41,000 --> 00:34:44,000
 So, at the beginning, of course, if we have the prior knowledge, right,

553
00:34:44,000 --> 00:34:46,000
 we can get the prior knowledge.

554
00:34:46,000 --> 00:34:49,000
 We can get the data from the demy as per our, whoever, right.

555
00:34:49,000 --> 00:34:55,000
 And we know to perform classification for this problem.

556
00:34:55,000 --> 00:34:57,000
 And we need to acquire what kind of data.

557
00:34:57,000 --> 00:34:58,000
 OK.

558
00:34:58,000 --> 00:34:59,000
 That's the best, right.

559
00:34:59,000 --> 00:35:02,000
 But, quite often, actually, we don't have this information.

560
00:35:02,000 --> 00:35:03,000
 OK.

561
00:35:03,000 --> 00:35:06,000
 So, during the data acquisition stage, or data collection stage,

562
00:35:06,000 --> 00:35:10,000
 we try our best to collect as much as possible.

563
00:35:10,000 --> 00:35:11,000
 OK.

564
00:35:11,000 --> 00:35:13,000
 As many features as possible.

565
00:35:13,000 --> 00:35:14,000
 OK.

566
00:35:14,000 --> 00:35:18,000
 But, actually, if we, you know, include too many features in the model,

567
00:35:18,000 --> 00:35:21,000
 and we could have some issues, OK.

568
00:35:21,000 --> 00:35:25,000
 In previous studies, we always assume all the features are used, right.

569
00:35:25,000 --> 00:35:28,000
 We have not talked about touch-dissue issue.

570
00:35:28,000 --> 00:35:30,000
 But this is a practical issue.

571
00:35:30,000 --> 00:35:31,000
 OK.

572
00:35:31,000 --> 00:35:34,000
 And given a problem, you collect the data.

573
00:35:34,000 --> 00:35:39,000
 But what data are relevant, what data should be collected?

574
00:35:39,000 --> 00:35:42,000
 And at the beginning, maybe you have no idea.

575
00:35:42,000 --> 00:35:47,000
 But then you try your best to collect as much as possible.

576
00:35:47,000 --> 00:35:48,000
 OK.

577
00:35:48,000 --> 00:35:55,000
 And then, actually, we can have too many features, too many attributes.

578
00:35:55,000 --> 00:35:56,000
 OK.

579
00:35:56,000 --> 00:35:59,000
 And then the data will become hard dimensional.

580
00:35:59,000 --> 00:36:00,000
 Hard dimensional.

581
00:36:00,000 --> 00:36:03,000
 And, actually, we could have some issues, actually,

582
00:36:03,000 --> 00:36:05,000
 related to this hard dimensional data.

583
00:36:05,000 --> 00:36:06,000
 OK.

584
00:36:06,000 --> 00:36:10,000
 And this is the problem is called, so the data is a hard dimensional.

585
00:36:11,000 --> 00:36:12,000
 OK.

586
00:36:12,000 --> 00:36:17,000
 And then, actually, we can have the so-called curse of the minority.

587
00:36:17,000 --> 00:36:18,000
 OK.

588
00:36:18,000 --> 00:36:22,000
 So the difficulty related to hard dimensional data is referred to,

589
00:36:22,000 --> 00:36:26,000
 actually, the curse of the minority.

590
00:36:26,000 --> 00:36:27,000
 OK.

591
00:36:27,000 --> 00:36:32,000
 So very hard dimensional kind of data can bring some challenges

592
00:36:32,000 --> 00:36:34,000
 for the machine learning.

593
00:36:34,000 --> 00:36:35,000
 OK.

594
00:36:35,000 --> 00:36:42,000
 And so this is a hard curse of the minority has two, actually, paths.

595
00:36:42,000 --> 00:36:45,000
 First, actually, the data is sparse.

596
00:36:45,000 --> 00:36:46,000
 The data sparse.

597
00:36:46,000 --> 00:36:47,000
 OK.

598
00:36:47,000 --> 00:36:49,000
 So sparsity is one issue.

599
00:36:49,000 --> 00:36:51,000
 Another is the distance and concentration.

600
00:36:51,000 --> 00:36:52,000
 OK.

601
00:36:52,000 --> 00:36:54,000
 So what is sparsity?

602
00:36:54,000 --> 00:36:55,000
 Sparsity of the data.

603
00:36:55,000 --> 00:36:57,000
 Actually, you can imagine, right?

604
00:36:57,000 --> 00:37:00,000
 So, we have about 100 or 200 students, right?

605
00:37:00,000 --> 00:37:03,000
 If all the students standing in one line,

606
00:37:03,000 --> 00:37:06,000
 if all of you come to the front, stand in one line,

607
00:37:06,000 --> 00:37:11,000
 in one dimension, then actually, you can imagine, right?

608
00:37:11,000 --> 00:37:15,000
 So they're all very crowded, right?

609
00:37:15,000 --> 00:37:19,000
 That means that the data densely actually is disputed.

610
00:37:19,000 --> 00:37:20,000
 OK.

611
00:37:20,000 --> 00:37:25,000
 So now, actually, you all see there's a 2D space, right?

612
00:37:25,000 --> 00:37:26,000
 At least 2D, right?

613
00:37:26,000 --> 00:37:29,000
 And then you have space between the students.

614
00:37:29,000 --> 00:37:30,000
 OK.

615
00:37:30,000 --> 00:37:31,000
 Not very crowded.

616
00:37:31,000 --> 00:37:34,000
 You can imagine if the space actually is 3D, some of the students

617
00:37:34,000 --> 00:37:37,000
 can see there, not 3D, right?

618
00:37:37,000 --> 00:37:39,000
 So the data, actually, everyone has one.

619
00:37:39,000 --> 00:37:42,000
 The distance between all of you, a student, will be big.

620
00:37:42,000 --> 00:37:43,000
 OK.

621
00:37:43,000 --> 00:37:47,000
 And if the data is harder, then, you know.

622
00:37:47,000 --> 00:37:51,000
 Then, actually, the distribution of the samples will be very sparse.

623
00:37:51,000 --> 00:37:53,000
 Very sparse.

624
00:37:53,000 --> 00:37:54,000
 OK.

625
00:37:54,000 --> 00:37:57,000
 And actually, previously, when we think about the tree,

626
00:37:57,000 --> 00:38:01,000
 we were designed or pen classified.

627
00:38:01,000 --> 00:38:04,000
 And normally, we assume that the samples in one class

628
00:38:04,000 --> 00:38:06,000
 are densely disputed.

629
00:38:06,000 --> 00:38:09,000
 The distribution is very compact, right?

630
00:38:09,000 --> 00:38:10,000
 Compact.

631
00:38:10,000 --> 00:38:12,000
 So in our future linear adjustment analysis,

632
00:38:12,000 --> 00:38:17,000
 we try to minimize, right, within class scatter.

633
00:38:17,000 --> 00:38:20,000
 Why we try to minimize within class scatter?

634
00:38:20,000 --> 00:38:23,000
 So this is because we hope that the data within the similar class

635
00:38:23,000 --> 00:38:28,000
 could be densely located in one region of the space.

636
00:38:28,000 --> 00:38:30,000
 So these are not assumptions, right?

637
00:38:30,000 --> 00:38:31,000
 Assumptions.

638
00:38:31,000 --> 00:38:32,000
 OK.

639
00:38:32,000 --> 00:38:37,000
 So densely disputed in one region or part of this space.

640
00:38:37,000 --> 00:38:39,000
 OK.

641
00:38:39,000 --> 00:38:42,000
 But here, because of hard deminial space, right,

642
00:38:42,000 --> 00:38:45,000
 hard deminality of the data, actually, the distribution of the data

643
00:38:45,000 --> 00:38:47,000
 is very sparse.

644
00:38:47,000 --> 00:38:48,000
 Very sparse.

645
00:38:48,000 --> 00:38:49,000
 OK.

646
00:38:49,000 --> 00:38:51,000
 The example that you can imagine, right?

647
00:38:51,000 --> 00:38:53,000
 Also, you stand in one line.

648
00:38:53,000 --> 00:38:55,000
 It's very dense, right?

649
00:38:55,000 --> 00:38:57,000
 But in two states, you have space.

650
00:38:57,000 --> 00:39:02,000
 You can imagine how about 3D, 10D, 100 dimensions.

651
00:39:02,000 --> 00:39:04,000
 That could be very sparse.

652
00:39:04,000 --> 00:39:05,000
 OK.

653
00:39:05,000 --> 00:39:08,000
 Then, actually, we have some issues, right?

654
00:39:08,000 --> 00:39:13,000
 And then another tree is called a distance concentration.

655
00:39:13,000 --> 00:39:16,000
 What does it mean of distance concentration?

656
00:39:16,000 --> 00:39:18,000
 So the meaning here is that, actually,

657
00:39:18,000 --> 00:39:23,000
 in the very hard deminial space, all the power distance,

658
00:39:23,000 --> 00:39:27,000
 power, I mean, the distance between the two samples, right?

659
00:39:27,000 --> 00:39:31,000
 The power distance may convert to the same values.

660
00:39:34,000 --> 00:39:37,000
 Actually, we hope a tree, just imagine a tree,

661
00:39:37,000 --> 00:39:41,000
 we hope, of course, we hope that the distance within the same class

662
00:39:41,000 --> 00:39:43,000
 should be similar, right?

663
00:39:43,000 --> 00:39:45,000
 So we try to minimize the scatter.

664
00:39:45,000 --> 00:39:52,000
 Meanwhile, we also try to maximize the between class difference,

665
00:39:52,000 --> 00:39:54,000
 the between class difference, right?

666
00:39:54,000 --> 00:39:56,000
 That means that the distance between the sample

667
00:39:56,000 --> 00:40:01,000
 in different classes, the difference, the distance could be big.

668
00:40:01,000 --> 00:40:04,000
 But now, even in the very hard deminial space, right?

669
00:40:04,000 --> 00:40:08,000
 The power distance, even the distance between two,

670
00:40:08,000 --> 00:40:11,000
 the sample in different classes,

671
00:40:11,000 --> 00:40:13,000
 and also the sample in the winning class,

672
00:40:13,000 --> 00:40:17,000
 they tend to have the same or similar values.

673
00:40:17,000 --> 00:40:21,000
 So this is the distance concentration.

674
00:40:21,000 --> 00:40:24,000
 Okay, then the data is very hard to separate, right?

675
00:40:24,000 --> 00:40:27,000
 Because the distance between, actually, the sample

676
00:40:27,000 --> 00:40:31,000
 within the same class or between different classes are all similar.

677
00:40:31,000 --> 00:40:35,000
 Then how to classify them, how to separate them?

678
00:40:35,000 --> 00:40:38,000
 Okay, so the crush on the mananulity,

679
00:40:38,000 --> 00:40:41,000
 the two aspects of this crush on the mananulity,

680
00:40:41,000 --> 00:40:44,000
 the distance concentration, and also the data capacity

681
00:40:44,000 --> 00:40:48,000
 all bring challenges to the pen classification,

682
00:40:48,000 --> 00:40:53,000
 not task or to the bring difficulties

683
00:40:53,000 --> 00:40:57,000
 or challenges to the classifier design.

684
00:40:57,000 --> 00:41:00,000
 Okay, and how to address this problem?

685
00:41:00,000 --> 00:41:05,000
 And of course, actually, one of the obvious solution

686
00:41:05,000 --> 00:41:10,000
 is to reduce the deminulity of the data, right?

687
00:41:10,000 --> 00:41:13,000
 And actually, whether or not that really, indeed,

688
00:41:13,000 --> 00:41:17,000
 then we can reduce it, and indeed, actually, we can reduce it.

689
00:41:17,000 --> 00:41:21,000
 And this is because, actually, during the data collection stage, right?

690
00:41:21,000 --> 00:41:27,000
 Maybe, actually, we have collected some features

691
00:41:27,000 --> 00:41:36,000
 which are actually irrelevant to the problem.

692
00:41:36,000 --> 00:41:43,000
 For example, if you want to classify a male or female student,

693
00:41:43,000 --> 00:41:47,000
 for example, okay, and with the color of the hair,

694
00:41:47,000 --> 00:41:50,000
 and maybe the color of the hair, at the beginning,

695
00:41:50,000 --> 00:41:52,000
 you collect all the information about a person, right?

696
00:41:52,000 --> 00:41:56,000
 You take pictures of the person, then you record the color of the hair,

697
00:41:56,000 --> 00:42:00,000
 you record the body weight, you record the height,

698
00:42:00,000 --> 00:42:04,000
 and you have every item of the,

699
00:42:04,000 --> 00:42:07,000
 and many items of the information, right?

700
00:42:07,000 --> 00:42:11,000
 You record the, but many of these features or attributes

701
00:42:11,000 --> 00:42:19,000
 are usually irrelevant to the classifier problem.

702
00:42:19,000 --> 00:42:22,000
 So this is one type of features.

703
00:42:22,000 --> 00:42:26,000
 Okay, certainly, now, we can remove this,

704
00:42:26,000 --> 00:42:29,000
 we can remove the top of features,

705
00:42:29,000 --> 00:42:32,000
 and then we can reduce the deminality of the data.

706
00:42:32,000 --> 00:42:35,000
 Okay, then another type of feature we can also remove.

707
00:42:35,000 --> 00:42:39,000
 So that is the redundant features, redundant.

708
00:42:39,000 --> 00:42:42,000
 Redundant features, that means, actually,

709
00:42:42,000 --> 00:42:49,000
 if one feature is included in the addition of one extra feature,

710
00:42:49,000 --> 00:42:53,000
 redundant feature, we're not bringing extra information,

711
00:42:53,000 --> 00:42:55,000
 because they are redundant.

712
00:42:55,000 --> 00:42:59,000
 Okay, so for this redundant feature, we can also remove.

713
00:42:59,000 --> 00:43:04,000
 Okay, so by removing, actually, the irrelevant,

714
00:43:04,000 --> 00:43:06,000
 all insignificant features,

715
00:43:06,000 --> 00:43:12,000
 and also by removing the removal of the redundant features,

716
00:43:12,000 --> 00:43:17,000
 and then we can reduce, actually, the deminality of the data.

717
00:43:17,000 --> 00:43:23,000
 And then, actually, these issues, like spasticity

718
00:43:23,000 --> 00:43:27,000
 and the traditional concentration,

719
00:43:27,000 --> 00:43:32,000
 actually, this problem can be elevated.

720
00:43:32,000 --> 00:43:36,000
 Okay, so that is just our topic, right?

721
00:43:36,000 --> 00:43:43,000
 Feature selection for deminality reduction and classification.

722
00:43:43,000 --> 00:43:46,000
 Okay, so, yeah, and of course,

723
00:43:46,000 --> 00:43:49,000
 besides these issues,

724
00:43:49,000 --> 00:43:52,000
 if we include too many features in the model,

725
00:43:52,000 --> 00:43:55,000
 we also have these computational issues, right?

726
00:43:55,000 --> 00:43:59,000
 Actually, the more features we have in the model,

727
00:43:59,000 --> 00:44:02,000
 the computational complexity is higher.

728
00:44:02,000 --> 00:44:08,000
 Okay, so computational complexity is also a consideration.

729
00:44:08,000 --> 00:44:10,000
 Okay.

730
00:44:10,000 --> 00:44:16,000
 So later we will see, actually, when we, you know, fit a model,

731
00:44:16,000 --> 00:44:20,000
 quite often, actually, we look at the ratio of the number of samples

732
00:44:20,000 --> 00:44:22,000
 to the number of features,

733
00:44:22,000 --> 00:44:25,000
 or to the number of parameters in the model.

734
00:44:25,000 --> 00:44:29,000
 Normally, for this ratio, the higher, the better.

735
00:44:29,000 --> 00:44:31,000
 Right?

736
00:44:31,000 --> 00:44:34,000
 So, in other words, if you have too many attributes,

737
00:44:34,000 --> 00:44:37,000
 and then you have to collect a large number of samples,

738
00:44:37,000 --> 00:44:41,000
 but in many scenarios, the collection of large numbers of samples

739
00:44:41,000 --> 00:44:45,000
 is very costly, or sometimes, actually, it is impossible.

740
00:44:45,000 --> 00:44:47,000
 Okay.

741
00:44:47,000 --> 00:44:50,000
 Because, actually, the data available could be very limited, right?

742
00:44:50,000 --> 00:44:54,000
 And so you cannot collect a large number of samples.

743
00:44:54,000 --> 00:44:55,000
 Okay.

744
00:44:55,000 --> 00:44:57,000
 So later we will talk about this issue further.

745
00:44:57,000 --> 00:44:59,000
 Okay.

746
00:44:59,000 --> 00:45:02,000
 And another tree is the generalization capacity.

747
00:45:02,000 --> 00:45:04,000
 And actually, this generalization capacity,

748
00:45:04,000 --> 00:45:07,000
 the model, right, relates to the ratio,

749
00:45:07,000 --> 00:45:10,000
 the generalization of the ratio of the number of samples

750
00:45:10,000 --> 00:45:12,000
 to the number of features.

751
00:45:12,000 --> 00:45:14,000
 Okay.

752
00:45:14,000 --> 00:45:16,000
 If we reduce the number of features,

753
00:45:16,000 --> 00:45:18,000
 and for a fixed number of samples,

754
00:45:18,000 --> 00:45:20,000
 and if we reduce the number of features,

755
00:45:20,000 --> 00:45:22,000
 then we can improve this ratio.

756
00:45:22,000 --> 00:45:25,000
 And then we can improve the generalization capability of the model.

757
00:45:25,000 --> 00:45:30,000
 Because for such a model, it is less likely to overfit the training data.

758
00:45:30,000 --> 00:45:34,000
 And then we have a higher chance to generalize better, right,

759
00:45:34,000 --> 00:45:37,000
 for the attesting data.

760
00:45:37,000 --> 00:45:38,000
 Okay.

761
00:45:38,000 --> 00:45:40,000
 So these are the considerations, right,

762
00:45:40,000 --> 00:45:45,000
 why we need, actually, to perform the deminotent reduction

763
00:45:45,000 --> 00:45:48,000
 from the computational point of view

764
00:45:48,000 --> 00:45:51,000
 from the generalization point of view.

765
00:45:51,000 --> 00:45:52,000
 Okay.

766
00:45:52,000 --> 00:45:55,000
 We have reasons to do the deminotent reduction.

767
00:45:55,000 --> 00:45:57,000
 Okay.

768
00:46:01,000 --> 00:46:03,000
 Okay.

769
00:46:03,000 --> 00:46:04,000
 Okay.

770
00:46:04,000 --> 00:46:09,000
 So this is the major task of the feature selection.

771
00:46:09,000 --> 00:46:10,000
 Okay.

772
00:46:10,000 --> 00:46:14,000
 So given actually a number of features,

773
00:46:14,000 --> 00:46:18,000
 normally the number, normally the larger number, now, okay.

774
00:46:18,000 --> 00:46:23,000
 And how can we select the most important one?

775
00:46:23,000 --> 00:46:24,000
 Of the features.

776
00:46:24,000 --> 00:46:27,000
 We could assume so many features, right,

777
00:46:27,000 --> 00:46:30,000
 so many, actually, variables, attributes.

778
00:46:30,000 --> 00:46:31,000
 Okay.

779
00:46:31,000 --> 00:46:33,000
 All of them are useful.

780
00:46:33,000 --> 00:46:34,000
 Okay.

781
00:46:34,000 --> 00:46:36,000
 Not all of them are important.

782
00:46:36,000 --> 00:46:37,000
 Okay.

783
00:46:37,000 --> 00:46:44,000
 So here, actually, how do we kind of select the important ones?

784
00:46:44,000 --> 00:46:45,000
 Okay.

785
00:46:45,000 --> 00:46:49,000
 And at the same time, actually, we should actually

786
00:46:49,000 --> 00:46:55,000
 retain as much as possible of the cloud discriminatory information,

787
00:46:55,000 --> 00:46:57,000
 discriminatory information.

788
00:46:57,000 --> 00:46:58,000
 Okay.

789
00:46:58,000 --> 00:47:02,000
 Actually, a feature should be discriminative or discriminatory.

790
00:47:02,000 --> 00:47:03,000
 Okay.

791
00:47:03,000 --> 00:47:08,000
 And just now I use one example, like a hair color, right?

792
00:47:08,000 --> 00:47:12,000
 The hair color, then all of them, most of us,

793
00:47:12,000 --> 00:47:16,000
 male or female, can all have no black color, right?

794
00:47:16,000 --> 00:47:20,000
 For this feature, and this feature does not provide

795
00:47:20,000 --> 00:47:22,000
 a discriminative information.

796
00:47:22,000 --> 00:47:23,000
 Okay.

797
00:47:23,000 --> 00:47:26,000
 Because, actually, you all have this value, right?

798
00:47:26,000 --> 00:47:28,000
 The same value for male or female.

799
00:47:28,000 --> 00:47:30,000
 We cannot use this value to discriminate,

800
00:47:30,000 --> 00:47:34,000
 to separate a male from a female student.

801
00:47:34,000 --> 00:47:35,000
 Okay.

802
00:47:35,000 --> 00:47:38,000
 But when we do the feature selection, when we, you know,

803
00:47:38,000 --> 00:47:43,000
 remove the redundant features and then remove the

804
00:47:43,000 --> 00:47:47,000
 insignificant features, we remove them.

805
00:47:47,000 --> 00:47:48,000
 Okay.

806
00:47:48,000 --> 00:47:49,000
 We should keep.

807
00:47:49,000 --> 00:47:50,000
 Why we remove them?

808
00:47:50,000 --> 00:47:53,000
 Because they are not discriminatory or they are redundant, right?

809
00:47:53,000 --> 00:47:59,000
 So when we remove them, we should keep or retain as much as

810
00:47:59,000 --> 00:48:02,000
 possible the discriminatory information.

811
00:48:02,000 --> 00:48:03,000
 Okay.

812
00:48:03,000 --> 00:48:06,000
 So this basically tells us the target of feature selection,

813
00:48:06,000 --> 00:48:07,000
 right?

814
00:48:07,000 --> 00:48:10,000
 You reduce the number of features, but you also need to keep

815
00:48:10,000 --> 00:48:14,000
 as much as possible the discriminatory information and

816
00:48:14,000 --> 00:48:17,000
 allowing all of the original features.

817
00:48:17,000 --> 00:48:18,000
 Okay.

818
00:48:18,000 --> 00:48:23,000
 The selection feature should not actually lose, okay,

819
00:48:23,000 --> 00:48:29,000
 should not lose as much of the discriminatory information.

820
00:48:29,000 --> 00:48:30,000
 Okay.

821
00:48:30,000 --> 00:48:34,000
 That means we need to remove the insignificant or

822
00:48:34,000 --> 00:48:40,000
 irrelevant features and then we should keep also the redundant

823
00:48:40,000 --> 00:48:41,000
 features, right?

824
00:48:41,000 --> 00:48:44,000
 And then keep the non-redundant and also the

825
00:48:44,000 --> 00:48:48,000
 significant features.

826
00:48:48,000 --> 00:48:51,000
 Of course, how to evaluate significance or the usefulness

827
00:48:51,000 --> 00:48:55,000
 of a feature, we'll talk about this later.

828
00:48:55,000 --> 00:48:56,000
 Okay.

829
00:48:56,000 --> 00:48:59,000
 So here we first now have a base idea about the target or the

830
00:48:59,000 --> 00:49:03,000
 goal of the feature selection for the minority reduction.

831
00:49:03,000 --> 00:49:04,000
 Okay.

832
00:49:04,000 --> 00:49:06,000
 Indeed, we use small numbers, right?

833
00:49:06,000 --> 00:49:09,000
 We can reduce the minority data.

834
00:49:09,000 --> 00:49:12,000
 Then we can solve the issues, right?

835
00:49:12,000 --> 00:49:15,000
 And then we can also reduce the number of features.

836
00:49:15,000 --> 00:49:16,000
 Okay.

837
00:49:16,000 --> 00:49:20,000
 So we have two aspects of the class of the minority.

838
00:49:20,000 --> 00:49:22,000
 But how to select?

839
00:49:22,000 --> 00:49:24,000
 Kind of arbitrary, right?

840
00:49:24,000 --> 00:49:27,000
 Kind of randomly select a small number, right?

841
00:49:27,000 --> 00:49:30,000
 Actually, we should actually carefully select a small number

842
00:49:30,000 --> 00:49:37,000
 of features so that the discriminatory information is kept

843
00:49:37,000 --> 00:49:40,000
 or retained as much as possible.

844
00:49:40,000 --> 00:49:41,000
 Okay.

845
00:49:41,000 --> 00:49:56,000
 So that's actually another concept here we need to understand.

846
00:49:56,000 --> 00:49:59,000
 It is a feature selection and a feature illustration.

847
00:49:59,000 --> 00:50:02,000
 And actually, the feature illustration and feature illustration

848
00:50:02,000 --> 00:50:06,000
 are two different methods for minority reduction.

849
00:50:06,000 --> 00:50:10,000
 And actually, the feature illustration is actually

850
00:50:10,000 --> 00:50:14,000
 about selecting a subset of the original features,

851
00:50:14,000 --> 00:50:16,000
 a subset of the original features.

852
00:50:16,000 --> 00:50:19,000
 So this is called a feature selection.

853
00:50:19,000 --> 00:50:20,000
 Okay.

854
00:50:20,000 --> 00:50:22,000
 And then what is feature illustration?

855
00:50:22,000 --> 00:50:29,000
 Actually, feature illustration is to create new features.

856
00:50:29,000 --> 00:50:33,000
 And actually, we try to actually compress the information

857
00:50:33,000 --> 00:50:36,000
 underlying the original full features, right,

858
00:50:36,000 --> 00:50:39,000
 into a small set of new features.

859
00:50:39,000 --> 00:50:42,000
 This is called a feature illustration.

860
00:50:42,000 --> 00:50:43,000
 Okay.

861
00:50:43,000 --> 00:50:45,000
 So we create new features.

862
00:50:45,000 --> 00:50:46,000
 Okay.

863
00:50:46,000 --> 00:50:51,000
 In the feature selection, we select a subset of the original features.

864
00:50:51,000 --> 00:50:55,000
 In the feature illustration, we create new features.

865
00:50:55,000 --> 00:50:56,000
 Okay.

866
00:50:56,000 --> 00:51:00,000
 And each of these new features is a function

867
00:51:00,000 --> 00:51:04,000
 of all of the original features.

868
00:51:04,000 --> 00:51:05,000
 Okay.

869
00:51:05,000 --> 00:51:08,000
 So maybe I use this diagram to illustrate this.

870
00:51:08,000 --> 00:51:09,000
 Okay.

871
00:51:09,000 --> 00:51:13,000
 So this is, you know, usually we have five features,

872
00:51:13,000 --> 00:51:15,000
 X, Y, S2, S3, S4, S5.

873
00:51:15,000 --> 00:51:16,000
 Okay.

874
00:51:16,000 --> 00:51:20,000
 So feature selection tries to select a subset.

875
00:51:20,000 --> 00:51:24,000
 So subset is just an X, Y, S2, S5.

876
00:51:24,000 --> 00:51:25,000
 Okay.

877
00:51:25,000 --> 00:51:28,000
 So these features, these features are important features,

878
00:51:28,000 --> 00:51:32,000
 not redundant, and also, you know, discriminative features.

879
00:51:32,000 --> 00:51:33,000
 Okay.

880
00:51:33,000 --> 00:51:35,000
 So we have this feature selection.

881
00:51:35,000 --> 00:51:38,000
 This is called feature selection, a subset of the original features.

882
00:51:38,000 --> 00:51:39,000
 All right.

883
00:51:39,000 --> 00:51:45,000
 So in this feature subset, all the original manuals of the features are also,

884
00:51:45,000 --> 00:51:47,000
 the manuals are retained.

885
00:51:47,000 --> 00:51:48,000
 Okay.

886
00:51:48,000 --> 00:51:50,000
 So we retain the original features.

887
00:51:50,000 --> 00:51:51,000
 Okay.

888
00:51:51,000 --> 00:51:54,000
 Of course, we retain the manuals of the original features.

889
00:51:54,000 --> 00:51:55,000
 So this is very important.

890
00:51:55,000 --> 00:52:00,000
 Actually, if you want to build a model that is, is planable, is planable.

891
00:52:00,000 --> 00:52:01,000
 Okay.

892
00:52:01,000 --> 00:52:05,000
 And feature extraction, just mentioned, create new features.

893
00:52:05,000 --> 00:52:08,000
 Again, originally we have these five features, right?

894
00:52:08,000 --> 00:52:13,000
 In the feature extraction, actually, we create new features, like V1, V2.

895
00:52:13,000 --> 00:52:17,000
 And each of these features is a function of the original features.

896
00:52:17,000 --> 00:52:18,000
 Okay.

897
00:52:18,000 --> 00:52:21,000
 You know, for V1, we have function F1.

898
00:52:21,000 --> 00:52:26,000
 For V2, this new feature, V2, we have a function F2.

899
00:52:26,000 --> 00:52:27,000
 Okay.

900
00:52:27,000 --> 00:52:30,000
 And this F1, F2 could be linear or nonlinear functions.

901
00:52:30,000 --> 00:52:33,000
 Could be linear, or could be nonlinear.

902
00:52:33,000 --> 00:52:34,000
 Okay.

903
00:52:34,000 --> 00:52:39,000
 And the commonly used principle of Kapun-Ny analysis, probably you heard this, right?

904
00:52:39,000 --> 00:52:40,000
 Or singular value decomposition.

905
00:52:40,000 --> 00:52:46,000
 So this technique belongs to the second type of the deminology reduction, feature extraction.

906
00:52:46,000 --> 00:52:52,000
 Because actually, in the principle of Kapun-Ny analysis, each of the new features is a linear

907
00:52:52,000 --> 00:52:56,000
 combination of all the original features.

908
00:52:56,000 --> 00:53:01,000
 F1, F2, actually are two different linear functions.

909
00:53:01,000 --> 00:53:05,000
 Two different, of course, the form of the linear function is the same, but just the

910
00:53:05,000 --> 00:53:09,000
 coefficient of the functions are nonlinear, are different.

911
00:53:09,000 --> 00:53:10,000
 Okay.

912
00:53:10,000 --> 00:53:15,000
 So V1, V2, actually, now, actually, the meaningless.

913
00:53:15,000 --> 00:53:23,000
 For example, actually, X1 is a gender, X2 is a nationality, S3 is your degree.

914
00:53:23,000 --> 00:53:25,000
 What is the meaning of the variable?

915
00:53:25,000 --> 00:53:31,000
 Your gender times 0.2, plus your nationality times 0.4, and plus your degree, plus, you

916
00:53:31,000 --> 00:53:34,000
 know, the new variable, right, new feature.

917
00:53:34,000 --> 00:53:35,000
 You don't have a meaning.

918
00:53:35,000 --> 00:53:40,000
 And then, of course, a model based on V1, V2 is hard to explain.

919
00:53:40,000 --> 00:53:42,000
 Hard to explain.

920
00:53:42,000 --> 00:53:43,000
 Okay.

921
00:53:43,000 --> 00:53:47,000
 So you want to build, actually, you know, explain them all model, and we prefer to use the super

922
00:53:47,000 --> 00:53:49,000
 feature selection.

923
00:53:49,000 --> 00:53:53,000
 Of course, actually, maybe feature extraction could be more efficient.

924
00:53:53,000 --> 00:53:55,000
 Could be more efficient.

925
00:53:55,000 --> 00:54:01,000
 Because, actually, it will use all of the information, right, X1, X1, X2, and X5.

926
00:54:01,000 --> 00:54:08,000
 So the limitation of this feature extraction is that the meanings of the original features

927
00:54:08,000 --> 00:54:10,000
 are lost.

928
00:54:10,000 --> 00:54:11,000
 Okay.

929
00:54:11,000 --> 00:54:18,000
 But the feature extraction, selection, can keep the meanings of the original features.

930
00:54:18,000 --> 00:54:19,000
 Okay.

931
00:54:19,000 --> 00:54:22,000
 So in this course, actually, we focus on the feature selection.

932
00:54:22,000 --> 00:54:26,000
 And because in other courses, normally, when we talk about the knowledge reduction, and

933
00:54:26,000 --> 00:54:31,000
 what they mean, actually, is the feature extraction, like a principle of carbon analysis.

934
00:54:31,000 --> 00:54:37,000
 In other courses, right, you probably learn PCA, but that's the feature extraction.

935
00:54:37,000 --> 00:54:42,000
 So in this course, actually, we focus on the feature selection.

936
00:54:42,000 --> 00:54:43,000
 Okay.

937
00:54:43,000 --> 00:54:46,000
 So, naturally, we have a 10-minute break.

938
00:54:46,000 --> 00:54:50,000
 After a break, we continue to talk about the evaluation of the features.

939
00:54:50,000 --> 00:54:53,000
 So now, we mentioned, insignificant or redundant, right.

940
00:54:53,000 --> 00:54:55,000
 So how to evaluate this?

941
00:54:55,000 --> 00:54:56,000
 Okay.

942
00:54:56,000 --> 00:55:05,000
 So next, actually, we will talk about this.

943
00:55:25,000 --> 00:55:32,000
 Okay.

944
00:55:55,000 --> 00:56:05,000
 Okay.

945
00:56:25,000 --> 00:56:35,000
 Okay.

946
00:56:55,000 --> 00:57:05,000
 Okay.

947
00:57:05,000 --> 00:57:15,000
 Okay.

948
00:57:15,000 --> 00:57:25,000
 Okay.

949
00:57:25,000 --> 00:57:35,000
 Okay.

950
00:57:35,000 --> 00:57:45,000
 Okay.

951
00:57:45,000 --> 00:57:55,000
 Okay.

952
00:57:55,000 --> 00:58:05,000
 Okay.

953
00:58:05,000 --> 00:58:15,000
 Okay.

954
00:58:15,000 --> 00:58:25,000
 Okay.

955
00:58:25,000 --> 00:58:35,000
 Okay.

956
00:58:35,000 --> 00:58:45,000
 Okay.

957
00:58:45,000 --> 00:58:55,000
 Okay.

958
00:58:55,000 --> 00:59:05,000
 Okay.

959
00:59:05,000 --> 00:59:15,000
 Okay.

960
00:59:15,000 --> 00:59:25,000
 Okay.

961
00:59:25,000 --> 00:59:35,000
 Okay.

962
00:59:35,000 --> 00:59:45,000
 Okay.

963
00:59:45,000 --> 00:59:55,000
 Okay.

964
00:59:55,000 --> 01:00:05,000
 Okay.

965
01:00:05,000 --> 01:00:15,000
 Okay.

966
01:00:15,000 --> 01:00:24,000
 Okay.

967
01:00:24,000 --> 01:00:34,000
 Okay.

968
01:00:34,000 --> 01:00:44,000
 Okay.

969
01:00:44,000 --> 01:00:53,000
 Okay.

970
01:00:53,000 --> 01:01:03,000
 Okay.

971
01:01:03,000 --> 01:01:13,000
 Okay.

972
01:01:13,000 --> 01:01:22,000
 Okay.

973
01:01:22,000 --> 01:01:32,000
 Okay.

974
01:01:32,000 --> 01:01:42,000
 Okay.

975
01:01:42,000 --> 01:01:47,000
 Okay.

976
01:01:47,000 --> 01:01:57,000
 Okay.

977
01:01:57,000 --> 01:02:07,000
 Okay.

978
01:02:07,000 --> 01:02:12,000
 Okay.

979
01:02:12,000 --> 01:02:22,000
 Okay.

980
01:02:22,000 --> 01:02:32,000
 Okay.

981
01:02:32,000 --> 01:02:37,000
 Okay.

982
01:02:37,000 --> 01:02:47,000
 Okay.

983
01:02:47,000 --> 01:02:57,000
 Okay.

984
01:02:57,000 --> 01:03:02,000
 Okay.

985
01:03:02,000 --> 01:03:12,000
 Okay.

986
01:03:12,000 --> 01:03:22,000
 Okay.

987
01:03:22,000 --> 01:03:27,000
 Okay.

988
01:03:27,000 --> 01:03:37,000
 Okay.

989
01:03:37,000 --> 01:03:47,000
 Okay.

990
01:03:47,000 --> 01:03:52,000
 Okay.

991
01:03:52,000 --> 01:04:02,000
 Okay.

992
01:04:02,000 --> 01:04:12,000
 Okay.

993
01:04:12,000 --> 01:04:17,000
 Okay.

994
01:04:17,000 --> 01:04:27,000
 Okay.

995
01:04:27,000 --> 01:04:37,000
 Okay.

996
01:04:37,000 --> 01:04:42,000
 Okay.

997
01:04:42,000 --> 01:04:52,000
 Okay.

998
01:04:52,000 --> 01:05:02,000
 Okay.

999
01:05:02,000 --> 01:05:07,000
 Okay.

1000
01:05:07,000 --> 01:05:17,000
 Okay.

1001
01:05:37,000 --> 01:05:47,000
 Okay.

1002
01:05:47,000 --> 01:05:57,000
 Okay.

1003
01:05:57,000 --> 01:06:02,000
 Okay.

1004
01:06:02,000 --> 01:06:12,000
 Okay.

1005
01:06:12,000 --> 01:06:22,000
 Okay.

1006
01:06:22,000 --> 01:06:27,000
 Okay.

1007
01:06:27,000 --> 01:06:37,000
 Okay.

1008
01:06:37,000 --> 01:06:47,000
 Okay.

1009
01:06:47,000 --> 01:06:56,000
 Okay.

1010
01:06:56,000 --> 01:07:05,000
 Okay.

1011
01:07:05,000 --> 01:07:07,000
 Okay.

1012
01:07:07,000 --> 01:07:14,000
 So, that's actually we look at the so-called picking phenomenon.

1013
01:07:14,000 --> 01:07:20,000
 So, this is also the reason why we want to do the feature oscillation, okay, or feature

1014
01:07:20,000 --> 01:07:21,000
 oscillation.

1015
01:07:21,000 --> 01:07:24,000
 So, this is the reason why we want to do the deminology reduction.

1016
01:07:24,000 --> 01:07:29,000
 So, here we assume we have a data set with n samples.

1017
01:07:29,000 --> 01:07:35,000
 And so, also we have a number of features, L. Okay.

1018
01:07:35,000 --> 01:07:40,000
 Of course, based on the data we can train a classifier.

1019
01:07:40,000 --> 01:07:45,000
 I think we can look at the performance of the classifier on the training or testing data.

1020
01:07:45,000 --> 01:07:46,000
 Right?

1021
01:07:46,000 --> 01:07:47,000
 Okay.

1022
01:07:47,000 --> 01:07:50,000
 So, of course, we assume this is a linear, right?

1023
01:07:50,000 --> 01:07:51,000
 Okay.

1024
01:07:51,000 --> 01:07:55,000
 So, this is a general form of a linear classifier, right?

1025
01:07:55,000 --> 01:08:00,000
 Either it's a nonlinear, it's a linear suborbital machine or linear, feature linear disk analysis.

1026
01:08:00,000 --> 01:08:03,000
 You know, the classifier have the same form, right?

1027
01:08:03,000 --> 01:08:05,000
 Just a different method.

1028
01:08:05,000 --> 01:08:09,000
 We have different ways to determine the value of W0 and W.

1029
01:08:09,000 --> 01:08:15,000
 So, for this linear classifier, actually, the parameter number in this model is just

1030
01:08:15,000 --> 01:08:17,000
 actually L plus 1.

1031
01:08:17,000 --> 01:08:21,000
 Because we have one additional term, that is the bias term, right?

1032
01:08:21,000 --> 01:08:22,000
 Bias term.

1033
01:08:22,000 --> 01:08:23,000
 Okay, W is 0.

1034
01:08:23,000 --> 01:08:28,000
 And for this W transpose, here W, here, the number of features, you know, the deminology

1035
01:08:28,000 --> 01:08:31,000
 of this W is just the same as X, right?

1036
01:08:31,000 --> 01:08:36,000
 So, if the number of features is L, then the number of features and parameters actually,

1037
01:08:36,000 --> 01:08:41,000
 or unknown parameters in the model, actually is M plus 1.

1038
01:08:41,000 --> 01:08:42,000
 Okay.

1039
01:08:42,000 --> 01:08:43,000
 So, okay.

1040
01:08:43,000 --> 01:08:44,000
 So, there's a tree we look at this curve.

1041
01:08:44,000 --> 01:08:45,000
 Look at this curve.

1042
01:08:45,000 --> 01:08:49,000
 So, the horizontal axis shows the number of features.

1043
01:08:49,000 --> 01:08:50,000
 Okay.

1044
01:08:50,000 --> 01:08:53,000
 And the vertical axis through the probability of an area.

1045
01:08:53,000 --> 01:08:57,000
 You can just think, oh, this is an area on the testing data.

1046
01:08:57,000 --> 01:09:01,000
 You can just imagine, probability of an area is just an area on the testing data.

1047
01:09:01,000 --> 01:09:02,000
 Okay.

1048
01:09:02,000 --> 01:09:09,000
 And actually, you can see, at the beginning, actually, the area is big, right?

1049
01:09:09,000 --> 01:09:11,000
 So, even a small number of features is used.

1050
01:09:11,000 --> 01:09:13,000
 For example, just one feature is used.

1051
01:09:13,000 --> 01:09:14,000
 Okay.

1052
01:09:14,000 --> 01:09:16,000
 And then we can have a big area.

1053
01:09:16,000 --> 01:09:18,000
 Big area, right?

1054
01:09:18,000 --> 01:09:24,000
 If we increase the number of features, right, from 1 to L1 for the first case, if the number

1055
01:09:24,000 --> 01:09:27,000
 of samples equals M1.

1056
01:09:27,000 --> 01:09:33,000
 And so, when the number of features L1, here, actually, we have this area.

1057
01:09:33,000 --> 01:09:34,000
 Okay.

1058
01:09:34,000 --> 01:09:35,000
 And this area is the minimum.

1059
01:09:35,000 --> 01:09:36,000
 Okay.

1060
01:09:36,000 --> 01:09:43,000
 After this L1 critical value, and if we continue to increase the value of L, actually, then

1061
01:09:43,000 --> 01:09:46,000
 the probability of area increase.

1062
01:09:46,000 --> 01:09:47,000
 Increase.

1063
01:09:47,000 --> 01:09:48,000
 Okay.

1064
01:09:48,000 --> 01:09:53,000
 And so, the best performance achieved at this L1, actually, L1.

1065
01:09:53,000 --> 01:09:54,000
 Okay.

1066
01:09:54,000 --> 01:09:56,000
 So, these are critical values.

1067
01:09:56,000 --> 01:09:57,000
 Okay.

1068
01:09:57,000 --> 01:09:58,000
 And so, these are three phenomena.

1069
01:09:58,000 --> 01:09:59,000
 It's called picking phenomena.

1070
01:09:59,000 --> 01:10:01,000
 Now, we have a picking performance.

1071
01:10:01,000 --> 01:10:03,000
 We have a picking performance, right?

1072
01:10:03,000 --> 01:10:09,000
 And when the number of features actually used in the model, they equal to a critical value

1073
01:10:09,000 --> 01:10:10,000
 L1.

1074
01:10:10,000 --> 01:10:11,000
 Okay.

1075
01:10:11,000 --> 01:10:18,000
 So, from this point of, this is a non-peckling phenomenon, and we can see the reason, right,

1076
01:10:18,000 --> 01:10:24,000
 the motivation for the demagnitude reduction or feature selection here.

1077
01:10:24,000 --> 01:10:25,000
 Okay.

1078
01:10:25,000 --> 01:10:32,000
 Because if we use all the features, and the generalization capability may be very bad.

1079
01:10:32,000 --> 01:10:33,000
 Okay.

1080
01:10:33,000 --> 01:10:36,000
 And we should select, actually, the number of features.

1081
01:10:36,000 --> 01:10:37,000
 Okay.

1082
01:10:37,000 --> 01:10:41,000
 And so, that, actually, the best performance could be achieved.

1083
01:10:41,000 --> 01:10:44,000
 Of course, this best performance is not for training data.

1084
01:10:44,000 --> 01:10:45,000
 Not for training.

1085
01:10:45,000 --> 01:10:50,000
 And we will not observe this non-peckling phenomenon on the training data.

1086
01:10:50,000 --> 01:10:55,000
 Normally, for the training data, the more features we use, the better the performance.

1087
01:10:55,000 --> 01:10:56,000
 Okay.

1088
01:10:56,000 --> 01:11:01,000
 So, this is the area, the area for the generalization, generalization area.

1089
01:11:01,000 --> 01:11:05,000
 That means the area under unseen testing data.

1090
01:11:05,000 --> 01:11:06,000
 Okay.

1091
01:11:06,000 --> 01:11:12,000
 And so, actually, then, in practice, how to determine this L?

1092
01:11:12,000 --> 01:11:17,000
 How we can find out, you know, this critical value, so that the performance is the best

1093
01:11:17,000 --> 01:11:23,000
 for the future data, for the testing data.

1094
01:11:23,000 --> 01:11:29,000
 And actually, now, we can select this critical value L based on the performance on the validation

1095
01:11:29,000 --> 01:11:30,000
 data.

1096
01:11:30,000 --> 01:11:35,000
 So, this number of critical values, right, L1, this L, actually, normally, you know, it's

1097
01:11:35,000 --> 01:11:39,000
 determined, it's considered as a hyper parameter.

1098
01:11:39,000 --> 01:11:46,000
 And hyper parameter, that means that we need to determine the value for this hyper parameter

1099
01:11:46,000 --> 01:11:49,000
 and using the validation data.

1100
01:11:49,000 --> 01:11:50,000
 Okay.

1101
01:11:50,000 --> 01:11:56,000
 So, normally, you know, we, in practice, we use one feature to train a classifier, for

1102
01:11:56,000 --> 01:11:57,000
 example.

1103
01:11:57,000 --> 01:11:58,000
 We have two features to train a classifier.

1104
01:11:58,000 --> 01:12:00,000
 You have three features to train a classifier.

1105
01:12:00,000 --> 01:12:04,000
 That is five or, you know, we increase the number of features.

1106
01:12:04,000 --> 01:12:09,000
 For each number of features, actually, we use and we train a parameter classifier.

1107
01:12:09,000 --> 01:12:13,000
 And then we check the performance on the validation data.

1108
01:12:13,000 --> 01:12:14,000
 Okay.

1109
01:12:14,000 --> 01:12:19,000
 And then, based on the performance on the validation data, and probably we can find, actually,

1110
01:12:19,000 --> 01:12:25,000
 this critical value, which will produce the best performance for the future unseen testing

1111
01:12:25,000 --> 01:12:26,000
 data.

1112
01:12:27,000 --> 01:12:30,000
 I just mentioned on the training data, you won't see such a curve.

1113
01:12:30,000 --> 01:12:36,000
 You won't see this a peak phenomenon because the, actually, the probability of error actually

1114
01:12:36,000 --> 01:12:38,000
 decrease more and more non-tonically normally.

1115
01:12:38,000 --> 01:12:43,000
 The more features you use in the model, the better performance on the training data.

1116
01:12:43,000 --> 01:12:44,000
 Okay.

1117
01:12:44,000 --> 01:12:47,000
 So, this is for the validation data, for the testing data.

1118
01:12:47,000 --> 01:12:50,000
 So, this is the generalization error.

1119
01:12:50,000 --> 01:12:51,000
 Okay.

1120
01:12:51,000 --> 01:12:58,000
 Because the ultimate goal for the modeling, right, is to use the model for future data,

1121
01:12:58,000 --> 01:12:59,000
 not for training data.

1122
01:12:59,000 --> 01:13:03,000
 For training data, you already have the class label, right, or you already know the class

1123
01:13:03,000 --> 01:13:04,000
 label.

1124
01:13:04,000 --> 01:13:06,000
 There is no need to use a model to predict.

1125
01:13:06,000 --> 01:13:10,000
 What we want is to use a model to predict the label of future data.

1126
01:13:10,000 --> 01:13:17,000
 So, we are concerned with the probability of error of the future data, the testing data.

1127
01:13:17,000 --> 01:13:18,000
 Okay.

1128
01:13:18,000 --> 01:13:20,000
 So, we can determine this.

1129
01:13:20,000 --> 01:13:21,000
 Okay.

1130
01:13:21,000 --> 01:13:22,000
 So, this is a peak phenomenon.

1131
01:13:22,000 --> 01:13:30,000
 And, actually, from here, actually, we know this diagram shows two scenarios, two curves,

1132
01:13:30,000 --> 01:13:31,000
 right?

1133
01:13:31,000 --> 01:13:34,000
 And one curve, actually, that is n equals n1.

1134
01:13:34,000 --> 01:13:37,000
 The number of samples equals n1.

1135
01:13:37,000 --> 01:13:44,000
 And the second curve actually shows the scenario, shows actually when n equals to n2.

1136
01:13:44,000 --> 01:13:49,000
 Actually, n2 is much greater than n1.

1137
01:13:49,000 --> 01:13:56,000
 So, and from this, actually, we can also see, actually, if a larger number of training data

1138
01:13:56,000 --> 01:13:58,000
 is used, n2, right?

1139
01:13:58,000 --> 01:14:07,000
 And then, actually, the critical value, L2, is greater because L2 here is greater than

1140
01:14:07,000 --> 01:14:08,000
 L1, right?

1141
01:14:08,000 --> 01:14:14,000
 So, the critical value is greater if the number of training samples is greater than more.

1142
01:14:14,000 --> 01:14:15,000
 Okay.

1143
01:14:15,000 --> 01:14:20,000
 So, the performance is better because the probability error is better, right?

1144
01:14:20,000 --> 01:14:27,000
 It's lower than the first scenario, where n1 is less than n2.

1145
01:14:27,000 --> 01:14:28,000
 Okay.

1146
01:14:28,000 --> 01:14:31,000
 So, this diagram is, first, actually, show the thing, right?

1147
01:14:31,000 --> 01:14:32,000
 The peak phenomenon.

1148
01:14:32,000 --> 01:14:39,000
 That means, actually, we should select a number of features so that we can achieve the best

1149
01:14:39,000 --> 01:14:42,000
 performance on the future data, testing data.

1150
01:14:42,000 --> 01:14:43,000
 Okay.

1151
01:14:43,000 --> 01:14:44,000
 We don't use all of them.

1152
01:14:44,000 --> 01:14:46,000
 We should perform feature selection.

1153
01:14:46,000 --> 01:14:51,000
 And this number, you know, finally, the number of features including the model, the best,

1154
01:14:51,000 --> 01:14:56,000
 actually, you know, it should be equal to this, actually, critical value, critical value.

1155
01:14:56,000 --> 01:14:57,000
 Okay.

1156
01:14:57,000 --> 01:14:59,000
 This is the first thing from this diagram, right?

1157
01:14:59,000 --> 01:15:03,000
 Then the second thing is that, actually, we should use, actually, a larger number of training

1158
01:15:03,000 --> 01:15:04,000
 samples.

1159
01:15:04,000 --> 01:15:10,000
 If we use a larger number of training samples, then the critical value, L2, is larger.

1160
01:15:10,000 --> 01:15:15,000
 And then also, actually, a better performance could be achieved.

1161
01:15:15,000 --> 01:15:16,000
 Okay.

1162
01:15:16,000 --> 01:15:19,000
 So, this is known in the machine learning program.

1163
01:15:19,000 --> 01:15:26,000
 We always try to keep the terminology of the data low, try to collect, actually, as many

1164
01:15:26,000 --> 01:15:28,000
 samples as possible.

1165
01:15:28,000 --> 01:15:29,000
 Okay.

1166
01:15:29,000 --> 01:15:31,000
 We increase n2.

1167
01:15:31,000 --> 01:15:35,000
 And then we also reduce the terminology.

1168
01:15:35,000 --> 01:15:42,000
 So, from these two, actually, you know, aspects, and we improve, actually, the generalization

1169
01:15:42,000 --> 01:15:44,000
 capability of the model.

1170
01:15:44,000 --> 01:15:49,000
 That means that we want to achieve the small, actually, probability of error for the future

1171
01:15:49,000 --> 01:15:52,000
 data, for the testing data.

1172
01:15:52,000 --> 01:15:53,000
 Okay.

1173
01:15:53,000 --> 01:15:59,000
 So, this is, actually, you know, whatever we can get, actually, from this, actually, from

1174
01:15:59,000 --> 01:16:02,000
 this, you know, figure, right, the picking phenomenon.

1175
01:16:02,000 --> 01:16:09,000
 And also, these are, you know, figures certainly give us, Godland, right, on the future selection,

1176
01:16:09,000 --> 01:16:12,000
 and also the training sample collection.

1177
01:16:12,000 --> 01:16:17,000
 We should try our best to collect as many as possible, the samples, right.

1178
01:16:17,000 --> 01:16:20,000
 For features, of course, at the beginning, we can collect.

1179
01:16:20,000 --> 01:16:26,000
 If you don't know what features are useful, we should collect as many features as possible.

1180
01:16:26,000 --> 01:16:30,000
 But then we need to go through the so-called terminology reduction.

1181
01:16:30,000 --> 01:16:33,000
 And in particular, here, the future selection process.

1182
01:16:33,000 --> 01:16:34,000
 Okay.

1183
01:16:34,000 --> 01:16:40,000
 And so that, actually, the number of features finally used in the model, actually, would

1184
01:16:40,000 --> 01:16:43,000
 be the, actually, the critical value.

1185
01:16:43,000 --> 01:16:49,000
 And so that the probability of error for the testing data is the minimum.

1186
01:16:49,000 --> 01:16:51,000
 Okay.

1187
01:16:51,000 --> 01:16:56,000
 So, picking phenomenon.

1188
01:16:57,000 --> 01:16:59,000
 So, picking phenomenon.

1189
01:16:59,000 --> 01:17:00,000
 Okay.

1190
01:17:00,000 --> 01:17:01,000
 You need to know these picking phenomenon.

1191
01:17:01,000 --> 01:17:06,000
 And also, from the picking phenomenon, actually, the Godland lines on the data collection,

1192
01:17:06,000 --> 01:17:07,000
 right.

1193
01:17:07,000 --> 01:17:08,000
 Okay.

1194
01:17:08,000 --> 01:17:09,000
 And, okay.

1195
01:17:09,000 --> 01:17:16,000
 So, this is just the thing I just talked about, actually, N2 greater than N1, actually,

1196
01:17:16,000 --> 01:17:22,000
 L2, critical value, L2, larger than N1, L1, and then, actually, even smaller areas are

1197
01:17:22,000 --> 01:17:23,000
 achieved.

1198
01:17:23,000 --> 01:17:24,000
 Okay.

1199
01:17:24,000 --> 01:17:30,000
 So, in practice, of course, sometimes, actually, the collection of a large number of samples,

1200
01:17:30,000 --> 01:17:32,000
 actually, is impossible.

1201
01:17:32,000 --> 01:17:33,000
 Right.

1202
01:17:33,000 --> 01:17:35,000
 All of them, sometimes, are very expensive.

1203
01:17:35,000 --> 01:17:36,000
 Okay.

1204
01:17:36,000 --> 01:17:39,000
 And then what we can get is only a small number of training data.

1205
01:17:39,000 --> 01:17:40,000
 Okay.

1206
01:17:40,000 --> 01:17:45,000
 In such a scenario, certainly, only a small number of features should be used.

1207
01:17:45,000 --> 01:17:46,000
 Small number, right.

1208
01:17:46,000 --> 01:17:47,000
 Small number.

1209
01:17:47,000 --> 01:17:51,000
 If you use a large number of, a small number of features, if you use a large number of features,

1210
01:17:51,000 --> 01:17:57,000
 and then the probability of error on the testing data could, is not the minimum.

1211
01:17:57,000 --> 01:17:58,000
 Okay.

1212
01:17:58,000 --> 01:18:01,000
 So, we should use a small number of samples.

1213
01:18:01,000 --> 01:18:05,000
 But if a larger number of training data is obtained, is available, then we should use

1214
01:18:05,000 --> 01:18:06,000
 more features.

1215
01:18:06,000 --> 01:18:07,000
 Okay.

1216
01:18:07,000 --> 01:18:11,000
 But also, of course, actually, we still need to do the terminology reduction, right.

1217
01:18:11,000 --> 01:18:14,000
 But actually, we can use more features.

1218
01:18:14,000 --> 01:18:19,000
 And then, with more features, we can even achieve even a better performance.

1219
01:18:19,000 --> 01:18:20,000
 Okay.

1220
01:18:20,000 --> 01:18:25,000
 So, in practice, the number of features normally determine, actually, based on the performance

1221
01:18:25,000 --> 01:18:27,000
 on the validation data.

1222
01:18:27,000 --> 01:18:31,000
 So, that's the reason, in practice, actually, the data is normally divided into three parts,

1223
01:18:31,000 --> 01:18:32,000
 right.

1224
01:18:32,000 --> 01:18:37,000
 Training data, validation data, and then the testing data.

1225
01:18:37,000 --> 01:18:38,000
 Okay.

1226
01:18:38,000 --> 01:18:41,000
 So, the training data is used to estimate the parameters of the model.

1227
01:18:41,000 --> 01:18:44,000
 Determine the, no, W, W0, right.

1228
01:18:44,000 --> 01:18:49,000
 So, this is the use of the training data.

1229
01:18:49,000 --> 01:18:55,000
 And how many features to include, and then we should actually, and this is a hyperparameter.

1230
01:18:55,000 --> 01:19:00,000
 We should determine the value of hyperparameter using the validation data.

1231
01:19:00,000 --> 01:19:02,000
 The hyperparameter are those kind of parameters.

1232
01:19:02,000 --> 01:19:06,000
 They're not directly estimated from the training data.

1233
01:19:06,000 --> 01:19:07,000
 Okay.

1234
01:19:07,000 --> 01:19:12,000
 I think in the regression, for example, in the regression problem, we have lambda, right,

1235
01:19:12,000 --> 01:19:24,000
 which is a weightage on the term for the penalization of the L1 norm or L2 norm, right, of the parameter

1236
01:19:24,000 --> 01:19:25,000
 vector.

1237
01:19:25,000 --> 01:19:26,000
 So, this lambda is also hyperparameter.

1238
01:19:26,000 --> 01:19:33,000
 We need to determine the value of this lambda based on the performance on the validation

1239
01:19:33,000 --> 01:19:34,000
 data.

1240
01:19:34,000 --> 01:19:35,000
 Okay.

1241
01:19:35,000 --> 01:19:36,000
 So, this kind of parameter are called hyperparameter.

1242
01:19:36,000 --> 01:19:37,000
 Okay.

1243
01:19:37,000 --> 01:19:44,000
 So, in other more complex models, and sometimes we have more than one hyperparameter.

1244
01:19:44,000 --> 01:19:51,000
 And then, we need to use actually validation training data to determine all of these hyperparameters.

1245
01:19:51,000 --> 01:19:53,000
 Okay.

1246
01:19:53,000 --> 01:19:55,000
 Okay.

1247
01:19:55,000 --> 01:20:02,000
 So, next, actually, we look at the individual feature evaluation.

1248
01:20:02,000 --> 01:20:03,000
 Okay.

1249
01:20:03,000 --> 01:20:09,000
 So, we know that our target of feature selection is to actually select the important ones, right,

1250
01:20:09,000 --> 01:20:10,000
 important ones.

1251
01:20:10,000 --> 01:20:15,000
 And, say, important ones, that means that we need to remove the unimportant ones, right,

1252
01:20:15,000 --> 01:20:21,000
 the insignificant ones, and also the redundant features.

1253
01:20:21,000 --> 01:20:22,000
 Okay.

1254
01:20:22,000 --> 01:20:28,000
 And, actually, normally, the feature selection or the feature evaluation is based on a set.

1255
01:20:28,000 --> 01:20:30,000
 It's a set-based feature selection.

1256
01:20:30,000 --> 01:20:34,000
 Because, actually, the feature set is finally used in the model.

1257
01:20:34,000 --> 01:20:35,000
 Okay.

1258
01:20:35,000 --> 01:20:37,000
 So, this is something like a scenario.

1259
01:20:37,000 --> 01:20:43,000
 If you want to select some students from this class, right, to attend a competition, this

1260
01:20:43,000 --> 01:20:49,000
 competition could, the question could actually, you know, from different fields, could from

1261
01:20:49,000 --> 01:20:55,000
 electrical, electronic engineering, from like physical, from math, from chemistry, right.

1262
01:20:55,000 --> 01:21:02,000
 So, you want to select a team, right, to represent the whole class, to attend this competition.

1263
01:21:02,000 --> 01:21:07,000
 And, certainly, we should actually select the student with actually personal skills,

1264
01:21:07,000 --> 01:21:11,000
 compensate, compensate, right, complementary.

1265
01:21:11,000 --> 01:21:15,000
 We cannot select students just actually based on each individual one.

1266
01:21:15,000 --> 01:21:21,000
 You are very good, right, your GPA is 4.9, or you're very good at the electrical and electronic

1267
01:21:21,000 --> 01:21:22,000
 engineering.

1268
01:21:22,000 --> 01:21:23,000
 And, that's like you.

1269
01:21:23,000 --> 01:21:26,000
 So, students, you know, have the same skill with you, right.

1270
01:21:26,000 --> 01:21:30,000
 Then, if you attend a competition, if the question about the science, about math, about

1271
01:21:30,000 --> 01:21:33,000
 physics, right, often you cannot answer, right.

1272
01:21:33,000 --> 01:21:41,000
 So, we should select the team, right, whose skills are complementary.

1273
01:21:41,000 --> 01:21:42,000
 Okay.

1274
01:21:42,000 --> 01:21:47,000
 We should not select the candidate based on each, just individually.

1275
01:21:47,000 --> 01:21:53,000
 Actually, this concept actually is also applicable to the future selection.

1276
01:21:53,000 --> 01:22:02,000
 Because, ultimately, the features are used as a team in the final model, right, together.

1277
01:22:02,000 --> 01:22:03,000
 Okay.

1278
01:22:03,000 --> 01:22:09,000
 But, actually, the future evaluation can still be first done by, you know, individually.

1279
01:22:09,000 --> 01:22:10,000
 Okay.

1280
01:22:10,000 --> 01:22:16,000
 And, actually, the later, when we talk about the future subset selection, and actually,

1281
01:22:16,000 --> 01:22:21,000
 normally, the future subset selection is a process, then we need to generate a larger

1282
01:22:21,000 --> 01:22:25,000
 number of, you know, candidate feature subset.

1283
01:22:25,000 --> 01:22:27,000
 And, then, we evaluate each of the future subset.

1284
01:22:27,000 --> 01:22:30,000
 And, then, we identify the best performing one.

1285
01:22:30,000 --> 01:22:31,000
 Okay.

1286
01:22:31,000 --> 01:22:38,000
 So, if we have a large number of features of code, actually, we can have a huge number

1287
01:22:38,000 --> 01:22:43,000
 of combinations of the features, right, of the features.

1288
01:22:43,000 --> 01:22:48,000
 And, then, actually, this will result in very high, you know, computational complexities.

1289
01:22:48,000 --> 01:22:51,000
 It is very computational demanding.

1290
01:22:51,000 --> 01:22:52,000
 Okay.

1291
01:22:52,000 --> 01:22:57,000
 So, in order to actually address this issue, at least, actually, if we use, actually, the

1292
01:22:57,000 --> 01:23:03,000
 individual feature evaluation, which can remove some of the features, what features?

1293
01:23:03,000 --> 01:23:08,000
 The insignificant or irrelevant features.

1294
01:23:08,000 --> 01:23:09,000
 Okay.

1295
01:23:09,000 --> 01:23:12,000
 So, these are the three individual feature evaluations.

1296
01:23:12,000 --> 01:23:18,000
 Often, they are used as the first step in the future subset selection.

1297
01:23:18,000 --> 01:23:19,000
 Okay.

1298
01:23:19,000 --> 01:23:25,000
 So, it's a known, actually, it's used as the first step.

1299
01:23:25,000 --> 01:23:32,000
 At least, we can remove, actually, the bad features or irrelevant features, insignificant

1300
01:23:32,000 --> 01:23:33,000
 features.

1301
01:23:33,000 --> 01:23:40,000
 And, then, individual feature evaluation cannot identify the redundant features, but it can

1302
01:23:40,000 --> 01:23:46,000
 identify, actually, the insignificant or irrelevant or useless features or bad features.

1303
01:23:46,000 --> 01:23:47,000
 Okay.

1304
01:23:47,000 --> 01:23:53,000
 So, this could be used as the first step in a future subset selection.

1305
01:23:53,000 --> 01:23:54,000
 Okay.

1306
01:23:54,000 --> 01:23:56,000
 In the future selection.

1307
01:23:56,000 --> 01:23:57,000
 Okay.

1308
01:23:57,000 --> 01:23:59,000
 So, these are the individual feature evaluations.

1309
01:23:59,000 --> 01:24:03,000
 So, evaluate the goodness of each feature individually, separately.

1310
01:24:03,000 --> 01:24:07,000
 Don't consider the combination of the features.

1311
01:24:07,000 --> 01:24:12,000
 So, this is called individual feature evaluation and selection.

1312
01:24:12,000 --> 01:24:13,000
 Okay.

1313
01:24:13,000 --> 01:24:18,000
 So, these will be used as the first step.

1314
01:24:18,000 --> 01:24:19,000
 Okay.

1315
01:24:19,000 --> 01:24:26,500
 And, so, next, actually, we will talk about how to evaluate the usefulness or the relevance

1316
01:24:26,500 --> 01:24:28,000
 of a feature.

1317
01:24:28,000 --> 01:24:29,000
 Okay.

1318
01:24:29,000 --> 01:24:34,000
 And, actually, we can evaluate the goodness of a feature based on two, actually, interpretations.

1319
01:24:34,000 --> 01:24:35,000
 Okay.

1320
01:24:35,000 --> 01:24:42,000
 The first interpretation is called separability, based on this, actually, interpretation, and

1321
01:24:42,000 --> 01:24:46,000
 which, actually, can identify the important ones.

1322
01:24:46,000 --> 01:24:51,000
 Then, another, actually, is based on a concept, is the relevance of a feature.

1323
01:24:51,000 --> 01:24:57,000
 The relevance of a feature to predict the class label of a sample.

1324
01:24:57,000 --> 01:24:58,000
 Relevance.

1325
01:24:58,000 --> 01:24:59,000
 Okay.

1326
01:24:59,000 --> 01:25:03,000
 So, we can evaluate the goodness of a feature based on these two different interpretations.

1327
01:25:03,000 --> 01:25:10,000
 Of course, actually, these different interpretations will have different evaluation metrics.

1328
01:25:10,000 --> 01:25:11,000
 Okay.

1329
01:25:11,000 --> 01:25:12,000
 Evaluate metrics.

1330
01:25:12,000 --> 01:25:14,000
 I mean, we have formula, right?

1331
01:25:14,000 --> 01:25:18,000
 We have formula to calculate the separability of a feature.

1332
01:25:19,000 --> 01:25:22,000
 And, also, the relevance of a feature.

1333
01:25:22,000 --> 01:25:25,000
 And then, based on these, you know, calculated value, right?

1334
01:25:25,000 --> 01:25:27,000
 And then, we can rank the features.

1335
01:25:27,000 --> 01:25:31,000
 And then, we can also identify the bad features.

1336
01:25:31,000 --> 01:25:32,000
 Okay.

1337
01:25:32,000 --> 01:25:33,000
 So, this is the end of the...

1338
01:25:33,000 --> 01:25:35,000
 So, next, we look at what is the class separability.

1339
01:25:35,000 --> 01:25:37,000
 And, actually, we have already used these features.

1340
01:25:37,000 --> 01:25:39,000
 There is no concept before.

1341
01:25:39,000 --> 01:25:44,000
 Actually, when we, you know, studied the feature linear dishman analysis, right?

1342
01:25:44,000 --> 01:25:47,000
 And, actually, the sample is the hard and no-speed.

1343
01:25:47,000 --> 01:25:52,000
 We want to find a W. And then, we project the sample on this W, right?

1344
01:25:52,000 --> 01:25:57,000
 And, of the projection, each sample is represented by one single value, right?

1345
01:25:57,000 --> 01:25:58,000
 Is a...

1346
01:25:58,000 --> 01:25:59,000
 Okay.

1347
01:25:59,000 --> 01:26:04,000
 Then, all the samples actually are distributed on this W.

1348
01:26:04,000 --> 01:26:05,000
 Okay.

1349
01:26:05,000 --> 01:26:07,000
 And then, we...

1350
01:26:07,000 --> 01:26:11,000
 We evaluate the goodness of the W, right?

1351
01:26:11,000 --> 01:26:14,000
 Based on, actually, two criteria.

1352
01:26:14,000 --> 01:26:19,000
 One is that the other projection of this W, if the W is good,

1353
01:26:19,000 --> 01:26:24,000
 then, actually, within class, the scatter is small.

1354
01:26:24,000 --> 01:26:29,000
 That means that within the same class, the samples are actually are very similar.

1355
01:26:29,000 --> 01:26:31,000
 So, this is called within class scatter, right?

1356
01:26:31,000 --> 01:26:35,000
 And then, between class, the difference should be big,

1357
01:26:35,000 --> 01:26:39,000
 of the projection onto this W.

1358
01:26:39,000 --> 01:26:46,000
 We use this class separability to evaluate the goodness of the projection W, right?

1359
01:26:46,000 --> 01:26:47,000
 So, finally, we use...

1360
01:26:47,000 --> 01:26:56,000
 We try to find the W that maximizes the ratio of the between class scatter to the within class scatter.

1361
01:26:56,000 --> 01:26:57,000
 Okay.

1362
01:26:57,000 --> 01:26:58,000
 So, this is not...

1363
01:26:58,000 --> 01:27:00,000
 The ratio is called separability.

1364
01:27:00,000 --> 01:27:03,000
 We have used this concept, right?

1365
01:27:03,000 --> 01:27:08,000
 But, actually, before we use this concept to evaluate the goodness of a W, right?

1366
01:27:08,000 --> 01:27:12,000
 We use this concept to evaluate each individual feature.

1367
01:27:12,000 --> 01:27:13,000
 Okay.

1368
01:27:13,000 --> 01:27:22,000
 And so, this is actually a sample along one...

1369
01:27:22,000 --> 01:27:25,000
 A distribution of the sample along one feature.

1370
01:27:25,000 --> 01:27:27,000
 Along one feature, right?

1371
01:27:27,000 --> 01:27:28,000
 Okay.

1372
01:27:28,000 --> 01:27:32,000
 So, different samples along this feature have different values.

1373
01:27:32,000 --> 01:27:39,000
 And if we see all the sample class 1 are located in this part of the axis,

1374
01:27:39,000 --> 01:27:44,000
 and then the sample in another class is located in another side of the species, right?

1375
01:27:44,000 --> 01:27:48,000
 And then, actually, we can actually use...

1376
01:27:48,000 --> 01:27:51,000
 We can look at the, you know, between class difference.

1377
01:27:51,000 --> 01:27:53,000
 How to evaluate the between class difference?

1378
01:27:53,000 --> 01:27:58,000
 So, certainly, one way is that we can use the difference between the two mean values, right?

1379
01:27:58,000 --> 01:28:01,000
 For all the sample in class 1 along this axis, right?

1380
01:28:01,000 --> 01:28:04,000
 X along this feature, we can have a mean value.

1381
01:28:04,000 --> 01:28:07,000
 And then, for sample in class 2, we can have a mean value.

1382
01:28:07,000 --> 01:28:08,000
 Okay.

1383
01:28:08,000 --> 01:28:13,000
 So, the difference between class scatter, the between class difference can be measured

1384
01:28:13,000 --> 01:28:18,000
 in this difference of these two mean values, right?

1385
01:28:18,000 --> 01:28:19,000
 So...

1386
01:28:19,000 --> 01:28:21,000
 And also, for some...

1387
01:28:21,000 --> 01:28:25,000
 Within the same class, we hope they are similar.

1388
01:28:25,000 --> 01:28:28,000
 So, these actually should be the within class scatter, right?

1389
01:28:28,000 --> 01:28:34,000
 Normally, within class scatter can be evaluated using the standard of the region, the sigma,

1390
01:28:34,000 --> 01:28:36,000
 or the variance, sigma squared.

1391
01:28:36,000 --> 01:28:37,000
 Okay.

1392
01:28:37,000 --> 01:28:42,000
 And then, actually, normally, the separability measure, feature separability measure,

1393
01:28:42,000 --> 01:28:49,000
 or feature ratio, it defines the ratio of these two, between class scatter to the within class scatter.

1394
01:28:49,000 --> 01:28:50,000
 Okay.

1395
01:28:50,000 --> 01:28:52,000
 Then, this is the feature ratio.

1396
01:28:52,000 --> 01:28:53,000
 Okay.

1397
01:28:53,000 --> 01:29:01,000
 So, M1, M2, M1 is the mean value of sample in class 1 along this feature, right?

1398
01:29:01,000 --> 01:29:04,000
 M2 is the mean value for sample in class 2.

1399
01:29:04,000 --> 01:29:10,000
 Sigma 1 is the square, is the variance for sample in class 1.

1400
01:29:10,000 --> 01:29:16,000
 And sigma 2 is sample in class 2 along this feature X.

1401
01:29:16,000 --> 01:29:17,000
 Okay.

1402
01:29:17,000 --> 01:29:23,000
 So, you can use this ratio to evaluate the goodness of feature, the larger the better.

1403
01:29:23,000 --> 01:29:25,000
 The larger the better.

1404
01:29:25,000 --> 01:29:26,000
 Okay.

1405
01:29:26,000 --> 01:29:27,000
 So, this is the feature ratio.

1406
01:29:27,000 --> 01:29:34,000
 But this concept here is between class difference, within class difference, between class scatter,

1407
01:29:34,000 --> 01:29:35,000
 within class scatter.

1408
01:29:35,000 --> 01:29:36,000
 Okay.

1409
01:29:36,000 --> 01:29:41,000
 So, now, actually, between class scatter is evaluated based on the distance between two,

1410
01:29:41,000 --> 01:29:43,000
 between two, and the mean values, right?

1411
01:29:43,000 --> 01:29:44,000
 Between two.

1412
01:29:44,000 --> 01:29:51,000
 Actually, you can also actually develop your own ideas, develop your own metrics for the

1413
01:29:51,000 --> 01:29:54,000
 evaluation of the between class scatter.

1414
01:29:54,000 --> 01:29:58,000
 For example, actually, I think if you think about the support weight machine, support weight

1415
01:29:58,000 --> 01:30:02,000
 machine actually use the distance between the closed samples, right?

1416
01:30:02,000 --> 01:30:03,000
 Closed samples.

1417
01:30:03,000 --> 01:30:09,000
 And then, the distance between this sample and this sample, this distance, actually, like

1418
01:30:09,000 --> 01:30:11,000
 no margin of separation, right?

1419
01:30:11,000 --> 01:30:16,000
 Actually, this margin of separation can be, actually, it's evaluated based on the distance

1420
01:30:16,000 --> 01:30:18,000
 between the two closed samples.

1421
01:30:18,000 --> 01:30:23,000
 So, this is also an evaluation of the between class difference, right?

1422
01:30:23,000 --> 01:30:25,000
 Between class difference.

1423
01:30:25,000 --> 01:30:29,000
 So, here, we try to minimize, or maximize the between class difference.

1424
01:30:29,000 --> 01:30:32,000
 But how to measure the between class difference?

1425
01:30:32,000 --> 01:30:34,000
 You can have a different metrics, right?

1426
01:30:34,000 --> 01:30:36,000
 We can use the mean difference.

1427
01:30:36,000 --> 01:30:42,000
 We can also look at the distance between two closed samples, right?

1428
01:30:42,000 --> 01:30:48,000
 Then, you can also create your own, your own metrics to evaluate the between class scatter

1429
01:30:48,000 --> 01:30:50,000
 metrics, between class scatters, right?

1430
01:30:50,000 --> 01:30:51,000
 Okay.

1431
01:30:51,000 --> 01:30:55,000
 So, we always have this concept between class scatters should be maximized.

1432
01:30:55,000 --> 01:31:00,000
 Meanwhile, actually, within class scatter, actually, we should minimize, right?

1433
01:31:00,000 --> 01:31:05,000
 We hope all the samples within a similar class, actually, are similar.

1434
01:31:05,000 --> 01:31:08,000
 So, these are within class scatters.

1435
01:31:08,000 --> 01:31:10,000
 So, these are mean, right?

1436
01:31:10,000 --> 01:31:15,000
 So, I think what I want to emphasize is that, actually, this concept between class scatter,

1437
01:31:15,000 --> 01:31:20,000
 within class scatter, okay, I think we should try to maximize between class scatter and

1438
01:31:20,000 --> 01:31:22,000
 the mean model within class scatter.

1439
01:31:22,000 --> 01:31:25,000
 We can also try to maximize the reach of the two.

1440
01:31:25,000 --> 01:31:27,000
 But how to evaluate the between class scatter?

1441
01:31:27,000 --> 01:31:29,000
 How to evaluate the within class scatter?

1442
01:31:29,000 --> 01:31:31,000
 We can have different ways.

1443
01:31:31,000 --> 01:31:33,000
 At least here, we have two ways, right?

1444
01:31:33,000 --> 01:31:39,000
 Based on the mean difference or based on the distance between the closed samples.

1445
01:31:39,000 --> 01:31:40,000
 Okay.

1446
01:31:40,000 --> 01:31:43,000
 So, this is the feature ratio.

1447
01:31:43,000 --> 01:31:45,000
 And, okay.

1448
01:31:45,000 --> 01:31:48,000
 So, these are two concepts.

1449
01:31:48,000 --> 01:31:51,000
 We hope, actually, they can have a larger interclass difference.

1450
01:31:51,000 --> 01:31:53,000
 Interclass, that means the between class.

1451
01:31:53,000 --> 01:31:55,000
 Intra class, within class.

1452
01:31:55,000 --> 01:31:56,000
 Okay.

1453
01:31:56,000 --> 01:31:57,000
 So, these are the concepts.

1454
01:31:57,000 --> 01:32:01,000
 So, we can actually talk about interclass or interclass.

1455
01:32:01,000 --> 01:32:05,000
 Sometimes, they use between class and within class.

1456
01:32:05,000 --> 01:32:06,000
 Okay.

1457
01:32:06,000 --> 01:32:08,000
 So, we emphasize these two concepts.

1458
01:32:08,000 --> 01:32:13,000
 But we can have different implementations of these two concepts.

1459
01:32:13,000 --> 01:32:15,000
 Okay.

1460
01:32:15,000 --> 01:32:19,000
 And, so here, the feature ratio, we use the mean, right?

1461
01:32:19,000 --> 01:32:20,000
 We use the mean.

1462
01:32:20,000 --> 01:32:22,000
 We use the distance concept, the distance.

1463
01:32:22,000 --> 01:32:23,000
 Okay.

1464
01:32:23,000 --> 01:32:28,000
 And, this is actually, you know, distance, it's meaningful only for continuous variables.

1465
01:32:28,000 --> 01:32:33,000
 Actually, before, you know, we actually in the second part of the, the second week of

1466
01:32:33,000 --> 01:32:37,000
 this course, we talk about data preparations, you know, cause, right?

1467
01:32:37,000 --> 01:32:40,000
 And, in part, actually, we talk about the different types of features.

1468
01:32:40,000 --> 01:32:42,000
 Now, we have the continuous features.

1469
01:32:42,000 --> 01:32:43,000
 We have discrete features.

1470
01:32:43,000 --> 01:32:49,000
 For discrete, we have the nominal and then we have the, we have the, you know, the ordinal,

1471
01:32:49,000 --> 01:32:50,000
 right?

1472
01:32:50,000 --> 01:32:52,000
 And, in particular, for nominal, right?

1473
01:32:52,000 --> 01:32:54,000
 The distance has no meaning.

1474
01:32:54,000 --> 01:32:57,000
 Actually, we have different, we have different value just to represent.

1475
01:32:57,000 --> 01:32:58,000
 They are different.

1476
01:32:58,000 --> 01:33:04,000
 But this value, actually, does not really reflect how different they are.

1477
01:33:04,000 --> 01:33:05,000
 Okay.

1478
01:33:05,000 --> 01:33:10,000
 So, here, this concept, actually, you know, this metric cannot be used for this kind of

1479
01:33:10,000 --> 01:33:11,000
 features.

1480
01:33:11,000 --> 01:33:17,000
 Maybe okay, and it's okay for ordinal data, but certainly not for nominal data.

1481
01:33:18,000 --> 01:33:20,000
 Because we don't have a mean, right?

1482
01:33:20,000 --> 01:33:28,000
 Mean value or distance, a distance metric, actually, are meaningless for nominal data.

1483
01:33:28,000 --> 01:33:29,000
 Okay.

1484
01:33:29,000 --> 01:33:35,000
 So, we use these features, we show for continuous features, the value continuous.

1485
01:33:35,000 --> 01:33:36,000
 Okay.

1486
01:33:36,000 --> 01:33:46,000
 And then, what, actually, you know, we can use for discrete features, in particular,

1487
01:33:46,000 --> 01:33:48,000
 for nominal, right?

1488
01:33:48,000 --> 01:33:51,000
 And, actually, we can use the so-called mutual information.

1489
01:33:51,000 --> 01:33:55,000
 And, actually, mutual information is based on the interpretation of the relevance.

1490
01:33:55,000 --> 01:33:57,000
 Relevance, okay.

1491
01:33:57,000 --> 01:34:07,000
 Relevance, that means, actually, how much information we can have, we can get from this feature

1492
01:34:07,000 --> 01:34:11,000
 and to predict for the class label.

1493
01:34:11,000 --> 01:34:13,000
 That is the relevance.

1494
01:34:13,000 --> 01:34:15,000
 The relevance, okay.

1495
01:34:15,000 --> 01:34:19,000
 Relevance, actually, of course, the relevance can be measured by different, we can use a

1496
01:34:19,000 --> 01:34:20,000
 correlation.

1497
01:34:20,000 --> 01:34:21,000
 Okay.

1498
01:34:21,000 --> 01:34:24,000
 But we can also use the so-called mutual information.

1499
01:34:24,000 --> 01:34:26,000
 So, relevance is a big concept.

1500
01:34:26,000 --> 01:34:34,000
 We can have different, you know, we can develop different metrics and to measure these relevance.

1501
01:34:34,000 --> 01:34:35,000
 Okay.

1502
01:34:35,000 --> 01:34:38,000
 But here, we just introduced the mutual information.

1503
01:34:38,000 --> 01:34:42,000
 So, this is particular for discrete data.

1504
01:34:42,000 --> 01:34:43,000
 Okay.

1505
01:34:44,000 --> 01:34:54,000
 So, a good feature should contain significant amount of information to decide the class

1506
01:34:54,000 --> 01:34:55,000
 label, right?

1507
01:34:55,000 --> 01:34:56,000
 Continue information.

1508
01:34:56,000 --> 01:34:57,000
 That means it is relevant.

1509
01:34:57,000 --> 01:34:58,000
 It is relevant.

1510
01:34:58,000 --> 01:34:59,000
 Okay.

1511
01:34:59,000 --> 01:35:04,000
 So, we use this relevance concept for feature evaluation.

1512
01:35:04,000 --> 01:35:10,000
 And, the mutual information is one of the metrics that we can use to evaluate the relevance.

1513
01:35:10,000 --> 01:35:16,000
 Correlation is also a kind of measure for the relevance.

1514
01:35:16,000 --> 01:35:17,000
 Okay.

1515
01:35:18,000 --> 01:35:22,000
 So, this is the definition of the mutual information.

1516
01:35:22,000 --> 01:35:23,000
 Okay.

1517
01:35:23,000 --> 01:35:28,000
 And so, X is a feature, is a one feature.

1518
01:35:28,000 --> 01:35:32,000
 And Y is a target variable, the class label.

1519
01:35:32,000 --> 01:35:33,000
 Okay.

1520
01:35:33,000 --> 01:35:37,000
 And so, Y, of course, can only take discrete values, right?

1521
01:35:37,000 --> 01:35:40,000
 For binary classification problem, only two possible values.

1522
01:35:40,000 --> 01:35:44,000
 In fact, for multiple, Y can only take limited values, right?

1523
01:35:44,000 --> 01:35:46,000
 Limited number of values, right?

1524
01:35:46,000 --> 01:35:48,000
 Possible value, limited.

1525
01:35:48,000 --> 01:35:49,000
 Okay.

1526
01:35:49,000 --> 01:35:52,000
 And X here is also a discrete variable.

1527
01:35:52,000 --> 01:35:56,000
 Taking a limit, taking a different, limited number of values, right?

1528
01:35:56,000 --> 01:35:58,000
 Just a few values.

1529
01:35:58,000 --> 01:35:59,000
 Okay.

1530
01:35:59,000 --> 01:36:07,000
 And then, this mutual information, M, I, X, Y, equals to E, X, plus E, Y, minus E, X,

1531
01:36:07,000 --> 01:36:08,000
 Y.

1532
01:36:08,000 --> 01:36:09,000
 E here is not the expectation.

1533
01:36:09,000 --> 01:36:11,000
 Expectation is the mean, right?

1534
01:36:11,000 --> 01:36:16,000
 Certainly, we just now, we have mentioned, actually, for discrete, in particular, for

1535
01:36:16,000 --> 01:36:18,000
 the nominal data, the mean is the meaningless.

1536
01:36:18,000 --> 01:36:22,000
 So, the X here is not the expectation, not the mean.

1537
01:36:22,000 --> 01:36:24,000
 Existitation is just the mean.

1538
01:36:24,000 --> 01:36:25,000
 Okay.

1539
01:36:25,000 --> 01:36:26,000
 So, X here, E here, no.

1540
01:36:26,000 --> 01:36:29,000
 E here means, actually, the entropy.

1541
01:36:29,000 --> 01:36:30,000
 Okay.

1542
01:36:30,000 --> 01:36:37,000
 So, entropy of the X plus entropy of Y minus the joint entropy of X and Y.

1543
01:36:37,000 --> 01:36:38,000
 Okay.

1544
01:36:38,000 --> 01:36:40,000
 So, this is the mutual information.

1545
01:36:40,000 --> 01:36:41,000
 Okay.

1546
01:36:41,000 --> 01:36:44,000
 Actually, entropy, we have used this concept before, right?

1547
01:36:44,000 --> 01:36:48,000
 In the class-finition tree, right?

1548
01:36:48,000 --> 01:36:55,000
 And we decided which, you know, feature to use for the nest node, okay?

1549
01:36:55,000 --> 01:36:58,000
 Or for the rule node and for the decision node, right?

1550
01:36:58,000 --> 01:37:00,000
 We're based on the mutual information.

1551
01:37:00,000 --> 01:37:01,000
 Okay.

1552
01:37:01,000 --> 01:37:04,000
 So, this is the, based on the entropy concept, entropy, right?

1553
01:37:04,000 --> 01:37:05,000
 Entropy.

1554
01:37:05,000 --> 01:37:07,000
 So, this is the entropy.

1555
01:37:07,000 --> 01:37:11,000
 Probability times the log of the probability.

1556
01:37:11,000 --> 01:37:12,000
 Okay.

1557
01:37:12,000 --> 01:37:15,000
 So, for feature X, it could take a few values, right?

1558
01:37:15,000 --> 01:37:17,000
 Either feature is a binary feature, like gender.

1559
01:37:17,000 --> 01:37:21,000
 This feature only has two possible values, male or female.

1560
01:37:21,000 --> 01:37:28,000
 And if the X is like a transportation mode, you know, if you come to NTU, you can buy

1561
01:37:28,000 --> 01:37:30,000
 a bus, you can buy a taxi, right?

1562
01:37:30,000 --> 01:37:32,000
 You can walk to NTU, right?

1563
01:37:32,000 --> 01:37:35,000
 And, yeah, so, at least you have three ways, right?

1564
01:37:35,000 --> 01:37:41,000
 Then, actually, the number, we have Ui, U2, U3, U1, U2, U3.

1565
01:37:41,000 --> 01:37:43,000
 They have three values.

1566
01:37:43,000 --> 01:37:44,000
 Okay.

1567
01:37:44,000 --> 01:37:48,000
 So, for each of the three values, or each of the two values, for each of the values, right?

1568
01:37:48,000 --> 01:37:52,000
 Because discrete, we can look at the probability.

1569
01:37:52,000 --> 01:37:54,000
 We can count, right?

1570
01:37:54,000 --> 01:38:01,000
 And the probability, then times the log of this probability.

1571
01:38:01,000 --> 01:38:02,000
 The summation.

1572
01:38:02,000 --> 01:38:04,000
 Summary to that means for all the three, right?

1573
01:38:04,000 --> 01:38:12,000
 When the probability, when the transportation mode equal to, you know, buy a taxi, right?

1574
01:38:12,000 --> 01:38:15,000
 Then log, this is probability.

1575
01:38:15,000 --> 01:38:19,000
 And plus the probability of taking a bus, okay?

1576
01:38:19,000 --> 01:38:23,000
 Then times the probability, log of the probability of taking a bus.

1577
01:38:23,000 --> 01:38:29,000
 And then plus the probability of walking to NTU times the log of the probability of walking to NTU.

1578
01:38:29,000 --> 01:38:30,000
 Okay.

1579
01:38:30,000 --> 01:38:32,000
 And because the probability, you know, is less than one, right?

1580
01:38:32,000 --> 01:38:33,000
 Less than one.

1581
01:38:33,000 --> 01:38:35,000
 Then the log will be negative.

1582
01:38:35,000 --> 01:38:38,000
 And then, so, finally, we put a negative value here.

1583
01:38:38,000 --> 01:38:41,000
 Then this entropy measure, actually, is a positive number.

1584
01:38:41,000 --> 01:38:42,000
 Positive.

1585
01:38:42,000 --> 01:38:43,000
 Okay.

1586
01:38:43,000 --> 01:38:46,000
 So this is an entropy, X, similar for Y, right?

1587
01:38:46,000 --> 01:38:48,000
 Y is a class label, right?

1588
01:38:48,000 --> 01:38:52,000
 So for binary class, class value problem, only two possible values.

1589
01:38:52,000 --> 01:38:55,000
 And then you can have a multiple class problem, right?

1590
01:38:55,000 --> 01:39:00,000
 And then, you know, this actually, the Y can take multiple values.

1591
01:39:00,000 --> 01:39:03,000
 But it's limited number of values.

1592
01:39:03,000 --> 01:39:05,000
 So we can use this entropy, right?

1593
01:39:05,000 --> 01:39:10,000
 And this formula to carry the entropy for X, Y.

1594
01:39:10,000 --> 01:39:13,000
 And then we can take the entropy for X, Y.

1595
01:39:13,000 --> 01:39:16,000
 So here, this may, you know, is a combination.

1596
01:39:16,000 --> 01:39:17,000
 Combination.

1597
01:39:17,000 --> 01:39:20,000
 So we need to take more of the computations.

1598
01:39:20,000 --> 01:39:21,000
 Okay.

1599
01:39:21,000 --> 01:39:30,000
 So, because we have two variables, X equal to mu J, Y equals to C, I, different values.

1600
01:39:30,000 --> 01:39:31,000
 So combination.

1601
01:39:31,000 --> 01:39:33,000
 This is a John's entropy.

1602
01:39:33,000 --> 01:39:34,000
 Okay.

1603
01:39:34,000 --> 01:39:40,000
 So we have the entropy X plus entropy Y minus the John entropy X and Y.

1604
01:39:40,000 --> 01:39:43,000
 And we can get the mutual information between X and Y.

1605
01:39:43,000 --> 01:39:44,000
 Okay.

1606
01:39:44,000 --> 01:39:51,000
 So this will be used in evaluation for the goodness of the feature.

1607
01:39:51,000 --> 01:39:52,000
 Okay.

1608
01:39:52,000 --> 01:39:56,000
 So this is for discrete.

1609
01:39:56,000 --> 01:39:58,000
 For discrete.

1610
01:39:58,000 --> 01:39:59,000
 Okay.

1611
01:39:59,000 --> 01:40:02,000
 So here, I just give one example for each type of feature.

1612
01:40:02,000 --> 01:40:05,000
 For continuous feature, we can use a feature as a ratio, right?

1613
01:40:05,000 --> 01:40:08,000
 For discrete features, we can use the mutual information.

1614
01:40:08,000 --> 01:40:11,000
 But actually, you know, we have the bigger concept, right?

1615
01:40:11,000 --> 01:40:15,000
 The relevance of the class separability.

1616
01:40:15,000 --> 01:40:16,000
 We have this big concept.

1617
01:40:16,000 --> 01:40:20,000
 And then, actually, we have some other metrics, right?

1618
01:40:20,000 --> 01:40:24,000
 For the evaluation of continuous features, for the evaluation of discrete features.

1619
01:40:24,000 --> 01:40:28,000
 But here, we just give one example for each type of the feature.

1620
01:40:28,000 --> 01:40:29,000
 Okay.

1621
01:40:29,000 --> 01:40:31,000
 Anyway, we have means, right?

1622
01:40:31,000 --> 01:40:34,000
 To evaluate the goodness of the feature.

1623
01:40:34,000 --> 01:40:37,000
 No matter what feature, kind of feature they are.

1624
01:40:37,000 --> 01:40:40,000
 Continuous, then we will continue the evaluation criteria.

1625
01:40:40,000 --> 01:40:43,000
 Discrete, then we will do discrete evaluation criteria.

1626
01:40:43,000 --> 01:40:44,000
 Okay.

1627
01:40:44,000 --> 01:40:47,000
 We have ways to evaluate the goodness of a feature.

1628
01:40:47,000 --> 01:40:48,000
 Okay.

1629
01:40:48,000 --> 01:40:51,000
 So this is the individual feature evaluation.

1630
01:40:51,000 --> 01:40:52,000
 Okay.

1631
01:40:52,000 --> 01:40:55,000
 So, next, actually, the feature subset.

1632
01:40:55,000 --> 01:40:59,000
 In practice, actually, we select feature subset.

1633
01:40:59,000 --> 01:41:00,000
 Okay.

1634
01:41:00,000 --> 01:41:03,000
 So, this is the feature subset selection.

1635
01:41:03,000 --> 01:41:08,000
 Okay.

1636
01:41:08,000 --> 01:41:11,000
 The feature subset selection can be defined as this.

1637
01:41:11,000 --> 01:41:13,000
 Give a full feature set.

1638
01:41:13,000 --> 01:41:14,000
 Okay.

1639
01:41:14,000 --> 01:41:16,000
 At the beginning, you have no idea, right?

1640
01:41:16,000 --> 01:41:19,000
 You collect as many features as possible, right?

1641
01:41:19,000 --> 01:41:24,000
 And because, actually, you are afraid of missing some important features, right?

1642
01:41:24,000 --> 01:41:27,000
 So, you collect as many as possible.

1643
01:41:27,000 --> 01:41:28,000
 Okay.

1644
01:41:28,000 --> 01:41:33,000
 And so, here, you assume we have a full set, full feature set, X1, X2, Xn.

1645
01:41:33,000 --> 01:41:34,000
 Okay.

1646
01:41:34,000 --> 01:41:45,000
 And our target objective is to select a subset that Z1, Z2, Zn.

1647
01:41:45,000 --> 01:41:48,000
 And Zn, Z1 is just one of the features, right?

1648
01:41:48,000 --> 01:41:49,000
 X1, Z2.

1649
01:41:49,000 --> 01:41:50,000
 Okay.

1650
01:41:50,000 --> 01:41:53,000
 So, this is a subset of the original features.

1651
01:41:53,000 --> 01:41:54,000
 Okay.

1652
01:41:54,000 --> 01:41:59,000
 And we still select such a subset that they produce the best class-future performance.

1653
01:41:59,000 --> 01:42:05,000
 Of course, the best class-future performance should be on the validation, on the, you know,

1654
01:42:05,000 --> 01:42:06,000
 the testing data, right?

1655
01:42:06,000 --> 01:42:09,000
 But, of course, the testing data is unseen, right?

1656
01:42:09,000 --> 01:42:11,000
 It's a future data.

1657
01:42:11,000 --> 01:42:15,000
 So, how we know these are the best performance on the validation, on the testing data.

1658
01:42:15,000 --> 01:42:20,000
 So, basically, at least, actually, in the training selection stage, we can look at the performance

1659
01:42:20,000 --> 01:42:25,000
 on the, actually, on the validation data.

1660
01:42:25,000 --> 01:42:26,000
 Okay.

1661
01:42:26,000 --> 01:42:28,000
 So, they produce the best performance.

1662
01:42:28,000 --> 01:42:30,000
 So, this is the target of future selection.

1663
01:42:30,000 --> 01:42:35,000
 We select a subset that can produce the best performance.

1664
01:42:35,000 --> 01:42:36,000
 Okay.

1665
01:42:36,000 --> 01:42:48,000
 And, okay, so, the peak of phenomenon, right?

1666
01:42:48,000 --> 01:42:53,000
 Actually, I'll read how about the features, actually, normally, the feature selection

1667
01:42:53,000 --> 01:43:01,000
 tries to remove, actually, the irrelevant feature, irrelevant or insignificant features.

1668
01:43:01,000 --> 01:43:04,000
 Maybe they are relevant, but actually, insignificant.

1669
01:43:04,000 --> 01:43:05,000
 Okay.

1670
01:43:05,000 --> 01:43:07,000
 So, this is one type of feature that we should remove.

1671
01:43:07,000 --> 01:43:10,000
 Then, another type of feature is redundant, right?

1672
01:43:10,000 --> 01:43:12,000
 Redundant features.

1673
01:43:12,000 --> 01:43:13,000
 Okay.

1674
01:43:13,000 --> 01:43:19,000
 And so, actually, you know, use the, you know, individual feature evaluation, and we can

1675
01:43:19,000 --> 01:43:24,000
 remove the redundant, the insignificant features, but we cannot remove the redundant features.

1676
01:43:24,000 --> 01:43:25,000
 Okay.

1677
01:43:25,000 --> 01:43:30,000
 In other words, if we, you know, rank the features, and just, actually, select the top rank features,

1678
01:43:30,000 --> 01:43:35,000
 based on the, you know, the best to the, to the, to the worst, right?

1679
01:43:35,000 --> 01:43:41,000
 And then, we select top K features, for example, top M features, L features.

1680
01:43:41,000 --> 01:43:47,000
 And, these features, actually, whether this could produce a good performance, I know.

1681
01:43:47,000 --> 01:43:48,000
 Okay.

1682
01:43:48,000 --> 01:43:49,000
 The answer is no.

1683
01:43:49,000 --> 01:43:56,000
 The reason is that, actually, these top rank features may contain redundancies, strong redundancies.

1684
01:43:56,000 --> 01:44:01,000
 Redundancies, that means that they have strong correlations, strong correlations.

1685
01:44:01,000 --> 01:44:02,000
 Okay.

1686
01:44:02,000 --> 01:44:09,000
 So, so here, actually, this is, this is, this is, is a strong correlation between the top

1687
01:44:09,000 --> 01:44:10,000
 rank features.

1688
01:44:10,000 --> 01:44:16,000
 So, this, actually, means that, that is, and, it's as strong as a severe redundancies.

1689
01:44:16,000 --> 01:44:17,000
 Okay.

1690
01:44:17,000 --> 01:44:22,000
 You include the feature, but these features basically have the same information, right,

1691
01:44:22,000 --> 01:44:23,000
 as other features.

1692
01:44:23,000 --> 01:44:29,000
 So, this feature, you know, discriminative information of this feature is already kept,

1693
01:44:29,000 --> 01:44:32,000
 captured, okay, by other features.

1694
01:44:32,000 --> 01:44:33,000
 Okay.

1695
01:44:33,000 --> 01:44:37,000
 Then, this feature is redundant, redundant.

1696
01:44:37,000 --> 01:44:38,000
 Okay.

1697
01:44:38,000 --> 01:44:42,000
 Now, of course, actually, you know, we shouldn't look at, you know, from this point of view,

1698
01:44:42,000 --> 01:44:44,000
 we should remove the redundant features.

1699
01:44:44,000 --> 01:44:49,000
 But from another point of view, actually, redundant, we should keep a bit of redundancy in the,

1700
01:44:49,000 --> 01:44:51,000
 in the features I've said.

1701
01:44:51,000 --> 01:44:56,000
 Because redundancy normally improve the robustness, robustness, right.

1702
01:44:56,000 --> 01:45:01,000
 Normally, you know, in some critical system, we have two set of systems, right.

1703
01:45:01,000 --> 01:45:06,000
 One set is, is a, is a 40, right.

1704
01:45:06,000 --> 01:45:08,000
 We can use another set, right.

1705
01:45:08,000 --> 01:45:11,000
 Redundancy, actually, can improve the robustness.

1706
01:45:11,000 --> 01:45:12,000
 Okay.

1707
01:45:12,000 --> 01:45:16,000
 So, you know, they're not absolute, right.

1708
01:45:16,000 --> 01:45:18,000
 We should remove all the redundancy.

1709
01:45:18,000 --> 01:45:21,000
 Sometimes, redundancy is kept in the feature.

1710
01:45:21,000 --> 01:45:24,000
 Actually, we can improve the robustness.

1711
01:45:24,000 --> 01:45:25,000
 Okay.

1712
01:45:25,000 --> 01:45:29,000
 So, we should actually look at this issue from two, you know, perspectives.

1713
01:45:29,000 --> 01:45:30,000
 Okay.

1714
01:45:30,000 --> 01:45:33,000
 So, this is the, the reason of called the tree.

1715
01:45:33,000 --> 01:45:35,000
 We see we try to remove the redundancy, right.

1716
01:45:35,000 --> 01:45:39,000
 And because of the pinot phenomenon.

1717
01:45:39,000 --> 01:45:40,000
 Okay.

1718
01:45:40,000 --> 01:45:46,000
 So, a good feature subset should have a maximum relevance and a minimum redundancy.

1719
01:45:47,000 --> 01:45:50,000
 So, this is the, you know, a good feature subset.

1720
01:45:50,000 --> 01:45:55,000
 And then of course, the tree in the feature subset selection, actually, you know, we

1721
01:45:55,000 --> 01:46:02,000
 should actually use these two, actually, aspects to evaluate the goodness of a feature subset.

1722
01:46:02,000 --> 01:46:05,000
 Re-relevance, okay.

1723
01:46:05,000 --> 01:46:07,000
 Should be maximized.

1724
01:46:07,000 --> 01:46:10,000
 And redundancy should be minimized.

1725
01:46:10,000 --> 01:46:11,000
 Okay.

1726
01:46:11,000 --> 01:46:12,000
 Okay.

1727
01:46:12,000 --> 01:46:15,000
 So, now that we consider the feature selection algorithm.

1728
01:46:15,000 --> 01:46:19,000
 Actually, a feature selection algorithm consists of two main components.

1729
01:46:19,000 --> 01:46:20,000
 Okay.

1730
01:46:20,000 --> 01:46:24,000
 And one component is called a feature is a set of all of them.

1731
01:46:24,000 --> 01:46:25,000
 Okay.

1732
01:46:25,000 --> 01:46:30,000
 And actually, the set of all of them actually, actually creates a set of all of them.

1733
01:46:30,000 --> 01:46:31,000
 Okay.

1734
01:46:31,000 --> 01:46:35,000
 So, we can see that the tree is a set of all of them.

1735
01:46:35,000 --> 01:46:36,000
 Okay.

1736
01:46:36,000 --> 01:46:43,000
 And actually, the set algorithm actually creates the candidate feature subset.

1737
01:46:43,000 --> 01:46:44,000
 Okay.

1738
01:46:44,000 --> 01:46:48,000
 We are creating a huge number of, you know, candidate features of set.

1739
01:46:48,000 --> 01:46:49,000
 Okay.

1740
01:46:49,000 --> 01:46:56,000
 And then the second component evaluation criterion is used to evaluate each of these generated

1741
01:46:56,000 --> 01:46:58,000
 candidate features of set.

1742
01:46:58,000 --> 01:46:59,000
 Okay.

1743
01:46:59,000 --> 01:47:02,000
 And then we identify the best performing feature subset.

1744
01:47:02,000 --> 01:47:03,000
 Okay.

1745
01:47:03,000 --> 01:47:05,000
 So, we need to have these two components.

1746
01:47:05,000 --> 01:47:08,000
 One is to generate candidate features of set.

1747
01:47:08,000 --> 01:47:09,000
 Okay.

1748
01:47:09,000 --> 01:47:13,000
 So, this is the function of the set algorithm.

1749
01:47:13,000 --> 01:47:14,000
 Okay.

1750
01:47:14,000 --> 01:47:18,000
 Then the second component is the evaluation criterion.

1751
01:47:18,000 --> 01:47:19,000
 Okay.

1752
01:47:19,000 --> 01:47:25,000
 And which is used to identify, right, the best performing features.

1753
01:47:25,000 --> 01:47:26,000
 Okay.

1754
01:47:26,000 --> 01:47:33,000
 And this diagram shows these actually the two components in a feature selection algorithm.

1755
01:47:33,000 --> 01:47:34,000
 Okay.

1756
01:47:34,000 --> 01:47:36,000
 So, we have, first of all, we have a full feature set.

1757
01:47:36,000 --> 01:47:37,000
 Right.

1758
01:47:37,000 --> 01:47:38,000
 This is a full feature set.

1759
01:47:38,000 --> 01:47:43,000
 And based on the full feature set, the set algorithm will create a large number of candidate

1760
01:47:43,000 --> 01:47:45,000
 features of set.

1761
01:47:45,000 --> 01:47:53,000
 All of these are possible as the, as the possibility as being selected as the final feature set.

1762
01:47:53,000 --> 01:47:54,000
 Okay.

1763
01:47:54,000 --> 01:47:59,000
 So, so actually we, we need to generate actually a lot of candidate features of set.

1764
01:47:59,000 --> 01:48:05,000
 Then the evaluation criterion to evaluate is used to evaluate each of the candidate features

1765
01:48:05,000 --> 01:48:06,000
 of set.

1766
01:48:06,000 --> 01:48:07,000
 Okay.

1767
01:48:07,000 --> 01:48:10,000
 Then based on the evaluation criterion, the value we calculated, right.

1768
01:48:10,000 --> 01:48:13,000
 And then we can identify the best of the best one.

1769
01:48:13,000 --> 01:48:14,000
 Okay.

1770
01:48:14,000 --> 01:48:19,000
 Then we can get the feature subset selected.

1771
01:48:19,000 --> 01:48:20,000
 Okay.

1772
01:48:20,000 --> 01:48:24,000
 So, these are the two components in the set algorithm.

1773
01:48:24,000 --> 01:48:27,000
 So, necessarily we look at each of the two.

1774
01:48:27,000 --> 01:48:28,000
 The set algorithm.

1775
01:48:28,000 --> 01:48:30,000
 So, what's set algorithm we can use?

1776
01:48:30,000 --> 01:48:36,000
 And then we'll also look at the evaluation criterion for feature subset, not for the

1777
01:48:36,000 --> 01:48:38,000
 individual feature.

1778
01:48:38,000 --> 01:48:39,000
 Okay.

1779
01:48:39,000 --> 01:48:44,000
 So, set algorithm here, the first one is the result of the search.

1780
01:48:44,000 --> 01:48:49,000
 The result of the search, we will use all possible subset.

1781
01:48:49,000 --> 01:48:54,000
 Actually, it can see all possible combinations.

1782
01:48:54,000 --> 01:48:55,000
 Okay.

1783
01:48:55,000 --> 01:49:01,000
 And for example, initially, you know, the full feature subset, full feature set can see

1784
01:49:01,000 --> 01:49:03,000
 all n features, right.

1785
01:49:03,000 --> 01:49:05,000
 X1, X2, X3, n, Xn.

1786
01:49:05,000 --> 01:49:06,000
 Okay.

1787
01:49:06,000 --> 01:49:13,000
 Based on these n features, we should create actually all the possible combinations with

1788
01:49:13,000 --> 01:49:15,000
 different number of features.

1789
01:49:15,000 --> 01:49:22,000
 For example, for one feature, for, if just for feature subset with one feature, then

1790
01:49:22,000 --> 01:49:27,000
 of course, we can have X1, we can X2, X3, Xn, until Xn, right.

1791
01:49:27,000 --> 01:49:29,000
 We can have these n subset.

1792
01:49:29,000 --> 01:49:33,000
 So, each of these n subset contains one feature.

1793
01:49:33,000 --> 01:49:39,000
 And then we can create feature subset that contains two features.

1794
01:49:39,000 --> 01:49:41,000
 And then we can have X1, X2.

1795
01:49:41,000 --> 01:49:42,000
 We have two features.

1796
01:49:42,000 --> 01:49:44,000
 These are feature subset.

1797
01:49:44,000 --> 01:49:45,000
 X1, X3.

1798
01:49:45,000 --> 01:49:48,000
 So, these are not the feature subset, right.

1799
01:49:48,000 --> 01:49:49,000
 So, we have two features.

1800
01:49:49,000 --> 01:49:50,000
 X1, X4.

1801
01:49:50,000 --> 01:49:51,000
 X1, Xn.

1802
01:49:51,000 --> 01:49:52,000
 Okay.

1803
01:49:52,000 --> 01:49:56,000
 Then we can have X2, X3.

1804
01:49:56,000 --> 01:49:57,000
 X2, X4.

1805
01:49:57,000 --> 01:49:58,000
 X2, X5.

1806
01:49:58,000 --> 01:49:59,000
 X2, Xn.

1807
01:49:59,000 --> 01:50:02,000
 Then we have X3, X4, right.

1808
01:50:02,000 --> 01:50:03,000
 X3, X5.

1809
01:50:03,000 --> 01:50:05,000
 We can have all of these combinations.

1810
01:50:05,000 --> 01:50:10,000
 So, these are only for the feature with three, a subset with two features.

1811
01:50:10,000 --> 01:50:13,000
 Then we can have a feature subset that has three features, right.

1812
01:50:13,000 --> 01:50:14,000
 X1, X2, X3.

1813
01:50:14,000 --> 01:50:15,000
 X1, X2, X4.

1814
01:50:15,000 --> 01:50:18,000
 So, we can have X1, X5, Xn, X4.

1815
01:50:18,000 --> 01:50:19,000
 Different combinations, right.

1816
01:50:19,000 --> 01:50:26,000
 So, actually these are the user research generate all possible combinations.

1817
01:50:26,000 --> 01:50:27,000
 All possible combinations.

1818
01:50:27,000 --> 01:50:29,000
 So, these are kind of feature subset.

1819
01:50:29,000 --> 01:50:32,000
 Actually, this number is huge normally.

1820
01:50:32,000 --> 01:50:37,000
 In particular, if this n is big, how big could this n?

1821
01:50:37,000 --> 01:50:43,000
 I can tell you, okay, this n sometimes could be over a few thousand.

1822
01:50:43,000 --> 01:50:50,000
 Actually, I used to work on the, you know, applying machine learning to the biomedical

1823
01:50:50,000 --> 01:50:51,000
 data.

1824
01:50:51,000 --> 01:50:53,000
 One area called bioinformatics.

1825
01:50:53,000 --> 01:51:00,000
 And so, they use actually the G-machrylary technology to measure the genes per levels.

1826
01:51:00,000 --> 01:51:02,000
 How many genes?

1827
01:51:02,000 --> 01:51:04,000
 Forty thousand.

1828
01:51:04,000 --> 01:51:08,000
 In other words, the number of features is forty thousand.

1829
01:51:08,000 --> 01:51:11,000
 But the number of samples is just a few hundred.

1830
01:51:11,000 --> 01:51:16,000
 Because each sample is just one patient, right, over a certain cancer, for example.

1831
01:51:16,000 --> 01:51:20,000
 So, the cancer in the patient is very limited, right.

1832
01:51:20,000 --> 01:51:23,000
 And so, the number of samples is a few hundred.

1833
01:51:23,000 --> 01:51:28,000
 But the number of features could be as high as forty thousand.

1834
01:51:28,000 --> 01:51:32,000
 You can imagine, right, forty thousand combinations.

1835
01:51:32,000 --> 01:51:34,000
 What's the number?

1836
01:51:34,000 --> 01:51:35,000
 Okay.

1837
01:51:35,000 --> 01:51:38,000
 So, these are the results of the search normally.

1838
01:51:38,000 --> 01:51:46,000
 I think, you know, it's used only when the number of samples, the number of kinds of

1839
01:51:46,000 --> 01:51:48,000
 features, right, is small.

1840
01:51:48,000 --> 01:51:52,000
 For example, you have ten, you have twenty features, right, you want to select features

1841
01:51:52,000 --> 01:51:53,000
 subset.

1842
01:51:53,000 --> 01:51:56,000
 Then you probably can think about these results of the search.

1843
01:51:56,000 --> 01:52:02,000
 And these results of the search method will certainly not miss the optimal features subset.

1844
01:52:02,000 --> 01:52:06,000
 They will create the features subset that are optimal, right.

1845
01:52:06,000 --> 01:52:09,000
 Because they create all the possible combinations.

1846
01:52:09,000 --> 01:52:14,000
 So, these all possible combinations include the optimal features subset.

1847
01:52:14,000 --> 01:52:18,000
 So, these methods will not miss the optimal features subset.

1848
01:52:18,000 --> 01:52:27,000
 But the problem is that if the N is large, and the computational complexity is high,

1849
01:52:27,000 --> 01:52:37,000
 then this method is not practical, okay, for the large N, not practical.

1850
01:52:37,000 --> 01:52:38,000
 Okay.

1851
01:52:38,000 --> 01:52:41,000
 In theory, of course, actually, this method is optimal.

1852
01:52:41,000 --> 01:52:54,000
 But in practice, actually, it's hard to implement for large N. Okay.

1853
01:52:54,000 --> 01:52:58,000
 Then, in practice, we normally use suboptimal method.

1854
01:52:58,000 --> 01:53:01,000
 This is the optimal method, is the search.

1855
01:53:01,000 --> 01:53:04,000
 We use the suboptimal method.

1856
01:53:04,000 --> 01:53:09,000
 So, the suboptimal method here is sequential.

1857
01:53:09,000 --> 01:53:11,000
 Sequential, okay.

1858
01:53:11,000 --> 01:53:14,000
 And we have a sequential forward selection.

1859
01:53:14,000 --> 01:53:18,000
 And so, this is the bottom up method.

1860
01:53:18,000 --> 01:53:19,000
 Sequential.

1861
01:53:19,000 --> 01:53:23,000
 That means, actually, we select the features sequentially.

1862
01:53:23,000 --> 01:53:26,000
 And this method is a E32 method.

1863
01:53:26,000 --> 01:53:29,000
 And at the each step, we just select one feature.

1864
01:53:29,000 --> 01:53:34,000
 For example, actually, in the first step, we select one feature.

1865
01:53:34,000 --> 01:53:41,000
 In the second select step, we select a second feature based on the first feature.

1866
01:53:41,000 --> 01:53:42,000
 Okay.

1867
01:53:42,000 --> 01:53:48,000
 Then, at the select step, we select the third feature based on the two previously selected features.

1868
01:53:48,000 --> 01:53:52,000
 This kind of, you know, sequential forward selection.

1869
01:53:52,000 --> 01:53:53,000
 Okay.

1870
01:53:53,000 --> 01:53:55,000
 And this is actually the bottom up method.

1871
01:53:55,000 --> 01:53:57,000
 We start from an empty set.

1872
01:53:57,000 --> 01:53:58,000
 Empty set.

1873
01:53:58,000 --> 01:53:59,000
 Right.

1874
01:53:59,000 --> 01:54:05,000
 Then, we re-grew the features subset from zero until a certain number.

1875
01:54:05,000 --> 01:54:06,000
 Okay.

1876
01:54:06,000 --> 01:54:08,000
 So, these are sequential forward search.

1877
01:54:08,000 --> 01:54:09,000
 Okay.

1878
01:54:09,000 --> 01:54:12,000
 Because empty set, every step we select one.

1879
01:54:12,000 --> 01:54:13,000
 Okay.

1880
01:54:13,000 --> 01:54:16,000
 So, the features are there, groups step by step.

1881
01:54:16,000 --> 01:54:17,000
 Okay.

1882
01:54:17,000 --> 01:54:21,000
 And so, then another method called sequential backward elimination.

1883
01:54:21,000 --> 01:54:24,000
 This is sequential backward elimination.

1884
01:54:24,000 --> 01:54:28,000
 That means that this method starts from the full set.

1885
01:54:28,000 --> 01:54:29,000
 Full set.

1886
01:54:29,000 --> 01:54:30,000
 Okay.

1887
01:54:30,000 --> 01:54:34,000
 Then, it sequentially removes the features step by step.

1888
01:54:34,000 --> 01:54:37,000
 At each step, it removes one feature.

1889
01:54:37,000 --> 01:54:38,000
 Okay.

1890
01:54:38,000 --> 01:54:40,000
 Start from the full feature set.

1891
01:54:40,000 --> 01:54:43,000
 Remove one feature at the first step.

1892
01:54:43,000 --> 01:54:44,000
 Okay.

1893
01:54:44,000 --> 01:54:46,000
 Then, we remove the second feature in the second step.

1894
01:54:46,000 --> 01:54:47,000
 Okay.

1895
01:54:47,000 --> 01:54:57,000
 And this is actually continued until the number of features or certain, you know, some criteria

1896
01:54:57,000 --> 01:54:58,000
 is satisfied.

1897
01:54:58,000 --> 01:54:59,000
 Okay.

1898
01:54:59,000 --> 01:55:02,500
 So, basically, this method is a top down method.

1899
01:55:02,500 --> 01:55:08,000
 From the full set, I then reduce to a small subset.

1900
01:55:08,000 --> 01:55:09,000
 Okay.

1901
01:55:09,000 --> 01:55:13,000
 So, in the forward selection, we start from an empty set.

1902
01:55:13,000 --> 01:55:15,000
 We re-grew the features subset.

1903
01:55:15,000 --> 01:55:19,000
 From empty set to a larger set.

1904
01:55:19,000 --> 01:55:20,000
 Okay.

1905
01:55:20,000 --> 01:55:23,000
 So, here, from the full set to a small set.

1906
01:55:23,000 --> 01:55:24,000
 Okay.

1907
01:55:24,000 --> 01:55:29,000
 So, this is called sequential backward elimination.

1908
01:55:29,000 --> 01:55:30,000
 Okay.

1909
01:55:30,000 --> 01:55:31,000
 Okay.

1910
01:55:31,000 --> 01:55:33,000
 So, it grows, right?

1911
01:55:33,000 --> 01:55:34,000
 It shrinks.

1912
01:55:34,000 --> 01:55:35,000
 The features set shrinks.

1913
01:55:35,000 --> 01:55:41,000
 Actually, one feature is removed at every step.

1914
01:55:41,000 --> 01:55:42,000
 Okay.

1915
01:55:42,000 --> 01:55:43,000
 And, actually, we have a break.

1916
01:55:43,000 --> 01:55:48,000
 After break, we talk about, actually, you know, the implementations of this sequential

1917
01:55:48,000 --> 01:55:53,000
 forward search selection and also the sequential backward elimination.

1918
01:55:53,000 --> 01:55:54,000
 Okay.

1919
01:55:54,000 --> 01:56:00,000
 And also, we will look at the evaluation criterion for the feature subset.

1920
01:56:00,000 --> 01:56:03,000
 We have learned how to evaluate the feature, right?

1921
01:56:03,000 --> 01:56:04,000
 Individually.

1922
01:56:04,000 --> 01:56:10,000
 So, the feature selection, just now we see such algorithm, a general feature subset,

1923
01:56:10,000 --> 01:56:11,000
 right?

1924
01:56:11,000 --> 01:56:15,000
 So, we have criterion to evaluate the feature subset, not feature individually.

1925
01:56:15,000 --> 01:56:16,000
 Okay.

1926
01:56:16,000 --> 01:56:21,000
 So, next, we will look at the implementation of the sequential forward search and also the

1927
01:56:21,000 --> 01:56:28,000
 sequential backward elimination and also the criterion for the feature subset evaluation.

1928
01:56:28,000 --> 01:56:35,000
 And, of course, the combination of the two to form a feature selection, actually, algorithm.

1929
01:56:35,000 --> 01:56:36,000
 Okay.

1930
01:56:36,000 --> 01:56:37,000
 So, next, we have break.

1931
01:56:37,000 --> 01:56:38,000
 Time is up.

1932
01:56:38,000 --> 01:56:39,000
 Okay.

1933
01:56:39,000 --> 01:56:40,000
 Okay.

1934
01:56:40,000 --> 01:56:41,000
 Okay.

1935
01:56:41,000 --> 01:56:42,000
 Okay.

1936
01:56:42,000 --> 01:56:43,000
 Okay.

1937
01:56:43,000 --> 01:56:44,000
 Okay.

1938
01:56:44,000 --> 01:56:45,000
 Okay.

1939
01:56:45,000 --> 01:56:46,000
 Okay.

1940
01:56:46,000 --> 01:56:47,000
 Okay.

1941
01:56:47,000 --> 01:56:48,000
 Okay.

1942
01:56:48,000 --> 01:56:49,000
 Okay.

1943
01:56:49,000 --> 01:56:50,000
 Okay.

1944
01:56:50,000 --> 01:56:51,000
 Okay.

1945
01:56:51,000 --> 01:56:52,000
 Okay.

1946
01:56:52,000 --> 01:56:53,000
 Okay.

1947
01:56:53,000 --> 01:56:54,000
 Okay.

1948
01:56:54,000 --> 01:56:55,000
 Okay.

1949
01:56:55,000 --> 01:56:56,000
 Okay.

1950
01:56:56,000 --> 01:56:57,000
 Okay.

1951
01:56:57,000 --> 01:56:58,000
 Okay.

1952
01:56:58,000 --> 01:56:59,000
 Okay.

1953
01:56:59,000 --> 01:57:00,000
 Okay.

1954
01:57:00,000 --> 01:57:01,000
 Okay.

1955
01:57:01,000 --> 01:57:02,000
 Okay.

1956
01:57:02,000 --> 01:57:03,000
 Okay.

1957
01:57:03,000 --> 01:57:04,000
 Okay.

1958
01:57:04,000 --> 01:57:05,000
 Okay.

1959
01:57:05,000 --> 01:57:06,000
 Okay.

1960
01:57:06,000 --> 01:57:07,000
 Okay.

1961
01:57:07,000 --> 01:57:08,000
 Okay.

1962
01:57:08,000 --> 01:57:09,000
 Okay.

1963
01:57:09,000 --> 01:57:10,000
 Okay.

1964
01:57:10,000 --> 01:57:11,000
 Okay.

1965
01:57:11,000 --> 01:57:12,000
 Okay.

1966
01:57:12,000 --> 01:57:13,000
 Okay.

1967
01:57:13,000 --> 01:57:14,000
 Okay.

1968
01:57:14,000 --> 01:57:15,000
 Okay.

1969
01:57:15,000 --> 01:57:16,000
 Okay.

1970
01:57:16,000 --> 01:57:17,000
 Okay.

1971
01:57:17,000 --> 01:57:18,000
 Okay.

1972
01:57:18,000 --> 01:57:19,000
 Okay.

1973
01:57:19,000 --> 01:57:20,000
 Okay.

1974
01:57:20,000 --> 01:57:21,000
 Okay.

1975
01:57:21,000 --> 01:57:22,000
 Okay.

1976
01:57:22,000 --> 01:57:23,000
 Okay.

1977
01:57:23,000 --> 01:57:24,000
 Okay.

1978
01:57:24,000 --> 01:57:25,000
 Okay.

1979
01:57:25,000 --> 01:57:26,000
 Okay.

1980
01:57:26,000 --> 01:57:27,000
 Okay.

1981
01:57:27,000 --> 01:57:28,000
 Okay.

1982
01:57:28,000 --> 01:57:29,000
 Okay.

1983
01:57:29,000 --> 01:57:30,000
 Okay.

1984
01:57:30,000 --> 01:57:31,000
 Okay.

1985
01:57:31,000 --> 01:57:32,000
 Okay.

1986
01:57:32,000 --> 01:57:33,000
 Okay.

1987
01:57:33,000 --> 01:57:34,000
 Okay.

1988
01:57:34,000 --> 01:57:35,000
 Okay.

1989
01:57:35,000 --> 01:57:36,000
 Okay.

1990
01:58:05,000 --> 01:58:06,000
 Okay.

1991
01:58:35,000 --> 01:58:36,000
 Okay.

1992
01:59:05,000 --> 01:59:06,000
 Okay.

1993
01:59:35,000 --> 01:59:36,000
 Okay.

1994
02:00:05,000 --> 02:00:06,000
 Okay.

1995
02:00:35,000 --> 02:00:36,000
 Okay.

1996
02:01:05,000 --> 02:01:06,000
 Okay.

1997
02:01:35,000 --> 02:01:36,000
 Okay.

1998
02:02:05,000 --> 02:02:06,000
 Okay.

1999
02:02:35,000 --> 02:02:36,000
 Okay.

2000
02:03:05,000 --> 02:03:06,000
 Okay.

2001
02:03:35,000 --> 02:03:36,000
 Okay.

2002
02:04:05,000 --> 02:04:06,000
 Okay.

2003
02:04:35,000 --> 02:04:36,000
 Okay.

2004
02:05:05,000 --> 02:05:06,000
 Okay.

2005
02:05:35,000 --> 02:05:40,840
 Okay, okay.

2006
02:05:40,840 --> 02:05:41,840
 So let's see.

2007
02:05:59,840 --> 02:06:00,840
 OK.

2008
02:06:00,840 --> 02:06:02,840
 So let's relook at the evaluation criteria

2009
02:06:02,840 --> 02:06:04,840
 for a feature subset.

2010
02:06:04,840 --> 02:06:06,840
 OK, not for any video feature, right?

2011
02:06:06,840 --> 02:06:07,840
 Features subset.

2012
02:06:13,840 --> 02:06:16,840
 And actually the first one, I think, is like

2013
02:06:16,840 --> 02:06:17,840
 separability measure, right?

2014
02:06:17,840 --> 02:06:19,840
 Just now when we talked about the individual feature,

2015
02:06:19,840 --> 02:06:21,840
 we had a separability measure.

2016
02:06:21,840 --> 02:06:24,840
 And actually we can generalize this, you know,

2017
02:06:24,840 --> 02:06:27,840
 separability measure to the feature subset.

2018
02:06:27,840 --> 02:06:28,840
 OK.

2019
02:06:29,840 --> 02:06:32,840
 And so this is kind of a measure, right?

2020
02:06:32,840 --> 02:06:37,840
 And also, you know, we have the other measures, OK?

2021
02:06:37,840 --> 02:06:40,840
 So this measure is called a colloquial performance.

2022
02:06:40,840 --> 02:06:43,840
 And if you recall, our target for feature extraction

2023
02:06:43,840 --> 02:06:47,840
 is to select, actually, a feature subset

2024
02:06:47,840 --> 02:06:51,840
 so that the best performance could be achieved, right?

2025
02:06:51,840 --> 02:06:53,840
 Of course, this best performance normally means

2026
02:06:53,840 --> 02:06:57,840
 the best performance on the future data, the testing data,

2027
02:06:57,840 --> 02:06:58,840
 right?

2028
02:06:58,840 --> 02:07:01,840
 But actually we can evaluate based on the,

2029
02:07:01,840 --> 02:07:03,840
 actually, you know, the validation data.

2030
02:07:03,840 --> 02:07:05,840
 So, OK.

2031
02:07:05,840 --> 02:07:07,840
 So this is our ultimate goal, right?

2032
02:07:07,840 --> 02:07:09,840
 So, essentially, we can use this, actually,

2033
02:07:09,840 --> 02:07:13,840
 as a criterion to evaluate the goodness of the feature set

2034
02:07:13,840 --> 02:07:15,840
 or subset, right?

2035
02:07:15,840 --> 02:07:18,840
 So the first type of the subset evaluation criterion

2036
02:07:18,840 --> 02:07:21,840
 is the colloquial performance.

2037
02:07:24,840 --> 02:07:27,840
 So normally this is not the performance on the training data,

2038
02:07:27,840 --> 02:07:29,840
 not on training data, right?

2039
02:07:29,840 --> 02:07:32,840
 So this is the evaluation criterion.

2040
02:07:32,840 --> 02:07:36,840
 And so in this approach, actually, we train a classifier.

2041
02:07:36,840 --> 02:07:39,840
 Because we need to have the colloquial performance, right?

2042
02:07:39,840 --> 02:07:42,840
 So that means, actually, for each feature subset,

2043
02:07:42,840 --> 02:07:45,840
 actually, we need to train a corresponding,

2044
02:07:45,840 --> 02:07:49,840
 actually, you know, the pattern classifier, OK?

2045
02:07:49,840 --> 02:07:52,840
 Then we look at the performance of the classifier

2046
02:07:52,840 --> 02:07:55,840
 on the validation data, OK?

2047
02:07:55,840 --> 02:07:59,840
 So this, actually, you know, is the,

2048
02:07:59,840 --> 02:08:01,840
 this is actually method.

2049
02:08:01,840 --> 02:08:04,840
 And, of course, actually, for some classifier,

2050
02:08:04,840 --> 02:08:07,840
 then the training is easy, right?

2051
02:08:07,840 --> 02:08:10,840
 But for some classifier, the training is a

2052
02:08:10,840 --> 02:08:12,840
 computational demanding.

2053
02:08:12,840 --> 02:08:14,840
 It involves a lot of competitions, OK?

2054
02:08:14,840 --> 02:08:17,840
 And then, of course, this method, actually,

2055
02:08:17,840 --> 02:08:20,840
 will have an issue of, you know,

2056
02:08:20,840 --> 02:08:21,840
 complexity, right?

2057
02:08:21,840 --> 02:08:24,840
 Computational complexity, OK?

2058
02:08:24,840 --> 02:08:30,840
 And in particular, and for complex classifiers,

2059
02:08:30,840 --> 02:08:33,840
 if you want to use, like, a neural network classifier,

2060
02:08:33,840 --> 02:08:35,840
 OK, then you need to train a neural network

2061
02:08:35,840 --> 02:08:37,840
 and for each of the features subset.

2062
02:08:37,840 --> 02:08:39,840
 And for a neural network, for example,

2063
02:08:39,840 --> 02:08:42,840
 multi-layer perceptual neural network,

2064
02:08:42,840 --> 02:08:45,840
 we need to train a neural network through many iterations.

2065
02:08:45,840 --> 02:08:47,840
 I think there are other iterations, right?

2066
02:08:47,840 --> 02:08:50,840
 So each training of a neural, you know,

2067
02:08:50,840 --> 02:08:52,840
 the training of a neural network, each neural network

2068
02:08:52,840 --> 02:08:57,840
 is time consuming and a competition only demanding, OK?

2069
02:08:57,840 --> 02:09:03,840
 So this is actually the limitation of this method.

2070
02:09:03,840 --> 02:09:06,840
 And also, actually, this method, you know,

2071
02:09:06,840 --> 02:09:09,840
 is chosen, is only, match the classifier.

2072
02:09:09,840 --> 02:09:12,840
 This chosen classifier, the best,

2073
02:09:12,840 --> 02:09:15,840
 because we use this specific classifier

2074
02:09:15,840 --> 02:09:17,840
 to evaluate the performance, right?

2075
02:09:17,840 --> 02:09:20,840
 So the chosen, actually, feature subset

2076
02:09:20,840 --> 02:09:23,840
 matches this, you know, classifier best.

2077
02:09:23,840 --> 02:09:25,840
 But if you use this selected feature subset

2078
02:09:25,840 --> 02:09:28,840
 for other application classifiers,

2079
02:09:28,840 --> 02:09:31,840
 then the performance may not be the best.

2080
02:09:31,840 --> 02:09:35,840
 OK, so a different classifier may use a trigger,

2081
02:09:35,840 --> 02:09:37,840
 you know, have a different, you know,

2082
02:09:37,840 --> 02:09:40,840
 best performing feature subset, OK?

2083
02:09:40,840 --> 02:09:42,840
 So this feature subset is selected

2084
02:09:42,840 --> 02:09:46,840
 is specific to those, to this specific, you know,

2085
02:09:46,840 --> 02:09:48,840
 dependent classifier.

2086
02:09:48,840 --> 02:09:53,840
 OK, so this classifier performance

2087
02:09:53,840 --> 02:09:56,840
 as the evaluation criterion for feature subset, OK?

2088
02:09:56,840 --> 02:09:58,840
 So others actually,

2089
02:09:58,840 --> 02:10:01,840
 the separability measure, separability measure.

2090
02:10:01,840 --> 02:10:05,840
 And separability measure is used in the evaluation

2091
02:10:05,840 --> 02:10:08,840
 of the individual feature,

2092
02:10:08,840 --> 02:10:11,840
 but we can extend this to the feature subset, OK?

2093
02:10:11,840 --> 02:10:17,840
 So how to evaluate the separability of a feature subset?

2094
02:10:17,840 --> 02:10:22,840
 So here, you know, we can have two, you know, methods.

2095
02:10:22,840 --> 02:10:25,840
 OK.

2096
02:10:25,840 --> 02:10:27,840
 So all these, actually, now,

2097
02:10:27,840 --> 02:10:29,840
 separability measures actually estimate

2098
02:10:29,840 --> 02:10:31,840
 the overlap between the distributions,

2099
02:10:31,840 --> 02:10:33,840
 overlap, OK?

2100
02:10:33,840 --> 02:10:37,840
 And of course, overlap normally is correspond to the distance,

2101
02:10:37,840 --> 02:10:44,840
 right, between, actually, the center points

2102
02:10:44,840 --> 02:10:46,840
 of the two clusters, OK?

2103
02:10:46,840 --> 02:10:48,840
 The two clusters.

2104
02:10:48,840 --> 02:10:50,840
 In other words, actually, you know,

2105
02:10:50,840 --> 02:10:53,840
 the center point, actually, is just the main vector, right?

2106
02:10:53,840 --> 02:10:56,840
 So certainly, actually, this overlap depends on the distance

2107
02:10:56,840 --> 02:10:58,840
 between the two main vectors.

2108
02:10:58,840 --> 02:11:04,840
 And also depends on the, maybe, like the scatter

2109
02:11:04,840 --> 02:11:07,840
 of each of the clusters.

2110
02:11:07,840 --> 02:11:10,840
 The scatter, right, within class scatter, right?

2111
02:11:10,840 --> 02:11:15,840
 And so this is the, the separability measure

2112
02:11:15,840 --> 02:11:18,840
 basically, actually, measure for feature evaluation,

2113
02:11:18,840 --> 02:11:20,840
 subset evaluation.

2114
02:11:20,840 --> 02:11:22,840
 And in this separability measure,

2115
02:11:22,840 --> 02:11:24,840
 we don't actually use the class feature performance.

2116
02:11:24,840 --> 02:11:27,840
 In other words, we don't need to train a classifier

2117
02:11:27,840 --> 02:11:30,840
 for each feature subset, OK?

2118
02:11:30,840 --> 02:11:33,840
 So compared with the previous method, right?

2119
02:11:33,840 --> 02:11:38,840
 Actually, this is a computational efficient, right?

2120
02:11:38,840 --> 02:11:41,840
 Because we don't need to train a model

2121
02:11:41,840 --> 02:11:44,840
 for each feature subset, OK?

2122
02:11:45,840 --> 02:11:48,840
 So this feature subset is also independent

2123
02:11:48,840 --> 02:11:50,840
 of the classifier, right?

2124
02:11:50,840 --> 02:11:52,840
 You select this feature subset, then you can,

2125
02:11:52,840 --> 02:11:55,840
 maybe, apply this, you can use a different,

2126
02:11:55,840 --> 02:11:58,840
 you know, classifier, OK?

2127
02:12:07,840 --> 02:12:08,840
 OK.

2128
02:12:08,840 --> 02:12:10,840
 So let's, what a metric we can use

2129
02:12:10,840 --> 02:12:12,840
 to measure this separability measure.

2130
02:12:12,840 --> 02:12:15,840
 So the first, actually, one is called

2131
02:12:15,840 --> 02:12:19,840
 a Mahalanobi distance-based separability measure.

2132
02:12:19,840 --> 02:12:22,840
 So we will have two types of a separability measure.

2133
02:12:22,840 --> 02:12:26,840
 The first type is a Mahalanobi distance, OK?

2134
02:12:26,840 --> 02:12:31,840
 Between the two, you know, mean vectors.

2135
02:12:31,840 --> 02:12:34,840
 So assume, you know, this is the space

2136
02:12:34,840 --> 02:12:38,840
 span by the two features, by two features.

2137
02:12:38,840 --> 02:12:39,840
 Assume there are two features.

2138
02:12:39,840 --> 02:12:40,840
 There are more, right?

2139
02:12:40,840 --> 02:12:42,840
 The concept is the same.

2140
02:12:42,840 --> 02:12:45,840
 For illustration, it's two features, OK?

2141
02:12:45,840 --> 02:12:48,840
 So for each of the classes,

2142
02:12:48,840 --> 02:12:52,840
 we can have the mean vector, which is the center point, right?

2143
02:12:52,840 --> 02:12:54,840
 We can have the mean vector.

2144
02:12:54,840 --> 02:12:57,840
 And the overlap of these mean vectors

2145
02:12:57,840 --> 02:12:59,840
 can be evaluated based on the distance

2146
02:12:59,840 --> 02:13:03,840
 between the two mean vectors, the two mean vectors, right?

2147
02:13:03,840 --> 02:13:07,840
 The centroid of the two, the two centroid

2148
02:13:07,840 --> 02:13:11,840
 of the two classes, the distance, OK?

2149
02:13:11,840 --> 02:13:16,840
 And here, what distance we should use?

2150
02:13:16,840 --> 02:13:21,840
 And actually, we should use the so-called Mahalanobi distance

2151
02:13:21,840 --> 02:13:26,840
 instead of the commonly used, including distance, OK?

2152
02:13:26,840 --> 02:13:29,840
 And this is because the Mahalanobi distance

2153
02:13:29,840 --> 02:13:32,840
 takes into account the correlation

2154
02:13:32,840 --> 02:13:36,840
 into the consideration, OK?

2155
02:13:37,840 --> 02:13:42,840
 So here, we assume we have M1 for class 1, right?

2156
02:13:42,840 --> 02:13:43,840
 M2 for class 2.

2157
02:13:43,840 --> 02:13:45,840
 These are the mean vectors, OK?

2158
02:13:45,840 --> 02:13:47,840
 So given, you know, as the training sample, right,

2159
02:13:47,840 --> 02:13:51,840
 we can just look at the mean vector along this two dimension,

2160
02:13:51,840 --> 02:13:54,840
 right, along the dimension determined by the subset, OK?

2161
02:13:54,840 --> 02:13:56,840
 We can have these two mean vectors.

2162
02:13:56,840 --> 02:13:58,840
 And then we can calculate the Mahalanobi distance

2163
02:13:58,840 --> 02:14:00,840
 between the two vectors.

2164
02:14:00,840 --> 02:14:03,840
 So this is the definition of the Mahalanobi distance.

2165
02:14:04,840 --> 02:14:09,840
 Actually, M1, M2, try to suppose C inverse M1, M2.

2166
02:14:09,840 --> 02:14:12,840
 C here is a correlation matrix.

2167
02:14:12,840 --> 02:14:15,840
 Correlation matrix, OK?

2168
02:14:15,840 --> 02:14:18,840
 If you don't have the correlation matrix,

2169
02:14:18,840 --> 02:14:20,840
 that means that if we don't consider correlation,

2170
02:14:20,840 --> 02:14:25,840
 then this C is just the identity matrix, right?

2171
02:14:25,840 --> 02:14:28,840
 Then this is just the including distance.

2172
02:14:28,840 --> 02:14:31,840
 But here, because actually, this is an inclusion of this tree

2173
02:14:31,840 --> 02:14:33,840
 in the worst of the correlation matrix,

2174
02:14:33,840 --> 02:14:36,840
 then the correlation or the redundancy of the features

2175
02:14:36,840 --> 02:14:40,840
 is considered in this evaluation.

2176
02:14:40,840 --> 02:14:43,840
 Actually, previously, we talked about the goal

2177
02:14:43,840 --> 02:14:45,840
 of feature subset selection, right?

2178
02:14:45,840 --> 02:14:49,840
 To remove the redundant features and also to remove

2179
02:14:49,840 --> 02:14:52,840
 actually the irrelevant features.

2180
02:14:52,840 --> 02:14:55,840
 Irrational features can be removed through the individual feature

2181
02:14:55,840 --> 02:14:58,840
 evaluation selection.

2182
02:14:58,840 --> 02:15:04,840
 But redundant features can be removed through this tree,

2183
02:15:04,840 --> 02:15:07,840
 Mahalanobi distance-based separability matrix,

2184
02:15:07,840 --> 02:15:09,840
 because this matrix takes into account

2185
02:15:09,840 --> 02:15:11,840
 the correlation between the features.

2186
02:15:11,840 --> 02:15:15,840
 Correlation means the redundancy.

2187
02:15:15,840 --> 02:15:17,840
 OK.

2188
02:15:17,840 --> 02:15:20,840
 So why we need to take this correlation into account?

2189
02:15:20,840 --> 02:15:23,840
 Actually, for example, there are two features.

2190
02:15:23,840 --> 02:15:25,840
 There are two features in the extreme case,

2191
02:15:25,840 --> 02:15:29,840
 two features identical, X1.

2192
02:15:29,840 --> 02:15:33,840
 Another feature is exactly the X1, same information.

2193
02:15:33,840 --> 02:15:37,840
 And actually, if we use actually, you're including distance, right?

2194
02:15:37,840 --> 02:15:40,840
 Actually, the distance based on the two features

2195
02:15:40,840 --> 02:15:42,840
 is larger than the distance based on one feature,

2196
02:15:42,840 --> 02:15:46,840
 although the two features are exactly the same.

2197
02:15:46,840 --> 02:15:49,840
 But if we use the monolithic distance, right,

2198
02:15:49,840 --> 02:15:53,840
 and then the features are set, right?

2199
02:15:53,840 --> 02:15:56,840
 Because there are no, this is the inclusion of the,

2200
02:15:56,840 --> 02:15:58,840
 in words of the chronometrics.

2201
02:15:58,840 --> 02:16:02,840
 And then there's no problem with the address.

2202
02:16:02,840 --> 02:16:03,840
 OK.

2203
02:16:03,840 --> 02:16:08,840
 So this is the one measure for the separability.

2204
02:16:08,840 --> 02:16:12,840
 Using the Mahalanobi distance between the two main vectors.

2205
02:16:12,840 --> 02:16:14,840
 Of course, this is only for the two class,

2206
02:16:14,840 --> 02:16:16,840
 class-future problem, right?

2207
02:16:16,840 --> 02:16:18,840
 If you have a multiple class, class-future problem,

2208
02:16:18,840 --> 02:16:21,840
 then we can evaluate the goodies of a feature subset.

2209
02:16:21,840 --> 02:16:24,840
 And based on the pair-wise, actually, you know,

2210
02:16:24,840 --> 02:16:25,840
 separability measures.

2211
02:16:25,840 --> 02:16:27,840
 For example, three classes.

2212
02:16:27,840 --> 02:16:29,840
 Then we, from class one, class two,

2213
02:16:29,840 --> 02:16:31,840
 we can have a, you know, a separability measure.

2214
02:16:31,840 --> 02:16:33,840
 Class one, class three, we can have.

2215
02:16:33,840 --> 02:16:35,840
 Then class two, class three, we can have.

2216
02:16:35,840 --> 02:16:37,840
 So it's a combination of pair-wise,

2217
02:16:37,840 --> 02:16:40,840
 actually, you know, separability measures.

2218
02:16:40,840 --> 02:16:42,840
 Then we get a sum of them.

2219
02:16:42,840 --> 02:16:46,840
 So this is for the multiple class, class-future problem.

2220
02:16:46,840 --> 02:16:47,840
 OK.

2221
02:16:47,840 --> 02:16:50,840
 So this is, you know, the Mahalanobi distance

2222
02:16:50,840 --> 02:16:53,840
 between separability measures.

2223
02:16:53,840 --> 02:16:55,840
 And then the next actually is the scatter

2224
02:16:55,840 --> 02:16:57,840
 between separability measures.

2225
02:16:57,840 --> 02:16:58,840
 Scatter.

2226
02:16:58,840 --> 02:17:00,840
 Actually scatter measures.

2227
02:17:00,840 --> 02:17:03,840
 We are familiar with this, right?

2228
02:17:03,840 --> 02:17:06,840
 When we started the Fisher-Linian Dispens Analysis,

2229
02:17:06,840 --> 02:17:08,840
 we have the scatter matrix.

2230
02:17:08,840 --> 02:17:12,840
 Within class scatter matrix, between class scatter matrix.

2231
02:17:12,840 --> 02:17:14,840
 So here we have two matrices, right?

2232
02:17:14,840 --> 02:17:18,840
 So we have SB, SW, between class, B, between class,

2233
02:17:18,840 --> 02:17:22,840
 and SW within class.

2234
02:17:22,840 --> 02:17:25,840
 Class one, we can have within class scatter matrix.

2235
02:17:25,840 --> 02:17:28,840
 Class two, we can have within class scatter matrix, right?

2236
02:17:28,840 --> 02:17:31,840
 Then we can, you know, S one plus S two, right?

2237
02:17:31,840 --> 02:17:34,840
 We can have this scatter matrix, SW,

2238
02:17:34,840 --> 02:17:36,840
 within class scatter matrix.

2239
02:17:36,840 --> 02:17:39,840
 So based on this, SB and SW,

2240
02:17:39,840 --> 02:17:43,840
 and we can derive actually a separability measure.

2241
02:17:43,840 --> 02:17:46,840
 So a separability measure must be a scalar.

2242
02:17:46,840 --> 02:17:49,840
 Must be a single, one scalar, right?

2243
02:17:49,840 --> 02:17:52,840
 So here's the input here.

2244
02:17:52,840 --> 02:17:54,840
 It's a matrix.

2245
02:17:54,840 --> 02:17:57,840
 So from home, we can, based on the matrix,

2246
02:17:57,840 --> 02:18:02,840
 to obtain one actually, you know, a scalar value, right?

2247
02:18:02,840 --> 02:18:05,840
 To denote the separabilities.

2248
02:18:05,840 --> 02:18:06,840
 OK.

2249
02:18:06,840 --> 02:18:09,840
 And actually here we can have two.

2250
02:18:09,840 --> 02:18:10,840
 OK.

2251
02:18:10,840 --> 02:18:12,840
 Mine is based on the trees.

2252
02:18:12,840 --> 02:18:15,840
 So the SB, the between class, right?

2253
02:18:15,840 --> 02:18:19,840
 SW, within class, right?

2254
02:18:19,840 --> 02:18:21,840
 So within class normally smaller, better, right?

2255
02:18:21,840 --> 02:18:25,840
 So even in scalar case, so this should be in a denominator, right?

2256
02:18:25,840 --> 02:18:28,840
 But now it's a multi-dimension.

2257
02:18:28,840 --> 02:18:30,840
 So not put in the denominator.

2258
02:18:30,840 --> 02:18:32,840
 So we use inverse.

2259
02:18:32,840 --> 02:18:33,840
 You need to understand the inverse.

2260
02:18:33,840 --> 02:18:36,840
 Inverse something like one divided by something, right?

2261
02:18:36,840 --> 02:18:38,840
 Scalar is one divided by something.

2262
02:18:38,840 --> 02:18:43,840
 If a matrix, a harder measure is inverse of the matrix.

2263
02:18:43,840 --> 02:18:44,840
 OK.

2264
02:18:44,840 --> 02:18:46,840
 So these are going to something like, you know,

2265
02:18:46,840 --> 02:18:50,840
 the between class divided by the within class, right?

2266
02:18:50,840 --> 02:18:54,840
 So we use the trees of these matrices.

2267
02:18:54,840 --> 02:18:57,840
 SW inverse times SB, you get a matrix.

2268
02:18:57,840 --> 02:18:59,840
 And then the trees of this matrix.

2269
02:18:59,840 --> 02:19:01,840
 So what is the trees of matrices?

2270
02:19:01,840 --> 02:19:06,840
 The summation of all the elements on the main diagonal.

2271
02:19:07,840 --> 02:19:09,840
 Main diagonal, matrix, main diagonal,

2272
02:19:09,840 --> 02:19:11,840
 all the elements, summation.

2273
02:19:11,840 --> 02:19:14,840
 So this is called a tree of a matrix.

2274
02:19:14,840 --> 02:19:16,840
 So we can use these trees, right?

2275
02:19:16,840 --> 02:19:18,840
 So from these trees we can get a scalar, right?

2276
02:19:18,840 --> 02:19:21,840
 Because all the diagonal elements, summation.

2277
02:19:21,840 --> 02:19:23,840
 This is a scalar.

2278
02:19:23,840 --> 02:19:25,840
 So from the matrix we get a scalar.

2279
02:19:25,840 --> 02:19:27,840
 Of course, they're not the only way, right?

2280
02:19:27,840 --> 02:19:29,840
 You can also think about other ways.

2281
02:19:29,840 --> 02:19:31,840
 Maybe they're not the best way, right?

2282
02:19:31,840 --> 02:19:33,840
 Maybe we can think other ways, OK?

2283
02:19:34,840 --> 02:19:36,840
 At least I can think about, you know,

2284
02:19:36,840 --> 02:19:38,840
 if we, for SW inverse SB, right,

2285
02:19:38,840 --> 02:19:41,840
 if I perform like a singular value decomposition,

2286
02:19:41,840 --> 02:19:45,840
 I get singular values, summation of the singular values,

2287
02:19:45,840 --> 02:19:50,840
 maybe it's a better evaluation, better than this.

2288
02:19:50,840 --> 02:19:51,840
 OK.

2289
02:19:51,840 --> 02:19:52,840
 Then just the trees.

2290
02:19:52,840 --> 02:19:53,840
 OK.

2291
02:19:53,840 --> 02:19:55,840
 So, but I think this is just one of the measures, right?

2292
02:19:55,840 --> 02:19:58,840
 It's not imperfect, imperfect.

2293
02:19:58,840 --> 02:20:00,840
 But you can think about other ways,

2294
02:20:00,840 --> 02:20:02,840
 but I can be in a better solution.

2295
02:20:02,840 --> 02:20:04,840
 You can think about, OK.

2296
02:20:04,840 --> 02:20:08,840
 So this is one measure.

2297
02:20:08,840 --> 02:20:12,840
 Another tree we based on the reach of the two trees, OK?

2298
02:20:12,840 --> 02:20:16,840
 The trees of the SB, the trees of the SB,

2299
02:20:16,840 --> 02:20:18,840
 with W, within class, right?

2300
02:20:18,840 --> 02:20:20,840
 Within class, we put a denominator, right?

2301
02:20:20,840 --> 02:20:22,840
 SW is the numerator.

2302
02:20:22,840 --> 02:20:27,840
 Because we hold SW as big as possible, the trees.

2303
02:20:27,840 --> 02:20:31,840
 So we can use this reach of the trees of the two metrics

2304
02:20:31,840 --> 02:20:35,840
 and as a separability measure, OK?

2305
02:20:35,840 --> 02:20:38,840
 So here these are separability measures.

2306
02:20:38,840 --> 02:20:41,840
 So from here we can see we don't actually include

2307
02:20:41,840 --> 02:20:43,840
 a pen and classifier in the loop, right?

2308
02:20:43,840 --> 02:20:47,840
 In the evaluation of the goodness of a feature subset.

2309
02:20:47,840 --> 02:20:50,840
 It's independent of the pen and classifiers, OK?

2310
02:20:50,840 --> 02:20:54,840
 So normally the computational complexity

2311
02:20:54,840 --> 02:20:57,840
 of this separability-based measure

2312
02:20:57,840 --> 02:21:01,840
 is lower than the classification performance-based measure.

2313
02:21:01,840 --> 02:21:03,840
 OK.

2314
02:21:06,840 --> 02:21:07,840
 OK.

2315
02:21:07,840 --> 02:21:10,840
 So we have studied the set algorithm

2316
02:21:10,840 --> 02:21:13,840
 and we have studied the evaluation criterion

2317
02:21:13,840 --> 02:21:15,840
 for feature subset evaluation.

2318
02:21:15,840 --> 02:21:17,840
 So next we need to combine the two, right?

2319
02:21:17,840 --> 02:21:21,840
 Combine the two to actually produce a feature

2320
02:21:22,840 --> 02:21:24,840
 subset selection algorithm.

2321
02:21:24,840 --> 02:21:25,840
 OK.

2322
02:21:25,840 --> 02:21:29,840
 So these are no feature subset selection algorithm.

2323
02:21:29,840 --> 02:21:37,840
 And so based on the which criterion, which type of criterion

2324
02:21:37,840 --> 02:21:40,840
 we use, actually these are feature subset selection

2325
02:21:40,840 --> 02:21:44,840
 algorithm actually are divided into two or three categories.

2326
02:21:44,840 --> 02:21:47,840
 The first category is called the filter method.

2327
02:21:47,840 --> 02:21:50,840
 In the filter method, we use the separability measure.

2328
02:21:50,840 --> 02:21:52,840
 Different separability measures.

2329
02:21:52,840 --> 02:21:54,840
 Here I just give you three examples, right?

2330
02:21:54,840 --> 02:21:58,840
 Monomy distance, I don't know the

2331
02:21:58,840 --> 02:22:02,840
 scatter-metrics-based value-based matrix, right?

2332
02:22:02,840 --> 02:22:04,840
 So we just give these three, right?

2333
02:22:04,840 --> 02:22:08,840
 And so these belong to the so-called filter method.

2334
02:22:08,840 --> 02:22:11,840
 We don't train classifiers.

2335
02:22:11,840 --> 02:22:12,840
 Filter.

2336
02:22:12,840 --> 02:22:13,840
 OK.

2337
02:22:13,840 --> 02:22:17,840
 And then the method based on the classification performance

2338
02:22:17,840 --> 02:22:22,840
 actually is the so-called wrapper method.

2339
02:22:22,840 --> 02:22:25,840
 It's a wrap around actually the classifier.

2340
02:22:25,840 --> 02:22:27,840
 So it's called wrapper method.

2341
02:22:27,840 --> 02:22:28,840
 OK.

2342
02:22:28,840 --> 02:22:31,840
 And then the last method actually is called embedded method.

2343
02:22:31,840 --> 02:22:35,840
 Embedded method, that means actually we have the so-called

2344
02:22:35,840 --> 02:22:38,840
 building mechanism to penalize the number of features

2345
02:22:38,840 --> 02:22:40,840
 in the model.

2346
02:22:40,840 --> 02:22:44,840
 For example, actually the last so-right, last so regression.

2347
02:22:45,840 --> 02:22:50,840
 So we penalize actually the nonzero coefficient.

2348
02:22:50,840 --> 02:22:51,840
 OK.

2349
02:22:51,840 --> 02:22:55,840
 We shrink the coefficient of features towards zero.

2350
02:22:55,840 --> 02:22:58,840
 But of course, not all the features in the coefficient

2351
02:22:58,840 --> 02:23:00,840
 will shrink to zero, right?

2352
02:23:00,840 --> 02:23:03,840
 If all of them shrink to zero, then actually

2353
02:23:03,840 --> 02:23:05,840
 the prediction area will be big.

2354
02:23:05,840 --> 02:23:08,840
 Actually, the loss function, we have two parts, right?

2355
02:23:08,840 --> 02:23:10,840
 One is the squared area.

2356
02:23:10,840 --> 02:23:13,840
 Another is the squared.

2357
02:23:13,840 --> 02:23:19,840
 Another is the L1 norm of the parameter vector.

2358
02:23:19,840 --> 02:23:22,840
 We see the absolute value of the parameter, right?

2359
02:23:22,840 --> 02:23:28,840
 So only the important, the unimportant features

2360
02:23:28,840 --> 02:23:33,840
 actually will shrink the coefficient to zero.

2361
02:23:33,840 --> 02:23:34,840
 OK.

2362
02:23:34,840 --> 02:23:39,840
 So it has a built-in mechanism to penalize actually

2363
02:23:39,840 --> 02:23:41,840
 the number of features in the model.

2364
02:23:41,840 --> 02:23:44,840
 So it also plays a rule of a feature selector.

2365
02:23:44,840 --> 02:23:48,840
 So it trains the model, parameter also selects

2366
02:23:48,840 --> 02:23:50,840
 actually the variables, the features.

2367
02:23:50,840 --> 02:23:51,840
 OK.

2368
02:23:51,840 --> 02:23:53,840
 So that is the embedded method.

2369
02:23:53,840 --> 02:23:55,840
 Embedded.

2370
02:23:55,840 --> 02:23:56,840
 OK.

2371
02:23:56,840 --> 02:23:58,840
 So we're going to classify in the loop.

2372
02:23:58,840 --> 02:24:02,840
 And first method, then we'll include a classifier.

2373
02:24:02,840 --> 02:24:03,840
 OK.

2374
02:24:03,840 --> 02:24:05,840
 So embedded method has a built-in mechanism actually

2375
02:24:05,840 --> 02:24:09,840
 to penalize the number of features in the model.

2376
02:24:09,840 --> 02:24:10,840
 OK.

2377
02:24:10,840 --> 02:24:12,840
 So that's actually, we look at, first,

2378
02:24:12,840 --> 02:24:14,840
 we look at the filter method.

2379
02:24:14,840 --> 02:24:15,840
 OK.

2380
02:24:15,840 --> 02:24:20,840
 And so I think the diagram is similar.

2381
02:24:20,840 --> 02:24:25,840
 Just the separability measure is used for the feature subset

2382
02:24:25,840 --> 02:24:26,840
 evaluation.

2383
02:24:26,840 --> 02:24:30,840
 It might not be decent measure, or the scatter metric

2384
02:24:30,840 --> 02:24:31,840
 is separability measures.

2385
02:24:31,840 --> 02:24:34,840
 It can be used to evaluate actually the good

2386
02:24:34,840 --> 02:24:36,840
 and the bad features of set.

2387
02:24:36,840 --> 02:24:37,840
 OK.

2388
02:24:40,840 --> 02:24:45,840
 And actually, this method, as we know, we normally don't

2389
02:24:45,840 --> 02:24:50,840
 actually use the resultive search, the optimal method.

2390
02:24:50,840 --> 02:24:54,840
 And because actually it is a computational,

2391
02:24:54,840 --> 02:24:55,840
 very intensive, right?

2392
02:24:55,840 --> 02:24:57,840
 Continually very intensive.

2393
02:24:57,840 --> 02:24:59,840
 So normally we use the suboptimal method,

2394
02:24:59,840 --> 02:25:02,840
 like sequential forward search or sequential backward

2395
02:25:02,840 --> 02:25:03,840
 elimination.

2396
02:25:03,840 --> 02:25:04,840
 OK.

2397
02:25:04,840 --> 02:25:07,840
 And in the forward selection, so on the backward elimination,

2398
02:25:07,840 --> 02:25:10,840
 actually it's a iterative procedure.

2399
02:25:10,840 --> 02:25:14,840
 And we remove the features, or we select the features,

2400
02:25:14,840 --> 02:25:16,840
 step by step.

2401
02:25:16,840 --> 02:25:17,840
 OK.

2402
02:25:17,840 --> 02:25:20,840
 So every step, we just select one feature.

2403
02:25:20,840 --> 02:25:25,840
 So, and let, in theory, this is the procedure repeated.

2404
02:25:25,840 --> 02:25:27,840
 Every step, we add one feature.

2405
02:25:28,840 --> 02:25:30,840
 Every step, we remove one feature.

2406
02:25:30,840 --> 02:25:34,840
 So this is actually a selection is a stop, or elimination

2407
02:25:34,840 --> 02:25:39,840
 is stopped until the stopping criterion is satisfied.

2408
02:25:39,840 --> 02:25:44,840
 The stopping criterion could be the preset number of features.

2409
02:25:44,840 --> 02:25:48,840
 If you know, OK, I want to select 50 features, right?

2410
02:25:48,840 --> 02:25:49,840
 OK.

2411
02:25:49,840 --> 02:25:52,840
 Then you just actually select 50 features.

2412
02:25:52,840 --> 02:25:53,840
 OK.

2413
02:25:53,840 --> 02:25:56,840
 And so then the number reach to 50, right?

2414
02:25:56,840 --> 02:25:57,840
 Then you stop.

2415
02:25:57,840 --> 02:25:58,840
 OK.

2416
02:26:00,840 --> 02:26:04,840
 Or actually, based on other, no, like, separate measure

2417
02:26:04,840 --> 02:26:06,840
 no longer improves.

2418
02:26:06,840 --> 02:26:10,840
 Actually, you cannot see no longer improves,

2419
02:26:10,840 --> 02:26:12,840
 but the improvement is trivial.

2420
02:26:12,840 --> 02:26:13,840
 It's trivial.

2421
02:26:13,840 --> 02:26:14,840
 OK.

2422
02:26:14,840 --> 02:26:17,840
 And so in practice, normally the separate measure

2423
02:26:17,840 --> 02:26:20,840
 actually increase, increase with more features.

2424
02:26:20,840 --> 02:26:25,840
 But just after critical value, the increase is trivial.

2425
02:26:25,840 --> 02:26:26,840
 OK.

2426
02:26:26,840 --> 02:26:28,840
 And actually, many actually, you know,

2427
02:26:28,840 --> 02:26:31,840
 no problem have similar phenomenon.

2428
02:26:31,840 --> 02:26:32,840
 OK.

2429
02:26:32,840 --> 02:26:35,840
 The more features we have, the better the performance.

2430
02:26:35,840 --> 02:26:39,840
 The more iterations we perform the training, right?

2431
02:26:39,840 --> 02:26:42,840
 And then the better the performance.

2432
02:26:42,840 --> 02:26:45,840
 So when to this, even in the actual annual,

2433
02:26:45,840 --> 02:26:47,840
 which we will study next week, right?

2434
02:26:47,840 --> 02:26:52,840
 And also, in the classroom, and also the more features

2435
02:26:52,840 --> 02:26:55,840
 we use, and then the better the performance on the training

2436
02:26:55,840 --> 02:26:56,840
 data.

2437
02:26:56,840 --> 02:27:01,840
 But just in a scenario, when you add one extra actual cluster,

2438
02:27:01,840 --> 02:27:06,840
 or you include one extra feature, or you train the model

2439
02:27:06,840 --> 02:27:11,840
 one extra iteration, and then the improvement is trivial.

2440
02:27:11,840 --> 02:27:14,840
 So normally, in such a scenario, we use the so-called,

2441
02:27:14,840 --> 02:27:18,840
 you know, the air ball method.

2442
02:27:18,840 --> 02:27:19,840
 OK.

2443
02:27:19,840 --> 02:27:21,840
 Air ball, right?

2444
02:27:21,840 --> 02:27:23,840
 At the beginning of the performance,

2445
02:27:23,840 --> 02:27:25,840
 you look at this area, right?

2446
02:27:25,840 --> 02:27:30,840
 The performance actually, you know, the area drop very fast,

2447
02:27:30,840 --> 02:27:31,840
 right?

2448
02:27:31,840 --> 02:27:34,840
 After this point, slow.

2449
02:27:34,840 --> 02:27:35,840
 This point, right?

2450
02:27:35,840 --> 02:27:38,840
 Correspond, this is the best point, which will stop here.

2451
02:27:38,840 --> 02:27:41,840
 So this is called the air ball method.

2452
02:27:41,840 --> 02:27:42,840
 OK.

2453
02:27:42,840 --> 02:27:43,840
 Very fast, then slow.

2454
02:27:43,840 --> 02:27:44,840
 OK.

2455
02:27:44,840 --> 02:27:46,840
 So this part, actually, we show based on this.

2456
02:27:46,840 --> 02:27:48,840
 So the performance improves, you know, based on the training

2457
02:27:48,840 --> 02:27:49,840
 data, the stability, right?

2458
02:27:49,840 --> 02:27:50,840
 Improved.

2459
02:27:50,840 --> 02:27:53,840
 But at the same point, the improvement is very small.

2460
02:27:53,840 --> 02:27:54,840
 OK.

2461
02:27:54,840 --> 02:27:55,840
 Then we can stop.

2462
02:27:55,840 --> 02:27:56,840
 OK.

2463
02:27:56,840 --> 02:28:02,840
 So this is the, you know, the filter method, and the

2464
02:28:02,840 --> 02:28:05,840
 reference method is similar, but every step is just, you know,

2465
02:28:05,840 --> 02:28:08,840
 based on the pan-classified performance to evaluate the

2466
02:28:08,840 --> 02:28:10,840
 goodness of a feature subset.

2467
02:28:10,840 --> 02:28:11,840
 OK.

2468
02:28:11,840 --> 02:28:13,840
 So also, actually, the solving criterion could be the

2469
02:28:13,840 --> 02:28:17,840
 number of features or the classification accuracy is the

2470
02:28:17,840 --> 02:28:18,840
 best.

2471
02:28:18,840 --> 02:28:19,840
 OK.

2472
02:28:19,840 --> 02:28:20,840
 Then we can stop.

2473
02:28:20,840 --> 02:28:21,840
 OK.

2474
02:28:21,840 --> 02:28:23,840
 So normally, we don't use the classification accuracy on the

2475
02:28:23,840 --> 02:28:24,840
 training data.

2476
02:28:24,840 --> 02:28:25,840
 We use the validation data.

2477
02:28:25,840 --> 02:28:28,840
 But you can still use the classification accuracy on the

2478
02:28:28,840 --> 02:28:29,840
 training data.

2479
02:28:29,840 --> 02:28:32,840
 But I just, again, will, again, follow the so-called air ball

2480
02:28:32,840 --> 02:28:33,840
 principle, right?

2481
02:28:33,840 --> 02:28:35,840
 At the beginning of the classification performance,

2482
02:28:35,840 --> 02:28:40,840
 we will improve very fast with more features added to the

2483
02:28:40,840 --> 02:28:41,840
 model, right?

2484
02:28:41,840 --> 02:28:44,840
 But after a point, this part is slow.

2485
02:28:44,840 --> 02:28:45,840
 Air ball.

2486
02:28:45,840 --> 02:28:46,840
 OK.

2487
02:28:46,840 --> 02:28:48,840
 So this is a point that we can stop.

2488
02:28:48,840 --> 02:28:49,840
 OK.

2489
02:28:49,840 --> 02:28:53,840
 So this is the reference method.

2490
02:28:53,840 --> 02:28:54,840
 OK.

2491
02:28:54,840 --> 02:28:59,840
 So that's actually, we look at the differential forward

2492
02:28:59,840 --> 02:29:00,840
 selection.

2493
02:29:00,840 --> 02:29:02,840
 We look at the implementation.

2494
02:29:02,840 --> 02:29:04,840
 How this is done.

2495
02:29:04,840 --> 02:29:05,840
 OK.

2496
02:29:05,840 --> 02:29:08,840
 And in the signal forward search, you know, we know this is

2497
02:29:08,840 --> 02:29:10,840
 a bottom up method.

2498
02:29:10,840 --> 02:29:12,840
 We start from empty set.

2499
02:29:12,840 --> 02:29:13,840
 OK.

2500
02:29:13,840 --> 02:29:15,840
 So usually, we have empty set.

2501
02:29:15,840 --> 02:29:17,840
 They are hard to select the first feature.

2502
02:29:17,840 --> 02:29:18,840
 OK.

2503
02:29:18,840 --> 02:29:19,840
 And actually, we don't know.

2504
02:29:19,840 --> 02:29:20,840
 We should try.

2505
02:29:20,840 --> 02:29:23,840
 For each of the features, kind of features, X, Y, X, X, X,

2506
02:29:23,840 --> 02:29:29,840
 X, we combine it with the empty set.

2507
02:29:29,840 --> 02:29:31,840
 Of course, the combination with the empty set, we add to the

2508
02:29:31,840 --> 02:29:34,840
 empty set, we adjust the individual feature, right?

2509
02:29:34,840 --> 02:29:37,840
 So the first step, actually, is the individual feature

2510
02:29:37,840 --> 02:29:40,840
 evaluation selection.

2511
02:29:40,840 --> 02:29:43,840
 In the first step, first, in the first step, we select one

2512
02:29:43,840 --> 02:29:44,840
 feature, right?

2513
02:29:44,840 --> 02:29:47,840
 And this feature is evaluated individually.

2514
02:29:47,840 --> 02:29:50,840
 Because you combine this feature with the empty set.

2515
02:29:50,840 --> 02:29:52,840
 That's the equivalent to us.

2516
02:29:52,840 --> 02:29:54,840
 There's a single feature, right?

2517
02:29:54,840 --> 02:29:55,840
 OK.

2518
02:29:55,840 --> 02:29:59,840
 So in the first step, we consider each feature as a

2519
02:29:59,840 --> 02:30:03,840
 candidate to be selected as the first feature.

2520
02:30:03,840 --> 02:30:04,840
 OK.

2521
02:30:04,840 --> 02:30:09,840
 And then we create a lot of feature subset candidates.

2522
02:30:09,840 --> 02:30:12,840
 But of course, each feature subset contains only one

2523
02:30:12,840 --> 02:30:13,840
 feature.

2524
02:30:13,840 --> 02:30:14,840
 Because this is the first step.

2525
02:30:14,840 --> 02:30:17,840
 Because we combine this feature with the empty set.

2526
02:30:17,840 --> 02:30:18,840
 OK.

2527
02:30:18,840 --> 02:30:22,840
 And then, this is the set algorithm, right?

2528
02:30:22,840 --> 02:30:23,840
 Sequential set.

2529
02:30:23,840 --> 02:30:27,840
 Actually, we combine each of the features with the

2530
02:30:27,840 --> 02:30:28,840
 previous feature.

2531
02:30:28,840 --> 02:30:31,840
 The previous feature is an empty set.

2532
02:30:31,840 --> 02:30:36,840
 And then we evaluate the gonies of the feature.

2533
02:30:36,840 --> 02:30:37,840
 OK.

2534
02:30:37,840 --> 02:30:41,840
 And so if we use the serrability measure, then we can just

2535
02:30:41,840 --> 02:30:45,840
 base on this, the trace of the between class scalar

2536
02:30:45,840 --> 02:30:48,840
 matrix over the trace of the within class scalar matrix,

2537
02:30:48,840 --> 02:30:51,840
 all based on the Mach-Norby distance measure, right?

2538
02:30:51,840 --> 02:30:53,840
 You can use only another measure.

2539
02:30:53,840 --> 02:30:54,840
 OK.

2540
02:30:54,840 --> 02:30:56,840
 Evaluate the gonies of an individual feature.

2541
02:30:56,840 --> 02:30:57,840
 OK.

2542
02:30:57,840 --> 02:30:59,840
 And actually, for individual feature, we can just use the

2543
02:30:59,840 --> 02:31:00,840
 feature ratio.

2544
02:31:00,840 --> 02:31:02,840
 We can just use the feature ratio.

2545
02:31:02,840 --> 02:31:03,840
 OK.

2546
02:31:03,840 --> 02:31:05,840
 And also, we can actually use the base on the

2547
02:31:05,840 --> 02:31:07,840
 performance classification performance.

2548
02:31:07,840 --> 02:31:09,840
 For each, we use just one feature, right?

2549
02:31:09,840 --> 02:31:11,840
 To train a classifier.

2550
02:31:11,840 --> 02:31:13,840
 For all the n features, right?

2551
02:31:13,840 --> 02:31:15,840
 We train n classifiers.

2552
02:31:15,840 --> 02:31:18,840
 Then we look at the performance of the n classifiers.

2553
02:31:18,840 --> 02:31:20,840
 Then we find the best performing one.

2554
02:31:20,840 --> 02:31:21,840
 OK.

2555
02:31:21,840 --> 02:31:24,840
 So the feature that has the largest separability measure,

2556
02:31:24,840 --> 02:31:29,840
 or the best classifier performance is selected.

2557
02:31:29,840 --> 02:31:34,840
 Then this feature is just the first feature selected.

2558
02:31:34,840 --> 02:31:35,840
 OK.

2559
02:31:35,840 --> 02:31:37,840
 So the first step basically is an individual feature

2560
02:31:37,840 --> 02:31:40,840
 evaluation, right?

2561
02:31:40,840 --> 02:31:42,840
 And then we select the best performing feature.

2562
02:31:42,840 --> 02:31:45,840
 So this is the first feature we have selected.

2563
02:31:45,840 --> 02:31:46,840
 OK.

2564
02:31:46,840 --> 02:31:49,840
 And then actually, come to the second step.

2565
02:31:49,840 --> 02:31:52,840
 In the second step, actually, we select the second feature.

2566
02:31:53,840 --> 02:31:55,840
 And how to select the second feature?

2567
02:31:55,840 --> 02:31:57,840
 The first feature is already selected, right?

2568
02:31:57,840 --> 02:32:02,840
 So we need to combine each of the remaining features.

2569
02:32:02,840 --> 02:32:04,840
 Totally, we have n features.

2570
02:32:04,840 --> 02:32:06,840
 After one feature is selected in the first step,

2571
02:32:06,840 --> 02:32:08,840
 we have a remaining a minus one feature.

2572
02:32:08,840 --> 02:32:11,840
 Each of these a minus one feature will be combined

2573
02:32:11,840 --> 02:32:14,840
 with the first selected feature.

2574
02:32:14,840 --> 02:32:18,840
 Then we can form a minus one feature subset.

2575
02:32:18,840 --> 02:32:21,840
 Each of the features subset have two features.

2576
02:32:22,840 --> 02:32:26,840
 And then we can evaluate the goodness of these two features.

2577
02:32:26,840 --> 02:32:27,840
 OK.

2578
02:32:27,840 --> 02:32:30,840
 Subset, a minus one feature subset.

2579
02:32:30,840 --> 02:32:32,840
 Each of them has two features.

2580
02:32:32,840 --> 02:32:33,840
 OK.

2581
02:32:33,840 --> 02:32:37,840
 And based on either the separability measure

2582
02:32:37,840 --> 02:32:41,840
 or the classifier performance.

2583
02:32:41,840 --> 02:32:42,840
 OK.

2584
02:32:42,840 --> 02:32:49,840
 Then we compare the goodness of the a minus one feature subset.

2585
02:32:49,840 --> 02:32:52,840
 Then we identify the best performing one, right?

2586
02:32:52,840 --> 02:32:54,840
 Either based on the separability measure

2587
02:32:54,840 --> 02:32:59,840
 or the classifier accuracy.

2588
02:32:59,840 --> 02:33:03,840
 Then we can select the second feature.

2589
02:33:03,840 --> 02:33:04,840
 OK.

2590
02:33:04,840 --> 02:33:05,840
 This is the second step, right?

2591
02:33:05,840 --> 02:33:06,840
 So these are the presets.

2592
02:33:06,840 --> 02:33:08,840
 We continue, right?

2593
02:33:08,840 --> 02:33:13,840
 Continue until the stopping criterion is satisfied.

2594
02:33:13,840 --> 02:33:14,840
 And just to mention, stopping criterion

2595
02:33:14,840 --> 02:33:19,840
 could be a number of features you preset.

2596
02:33:19,840 --> 02:33:20,840
 OK.

2597
02:33:20,840 --> 02:33:22,840
 It could be based on the separability measure

2598
02:33:22,840 --> 02:33:24,840
 increment, the improvement.

2599
02:33:24,840 --> 02:33:28,840
 If trivial, we can stop.

2600
02:33:28,840 --> 02:33:32,840
 Or we can be some of the classifier performance, right?

2601
02:33:32,840 --> 02:33:36,840
 On the validation data.

2602
02:33:36,840 --> 02:33:37,840
 OK.

2603
02:33:37,840 --> 02:33:43,840
 So this is the sequential forward selection.

2604
02:33:44,840 --> 02:33:48,840
 And next, we look at the sequential backward elimination.

2605
02:33:48,840 --> 02:33:49,840
 Backward elimination.

2606
02:33:49,840 --> 02:33:53,840
 By the way, backward elimination is the top down method, right?

2607
02:33:53,840 --> 02:33:55,840
 We start from the full set.

2608
02:33:55,840 --> 02:33:56,840
 OK.

2609
02:33:56,840 --> 02:33:59,840
 Then we remove the features one by one.

2610
02:33:59,840 --> 02:34:02,840
 Actually, each step we remove one feature.

2611
02:34:02,840 --> 02:34:03,840
 OK.

2612
02:34:03,840 --> 02:34:06,840
 Then the first step, which feature we should remove.

2613
02:34:06,840 --> 02:34:07,840
 I have no idea.

2614
02:34:07,840 --> 02:34:08,840
 We should try, right?

2615
02:34:08,840 --> 02:34:09,840
 Try one by one.

2616
02:34:09,840 --> 02:34:10,840
 OK.

2617
02:34:10,840 --> 02:34:15,840
 So at the first step, each of the features

2618
02:34:15,840 --> 02:34:19,840
 will be considered as a candidate to be removed.

2619
02:34:19,840 --> 02:34:20,840
 OK.

2620
02:34:20,840 --> 02:34:24,840
 Then we will create n subset.

2621
02:34:24,840 --> 02:34:31,840
 Each of the subset actually contains n monomount features.

2622
02:34:31,840 --> 02:34:34,840
 The first subset, I just know, remove x1.

2623
02:34:34,840 --> 02:34:35,840
 OK.

2624
02:34:35,840 --> 02:34:39,840
 Then we have x1, x2, x3, and your xn.

2625
02:34:39,840 --> 02:34:42,840
 The second feature subset removes x2.

2626
02:34:42,840 --> 02:34:46,840
 Then we have x1, x3, x4, and your xn.

2627
02:34:46,840 --> 02:34:49,840
 So we have n feature subset.

2628
02:34:49,840 --> 02:34:54,840
 Each feature subset contains n monomount features.

2629
02:34:54,840 --> 02:34:55,840
 OK.

2630
02:34:55,840 --> 02:34:57,840
 So this is the search, right?

2631
02:34:57,840 --> 02:35:00,840
 The search algorithm creates this candidate feature subset.

2632
02:35:00,840 --> 02:35:01,840
 OK.

2633
02:35:01,840 --> 02:35:04,840
 Then the next, we use the evaluation criterion

2634
02:35:04,840 --> 02:35:08,840
 to evaluate each of the features subset.

2635
02:35:08,840 --> 02:35:12,840
 We can either, based on the vulnerability measure,

2636
02:35:12,840 --> 02:35:16,840
 or based on the class feature accuracy.

2637
02:35:16,840 --> 02:35:19,840
 So filter method or the wrapper method, right?

2638
02:35:19,840 --> 02:35:24,840
 And then we can identify which feature is the least important.

2639
02:35:24,840 --> 02:35:25,840
 Less important.

2640
02:35:25,840 --> 02:35:28,840
 That means the removal of that feature does not

2641
02:35:28,840 --> 02:35:31,840
 affect the performance much.

2642
02:35:31,840 --> 02:35:32,840
 OK.

2643
02:35:32,840 --> 02:35:37,840
 Actually, in practice, we just compare which feature

2644
02:35:38,840 --> 02:35:42,840
 has the worst performance.

2645
02:35:42,840 --> 02:35:44,840
 It has the worst performance.

2646
02:35:44,840 --> 02:35:47,840
 We look at the vulnerability measure,

2647
02:35:47,840 --> 02:35:49,840
 or we look at the class accuracy.

2648
02:35:49,840 --> 02:35:52,840
 Which feature subset at least has the worst performance.

2649
02:35:52,840 --> 02:35:54,840
 And then we just remove that feature.

2650
02:35:57,840 --> 02:36:00,840
 So this is the first step.

2651
02:36:00,840 --> 02:36:01,840
 We remove one feature.

2652
02:36:01,840 --> 02:36:05,840
 Assume, actually, that one is removed.

2653
02:36:05,840 --> 02:36:06,840
 OK.

2654
02:36:09,840 --> 02:36:11,840
 And then, next, actually, we should actually

2655
02:36:11,840 --> 02:36:13,840
 remove other features, right?

2656
02:36:13,840 --> 02:36:15,840
 We should remove other first features removed.

2657
02:36:15,840 --> 02:36:17,840
 Then the next step, second step, we

2658
02:36:17,840 --> 02:36:19,840
 have M1 and 1 kind of features, right?

2659
02:36:19,840 --> 02:36:23,840
 So we will remove, actually, each of the candidate features.

2660
02:36:23,840 --> 02:36:25,840
 We get M1 and 1 features subset.

2661
02:36:25,840 --> 02:36:29,840
 Each feature subset has M1 and 2 features.

2662
02:36:29,840 --> 02:36:30,840
 OK.

2663
02:36:30,840 --> 02:36:31,840
 Then we evaluate.

2664
02:36:31,840 --> 02:36:33,840
 We repeat this, probably evaluate the performance,

2665
02:36:33,840 --> 02:36:35,840
 based on the vulnerability measure,

2666
02:36:35,840 --> 02:36:37,840
 or the classification accuracy.

2667
02:36:37,840 --> 02:36:38,840
 OK.

2668
02:36:38,840 --> 02:36:43,840
 And then, actually, we can identify the feature, right?

2669
02:36:43,840 --> 02:36:45,840
 The feature subset.

2670
02:36:45,840 --> 02:36:52,840
 And then we will continue this process, right?

2671
02:36:52,840 --> 02:36:55,840
 And until the summing criteria is satisfied.

2672
02:36:57,840 --> 02:36:58,840
 OK.

2673
02:36:58,840 --> 02:37:01,840
 So this is, actually, the sequential back-wet elimination.

2674
02:37:01,840 --> 02:37:06,840
 Sequentially, step by step, each step remove one.

2675
02:37:06,840 --> 02:37:12,840
 Of course, this is the classical forward search

2676
02:37:12,840 --> 02:37:14,840
 and the back-wet elimination.

2677
02:37:14,840 --> 02:37:17,840
 And sometimes, they combine the two together.

2678
02:37:17,840 --> 02:37:18,840
 OK.

2679
02:37:18,840 --> 02:37:23,840
 For example, they select one, two, three features.

2680
02:37:23,840 --> 02:37:24,840
 OK.

2681
02:37:24,840 --> 02:37:29,840
 Then, based on the three features, they apply the back-wet elimination,

2682
02:37:29,840 --> 02:37:31,840
 remove one features.

2683
02:37:31,840 --> 02:37:32,840
 Then we can get two.

2684
02:37:32,840 --> 02:37:33,840
 OK.

2685
02:37:33,840 --> 02:37:37,840
 Then from two, then we select that fourth, fifth.

2686
02:37:37,840 --> 02:37:41,840
 And then, based on the fifth, they apply the back-wet elimination,

2687
02:37:41,840 --> 02:37:42,840
 remove one features.

2688
02:37:42,840 --> 02:37:45,840
 So they combine the forward and backward together.

2689
02:37:45,840 --> 02:37:47,840
 So these are some variants, right?

2690
02:37:47,840 --> 02:37:51,840
 They combine the two forward search and back-wet elimination.

2691
02:37:51,840 --> 02:37:54,840
 So there are some variants, actually,

2692
02:37:54,840 --> 02:37:56,840
 in the literature.

2693
02:37:56,840 --> 02:37:57,840
 OK.

2694
02:37:57,840 --> 02:38:00,840
 And this is the second, actually, type.

2695
02:38:00,840 --> 02:38:04,840
 So in practice, which one we should use?

2696
02:38:04,840 --> 02:38:06,840
 And then, dependent applications.

2697
02:38:06,840 --> 02:38:07,840
 OK.

2698
02:38:07,840 --> 02:38:09,840
 For example, I mentioned before, right?

2699
02:38:09,840 --> 02:38:13,840
 Assume, actually, we have a very large, very hard-minor data.

2700
02:38:13,840 --> 02:38:16,840
 Just like, for example, 40,000 genes.

2701
02:38:16,840 --> 02:38:20,840
 But we know, only a few genes, actually, are related to cancer.

2702
02:38:20,840 --> 02:38:21,840
 OK.

2703
02:38:21,840 --> 02:38:26,840
 So in the transfer scenario, actually, we should use a forward search.

2704
02:38:27,840 --> 02:38:28,840
 Forward search.

2705
02:38:28,840 --> 02:38:29,840
 Right?

2706
02:38:29,840 --> 02:38:31,840
 Because, actually, if you want to identify 10 genes,

2707
02:38:31,840 --> 02:38:33,840
 you just need to repeat the process, right?

2708
02:38:33,840 --> 02:38:34,840
 10 times.

2709
02:38:34,840 --> 02:38:35,840
 Then you select 10 genes.

2710
02:38:35,840 --> 02:38:38,840
 If you use a back-wet elimination, you need to remove,

2711
02:38:38,840 --> 02:38:40,840
 you need to repeat 10 times.

2712
02:38:40,840 --> 02:38:42,840
 40,000 minus 10.

2713
02:38:42,840 --> 02:38:44,840
 You need to repeat so many times.

2714
02:38:44,840 --> 02:38:45,840
 OK.

2715
02:38:45,840 --> 02:38:47,840
 So this is another skill.

2716
02:38:47,840 --> 02:38:48,840
 OK.

2717
02:38:48,840 --> 02:38:51,840
 Another is that, actually, if you know the feature subset,

2718
02:38:51,840 --> 02:38:58,840
 very few features are redundant or insignificant.

2719
02:38:58,840 --> 02:39:02,840
 And most of them should be kept, should be retained,

2720
02:39:02,840 --> 02:39:04,840
 in the final features subset.

2721
02:39:04,840 --> 02:39:07,840
 In such a scenario, we should adopt the back-wet elimination.

2722
02:39:07,840 --> 02:39:10,840
 Because the back-wet elimination, just only we need to perform

2723
02:39:10,840 --> 02:39:13,840
 these a few steps, then we will stop.

2724
02:39:13,840 --> 02:39:17,840
 If you use a forward selection, you need to repeat many, many times.

2725
02:39:17,840 --> 02:39:20,840
 You need to perform this selection process many steps.

2726
02:39:21,840 --> 02:39:23,840
 So which to follow, which to use,

2727
02:39:23,840 --> 02:39:26,840
 and then depend on the specific application.

2728
02:39:26,840 --> 02:39:28,840
 OK.

2729
02:39:28,840 --> 02:39:32,840
 So this is the tool, actually, back-wet elimination,

2730
02:39:32,840 --> 02:39:34,840
 forward search, right?

2731
02:39:34,840 --> 02:39:36,840
 Forward selection, back-wet elimination.

2732
02:39:36,840 --> 02:39:39,840
 And the last one is the embedded method, right?

2733
02:39:39,840 --> 02:39:43,840
 Actually, we already landed in the regression, right?

2734
02:39:43,840 --> 02:39:45,840
 The last old regression.

2735
02:39:45,840 --> 02:39:53,840
 So we penalized the size of the coefficient, right?

2736
02:39:53,840 --> 02:39:55,840
 Absolute value of the coefficient.

2737
02:39:55,840 --> 02:39:59,840
 So the penalization will shrink the unimportant,

2738
02:39:59,840 --> 02:40:04,840
 actually, or redundant features, coefficient, towards zeros.

2739
02:40:04,840 --> 02:40:05,840
 OK.

2740
02:40:05,840 --> 02:40:07,840
 So this is a Buri-E entry mechanism.

2741
02:40:07,840 --> 02:40:11,840
 But actually, we know for the L2 norm penalization, right?

2742
02:40:11,840 --> 02:40:14,840
 For the L2, this is L1 norm.

2743
02:40:14,840 --> 02:40:17,840
 L2 norm, actually, we know the solution is easy, right?

2744
02:40:17,840 --> 02:40:18,840
 It's easy.

2745
02:40:18,840 --> 02:40:20,840
 We have the analytical solution.

2746
02:40:20,840 --> 02:40:21,840
 Analytical solution.

2747
02:40:21,840 --> 02:40:26,840
 We have a formula to calculate the theta value, right?

2748
02:40:26,840 --> 02:40:29,840
 But this, actually, we know for this, actually,

2749
02:40:29,840 --> 02:40:35,840
 L1 norm penalization, we need to use a numerical method to solve.

2750
02:40:35,840 --> 02:40:39,840
 We don't have a formula to calculate this parameter directly.

2751
02:40:39,840 --> 02:40:41,840
 We need to also use a non-neutral method,

2752
02:40:41,840 --> 02:40:45,840
 a iterative method to find the values of theta.

2753
02:40:45,840 --> 02:40:47,840
 Then based on theta, we know which is zero, right?

2754
02:40:47,840 --> 02:40:49,840
 Or close to zero.

2755
02:40:49,840 --> 02:40:51,840
 Then we see these are important.

2756
02:40:51,840 --> 02:40:53,840
 Are they redundant or insignificant?

2757
02:40:53,840 --> 02:41:00,840
 Then we only keep those features whose coefficient theta is big.

2758
02:41:00,840 --> 02:41:01,840
 OK.

2759
02:41:01,840 --> 02:41:04,840
 So this is an embedded method, right?

2760
02:41:04,840 --> 02:41:05,840
 So, yeah.

2761
02:41:05,840 --> 02:41:07,840
 So last old, yeah.

2762
02:41:07,840 --> 02:41:10,840
 I think that's all for the feature selection.

2763
02:41:10,840 --> 02:41:15,840
 And so feature selection is a denominator reduction technique, right?

2764
02:41:15,840 --> 02:41:18,840
 And we can keep the meanings of the original features.

2765
02:41:18,840 --> 02:41:25,840
 So this is very important for applications that need explainability of the model.

2766
02:41:25,840 --> 02:41:26,840
 OK.

2767
02:41:26,840 --> 02:41:28,840
 But in some applications, actually, we don't need this.

2768
02:41:28,840 --> 02:41:30,840
 We just want to use the model to predict.

2769
02:41:30,840 --> 02:41:31,840
 OK.

2770
02:41:31,840 --> 02:41:34,840
 We don't want to use the model to interpret, to explain.

2771
02:41:34,840 --> 02:41:37,840
 And then we can use a feature extraction.

2772
02:41:37,840 --> 02:41:42,840
 And in some time, actually, scenario, feature extraction, which combines all the features,

2773
02:41:42,840 --> 02:41:43,840
 right?

2774
02:41:43,840 --> 02:41:49,840
 Actually, it's more efficient and effective than feature selection.

2775
02:41:49,840 --> 02:41:52,840
 But feature selection also has its advantage, right?

2776
02:41:52,840 --> 02:41:58,840
 Keeping the meaning of the original features.

2777
02:41:58,840 --> 02:41:59,840
 OK.

2778
02:41:59,840 --> 02:42:00,840
 OK.

2779
02:42:00,840 --> 02:42:01,840
 So that's all.

2780
02:42:01,840 --> 02:42:04,840
 So next, actually, maybe I briefly talk about our quiz number two.

2781
02:42:04,840 --> 02:42:05,840
 Right?

2782
02:42:05,840 --> 02:42:07,840
 Quiz two is on the Saturday, right?

2783
02:42:07,840 --> 02:42:09,840
 The night of November.

2784
02:42:09,840 --> 02:42:12,840
 And I read the topic covered in the quiz.

2785
02:42:12,840 --> 02:42:13,840
 There should be those.

2786
02:42:13,840 --> 02:42:15,840
 There have not been covered in the assignment.

2787
02:42:15,840 --> 02:42:18,840
 In the assignment, we have the basic decision rule, right?

2788
02:42:18,840 --> 02:42:20,840
 Then we have the nine-year base.

2789
02:42:20,840 --> 02:42:22,840
 We have the support vectors.

2790
02:42:22,840 --> 02:42:25,840
 This have already covered in the assignment.

2791
02:42:25,840 --> 02:42:28,840
 Then we will not test this part in the quiz.

2792
02:42:28,840 --> 02:42:34,840
 Then in the quiz, we could test, actually, like the classification tree and the feature

2793
02:42:34,840 --> 02:42:35,840
 linear different analysis.

2794
02:42:35,840 --> 02:42:42,840
 And then the evolution of the classification performance and also the feature selection

2795
02:42:42,840 --> 02:42:45,840
 and also maybe the classification, right?

2796
02:42:45,840 --> 02:42:49,840
 Classification will be studied next week.

2797
02:42:49,840 --> 02:42:54,840
 And the time duration is one hour.

2798
02:42:54,840 --> 02:42:59,840
 And the question type could be like some could be computation, calculation.

2799
02:42:59,840 --> 02:43:02,840
 You need to calculate something, right?

2800
02:43:02,840 --> 02:43:03,840
 Some like descriptions.

2801
02:43:03,840 --> 02:43:06,840
 I ask questions and then you describe them.

2802
02:43:06,840 --> 02:43:12,840
 Then you are understanding of the question, right?

2803
02:43:12,840 --> 02:43:15,840
 You are understanding of the concept.

2804
02:43:15,840 --> 02:43:17,840
 So there could be description questions.

2805
02:43:17,840 --> 02:43:21,840
 It could be like a calculation question.

2806
02:43:21,840 --> 02:43:28,840
 And the topic should be those that have not been covered in our assignment.

2807
02:43:28,840 --> 02:43:29,840
 Okay.

2808
02:43:29,840 --> 02:43:36,840
 So later, actually, I think, yeah, maybe next week I will actually email you again about

2809
02:43:36,840 --> 02:43:39,840
 the location of each student.

2810
02:43:39,840 --> 02:43:40,840
 Actually, I will put a tree.

2811
02:43:40,840 --> 02:43:44,840
 And because actually we have three lecture theaters, right?

2812
02:43:44,840 --> 02:43:47,840
 So we'll assign you to different lecture theaters.

2813
02:43:47,840 --> 02:43:51,840
 And you can also write your lecture data and the venue for the quiz.

2814
02:43:51,840 --> 02:43:52,840
 Okay.

2815
02:43:52,840 --> 02:43:53,840
 Okay.

2816
02:43:53,840 --> 02:43:54,840
 Yeah.

2817
02:43:54,840 --> 02:43:55,840
 Thank you.

2818
02:44:17,840 --> 02:44:24,840
 Thank you.

2819
02:44:47,840 --> 02:44:54,840
 Thank you.

2820
02:45:17,840 --> 02:45:27,840
 Thank you.

2821
02:45:47,840 --> 02:45:57,840
 Thank you.

2822
02:46:17,840 --> 02:46:27,840
 Thank you.

2823
02:46:47,840 --> 02:46:57,840
 Thank you.

2824
02:47:17,840 --> 02:47:27,840
 Thank you.

2825
02:47:47,840 --> 02:47:57,840
 Thank you.

2826
02:48:17,840 --> 02:48:27,840
 Thank you.

2827
02:48:47,840 --> 02:48:57,840
 Thank you.

2828
02:49:17,840 --> 02:49:27,840
 Thank you.

2829
02:49:47,840 --> 02:49:57,840
 Thank you.

2830
02:50:17,840 --> 02:50:27,840
 Thank you.

2831
02:50:47,840 --> 02:50:57,840
 Thank you.

2832
02:51:17,840 --> 02:51:27,840
 Thank you.

2833
02:51:47,840 --> 02:51:57,840
 Thank you.

2834
02:52:17,840 --> 02:52:27,840
 Thank you.

2835
02:52:47,840 --> 02:52:57,840
 Thank you.

2836
02:53:17,840 --> 02:53:27,840
 Thank you.

2837
02:53:47,840 --> 02:53:57,840
 Thank you.

2838
02:54:17,840 --> 02:54:27,840
 Thank you.

2839
02:54:47,840 --> 02:54:57,840
 Thank you.

2840
02:55:17,840 --> 02:55:27,840
 Thank you.

2841
02:55:47,840 --> 02:55:57,840
 Thank you.

2842
02:56:17,840 --> 02:56:27,840
 Thank you.

2843
02:56:47,840 --> 02:56:57,840
 Thank you.

2844
02:57:17,840 --> 02:57:27,840
 Thank you.

2845
02:57:47,840 --> 02:57:57,840
 Thank you.

2846
02:58:17,840 --> 02:58:27,840
 Thank you.

2847
02:58:47,840 --> 02:58:57,840
 Thank you.

2848
02:59:17,840 --> 02:59:27,840
 Thank you.

2849
02:59:47,840 --> 02:59:57,840
 Thank you.

