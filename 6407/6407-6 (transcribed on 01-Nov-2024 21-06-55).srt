1
00:00:00,000 --> 00:00:03,520
 具体系的整个道理 提供你的解解等等HE IDEA 体式 펙算

2
00:00:03,680 --> 00:00:04,960
 感觉不会提到一个 不明护性的感觉

3
00:00:04,960 --> 00:00:34,680
 So today is the mid-autumn festival, so I wish I

4
00:01:34,680 --> 00:01:36,680
 would like to introduce you to the festival.

5
00:01:36,680 --> 00:01:42,680
 So here we first can see the one example and here shows two types of fish.

6
00:01:42,680 --> 00:01:50,680
 One type of fish is called sea bass, the left one, and the right one is salmon.

7
00:01:50,680 --> 00:01:57,680
 Of course, for this country we have no problem to identify the two types of fish, right?

8
00:01:57,680 --> 00:02:01,680
 And we can separate them.

9
00:02:01,680 --> 00:02:08,680
 In any manner, once the scenario, that is the Yemf Dachshundah, and there are two types of fish,

10
00:02:08,680 --> 00:02:13,680
 and we want to separate them and then pack them separately.

11
00:02:13,680 --> 00:02:19,680
 And I just mentioned human, of course, there is no problem to separate the two types of fish,

12
00:02:19,680 --> 00:02:23,680
 and of course we can do the separation of the two types of fish manually.

13
00:02:23,680 --> 00:02:29,680
 But here we want to design a machine learning system, two separate types of fish.

14
00:02:29,680 --> 00:02:40,680
 So for all the machine learning systems, of course, firstly we need to collect data, right?

15
00:02:40,680 --> 00:02:44,680
 And then we need to label the data.

16
00:02:44,680 --> 00:02:49,680
 So in the last week, during the data conversion part, we know in machine learning system that

17
00:02:49,680 --> 00:02:57,680
 we need to collect data, and of course we need to collect data.

18
00:02:57,680 --> 00:03:05,680
 And so we can take a lot of pictures of different salmon fish and also sea bass, right?

19
00:03:05,680 --> 00:03:10,680
 And then we need to select the features from the fish.

20
00:03:10,680 --> 00:03:15,680
 And of course, if you just compare the two pictures,

21
00:03:15,680 --> 00:03:21,680
 and we know the sea bass, the left one, actually is smaller than the right one.

22
00:03:21,680 --> 00:03:27,680
 And also the color or the latin is of the fish, which is a lot in a different, okay?

23
00:03:27,680 --> 00:03:30,680
 So at least we can select two features, right?

24
00:03:30,680 --> 00:03:34,680
 One is the size of the fish, another is the latin is of the color of the fish.

25
00:03:34,680 --> 00:03:38,680
 It's a feature to display the two types of features, okay?

26
00:03:38,680 --> 00:03:40,680
 The two types of fish, okay?

27
00:03:40,680 --> 00:03:46,680
 So after we have selected the features, determine what characteristics attribute we want to use,

28
00:03:46,680 --> 00:03:51,680
 and then the next step is to design a kind of classification,

29
00:03:51,680 --> 00:03:56,680
 design a kind of classification or with more kind of classifier, okay?

30
00:03:56,680 --> 00:03:59,680
 And of course, I just mentioned many methods.

31
00:03:59,680 --> 00:04:07,680
 Here we have been studying the step-to-step approach for actually the classification of different objects.

32
00:04:07,680 --> 00:04:12,680
 And the set of approach, let me, we make decisions,

33
00:04:12,680 --> 00:04:19,680
 we make a classification of samples based on the probabilities.

34
00:04:19,680 --> 00:04:21,680
 Probabilities.

35
00:04:21,680 --> 00:04:27,680
 Okay, so today I think we've studied a few concepts relating to probabilities, okay?

36
00:04:27,680 --> 00:04:31,680
 And the first concept would be the power probabilities.

37
00:04:31,680 --> 00:04:37,680
 So here we assume actually the omega-g-node class, okay?

38
00:04:37,680 --> 00:04:44,680
 And omega-1 actually for C-base, omega-2 the class 2, and this for the sample, okay?

39
00:04:44,680 --> 00:04:51,680
 And so the first concept by the probability is called the power probability.

40
00:04:51,680 --> 00:04:55,680
 So for salmon fish, we have power probability of omega-2,

41
00:04:55,680 --> 00:05:00,680
 and for the C-base, we also have power probabilities, okay?

42
00:05:00,680 --> 00:05:03,680
 And if there is no other type of fish, only these two types of fish,

43
00:05:03,680 --> 00:05:11,680
 then the summation of these two power probabilities should be 1, okay?

44
00:05:11,680 --> 00:05:15,680
 Okay, so these are first concept probabilities.

45
00:05:15,680 --> 00:05:19,680
 And there's something like this, I just give one example, right?

46
00:05:19,680 --> 00:05:25,680
 Actually you see in this class actually male students, female students, right?

47
00:05:25,680 --> 00:05:33,680
 And if the female student accounts for 40%, and the male student accounts for 60%, okay?

48
00:05:33,680 --> 00:05:39,680
 And then the power probability for male students is 0.6, or 60%,

49
00:05:39,680 --> 00:05:44,680
 and the female student is 0.4, or 40%.

50
00:05:44,680 --> 00:05:50,680
 So these power probabilities actually can be estimated based on the cost, right?

51
00:05:50,680 --> 00:05:58,680
 And then the percentage of the student male or female in this class will be 0.4.

52
00:05:58,680 --> 00:06:01,680
 So these are power probabilities.

53
00:06:01,680 --> 00:06:06,680
 So if a student, I know some of you are not coming here, they probably are on the way.

54
00:06:06,680 --> 00:06:11,680
 If you ask you to guess, what is the gender of the next coming in student?

55
00:06:11,680 --> 00:06:18,680
 You guess, right? You have not seen his face, you just guess what the next coming in student.

56
00:06:18,680 --> 00:06:20,680
 What gender you are guessing?

57
00:06:20,680 --> 00:06:23,680
 Male, right? One male.

58
00:06:23,680 --> 00:06:26,680
 Actually based on the power probability, right?

59
00:06:26,680 --> 00:06:30,680
 Because the gene, based on the, yeah, that's right.

60
00:06:30,680 --> 00:06:33,680
 The correct question, the prediction of the gender.

61
00:06:33,680 --> 00:06:34,680
 Yes, indeed.

62
00:06:34,680 --> 00:06:41,680
 Because the gene, the gender, the male actually has a larger probability, right?

63
00:06:41,680 --> 00:06:46,680
 So you make the prediction based on this, actually, larger probabilities.

64
00:06:46,680 --> 00:06:51,680
 And then, less likely, actually, we can make a wrong decision.

65
00:06:51,680 --> 00:06:54,680
 So this is the power probability.

66
00:06:54,680 --> 00:07:00,680
 But of course, actually, if you just make a power probability, every time, actually, you will,

67
00:07:00,680 --> 00:07:04,680
 actually, guess the next student is a male student.

68
00:07:04,680 --> 00:07:10,680
 And then you have a 40%, actually, 40% chances to make a wrong decision, right?

69
00:07:10,680 --> 00:07:11,680
 Wrong predictions.

70
00:07:11,680 --> 00:07:15,680
 Actually, that's a very high prediction error.

71
00:07:15,680 --> 00:07:19,680
 Okay, so in practical scenarios, so besides this power probability,

72
00:07:19,680 --> 00:07:21,680
 we need to have some other information, right?

73
00:07:21,680 --> 00:07:27,680
 So the next coming student, maybe you see his face, her face,

74
00:07:27,680 --> 00:07:33,680
 or you know his height or her height, and then you know this information, certainly,

75
00:07:33,680 --> 00:07:38,680
 can help you to make a correct decision, correct predictions.

76
00:07:38,680 --> 00:07:39,680
 Okay.

77
00:07:39,680 --> 00:07:45,680
 So here, actually, if we just make the decision based on the power probability,

78
00:07:45,680 --> 00:07:51,680
 then in power probability of omega-1 from class 1, greater than class 2,

79
00:07:51,680 --> 00:07:54,680
 then we predict omega-1, right?

80
00:07:54,680 --> 00:07:58,680
 But then, actually, we have a high chances, right, to make wrong decisions.

81
00:07:58,680 --> 00:08:05,680
 Because everyone, every, actually, you know, the sample will be classified to the class

82
00:08:05,680 --> 00:08:09,680
 that has higher power probabilities.

83
00:08:09,680 --> 00:08:11,680
 Okay.

84
00:08:11,680 --> 00:08:18,680
 So for this example, so we are not asked to make decisions just for some probability.

85
00:08:18,680 --> 00:08:21,680
 Instead, we will provide some information.

86
00:08:21,680 --> 00:08:22,680
 Okay.

87
00:08:22,680 --> 00:08:27,680
 At least we will provide some information about the size of the fish.

88
00:08:27,680 --> 00:08:29,680
 Okay, so how long it is.

89
00:08:29,680 --> 00:08:36,680
 And then we will also provide some information about the color, the lateness of the fish.

90
00:08:36,680 --> 00:08:37,680
 Okay.

91
00:08:37,680 --> 00:08:45,680
 So if this feature, if we, one of the features, we just denote by the egg, right?

92
00:08:45,680 --> 00:08:53,680
 So based on the eggs, based on the coming in to the eggs, based on the length of the size of the fish,

93
00:08:53,680 --> 00:08:57,680
 based on the lateness of the fish, we will make decisions.

94
00:08:57,680 --> 00:08:58,680
 Okay.

95
00:08:58,680 --> 00:09:02,680
 And actually, when we make decisions, not only we use the probabilities,

96
00:09:02,680 --> 00:09:05,680
 but also we need to use, actually, another probability.

97
00:09:05,680 --> 00:09:11,680
 So that is probably, actually, is called the class conditional probability density function.

98
00:09:11,680 --> 00:09:13,680
 So this is another concept.

99
00:09:13,680 --> 00:09:16,680
 Class conditional probability density function.

100
00:09:16,680 --> 00:09:18,680
 So what is the probability of eggs?

101
00:09:18,680 --> 00:09:20,680
 Given class.

102
00:09:20,680 --> 00:09:21,680
 Okay.

103
00:09:21,680 --> 00:09:23,680
 So given some fish.

104
00:09:23,680 --> 00:09:28,680
 So what is the distribution of the lateness of the color?

105
00:09:28,680 --> 00:09:32,680
 What is the distribution of the size?

106
00:09:32,680 --> 00:09:33,680
 Okay.

107
00:09:33,680 --> 00:09:37,680
 So given, actually, the male student, given this cloud,

108
00:09:37,680 --> 00:09:41,680
 so what is the distribution of the height, right?

109
00:09:41,680 --> 00:09:42,680
 For female student.

110
00:09:42,680 --> 00:09:50,680
 So given male student, so what is the distribution of the height from male student?

111
00:09:50,680 --> 00:09:55,680
 So this is called class conditional probability density function.

112
00:09:55,680 --> 00:09:57,680
 Density function.

113
00:09:57,680 --> 00:10:05,680
 And actually, later we will see, actually, given a sample with known information like lateness,

114
00:10:05,680 --> 00:10:09,680
 or we see the size x, okay.

115
00:10:09,680 --> 00:10:14,680
 And based on this x, we need to calculate, actually, another probability,

116
00:10:14,680 --> 00:10:18,680
 which is called, actually, the posterior probability.

117
00:10:18,680 --> 00:10:28,680
 So here, actually, this diagram just shows, actually, the different distribution of the two,

118
00:10:28,680 --> 00:10:31,680
 these two, actually, class conditional probability density functions, right?

119
00:10:31,680 --> 00:10:36,680
 So for the red one, this shows, actually, the distribution of the x,

120
00:10:36,680 --> 00:10:42,680
 the distribution of the size, the distribution of the lateness of the salmon fish.

121
00:10:42,680 --> 00:10:49,680
 And the black one shows the lateness, obviously, the size of the sea bass.

122
00:10:49,680 --> 00:10:52,680
 So this is called class conditional probability density functions.

123
00:10:52,680 --> 00:11:01,680
 And actually, we need both the information and also the incoming x to make predictions.

124
00:11:01,680 --> 00:11:03,680
 Okay.

125
00:11:03,680 --> 00:11:06,680
 So here, suppose we know these three information, right?

126
00:11:06,680 --> 00:11:09,680
 First, the probability, I just mentioned, right?

127
00:11:09,680 --> 00:11:16,680
 Then the second one, the conditional probability density function.

128
00:11:16,680 --> 00:11:22,680
 Then the third one, actually, is the information of the specific object, right?

129
00:11:22,680 --> 00:11:24,680
 For salmon fish, okay.

130
00:11:24,680 --> 00:11:27,680
 So for fish, we don't know whether it's salmon fish or sea bass.

131
00:11:27,680 --> 00:11:29,680
 Okay, we know the size of the fish.

132
00:11:29,680 --> 00:11:31,680
 This size is due to the x.

133
00:11:31,680 --> 00:11:35,680
 Obviously, we know the lateness, the color of the fish.

134
00:11:35,680 --> 00:11:44,680
 Okay, so based on these three information, we will make predictions of the class of the salmon fish.

135
00:11:44,680 --> 00:11:47,680
 Okay.

136
00:11:47,680 --> 00:11:49,680
 Then, of course, actually, how to make the decision.

137
00:11:49,680 --> 00:11:52,680
 Give me this information, how to predict.

138
00:11:52,680 --> 00:11:54,680
 Okay, so we gave an x.

139
00:11:54,680 --> 00:11:58,680
 Now we have an x because the information is provided, right?

140
00:11:58,680 --> 00:11:59,680
 We see this fish.

141
00:11:59,680 --> 00:12:02,680
 Okay, or the computer sees this fish.

142
00:12:02,680 --> 00:12:04,680
 So we know the information of x.

143
00:12:04,680 --> 00:12:18,680
 Actually, we can predict the probability of this object belonging to a certain class, right?

144
00:12:18,680 --> 00:12:21,680
 And then based on the so-called base theorem.

145
00:12:21,680 --> 00:12:23,680
 Base theorem.

146
00:12:23,680 --> 00:12:24,680
 Okay.

147
00:12:24,680 --> 00:12:31,680
 So, actually, probability of omega j, given x.

148
00:12:31,680 --> 00:12:35,680
 Given x, that means the x is provided is known.

149
00:12:35,680 --> 00:12:40,680
 So what is the probability of this object belonging to class omega j?

150
00:12:40,680 --> 00:12:44,680
 So this is the p of omega j given x.

151
00:12:44,680 --> 00:12:49,680
 The probability of omega j, the probability of class omega j, the probability of salmon fish,

152
00:12:49,680 --> 00:12:55,680
 the probability of sea bass, the length of the size of the fish,

153
00:12:55,680 --> 00:12:59,680
 and given the lateness of the fish.

154
00:12:59,680 --> 00:13:03,680
 Okay, so this is the, this is the, this part, okay.

155
00:13:03,680 --> 00:13:05,680
 So this p of omega j x.

156
00:13:05,680 --> 00:13:11,680
 Okay, and actually based on the base theorem, and this equals the numerator part here,

157
00:13:11,680 --> 00:13:16,680
 actually, is the joint probability of x and the omega j.

158
00:13:16,680 --> 00:13:21,680
 Okay, so the joint probability can be calculated by the probability of x,

159
00:13:21,680 --> 00:13:25,680
 given omega j times the probability of omega j.

160
00:13:25,680 --> 00:13:28,680
 So omega j here is just a p of omega j is a prior probability.

161
00:13:28,680 --> 00:13:33,680
 We just introduce the prior probability of omega j.

162
00:13:33,680 --> 00:13:37,680
 Okay, and then divided by this px.

163
00:13:37,680 --> 00:13:42,680
 This px is called total probability of x.

164
00:13:42,680 --> 00:13:46,680
 And actually this total probability of x can be calculated.

165
00:13:46,680 --> 00:13:48,680
 Actually, you know this formula.

166
00:13:48,680 --> 00:13:52,680
 Actually, this is the summation of two joint probabilities.

167
00:13:52,680 --> 00:13:55,680
 One is the joint probability of x with omega one.

168
00:13:55,680 --> 00:13:58,680
 And that is the joint probability of x with omega two.

169
00:13:58,680 --> 00:14:04,680
 So this summation is the joint, is the total probability of x.

170
00:14:04,680 --> 00:14:07,680
 So this is the base theorem.

171
00:14:07,680 --> 00:14:12,680
 Probably when you are in an integrated, right, to the integrated study, actually,

172
00:14:12,680 --> 00:14:16,680
 you learn this base theorem, but you don't know how to apply this, right.

173
00:14:16,680 --> 00:14:19,680
 But now, actually, even though you apply this base theorem,

174
00:14:19,680 --> 00:14:22,680
 you can apply this base theorem to Michelin.

175
00:14:22,680 --> 00:14:27,680
 To actually make a prediction of the class of the samples.

176
00:14:27,680 --> 00:14:33,680
 So this is p of omega jx is called the posterior probability.

177
00:14:33,680 --> 00:14:38,680
 And actually, we make decisions based on the posterior probabilities,

178
00:14:38,680 --> 00:14:42,680
 rather than the prior probabilities.

179
00:14:42,680 --> 00:14:47,680
 But the posterior probability is calculated based on the prior probability

180
00:14:47,680 --> 00:14:54,680
 and the class of the probability density function.

181
00:14:54,680 --> 00:14:55,680
 OK.

182
00:14:55,680 --> 00:15:05,680
 So this is actually, this is the base theorem.

183
00:15:05,680 --> 00:15:11,680
 So now we assume, actually, just now we already showed a diagram

184
00:15:11,680 --> 00:15:19,680
 about the class of the kind of probability density function for omega one and for omega two.

185
00:15:19,680 --> 00:15:24,680
 Now we assume, actually, the prior probability for omega one is to the,

186
00:15:24,680 --> 00:15:27,680
 the prior probability for omega two is one dot.

187
00:15:27,680 --> 00:15:33,680
 And then, actually, based on the, this prior probability and the class

188
00:15:33,680 --> 00:15:38,680
 kind of probability density function, we can calculate, actually, the posterior probabilities.

189
00:15:39,680 --> 00:15:41,680
 So this is something like that.

190
00:15:41,680 --> 00:15:43,680
 This is the posterior probability.

191
00:15:43,680 --> 00:15:51,680
 And naturally, omega, the red one shows the posterior probability

192
00:15:51,680 --> 00:15:59,680
 and for the class two, omega two, and the black one shows the posterior probability

193
00:15:59,680 --> 00:16:00,680
 for class one.

194
00:16:00,680 --> 00:16:03,680
 The horizontal here is just the x, right?

195
00:16:03,680 --> 00:16:04,680
 x.

196
00:16:04,680 --> 00:16:07,680
 And then the vertical one shows the posterior probability.

197
00:16:07,680 --> 00:16:08,680
 OK.

198
00:16:08,680 --> 00:16:14,680
 And actually, from here you can see, and maybe in the range, right, from nine,

199
00:16:14,680 --> 00:16:21,680
 from nine to around 9.8, 9 to 9.8, actually, the red one, the red, you know,

200
00:16:21,680 --> 00:16:23,680
 is above this black one, right?

201
00:16:23,680 --> 00:16:29,680
 So this means, actually, the probability, the posterior probability of class two is greater

202
00:16:29,680 --> 00:16:34,680
 than posterior probability of class one.

203
00:16:34,680 --> 00:16:42,680
 And similarly, then from in the range, from like 9.8 to around 11.2, maybe, OK.

204
00:16:42,680 --> 00:16:50,680
 And then the posterior probability of class one, given x, is greater than posterior

205
00:16:50,680 --> 00:16:55,680
 probability of omega two, given x.

206
00:16:55,680 --> 00:17:02,680
 So this is posterior probability, which is calculated, actually, based on the prior

207
00:17:02,680 --> 00:17:08,679
 probability and the class kind of probability that is found first.

208
00:17:08,679 --> 00:17:09,679
 OK.

209
00:17:09,679 --> 00:17:10,679
 OK.

210
00:17:10,679 --> 00:17:13,679
 So this is the base posterior rule.

211
00:17:13,679 --> 00:17:14,679
 OK.

212
00:17:14,679 --> 00:17:20,679
 So after we have calculated the posterior probability, then what to make the posterior?

213
00:17:20,679 --> 00:17:22,679
 How to make perceptions.

214
00:17:22,679 --> 00:17:23,679
 OK.

215
00:17:23,679 --> 00:17:25,679
 So this is the base posterior rule.

216
00:17:25,679 --> 00:17:27,679
 We decide omega one.

217
00:17:27,680 --> 00:17:31,680
 We classify the sample to class omega one.

218
00:17:31,680 --> 00:17:38,680
 If the probability, the posterior probability, belonging to class one, is greater than the

219
00:17:38,680 --> 00:17:40,680
 probability belonging to class two.

220
00:17:40,680 --> 00:17:42,680
 So here we, again, we base on the probability, right?

221
00:17:42,680 --> 00:17:44,680
 We say which probability is higher.

222
00:17:44,680 --> 00:17:48,680
 And then we assign the sample to that class.

223
00:17:48,680 --> 00:17:53,680
 Previously, we just used actually the prior probability, right?

224
00:17:53,680 --> 00:18:01,680
 And then the prior probability could have a higher chance of making wrong predictions.

225
00:18:01,680 --> 00:18:09,680
 But actually, the posterior making, based on the posterior probability, could have a higher

226
00:18:09,680 --> 00:18:12,680
 accuracy of that whole mechanism.

227
00:18:12,680 --> 00:18:18,680
 We make decisions based on the posterior probability, rather than prior probabilities.

228
00:18:18,680 --> 00:18:19,680
 OK.

229
00:18:20,680 --> 00:18:24,680
 So this is the base posterior rule.

230
00:18:24,680 --> 00:18:25,680
 OK.

231
00:18:25,680 --> 00:18:29,680
 So I think this rule is reasonable, right?

232
00:18:29,680 --> 00:18:33,680
 Because the probability of making, or the posterior making, based on the probability, of course,

233
00:18:33,680 --> 00:18:36,680
 if you look at which probability is greater than that, right?

234
00:18:36,680 --> 00:18:39,680
 And then we assign the sample to that class.

235
00:18:39,680 --> 00:18:43,680
 So these are very reasonable, and the posterior rule, right?

236
00:18:43,680 --> 00:18:44,680
 It's very intuitive.

237
00:18:44,680 --> 00:18:45,680
 OK.

238
00:18:46,680 --> 00:18:51,680
 And actually, just now we see the base posterior rule, right?

239
00:18:51,680 --> 00:18:59,680
 Actually, when we calculate the base theorem, and in the calculation of the posterior probability,

240
00:18:59,680 --> 00:19:05,680
 and we see in the denominator, denominator, right, is a Px.

241
00:19:05,680 --> 00:19:14,680
 And this Px, actually, is a common factor for both omega one and omega two.

242
00:19:14,680 --> 00:19:21,680
 So in practice, actually, we may not have to calculate this, actually, the Px, because

243
00:19:21,680 --> 00:19:23,680
 this Px is just a skin factor, right?

244
00:19:23,680 --> 00:19:30,680
 So whether or not we have these Px, or without having these Px, actually, this will not affect

245
00:19:30,680 --> 00:19:34,680
 the posterior, because this is just a skin factor.

246
00:19:34,680 --> 00:19:35,680
 OK.

247
00:19:35,680 --> 00:19:37,680
 So now we have such a few worries, right?

248
00:19:37,680 --> 00:19:46,680
 If you don't want to calculate the posterior probability, and if you want to ignore the

249
00:19:46,680 --> 00:19:54,680
 skin factor, right, the Px, we can just basically join the probability to make these series.

250
00:19:54,680 --> 00:19:59,680
 If the Px for omega one, tan P for omega one, this is a numerator part, right?

251
00:19:59,680 --> 00:20:04,680
 So now we see the posterior probability has a numerator part divided by the denominator.

252
00:20:04,680 --> 00:20:09,680
 The denominator is a common denominator for all classes, so we can ignore.

253
00:20:09,680 --> 00:20:12,680
 We just see the numerator part, OK?

254
00:20:12,680 --> 00:20:17,680
 The numerator part, actually, is the joint probability.

255
00:20:17,680 --> 00:20:23,680
 So this is the awareness of the, this awareness, right?

256
00:20:23,680 --> 00:20:30,680
 Actually, originally, we tried to use the posterior probability.

257
00:20:30,680 --> 00:20:35,680
 So now, actually, after analysis, and actually, we know, we can use the joint probability

258
00:20:35,680 --> 00:20:37,680
 to make these series.

259
00:20:37,680 --> 00:20:43,680
 So we don't have to calculate the Px, so we can actually reduce, actually, the computational

260
00:20:43,680 --> 00:20:46,680
 cost, right?

261
00:20:46,680 --> 00:20:49,680
 OK.

262
00:20:49,680 --> 00:20:53,680
 So just now we just actually, we have an x, right?

263
00:20:53,680 --> 00:20:55,680
 This x is a scalar.

264
00:20:55,680 --> 00:20:58,680
 Then in other words, we just use one attribute.

265
00:20:58,680 --> 00:20:59,680
 Just a number, right?

266
00:20:59,680 --> 00:21:01,680
 We just use one attribute.

267
00:21:01,680 --> 00:21:05,680
 So we let small, you know, in the lower case, x, right?

268
00:21:05,680 --> 00:21:10,680
 And so this actually is a one attribute, one feature.

269
00:21:10,680 --> 00:21:12,680
 So this is just for low-fission property.

270
00:21:12,680 --> 00:21:20,680
 When you practice, actually, you know the feature, the number will be much more than one.

271
00:21:20,680 --> 00:21:24,680
 In many applications, the feature will be over 100.

272
00:21:24,680 --> 00:21:26,680
 It will be over 100.

273
00:21:26,680 --> 00:21:32,680
 Actually, in some biomedical applications, like gene, no data analysis, and the demotivation

274
00:21:32,680 --> 00:21:36,680
 will be as high as 40,000.

275
00:21:36,680 --> 00:21:44,680
 So we need to extend, actually, from one attribute, one dimension to multi-dimensions, to multiple

276
00:21:44,680 --> 00:21:46,680
 features, multiple attributes.

277
00:21:46,680 --> 00:21:49,680
 So this is one extension, right?

278
00:21:49,680 --> 00:21:50,680
 One generalization.

279
00:21:50,680 --> 00:21:53,680
 So another is that just now, actually, we just have two classes.

280
00:21:53,680 --> 00:21:56,680
 In the example, we see male student, female student.

281
00:21:56,680 --> 00:21:58,680
 Actually, it's a binary, right?

282
00:21:58,680 --> 00:21:59,680
 Just two classes.

283
00:21:59,680 --> 00:22:02,680
 Now we see the fish, right?

284
00:22:02,680 --> 00:22:04,680
 The sea bass, a sun.

285
00:22:04,680 --> 00:22:06,680
 Just two classes.

286
00:22:06,680 --> 00:22:11,680
 So actually, of course, in many practical applications, actually, the number of classes

287
00:22:11,680 --> 00:22:15,680
 will be much more than two.

288
00:22:16,680 --> 00:22:23,680
 So we can also generalize the number of classes to multiple classes, rather than just two.

289
00:22:23,680 --> 00:22:25,680
 So we can extend this, right?

290
00:22:25,680 --> 00:22:31,680
 We can have this extension or generalization in these two ways.

291
00:22:31,680 --> 00:22:38,680
 So now, we use X to denote the feature vector.

292
00:22:38,680 --> 00:22:44,680
 So the input now is no longer one scalar, actually, it's a vector.

293
00:22:44,680 --> 00:22:51,680
 So not only just actually the size of the fish or the lateness, actually, we use a booth.

294
00:22:51,680 --> 00:22:55,680
 Or even we can use additional information.

295
00:22:55,680 --> 00:23:02,680
 So the number of features or the number of attributes is more than two or more than one.

296
00:23:02,680 --> 00:23:04,680
 So we use X.

297
00:23:04,680 --> 00:23:08,680
 Actually, this is a bone-faced look, is a letter, right?

298
00:23:08,680 --> 00:23:13,680
 Denote, actually, a feature vector.

299
00:23:13,680 --> 00:23:16,680
 And the number of classes here is a C.

300
00:23:16,680 --> 00:23:21,680
 So we have omega one, omega two, and omega C.

301
00:23:21,680 --> 00:23:26,680
 And then how to have the posterior probability?

302
00:23:26,680 --> 00:23:32,680
 So based on the base theorem and the posterior probability of omega z,

303
00:23:32,680 --> 00:23:42,680
 given X equal to the Px given omega z times the probability of P omega z.

304
00:23:42,680 --> 00:23:44,680
 The power probability of P will be omega z.

305
00:23:44,680 --> 00:23:47,680
 Then we divide by the total probability of X.

306
00:23:47,680 --> 00:23:55,680
 So now, actually, you see the Px is a summation of many John's probabilities.

307
00:23:55,680 --> 00:24:01,680
 Actually, j from one to C, rather than from j to one to two, right?

308
00:24:01,680 --> 00:24:08,680
 Because here is a C class, class-linear problem, rather than a two-class class-linear problem.

309
00:24:08,680 --> 00:24:14,680
 So the total probability of X, actually, is a summation of C John's probabilities.

310
00:24:14,680 --> 00:24:19,680
 Probability of X with omega one, with omega two, with omega C.

311
00:24:19,680 --> 00:24:23,680
 Then summation, that will be X, the total probability of X.

312
00:24:23,680 --> 00:24:34,680
 So this is actually the posterior probability of omega z given X.

313
00:24:34,680 --> 00:24:38,680
 So in practice, we always have X, right?

314
00:24:38,680 --> 00:24:42,680
 Because when we make a decision, we must know the information about the object.

315
00:24:42,680 --> 00:24:48,680
 So the information, actually, is contained in the feature vector X.

316
00:24:48,680 --> 00:24:54,680
 So based on this X, what is the probability that the object belongs to omega z?

317
00:24:54,680 --> 00:24:57,680
 So this is called posterior probability.

318
00:25:05,680 --> 00:25:11,680
 And actually, now we introduce another concept, which is called a dissimilar function.

319
00:25:11,680 --> 00:25:15,680
 A dissimilar function is a general concept in my shelenian.

320
00:25:15,680 --> 00:25:23,680
 Actually, it's a general, not just for the base-design theory.

321
00:25:23,680 --> 00:25:25,680
 Base-design theory is a statistical approach.

322
00:25:25,680 --> 00:25:30,680
 Actually, even for other approaches, we have a dissimilar function.

323
00:25:30,680 --> 00:25:36,680
 A dissimilar function is used to separate different classes.

324
00:25:36,680 --> 00:25:47,680
 So in this example, in this statistical approach, the dissimilar function can be just posterior probabilities.

325
00:25:47,680 --> 00:25:51,680
 And then we introduce this concept, right?

326
00:25:51,680 --> 00:25:53,680
 So this is a general concept.

327
00:25:53,680 --> 00:25:59,680
 So for given input, the input is a vector from X1 to Xt.

328
00:25:59,680 --> 00:26:08,680
 And then we calculate c dissimilar functions, g1x, g2x, and gcx.

329
00:26:08,680 --> 00:26:11,680
 Then we get g1x, g2, gcx.

330
00:26:11,680 --> 00:26:18,680
 Then we try to identify which has the maximum value.

331
00:26:18,680 --> 00:26:28,680
 If the gx has the maximum value, then we assign the sample x to class i.

332
00:26:29,680 --> 00:26:35,680
 So this is a general for any type of classification algorithm.

333
00:26:35,680 --> 00:26:37,680
 We use a dissimilar function.

334
00:26:37,680 --> 00:26:41,680
 So based on the dissimilar function, we calculate the current dissimilar function.

335
00:26:41,680 --> 00:26:45,680
 Then we compare the size of the dissimilar function for g1x.

336
00:26:45,680 --> 00:26:48,680
 Then we add the one with the maximum value.

337
00:26:48,680 --> 00:26:51,680
 Then we assign that sample to that class.

338
00:26:52,680 --> 00:26:55,680
 So this is a special scenario.

339
00:26:55,680 --> 00:27:06,680
 And then the gx is just a posterior probability.

340
00:27:06,680 --> 00:27:08,680
 So this is a special case.

341
00:27:08,680 --> 00:27:12,680
 The posterior probability is the dissimilar function.

342
00:27:12,680 --> 00:27:15,680
 We use a dissimilar probability.

343
00:27:16,680 --> 00:27:22,680
 And actually, just now we showed, we know in the calculation of the dissimilar probability.

344
00:27:22,680 --> 00:27:27,680
 And we have a common scaling factor, Px.

345
00:27:27,680 --> 00:27:30,680
 Actually, we can simply ignore this Px.

346
00:27:30,680 --> 00:27:36,680
 Then make decisions or make predictions based on the actually the jump probabilities.

347
00:27:36,680 --> 00:27:43,680
 In other words, actually, this jump probability could also be used as a dissimilar function.

348
00:27:44,680 --> 00:27:50,680
 So this could be like the gx, the numerator part, right?

349
00:27:50,680 --> 00:27:52,680
 In the calculation of the posterior probability.

350
00:27:52,680 --> 00:27:55,680
 This is a jump probability of x and omega j.

351
00:27:55,680 --> 00:28:02,680
 It could be used as actually the dissimilar function.

352
00:28:03,680 --> 00:28:13,680
 Another variant is the log of the jump probability.

353
00:28:13,680 --> 00:28:21,680
 So now it's a multiplication of the class-cannot-probability density function and the probability.

354
00:28:21,680 --> 00:28:22,680
 The multiplication.

355
00:28:22,680 --> 00:28:30,680
 If we take the logarithm operation, and then actually this becomes the logarithm of the class-cannot-probability density function,

356
00:28:31,680 --> 00:28:32,680
 plus, right?

357
00:28:32,680 --> 00:28:41,680
 Then the logarithm of the probability of omega j.

358
00:28:41,680 --> 00:28:46,680
 So actually, in practice, we can use both.

359
00:28:46,680 --> 00:28:56,680
 We can use all the three, the probability, the posterior probability, or the log of the jump probability.

360
00:28:57,680 --> 00:29:02,680
 And actually, we don't actually in practice, we normally just use these two.

361
00:29:02,680 --> 00:29:11,680
 We don't need to calculate the probability of px, the common, actually, scaling factor.

362
00:29:11,680 --> 00:29:32,680
 And actually, for 40p omega j, the probability, already using one example, in this class,

363
00:29:32,680 --> 00:29:39,680
 like 40 percent of the students are female students, 60 percent of the students are male students.

364
00:29:39,680 --> 00:29:46,680
 And then we know the probability for female is 0.4, and the probability for male is 0.6.

365
00:29:46,680 --> 00:29:56,680
 So I think we can easily estimate the probability based on the counts of the number of samples in each class.

366
00:29:56,680 --> 00:29:59,680
 Then we can calculate the percentage, the proportion.

367
00:29:59,680 --> 00:30:01,680
 So that is the probability.

368
00:30:02,680 --> 00:30:10,680
 But here, how to find this class-cannot-probability density, px omega j?

369
00:30:10,680 --> 00:30:15,680
 Because this one, we can easily obtain, right?

370
00:30:15,680 --> 00:30:22,680
 So how to obtain the px omega j, the class-conditional probability density function?

371
00:30:23,680 --> 00:30:26,680
 How to obtain this?

372
00:30:26,680 --> 00:30:29,680
 First, actually, density function, right?

373
00:30:29,680 --> 00:30:33,680
 I think even in the literature, if you know, I think even in the statistics, right,

374
00:30:33,680 --> 00:30:42,680
 when you study probability and statistics, actually, we know there are many type of density functions,

375
00:30:42,680 --> 00:30:44,680
 or probability mass functions.

376
00:30:44,680 --> 00:30:50,680
 And actually, in the last week, we studied the types of features, right?

377
00:30:50,680 --> 00:30:55,680
 The features could be discrete or could be continuous.

378
00:30:55,680 --> 00:31:01,680
 And actually, for continuous, for discrete, we could have very different density functions or mass functions.

379
00:31:01,680 --> 00:31:07,680
 For continuous features, now we can have normal distribution function, right?

380
00:31:07,680 --> 00:31:11,680
 We can assume the data for normal distribution or Gaussian distribution.

381
00:31:11,680 --> 00:31:16,680
 And if the data is discrete, then, of course, we cannot use a normal distribution,

382
00:31:16,680 --> 00:31:19,680
 but we can use other type of distributions, okay?

383
00:31:19,680 --> 00:31:22,680
 For example, we can use the Poisson distribution.

384
00:31:22,680 --> 00:31:25,680
 We can use the binomial distribution.

385
00:31:25,680 --> 00:31:32,680
 Binomial distribution or Poisson distribution are for discrete random variables or discrete features.

386
00:31:32,680 --> 00:31:37,680
 So now, we just focus, we first focus on continuous features.

387
00:31:37,680 --> 00:31:39,680
 Okay, continuous features.

388
00:31:39,680 --> 00:31:45,680
 Of course, continuous features can also follow and can have different type of density functions.

389
00:31:45,680 --> 00:31:48,680
 Some could be like a gamma density function.

390
00:31:48,680 --> 00:31:56,680
 Okay, but in practice, normally, we assume the data follows normal distribution, normal.

391
00:31:56,680 --> 00:32:05,680
 Okay, although, no stress is speaking, maybe the data does not follow a normal density function.

392
00:32:05,680 --> 00:32:12,680
 But we can use, we can assume, then, we use the normal distribution, normal density function.

393
00:32:12,680 --> 00:32:15,680
 And then, the good performance still can be achieved.

394
00:32:16,680 --> 00:32:27,680
 Okay, and later today, we will see if a sample, if the distribution of the data or the density function, density,

395
00:32:27,680 --> 00:32:30,680
 does not follow a normal distribution function.

396
00:32:30,680 --> 00:32:33,680
 But actually, we can use the so-called Gaussian-Mission model.

397
00:32:33,680 --> 00:32:39,680
 We can use multiple Gaussian functions to construct the density function.

398
00:32:39,680 --> 00:32:49,680
 Okay, so in the basic decision rule, the key problem is the construction of the estimation of the class-cognitive-property-density function.

399
00:32:49,680 --> 00:32:51,680
 Class-cognitive-property-density function.

400
00:32:51,680 --> 00:32:53,680
 But here, we simplify the problem.

401
00:32:53,680 --> 00:33:01,680
 We assume, actually, the class-cognitive-property-density function is a Gaussian function or normal function.

402
00:33:01,680 --> 00:33:05,680
 Okay, so we assume this.

403
00:33:06,680 --> 00:33:12,680
 Okay, and then this actually just shows a univariate normal density function.

404
00:33:12,680 --> 00:33:14,680
 Univariate, that means a one-single feature.

405
00:33:14,680 --> 00:33:18,680
 Okay, for a normal density function, we have two parameters, right?

406
00:33:18,680 --> 00:33:24,680
 One is the mu, which is the mean value, and another is the sigma, which is the standard variable.

407
00:33:24,680 --> 00:33:26,680
 The sigma square is the virus.

408
00:33:26,680 --> 00:33:32,680
 Okay, so for a normal-dimensional function, normal-density function, we have two parameters.

409
00:33:32,680 --> 00:33:35,680
 One is mu, and another is sigma.

410
00:33:35,680 --> 00:33:39,680
 Okay, so this is a univariate normal density function.

411
00:33:39,680 --> 00:33:44,680
 Okay, and so this is the definition of the mu, like mean value, right?

412
00:33:44,680 --> 00:33:49,680
 The mean, and so this is the definition of the virus.

413
00:33:49,680 --> 00:33:51,680
 Sigma is called a standard variant.

414
00:33:51,680 --> 00:33:56,680
 Okay, and this diagram just shows, actually, this is a normal distribution, right?

415
00:33:56,680 --> 00:34:01,680
 The mu, the horizontal, actually, is true to the value of x.

416
00:34:01,680 --> 00:34:04,680
 And the vertical shows the probability.

417
00:34:04,680 --> 00:34:11,680
 Probability, and normally the probability of the mean value is the maximum, right?

418
00:34:11,680 --> 00:34:16,680
 And actually, so sigma, what's the mean of mu?

419
00:34:16,680 --> 00:34:20,680
 Mu, what is mu? The mean value, right? Sigma is the standard variant.

420
00:34:20,680 --> 00:34:30,680
 Okay, so actually, either way, from mu minus sigma to mu plus sigma, actually, we have 68% of the data.

421
00:34:30,679 --> 00:34:37,679
 And the integration from mu minus to sigma to mu plus to sigma, we have 95% of the data.

422
00:34:37,679 --> 00:34:40,679
 Okay, just one example, right?

423
00:34:40,679 --> 00:34:47,679
 We see, after the exam, right, for any cost normally for this cost, for example, we see,

424
00:34:47,679 --> 00:34:54,679
 the average of the GPA is 3, the mean GPA for this cost is 3.

425
00:34:55,679 --> 00:35:00,680
 Okay, and then we see a standard variant, which is here from 5, right, here from 5.

426
00:35:00,680 --> 00:35:06,680
 Okay, and then, that means actually, the 68% of the students in this class, actually,

427
00:35:06,680 --> 00:35:10,680
 will have a GPA from 2.5 to 3.5.

428
00:35:10,680 --> 00:35:18,680
 Okay, and 95% of the students will have a GPA either range from actually like 2 and 2.4.

429
00:35:18,680 --> 00:35:25,680
 Right, so this is the mean, right, so this is the normal distribution of mu and sigma, the standard agents.

430
00:35:25,680 --> 00:35:32,680
 Okay, so this is the univariate, just one single feature, one single attribute.

431
00:35:32,680 --> 00:35:38,680
 And for normal functions here, for this one, we just have one parameter, or two parameters.

432
00:35:38,680 --> 00:35:43,680
 One is the mean value, and that is the variance for standard agent.

433
00:35:44,680 --> 00:35:54,680
 Okay, of course, we need to generate multiple dimensions, right, not just one dimension, we have feature vectors.

434
00:35:54,680 --> 00:36:04,680
 So this is the inspiration of the class canary plug density function, right?

435
00:36:04,680 --> 00:36:08,680
 So this is a multivariate normal density function.

436
00:36:08,680 --> 00:36:15,680
 Okay, so in this formula, we have this sigma, remember this sigma means actually, this is a capital letter of sigma,

437
00:36:15,680 --> 00:36:19,680
 just now we use the sigma, right, sigma, we see the standard agent.

438
00:36:19,680 --> 00:36:24,680
 So this is a capital letter of sigma, so it means the covariant matrix.

439
00:36:24,680 --> 00:36:33,680
 So this sigma is a covariant matrix, and this is a mu, a new bone phase, bone phase means a vector, it's a mean vector.

440
00:36:34,680 --> 00:36:41,680
 It no longer is a mean value, it's a mean vector, and it no longer is a standard agent of variance,

441
00:36:41,680 --> 00:36:44,680
 but now it's a covariant matrix, sigma.

442
00:36:44,680 --> 00:36:49,680
 Okay, so this is a capital letter of sigma, right, so this is sigma, okay.

443
00:36:49,680 --> 00:36:57,680
 And so this is sigma, the determinant of sigma, not absolute value, right, because this is a matrix, not absolute value,

444
00:36:57,680 --> 00:37:04,680
 it's a determinant, okay, and then this is the inverse of this covariant matrix, sigma.

445
00:37:04,680 --> 00:37:11,680
 So in this formula, we see one over two pi under the power d half of d,

446
00:37:11,680 --> 00:37:18,680
 and then the determinant, right, the square root of the determinant of sigma.

447
00:37:18,680 --> 00:37:25,680
 Okay, so this is the denominator of the denominator, okay, then it's a connectional x minus mu transpose,

448
00:37:25,680 --> 00:37:31,680
 then times actually the inverse of the covariant matrix times x minus mu.

449
00:37:31,680 --> 00:37:36,680
 Okay, so in this multivariate normal density function, we have two parameters.

450
00:37:36,680 --> 00:37:42,680
 One is the mean vector, mean vector has a denominator of d, right,

451
00:37:42,680 --> 00:37:47,680
 and the other is the sum of the denominator of d, then it also has a denominator of d,

452
00:37:47,680 --> 00:37:53,680
 because mean is just the average of all the vectors, right, so we still get a vector.

453
00:37:53,680 --> 00:38:04,680
 The current matrix is a d by d matrix, but this current matrix normally is a symmetric, right, symmetric.

454
00:38:04,680 --> 00:38:10,680
 So d by d values, then d by d by 2, we use symmetric.

455
00:38:10,680 --> 00:38:16,680
 Okay, so these are the parameter numbers in sigma, okay.

456
00:38:16,680 --> 00:38:21,680
 So this is the multivariate normal density function.

457
00:38:21,680 --> 00:38:29,680
 Okay, so that means that just now we see we want to need the base decision rule,

458
00:38:29,680 --> 00:38:33,680
 and we need to calculate the posterior probability, all the drown probability,

459
00:38:33,680 --> 00:38:39,680
 all the log of the drown probability, but in all of the three, actually, probabilities,

460
00:38:39,680 --> 00:38:43,680
 we need to have the density function, okay.

461
00:38:43,680 --> 00:38:48,680
 We need to have this multivariate normal density function.

462
00:38:48,680 --> 00:38:52,680
 We will assume the data follows normal distribution.

463
00:38:52,680 --> 00:38:59,680
 We can have this, okay, and in other words, we need to have this, but we are not given this.

464
00:38:59,680 --> 00:39:06,680
 Instead, in practice, we are given data, we are given labeled final data, okay.

465
00:39:06,680 --> 00:39:13,680
 And then the issue is how to estimate this class kind of probability density function

466
00:39:13,680 --> 00:39:23,680
 from the given labeled final data.

467
00:39:23,680 --> 00:39:27,680
 So this is a private estimation for the density function, right.

468
00:39:27,680 --> 00:39:31,680
 And actually how to estimate, if we assume the following normal distribution,

469
00:39:31,680 --> 00:39:39,680
 actually we just need to use the label train data to estimate the mean vectors

470
00:39:39,680 --> 00:39:41,680
 and also the parameters, okay.

471
00:39:41,680 --> 00:39:52,680
 Once these two parameters are determined, and then the Gaussian normal density functions are determined, okay.

472
00:39:52,680 --> 00:39:55,680
 And then we can use this density function, right.

473
00:39:55,680 --> 00:39:59,680
 For any given x, we can substitute into this density function

474
00:39:59,680 --> 00:40:06,680
 because actually for mu and sigma, if they are knowing, right, they are estimating from the train data.

475
00:40:06,680 --> 00:40:14,680
 Then for any data, either train you or testing, right, we have x, we can substitute into this formula

476
00:40:14,680 --> 00:40:22,680
 to calculate actually the conditional density function, right, p x given omega.

477
00:40:22,680 --> 00:40:25,680
 And then we multiply the probabilities.

478
00:40:25,680 --> 00:40:31,680
 And based on that, you know, we can make prediction of the class, okay.

479
00:40:31,680 --> 00:40:37,680
 So here we assume the density function is a Gaussian function or normal density function.

480
00:40:37,680 --> 00:40:46,680
 Then the key issue here is to estimate the mean vector and the corollary metric from the train data.

481
00:40:46,680 --> 00:40:49,680
 So this is a key issue, okay.

482
00:40:56,680 --> 00:41:00,680
 So that's how to estimate, how to estimate.

483
00:41:00,680 --> 00:41:02,680
 So this is how to estimate.

484
00:41:10,680 --> 00:41:14,680
 Of course, once they are estimated, we just assume the estimation,

485
00:41:14,680 --> 00:41:18,680
 we consider the estimation as true value, right,

486
00:41:18,680 --> 00:41:27,680
 for given any x, actually we can substitute into a formula to obtain the density function, right, p x given omega.

487
00:41:27,680 --> 00:41:30,680
 But here is how to estimate.

488
00:41:30,680 --> 00:41:37,680
 Probably you already know how to calculate the mean, right, actually the mean, how to calculate the corollary metric.

489
00:41:37,680 --> 00:41:41,680
 But here we learn one method for the parameter estimation.

490
00:41:41,680 --> 00:41:44,680
 This method is commonly used in machine learning.

491
00:41:44,680 --> 00:41:52,680
 And actually this method is called maximum likelihood method, maximum likelihood parameter estimation.

492
00:41:58,680 --> 00:42:04,680
 Suppose actually we have, now we just need to, we have done two extensions, right, or generalization.

493
00:42:04,680 --> 00:42:12,680
 Why is that we generalize from one single attribute or feature to a vector, right, to multiple features?

494
00:42:13,680 --> 00:42:19,680
 And also we generate the problem from a two class problem to a multiple class problem.

495
00:42:19,680 --> 00:42:21,680
 We have C classes.

496
00:42:21,680 --> 00:42:26,680
 So for each of the classes, we have a train of data.

497
00:42:26,680 --> 00:42:34,680
 Because the tree, the data is labeled train of data, right, so you first and then know which example belongs to omega 1,

498
00:42:34,680 --> 00:42:38,680
 which example belongs to omega 2, which example belongs to omega c.

499
00:42:38,680 --> 00:42:47,680
 Okay, so for each of the classes, we have a data set, d1, d2 and dc.

500
00:42:47,680 --> 00:42:51,680
 d1 for class omega 1, d2 for class omega 2.

501
00:42:51,680 --> 00:42:53,680
 We have this set, right.

502
00:42:53,680 --> 00:43:00,680
 And here we also assume that samples are joined independently.

503
00:43:00,680 --> 00:43:03,680
 We have this sample in d1.

504
00:43:03,680 --> 00:43:13,680
 Okay, and the tree, all the samples are joined independently, right,

505
00:43:13,680 --> 00:43:15,680
 joined independently.

506
00:43:15,680 --> 00:43:19,680
 And based on this tree, probably data function px omega j.

507
00:43:19,680 --> 00:43:23,680
 So this is a class kernel density function, right, based on this.

508
00:43:23,680 --> 00:43:24,680
 Okay.

509
00:43:24,680 --> 00:43:30,680
 And also the sample in one class is not affected by the sample in another class.

510
00:43:31,680 --> 00:43:32,680
 Okay.

511
00:43:32,680 --> 00:43:38,680
 In the classification, for example, you choose a tree, you want to get some data, right,

512
00:43:38,680 --> 00:43:44,680
 and you want to build a tree, a model to classify a sample, right, male or female student.

513
00:43:44,680 --> 00:43:50,680
 When you do the data collection for female students, you are not affected by the data collection of male students.

514
00:43:50,680 --> 00:43:55,680
 So the data collection for d1, d2, dn, actually are independent.

515
00:43:55,680 --> 00:43:58,680
 Independent, okay.

516
00:43:58,680 --> 00:44:07,680
 And, okay, so now actually, so for each of the classes, we have one density function, right,

517
00:44:07,680 --> 00:44:14,680
 because for each of the classes, we have one male vector, and we have one, actually, correct matrix.

518
00:44:14,680 --> 00:44:15,680
 Okay.

519
00:44:15,680 --> 00:44:23,680
 And actually the estimation for class 1, actually the same as the, the same as the estimation for class 2,

520
00:44:23,680 --> 00:44:26,680
 for class 3, omega 3, or omega c.

521
00:44:26,680 --> 00:44:27,680
 Okay.

522
00:44:27,680 --> 00:44:32,680
 So actually all of these problems, it can be summarized into one single problem, that is,

523
00:44:32,680 --> 00:44:39,680
 given, like I said, d, given, like I said, d, right, belonging to the same class, okay.

524
00:44:39,680 --> 00:44:44,680
 This d could be d1, could be d2, could be dc, right, okay.

525
00:44:44,680 --> 00:44:50,680
 And all the, actually the sample in this class are drawn independently, independently.

526
00:44:50,680 --> 00:44:51,680
 Okay.

527
00:44:51,680 --> 00:45:01,680
 And then actually from this sample, actually, we want to estimate, we want to estimate the parameter theta.

528
00:45:01,680 --> 00:45:02,680
 Okay.

529
00:45:02,680 --> 00:45:08,680
 And the theta includes, actually, the mu and the, actually, the sigma.

530
00:45:08,680 --> 00:45:10,680
 Okay.

531
00:45:10,680 --> 00:45:16,680
 So the problem is, given, like I said, d, right, d, like I said, d.

532
00:45:16,680 --> 00:45:21,680
 So how to estimate the mean vector and the current matrix?

533
00:45:21,680 --> 00:45:24,680
 So these are our problems.

534
00:45:24,680 --> 00:45:25,680
 Okay.

535
00:45:25,680 --> 00:45:30,680
 My data says how to estimate, actually, the mean vector and the current matrix?

536
00:45:30,680 --> 00:45:32,680
 How to estimate?

537
00:45:32,680 --> 00:45:37,680
 Of course, probably in your mind, you already have the formula, right, to calculate the mean vector,

538
00:45:37,680 --> 00:45:43,680
 which is an average, right, of all the vector, your sum, all the vector, and then you want to have a number of samples.

539
00:45:43,680 --> 00:45:48,680
 And also, for current matrix, you also have the formula in mind to calculate, right.

540
00:45:48,680 --> 00:45:51,680
 But here, actually, we want to learn a general method.

541
00:45:51,680 --> 00:45:57,680
 So how to find, actually, the estimate of the parameters based on the so-called principle,

542
00:45:57,680 --> 00:46:05,680
 principle, makes them line-in-hole method, makes them line-in-hole.

543
00:46:05,680 --> 00:46:06,680
 Okay.

544
00:46:06,680 --> 00:46:09,680
 So, here, focus here is to study this method.

545
00:46:09,680 --> 00:46:10,680
 Okay.

546
00:46:10,680 --> 00:46:16,680
 I suppose d contains, so this sample d, right, d could be d1, d2, and average d, right,

547
00:46:16,680 --> 00:46:20,680
 average d, like I said, but all the data belong to the same class.

548
00:46:20,680 --> 00:46:21,680
 Okay.

549
00:46:21,680 --> 00:46:24,680
 And we have x, y, x2, xn, n samples.

550
00:46:24,680 --> 00:46:25,680
 Okay.

551
00:46:25,680 --> 00:46:32,680
 And for example, of course, actually, no, we have probability to generate, right, we have probability to generate.

552
00:46:32,680 --> 00:46:33,680
 Okay.

553
00:46:33,680 --> 00:46:36,680
 So that is just a density function, right, a density function.

554
00:46:36,680 --> 00:46:41,680
 For given, no, data, and we have probability to generate this.

555
00:46:41,680 --> 00:46:42,680
 Okay.

556
00:46:42,680 --> 00:46:47,680
 And actually, just now we showed the normal density function, right.

557
00:46:47,680 --> 00:46:50,680
 Actually, we know actually for mu, we have sigma, right.

558
00:46:50,680 --> 00:46:52,680
 Actually, different mu and different sigma.

559
00:46:52,680 --> 00:46:59,680
 Even if we have a similar x, and then we can have a different values of p, x, omega, right.

560
00:46:59,680 --> 00:47:03,680
 Because actually, the position in the figure would be different.

561
00:47:03,680 --> 00:47:04,680
 Okay.

562
00:47:04,680 --> 00:47:06,680
 If the mu and sigma are different.

563
00:47:06,680 --> 00:47:07,680
 Okay.

564
00:47:07,680 --> 00:47:10,680
 That means that the probability would be different.

565
00:47:10,680 --> 00:47:11,680
 Okay.

566
00:47:11,680 --> 00:47:18,680
 And so this is the, so for each of our example, like p1, or average sample pk, so this is the

567
00:47:18,680 --> 00:47:21,680
 probability of having this sample in the dataset.

568
00:47:21,680 --> 00:47:26,680
 pxk given theta, given the density function.

569
00:47:26,680 --> 00:47:30,680
 So what is the probability of having this data, sk?

570
00:47:30,680 --> 00:47:34,680
 Now, we have n samples, where sy, s2, and n.

571
00:47:34,680 --> 00:47:40,680
 So the total probability here is just a multiplication of all the probabilities.

572
00:47:40,680 --> 00:47:47,680
 Probability of having sample number one times the probability of sample two times the probability

573
00:47:47,680 --> 00:47:53,680
 of x3, probability until xk, xn.

574
00:47:53,680 --> 00:48:02,680
 So this is the probability of having this dataset, gaining the mu and the sigma.

575
00:48:02,680 --> 00:48:09,680
 And actually for this same set of data, right, and the different mu and the different sigma,

576
00:48:09,680 --> 00:48:11,680
 we will have a different probability.

577
00:48:11,680 --> 00:48:15,680
 Because actually, the same value of x, if you substitute into the formula, different mu,

578
00:48:15,680 --> 00:48:20,680
 different sigma, certainly we will, of course, actually give a different value of p, xk,

579
00:48:20,680 --> 00:48:21,680
 or theta, right.

580
00:48:21,680 --> 00:48:24,680
 So then the multiplication will be different.

581
00:48:24,680 --> 00:48:30,680
 And actually, this is a PED, right, this is the probability of having this given dataset

582
00:48:30,680 --> 00:48:34,680
 and the given, and theta, right, given theta.

583
00:48:34,680 --> 00:48:42,680
 And this actually is called the likelihood of theta with respect to this dataset d.

584
00:48:42,680 --> 00:48:44,680
 This is called likelihood.

585
00:48:44,680 --> 00:48:47,680
 So why we have this dataset?

586
00:48:47,680 --> 00:48:52,680
 This is because this likelihood is the maximum.

587
00:48:52,680 --> 00:49:05,680
 So maximum, so this is the best idea of the maximum likelihood time to estimate it.

588
00:49:05,680 --> 00:49:12,680
 So the maximum likelihood estimate of theta is the value of theta that maximizes this probability.

589
00:49:12,680 --> 00:49:13,680
 Why we have this?

590
00:49:13,680 --> 00:49:20,680
 Because you have the maximum probability, actually, you know, of having this d, probably

591
00:49:20,680 --> 00:49:23,680
 at the maximum, okay.

592
00:49:23,680 --> 00:49:29,680
 So of course, actually, given a different value of theta, we can have a different of this

593
00:49:29,680 --> 00:49:30,680
 PED's theta.

594
00:49:30,680 --> 00:49:38,680
 So we want to find the value of theta so that this probability of PED is the maximum.

595
00:49:38,680 --> 00:49:46,680
 This is the best idea of maximum likelihood parameter estimation.

596
00:49:46,680 --> 00:49:54,680
 Okay, so actually, okay, now I think maybe that's actually we have a, okay, a break.

597
00:49:54,680 --> 00:49:56,680
 We have a break, okay, ten minute break.

598
00:49:56,680 --> 00:50:01,680
 And the other break, actually, we look into the details of the deviation of the estimation

599
00:50:01,680 --> 00:50:07,680
 for parameter mu and sigma.

600
00:50:07,680 --> 00:50:09,680
 Actually, here I play music.

601
00:50:09,680 --> 00:50:11,680
 I think today is a festival, right?

602
00:50:11,680 --> 00:50:12,680
 It's a very special day.

603
00:50:12,680 --> 00:50:16,680
 And the treat is one of my favorite songs, okay.

604
00:50:16,680 --> 00:50:23,680
 And also the singer, actually, a singer, a radio singer, a singer, we play this.

605
00:50:23,680 --> 00:50:25,680
 I'm not very sure what the...

606
00:50:25,680 --> 00:50:30,680
 Let me ask you.

607
00:50:30,680 --> 00:50:32,680
 Can you hear?

608
00:50:32,680 --> 00:50:34,680
 Can you hear?

609
00:50:34,680 --> 00:50:36,680
 One of them.

610
00:50:36,680 --> 00:50:40,680
 Oh, cannot, right?

611
00:50:40,680 --> 00:50:43,680
 I'm still in the middle of this.

612
00:50:43,680 --> 00:50:45,680
 I'm still in the middle of this.

613
00:50:45,680 --> 00:50:48,680
 Hey, cannot, right?

614
00:50:48,680 --> 00:50:49,680
 See?

615
00:50:49,680 --> 00:50:50,680
 Okay.

616
00:50:50,680 --> 00:50:52,680
 Okay.

617
00:51:20,680 --> 00:51:21,680
 Okay.

618
00:51:39,680 --> 00:51:43,680
 This doesn't mean that we can't put together.

619
00:51:43,680 --> 00:51:46,680
 These are parameters, parameters.

620
00:51:46,680 --> 00:51:48,680
 These things are parameters of mu and sigma.

621
00:51:48,680 --> 00:51:52,680
 All the conditions, all the parameters.

622
00:51:52,680 --> 00:52:01,680
 So it means, given this data set, we try different mu and sigma.

623
00:52:01,680 --> 00:52:12,680
 This combination of mu and sigma, so that's maximum.

624
00:52:12,680 --> 00:52:13,680
 That's fine.

625
00:52:13,680 --> 00:52:14,680
 Yeah.

626
00:52:18,680 --> 00:52:19,680
 Okay.

627
00:52:48,680 --> 00:52:49,680
 Okay.

628
00:53:18,680 --> 00:53:19,680
 Okay.

629
00:53:48,680 --> 00:53:49,680
 Okay.

630
00:54:18,680 --> 00:54:40,680
 I can't figure where to get that.

631
00:54:40,680 --> 00:54:47,080
 probably there's no wine another probably no air actually the market here right

632
00:54:47,080 --> 00:54:51,080
 like follow about another wine right

633
00:54:51,080 --> 00:54:57,080
 all the sample right probably of wine sample

634
00:54:57,080 --> 00:55:02,279
 then probably another sample right just a few probably he'll go to the first step

635
00:55:02,279 --> 00:55:05,879
 and then again the wine sample

636
00:55:06,880 --> 00:55:11,880
 it's a wine event

637
00:55:11,880 --> 00:55:13,880
 consider many sample

638
00:55:23,880 --> 00:55:27,880
 if a submission is a property of X1 or X2 or X3

639
00:55:27,880 --> 00:55:29,880
 not X1 to X2

640
00:55:29,880 --> 00:55:33,880
 I'm going to have a concept is under

641
00:55:59,880 --> 00:56:28,240
 P

642
00:56:59,880 --> 00:57:01,880
 I'm not sure if you can hear me.

643
00:57:01,880 --> 00:57:02,880
 I'm not sure.

644
00:57:02,880 --> 00:57:03,880
 I'm not sure.

645
00:57:03,880 --> 00:57:04,880
 I'm not sure.

646
00:57:04,880 --> 00:57:05,880
 I'm not sure.

647
00:57:05,880 --> 00:57:06,880
 I'm not sure.

648
00:57:06,880 --> 00:57:07,880
 I'm not sure.

649
00:57:07,880 --> 00:57:08,880
 I'm not sure.

650
00:57:08,880 --> 00:57:09,880
 I'm not sure.

651
00:57:09,880 --> 00:57:10,880
 I'm not sure.

652
00:57:10,880 --> 00:57:11,880
 I'm not sure.

653
00:57:11,880 --> 00:57:12,880
 I'm not sure.

654
00:57:12,880 --> 00:57:13,880
 I'm not sure.

655
00:57:13,880 --> 00:57:14,880
 I'm not sure.

656
00:57:14,880 --> 00:57:15,880
 I'm not sure.

657
00:57:15,880 --> 00:57:16,880
 I'm not sure.

658
00:57:16,880 --> 00:57:17,880
 I'm not sure.

659
00:57:17,880 --> 00:57:18,880
 I'm not sure.

660
00:57:18,880 --> 00:57:19,880
 I'm not sure.

661
00:57:19,880 --> 00:57:20,880
 I'm not sure.

662
00:57:20,880 --> 00:57:21,880
 I'm not sure.

663
00:57:21,880 --> 00:57:22,880
 I'm not sure.

664
00:57:22,880 --> 00:57:23,880
 I'm not sure.

665
00:57:23,880 --> 00:57:24,880
 I'm not sure.

666
00:57:24,880 --> 00:57:25,880
 I'm not sure.

667
00:57:25,880 --> 00:57:26,880
 I'm not sure.

668
00:57:26,880 --> 00:57:27,880
 I'm not sure.

669
00:57:27,880 --> 00:57:28,880
 I'm not sure.

670
00:57:29,880 --> 00:57:30,880
 I'm not sure.

671
00:57:52,880 --> 00:57:53,880
 I'm not sure.

672
00:57:53,880 --> 00:57:54,880
 I'm not sure.

673
00:57:55,880 --> 00:57:57,880
 I can't read the links properly.

674
00:58:01,880 --> 00:58:02,880
 He's embarrassed.

675
00:58:02,880 --> 00:58:03,880
 I'm not sure.

676
00:58:04,880 --> 00:58:05,880
 So embarrassed.

677
00:58:07,880 --> 00:58:09,880
 I can't read fertilistic 24 hours.

678
00:58:11,880 --> 00:58:14,880
 The trick is I didn't have a penlap I can see it well.

679
00:58:14,880 --> 00:58:15,880
 That's why I'm not sure.

680
00:58:18,880 --> 00:58:19,880
 I'm not sure.

681
00:58:20,880 --> 00:58:21,880
 I'm not sure.

682
00:58:21,880 --> 00:58:22,880
 I'm not sure.

683
00:58:22,880 --> 00:58:24,880
 This is the first time I've seen a class like this.

684
00:58:24,880 --> 00:58:26,880
 This is the first time I've seen a class like this.

685
00:58:26,880 --> 00:58:28,880
 This is the first time I've seen a class like this.

686
00:58:28,880 --> 00:58:30,880
 This is the first time I've seen a class like this.

687
00:58:30,880 --> 00:58:32,880
 This is the first time I've seen a class like this.

688
00:58:32,880 --> 00:58:34,880
 This is the first time I've seen a class like this.

689
00:58:34,880 --> 00:58:36,880
 This is the first time I've seen a class like this.

690
00:58:36,880 --> 00:58:38,880
 This is the first time I've seen a class like this.

691
00:58:38,880 --> 00:58:40,880
 This is the first time I've seen a class like this.

692
00:58:40,880 --> 00:58:42,880
 This is the first time I've seen a class like this.

693
00:58:42,880 --> 00:58:44,880
 This is the first time I've seen a class like this.

694
00:58:44,880 --> 00:58:46,880
 This is the first time I've seen a class like this.

695
00:58:46,880 --> 00:58:48,880
 This is the first time I've seen a class like this.

696
00:58:48,880 --> 00:58:50,880
 This is the first time I've seen a class like this.

697
00:58:50,880 --> 00:58:52,880
 This is the first time I've seen a class like this.

698
00:58:52,880 --> 00:58:54,880
 This has aоР equation for seventeen units.

699
00:58:54,880 --> 00:58:56,880
 This has a Luiza equation for seventeen units.

700
00:59:02,880 --> 00:59:04,880
 This has a 가지고 equation for seventeen units.

701
00:59:04,880 --> 00:59:06,880
 This has a ox Pokemon,

702
00:59:06,880 --> 00:59:08,880
 This has a llama Parker,

703
00:59:11,880 --> 00:59:13,880
 This has a Tenemosba formula forix too.

704
00:59:13,880 --> 00:59:15,880
 This has the им中 fifth form.

705
00:59:15,880 --> 00:59:17,880
 This has the X, one isakwash,

706
00:59:17,880 --> 00:59:19,880
 This has two configure isokwash,

707
00:59:19,880 --> 00:59:21,880
 Okay, okay, a sample.

708
00:59:21,880 --> 00:59:23,880
 Hmm.

709
00:59:23,880 --> 00:59:25,880
 Okay, a data sample.

710
00:59:25,880 --> 00:59:27,880
 Thank you.

711
00:59:27,880 --> 00:59:29,880
 Okay.

712
00:59:29,880 --> 00:59:31,880
 Hmm.

713
00:59:31,880 --> 00:59:33,880
 Okay.

714
00:59:33,880 --> 00:59:35,880
 Hmm.

715
00:59:35,880 --> 00:59:37,880
 Sure.

716
00:59:37,880 --> 00:59:39,880
 Hmm.

717
00:59:39,880 --> 00:59:41,880
 Hmm.

718
00:59:41,880 --> 00:59:43,880
 Hmm.

719
00:59:43,880 --> 00:59:45,880
 Hmm.

720
00:59:45,880 --> 00:59:47,880
 Hmm.

721
00:59:47,880 --> 00:59:49,880
 Hmm.

722
00:59:49,880 --> 00:59:51,880
 Hmm.

723
00:59:51,880 --> 00:59:53,880
 Hmm.

724
00:59:53,880 --> 00:59:55,880
 Hmm.

725
00:59:55,880 --> 00:59:57,880
 Hmm.

726
00:59:57,880 --> 00:59:59,880
 Hmm.

727
00:59:59,880 --> 01:00:01,880
 Hmm.

728
01:00:01,880 --> 01:00:03,880
 Hmm.

729
01:00:03,880 --> 01:00:05,880
 Hmm.

730
01:00:05,880 --> 01:00:07,880
 Ok, hmm.

731
01:00:07,880 --> 01:00:11,880
 OK, so this is a probability of having D data set D, right?

732
01:00:11,880 --> 01:00:13,880
 So the X, Y are the samples.

733
01:00:13,880 --> 01:00:15,880
 Not between each X,

734
01:00:15,880 --> 01:00:21,240
 is a vector, is a feature, is a sample.

735
01:00:21,240 --> 01:00:23,520
 Contain the values of each feature.

736
01:00:23,520 --> 01:00:24,960
 So these are samples.

737
01:00:24,960 --> 01:00:29,600
 And so the probability of having the data set D

738
01:00:29,600 --> 01:00:33,320
 actually is the multiplication of the probability of having x1,

739
01:00:33,320 --> 01:00:37,760
 x2, x, and until x1.

740
01:00:37,760 --> 01:00:39,120
 One multiplication.

741
01:00:39,120 --> 01:00:43,160
 Because this is under, we have this probability of having x1,

742
01:00:43,160 --> 01:00:47,040
 and x2, and x3, and and xc.

743
01:00:47,040 --> 01:00:51,200
 And the x1, x2, x, and until xn are independent.

744
01:00:51,200 --> 01:00:56,200
 So we can have this formula to calculate the probabilities.

745
01:00:56,200 --> 01:00:57,279
 Then this is a tree.

746
01:00:57,279 --> 01:01:01,440
 So this is called the likelihood of theta

747
01:01:01,440 --> 01:01:06,960
 with respect to the data set D. Why we have the data set here?

748
01:01:06,960 --> 01:01:12,440
 Because actually the probability, the likelihood is maximum.

749
01:01:12,440 --> 01:01:15,160
 So this is a basic idea of maximum likelihood parameter

750
01:01:15,160 --> 01:01:16,560
 estimation.

751
01:01:16,560 --> 01:01:22,080
 We want to estimate the parameters of mu and the sigma.

752
01:01:22,080 --> 01:01:27,560
 That's the maximized and this is P d theta.

753
01:01:27,560 --> 01:01:31,680
 For different theta and the mu and for the same x,

754
01:01:31,680 --> 01:01:34,200
 actually we can have different values of P, right?

755
01:01:34,200 --> 01:01:36,320
 The P probabilities.

756
01:01:36,320 --> 01:01:39,360
 We want to find the mu and theta.

757
01:01:39,360 --> 01:01:43,960
 That the probability P d theta is the maximum.

758
01:01:47,400 --> 01:01:50,280
 So now, because here is a multiplication, right?

759
01:01:50,280 --> 01:01:51,520
 Multiplication.

760
01:01:51,520 --> 01:01:57,560
 And then later we want to find the deviation of the loss function

761
01:01:57,560 --> 01:01:58,920
 with respect to the parameter.

762
01:01:58,920 --> 01:02:01,280
 It could be very complicated.

763
01:02:01,280 --> 01:02:05,000
 So we convert the multiplication into summation

764
01:02:05,000 --> 01:02:08,960
 by taking the logarithm operation.

765
01:02:09,000 --> 01:02:12,600
 So here, of course, we use the so-called Neckar logarithm,

766
01:02:12,600 --> 01:02:13,160
 right?

767
01:02:13,160 --> 01:02:14,640
 Lawing.

768
01:02:14,640 --> 01:02:18,560
 So we define this loss function as the lawing,

769
01:02:18,560 --> 01:02:21,920
 the logarithm of P d theta.

770
01:02:21,920 --> 01:02:27,440
 So previously, it's a multiplication of the n probabilities,

771
01:02:27,440 --> 01:02:27,920
 right?

772
01:02:27,920 --> 01:02:33,040
 But now, it becomes a summation of n probabilities.

773
01:02:33,040 --> 01:02:36,360
 So this is a summation from k to 1,

774
01:02:36,360 --> 01:02:45,000
 the logarithm of P x k and theta, k from 1 to n.

775
01:02:45,000 --> 01:02:46,800
 So this is a loss function.

776
01:02:46,800 --> 01:02:50,120
 And that's actually we try to find the value of theta,

777
01:02:50,120 --> 01:02:53,960
 including mu and the sigma, right?

778
01:02:53,960 --> 01:02:59,400
 So that this loss function, L theta, is maximized.

779
01:02:59,400 --> 01:03:01,600
 I think how to perform omission.

780
01:03:01,600 --> 01:03:03,680
 I think your university, Yale 1, already

781
01:03:03,680 --> 01:03:06,040
 studied this, actually, you already

782
01:03:06,040 --> 01:03:08,240
 know how to perform omission, right?

783
01:03:08,240 --> 01:03:11,440
 Actually, at least we know the first order derivative

784
01:03:11,440 --> 01:03:14,480
 of actually the loss function, object function,

785
01:03:14,480 --> 01:03:19,520
 with respect to the parameter should be 0, right?

786
01:03:19,520 --> 01:03:21,200
 OK.

787
01:03:21,200 --> 01:03:26,960
 So this we are going to actually solve this problem.

788
01:03:26,960 --> 01:03:29,960
 And so this theta is the one that makes

789
01:03:29,960 --> 01:03:33,640
 the maximum the loss function f theta.

790
01:03:33,640 --> 01:03:39,320
 And actually, initially, we want to maximize the probability,

791
01:03:39,320 --> 01:03:39,920
 right?

792
01:03:39,920 --> 01:03:41,040
 The probability.

793
01:03:41,040 --> 01:03:45,800
 But later, we want to maximize the log of the probabilities.

794
01:03:45,800 --> 01:03:47,360
 And because of the log operation,

795
01:03:47,360 --> 01:03:48,800
 we are converting the multiplication

796
01:03:48,800 --> 01:03:51,400
 into summation, right?

797
01:03:51,400 --> 01:03:53,240
 But actually, when we have the conversion,

798
01:03:53,240 --> 01:03:56,040
 whether the solution we find the same,

799
01:03:56,040 --> 01:03:58,440
 we are also maximizing the original P.

800
01:03:58,440 --> 01:04:03,120
 The y maximizing L, whether maximizing the P,

801
01:04:03,120 --> 01:04:04,520
 the probability.

802
01:04:04,520 --> 01:04:06,200
 And actually, the logarithm operation

803
01:04:06,200 --> 01:04:09,720
 is a monotonic function, right?

804
01:04:09,720 --> 01:04:12,840
 Actually, monotonic increasing or decreasing.

805
01:04:12,840 --> 01:04:17,560
 So the function, the theta, the value theta and sigma,

806
01:04:17,560 --> 01:04:21,280
 that maximizes the long likelihood function.

807
01:04:21,280 --> 01:04:26,880
 We also maximize the likelihood function.

808
01:04:27,360 --> 01:04:33,520
 So there are no problems to take this L theta,

809
01:04:33,520 --> 01:04:39,720
 the logarithm of the likelihood as the objective function

810
01:04:39,720 --> 01:04:40,520
 to optimize.

811
01:04:46,560 --> 01:04:50,440
 So the necessary condition for the maximum likelihood estimate

812
01:04:50,440 --> 01:04:52,320
 of theta is this.

813
01:04:52,320 --> 01:04:56,840
 The partial derivative of L with respect to theta is 0.

814
01:04:56,880 --> 01:04:58,320
 So this is the condition, right?

815
01:04:58,320 --> 01:05:03,080
 Actually, the partial derivative of the log function

816
01:05:03,080 --> 01:05:05,480
 or the coefficient function or the objective function

817
01:05:05,480 --> 01:05:08,080
 with respect to the parameter v0.

818
01:05:08,080 --> 01:05:12,240
 So this is the necessary condition for optimal value, right?

819
01:05:12,240 --> 01:05:14,560
 This is the necessary condition.

820
01:05:14,560 --> 01:05:19,280
 So there's a trivial, I'll try to find the truth.

821
01:05:19,280 --> 01:05:26,800
 What is the derivative of the long likelihood function?

822
01:05:26,800 --> 01:05:28,760
 With respect to the parameter v0.

823
01:05:28,760 --> 01:05:31,760
 Actually, the theta here includes two parts, right?

824
01:05:31,760 --> 01:05:35,960
 Just one is the mu, another is sigma, the mean vector

825
01:05:35,960 --> 01:05:38,640
 and the current matrix.

826
01:05:38,640 --> 01:05:40,840
 So here, we first consider the first scenario.

827
01:05:40,840 --> 01:05:44,200
 The Gaugrin function with an unknown mu, with an unknown mu,

828
01:05:44,200 --> 01:05:46,400
 we assume that the sigma here is known.

829
01:05:46,400 --> 01:05:46,880
 It's known.

830
01:05:46,880 --> 01:05:48,800
 It's constant, OK?

831
01:05:48,800 --> 01:05:52,600
 So let's treat with how to maximize.

832
01:05:52,600 --> 01:05:55,800
 So here, from this formula, we see this is the summation,

833
01:05:56,800 --> 01:05:58,520
 from k to 1.

834
01:05:58,520 --> 01:06:03,280
 Then the partial derivative of the log function,

835
01:06:03,280 --> 01:06:09,440
 p x k with respect to theta.

836
01:06:09,440 --> 01:06:11,920
 So theta here, we assume just mu.

837
01:06:11,920 --> 01:06:15,400
 We assume the sigma is known.

838
01:06:15,400 --> 01:06:17,440
 We want to find the mu.

839
01:06:17,440 --> 01:06:19,280
 So then, we treat the function here

840
01:06:19,280 --> 01:06:24,800
 becoming the p x k and the unknown parameter mu.

841
01:06:24,840 --> 01:06:29,320
 We just consider mu as an unknown parameter to estimate.

842
01:06:29,320 --> 01:06:31,040
 And then, so this is the function.

843
01:06:31,040 --> 01:06:32,040
 We still have sigma.

844
01:06:32,040 --> 01:06:34,600
 Here, we assume the sigma, the current matrix is known.

845
01:06:37,720 --> 01:06:40,800
 And then, so this is the log, right?

846
01:06:40,800 --> 01:06:44,840
 This is the log of operation and log probability.

847
01:06:44,840 --> 01:06:45,880
 So this is the log, right?

848
01:06:45,880 --> 01:06:48,520
 So this part, actually, this is not a function mu, right?

849
01:06:48,520 --> 01:06:49,800
 It's not a function mu.

850
01:06:49,800 --> 01:06:52,040
 We assume these are constant.

851
01:06:52,040 --> 01:06:56,240
 And then, only this part is a function mu.

852
01:06:56,240 --> 01:06:59,320
 So we want to find the partial derivative of this log

853
01:06:59,320 --> 01:07:00,080
 probability, right?

854
01:07:00,080 --> 01:07:05,520
 We just look at actually the partial derivative of this part.

855
01:07:05,520 --> 01:07:08,000
 So this part, actually, leads to the derivative

856
01:07:08,000 --> 01:07:11,759
 or differential action of a scalar with a vector.

857
01:07:11,759 --> 01:07:13,920
 Actually, this is a scalar, right?

858
01:07:13,920 --> 01:07:15,720
 This is a scalar.

859
01:07:15,720 --> 01:07:18,800
 So how to find the partial derivative?

860
01:07:18,800 --> 01:07:20,800
 How to differentiate?

861
01:07:21,000 --> 01:07:24,080
 Sometimes, different notion of a scalar variable

862
01:07:24,080 --> 01:07:26,520
 with respect to a vector.

863
01:07:26,520 --> 01:07:30,480
 I think you started this in the linear algebra course, right?

864
01:07:30,480 --> 01:07:33,720
 So I think this is very straightforward.

865
01:07:33,720 --> 01:07:36,520
 So we can kind of fix this part.

866
01:07:36,520 --> 01:07:39,560
 Then we look at the derivative with the mu, right?

867
01:07:39,560 --> 01:07:40,600
 Then we fix this part.

868
01:07:40,600 --> 01:07:44,040
 We look at the derivative of this part with respect to mu.

869
01:07:44,040 --> 01:07:46,160
 And then, actually, this is the mu.

870
01:07:46,160 --> 01:07:48,920
 This is the partial derivative of the log.

871
01:07:48,920 --> 01:07:56,040
 Actually, the log of p xk mu with respect to mu.

872
01:07:56,040 --> 01:08:01,480
 So this is just the k.

873
01:08:01,480 --> 01:08:04,200
 So actually, so k, right?

874
01:08:04,200 --> 01:08:06,080
 k is from 1 to n.

875
01:08:06,080 --> 01:08:07,520
 So we get the summation.

876
01:08:07,520 --> 01:08:13,520
 So this is the partial derivative of the log's function,

877
01:08:13,520 --> 01:08:18,720
 the log likelihood function with respect to the mu and mu.

878
01:08:19,880 --> 01:08:23,520
 And based on the analysis done now,

879
01:08:23,520 --> 01:08:25,520
 and this actually, first of all, should be 0.

880
01:08:25,520 --> 01:08:28,160
 This is a natural condition for optimal solution.

881
01:08:28,160 --> 01:08:29,920
 This should be 0.

882
01:08:29,920 --> 01:08:31,840
 It should be 0.

883
01:08:31,840 --> 01:08:34,840
 And then, actually, we let it 0, right?

884
01:08:34,840 --> 01:08:37,880
 So this sigma inverse, actually, is a common factor, right?

885
01:08:37,880 --> 01:08:40,920
 We can actually move out.

886
01:08:40,920 --> 01:08:45,000
 And then we multiply, actually, the current metric.

887
01:08:45,000 --> 01:08:47,040
 Then this will become a tree, you know?

888
01:08:47,800 --> 01:08:49,279
 Just like cancel, right?

889
01:08:49,279 --> 01:08:51,880
 So from here, from this equation,

890
01:08:51,880 --> 01:08:56,439
 you can either obtain this is the estimation of the mu.

891
01:08:56,439 --> 01:08:58,720
 And actually, just average, probably already

892
01:08:58,720 --> 01:09:00,040
 know the result, right?

893
01:09:00,040 --> 01:09:04,760
 But actually, I want to show you the method

894
01:09:04,760 --> 01:09:07,640
 for parameter estimation, that is maximum log likelihood

895
01:09:07,640 --> 01:09:08,840
 parameter estimation.

896
01:09:08,840 --> 01:09:11,640
 So this is a very commonly used method, actually,

897
01:09:11,640 --> 01:09:12,960
 in machine learning.

898
01:09:12,960 --> 01:09:16,560
 Maximum log likelihood parameter estimation.

899
01:09:16,600 --> 01:09:18,760
 So through this preset, right?

900
01:09:18,760 --> 01:09:25,160
 So first, actually, we construct the likelihood function.

901
01:09:25,160 --> 01:09:27,160
 And then, actually, for convenience,

902
01:09:27,160 --> 01:09:29,920
 we use the log of the likelihood function

903
01:09:29,920 --> 01:09:32,680
 as the object function to optimize.

904
01:09:32,680 --> 01:09:34,800
 So finally, through the optimization

905
01:09:34,800 --> 01:09:37,480
 of the natural condition, natural condition,

906
01:09:37,480 --> 01:09:40,200
 the frontal derivative of the cost function

907
01:09:40,200 --> 01:09:42,280
 with respect to the coefficient parameter

908
01:09:42,280 --> 01:09:43,280
 should be 0.

909
01:09:43,280 --> 01:09:45,880
 So based on this condition, we finally

910
01:09:45,920 --> 01:09:50,319
 obtain the estimate of the mean vector.

911
01:09:50,319 --> 01:09:54,040
 So this is a cap, where mu cap means the estimate

912
01:09:54,040 --> 01:09:55,640
 of the mean vector.

913
01:09:55,640 --> 01:10:02,160
 So it's just the average of the n vectors, the average

914
01:10:02,160 --> 01:10:03,640
 of the n samples.

915
01:10:03,640 --> 01:10:05,360
 So this is the mean, right?

916
01:10:05,360 --> 01:10:08,440
 This is the mean vector.

917
01:10:08,440 --> 01:10:14,040
 So this is the case when mu is unknown, but sigma is known.

918
01:10:14,040 --> 01:10:18,160
 And the necessary is this, when the sigma and the mu are

919
01:10:18,160 --> 01:10:19,080
 unknown.

920
01:10:19,080 --> 01:10:21,360
 And also, through similar process,

921
01:10:21,360 --> 01:10:26,200
 finally, we can get an estimate of the current matrix.

922
01:10:29,080 --> 01:10:31,320
 I probably already know the result.

923
01:10:31,320 --> 01:10:38,760
 But I think one thing I want to mention is that here is a 1 over n.

924
01:10:38,760 --> 01:10:42,240
 For current matrix estimation, how often

925
01:10:42,280 --> 01:10:45,120
 you see 1 over n minus 1?

926
01:10:45,120 --> 01:10:49,040
 But actually, this is called maximum likelihood

927
01:10:49,040 --> 01:10:51,840
 estimate of the current matrix.

928
01:10:51,840 --> 01:10:55,120
 If 1 over n minus 1, that is called

929
01:10:55,120 --> 01:10:59,040
 unbiased estimate of the current matrix.

930
01:10:59,040 --> 01:11:01,840
 Unbiased.

931
01:11:01,840 --> 01:11:03,840
 So this is a big difference, right?

932
01:11:03,840 --> 01:11:07,000
 Of course, nowadays normally we use data around large data,

933
01:11:07,000 --> 01:11:08,960
 at least 100,000.

934
01:11:08,960 --> 01:11:16,720
 So 1 over n minus 1 or 1 over n does not make much difference.

935
01:11:16,720 --> 01:11:20,440
 Because the 3n could be either 1,000, 10,000.

936
01:11:20,440 --> 01:11:24,600
 So 1 over n or 1 over n minus 1 does not make much difference.

937
01:11:24,600 --> 01:11:27,360
 But actually, of course, in theory,

938
01:11:27,360 --> 01:11:30,400
 we see why it's called actually the maximum likelihood

939
01:11:30,400 --> 01:11:33,080
 parameter estimate.

940
01:11:33,080 --> 01:11:37,520
 Another is unbiased estimate.

941
01:11:37,520 --> 01:11:40,680
 So this is the estimation for the mean vector

942
01:11:40,680 --> 01:11:41,840
 and the current matrix.

943
01:11:41,840 --> 01:11:43,920
 So gain-metronial samples, I think

944
01:11:43,920 --> 01:11:47,280
 this estimation of the mean vector and the current

945
01:11:47,280 --> 01:11:49,800
 matrix are quite straightforward, right?

946
01:11:49,800 --> 01:11:53,080
 So one of these two parameters are determined.

947
01:11:53,080 --> 01:12:00,640
 And then the conditional density function is determined.

948
01:12:00,640 --> 01:12:03,120
 Because we assume density function

949
01:12:03,120 --> 01:12:07,920
 is a Gaussian function, which just has two parameters.

950
01:12:07,920 --> 01:12:10,519
 One is mu, the mean vector.

951
01:12:10,519 --> 01:12:12,599
 Another is the current matrix.

952
01:12:12,599 --> 01:12:15,519
 So now these two parameters are estimated.

953
01:12:15,519 --> 01:12:19,840
 And then actually the density function is determined.

954
01:12:19,840 --> 01:12:22,920
 And then we can use the jump probability, right,

955
01:12:22,920 --> 01:12:25,480
 of the x and omega, which is just

956
01:12:25,480 --> 01:12:29,320
 the model product of the density function

957
01:12:29,320 --> 01:12:31,800
 with the probability.

958
01:12:31,800 --> 01:12:33,880
 Use this as a distribution function

959
01:12:33,880 --> 01:12:38,680
 to perform classification of the samples.

960
01:12:38,680 --> 01:12:46,000
 So I think this is the basic theory, basic decision theory.

961
01:12:46,000 --> 01:12:51,400
 And the core issue is the estimate of the density function.

962
01:12:51,400 --> 01:12:55,280
 But you will assume the sample for a normal distribution.

963
01:12:55,280 --> 01:12:59,200
 And then this becomes a problem is reduced

964
01:12:59,200 --> 01:13:04,920
 to the estimate the current matrix and the mean vector.

965
01:13:04,920 --> 01:13:08,080
 Although in practice, the sample may not

966
01:13:08,080 --> 01:13:10,519
 necessarily follow normal distribution.

967
01:13:10,519 --> 01:13:13,519
 But if you assume the following normal distribution,

968
01:13:13,519 --> 01:13:16,599
 and we use this to design the panic classifier,

969
01:13:16,599 --> 01:13:18,360
 I think that normally the performance is good.

970
01:13:22,080 --> 01:13:28,920
 So this is the primary estimate for the density function.

971
01:13:29,120 --> 01:13:32,560
 Now actually I show you one example.

972
01:13:32,560 --> 01:13:35,800
 So I generate the examples for two classes.

973
01:13:35,800 --> 01:13:43,280
 And so I just use the normal density function

974
01:13:43,280 --> 01:13:47,280
 to generate the random sample.

975
01:13:47,280 --> 01:13:52,960
 So we have 200, and each class has 100 samples.

976
01:13:52,960 --> 01:13:57,920
 And so now we want to actually build

977
01:13:57,920 --> 01:14:01,840
 density function for each of the two classes.

978
01:14:01,840 --> 01:14:04,920
 And because we assume the following normal distribution,

979
01:14:04,920 --> 01:14:09,600
 then the problem is reduced to the estimation

980
01:14:09,600 --> 01:14:13,120
 of the mean vector and the current matrix.

981
01:14:13,120 --> 01:14:17,080
 If you use other type of a density function,

982
01:14:17,080 --> 01:14:20,200
 then there could be a much more number of parameters.

983
01:14:20,200 --> 01:14:23,559
 Then the primary estimation could be much more challenging.

984
01:14:23,559 --> 01:14:26,160
 But here we assume normal distribution.

985
01:14:26,160 --> 01:14:29,400
 Then we're just going to estimate the mean vector

986
01:14:29,400 --> 01:14:31,080
 and the current matrix.

987
01:14:31,080 --> 01:14:32,720
 Just use a formula to calculate.

988
01:14:36,320 --> 01:14:42,000
 So this is just actually like we use all the sample in class 100

989
01:14:42,000 --> 01:14:44,800
 get a summation, then divide by n1.

990
01:14:44,800 --> 01:14:46,599
 So finally we get the mean vector.

991
01:14:46,600 --> 01:14:50,400
 So this is an estimate, mu1 cap.

992
01:14:50,400 --> 01:14:53,960
 So this is an estimate for the mean vector of class 1.

993
01:14:53,960 --> 01:14:59,000
 So this is the mean vector estimate for the class 2.

994
01:14:59,000 --> 01:15:00,640
 You just get the summation.

995
01:15:00,640 --> 01:15:03,760
 So in the computer, you know either meta-level or Python.

996
01:15:03,760 --> 01:15:05,000
 You just use the mean.

997
01:15:05,000 --> 01:15:10,400
 You can easily calculate the two vectors for the two classes.

998
01:15:10,400 --> 01:15:13,080
 And then based on the mu1, mu2, and of course,

999
01:15:13,080 --> 01:15:17,640
 if you can get the current matrix sigma 1 and sigma 2.

1000
01:15:17,640 --> 01:15:21,320
 So of course, if you're in a function or in a turbo,

1001
01:15:21,320 --> 01:15:23,960
 and you normally have the functions to call,

1002
01:15:23,960 --> 01:15:28,519
 to get the current matrix directly.

1003
01:15:28,519 --> 01:15:30,519
 So these are current matrices.

1004
01:15:30,519 --> 01:15:33,200
 These are current matrices.

1005
01:15:33,200 --> 01:15:36,200
 So here SK1.

1006
01:15:40,200 --> 01:15:41,200
 So these are current matrices.

1007
01:15:43,760 --> 01:15:45,760
 n1, sigma n1, class 1.

1008
01:15:45,760 --> 01:15:49,200
 Here is n2, sigma n2, class 2.

1009
01:15:49,200 --> 01:15:51,720
 100 is the example.

1010
01:15:51,720 --> 01:15:55,960
 Out of that, actually, based on mu1, mu2, sigma 1, sigma 2,

1011
01:15:55,960 --> 01:15:59,880
 we can construct actually the density function.

1012
01:15:59,880 --> 01:16:02,360
 And in the density function, actually, and also, actually,

1013
01:16:02,360 --> 01:16:04,640
 we have a trigger determinant.

1014
01:16:04,640 --> 01:16:07,400
 A trigger determinant of a matrix is a scalar.

1015
01:16:07,400 --> 01:16:09,720
 You know how to calculate the determinant.

1016
01:16:09,720 --> 01:16:11,040
 So not absolutely.

1017
01:16:12,000 --> 01:16:15,120
 The determinant of a matrix, sigma 1, actually,

1018
01:16:15,120 --> 01:16:17,240
 of this determinant.

1019
01:16:17,240 --> 01:16:19,840
 So after that, actually, we can obtain

1020
01:16:19,840 --> 01:16:23,000
 actually the conditional probability density function

1021
01:16:23,000 --> 01:16:27,480
 for class 1, px, gamma, omega 1.

1022
01:16:27,480 --> 01:16:29,600
 So the mu1 is estimated.

1023
01:16:29,600 --> 01:16:31,240
 So I did not call it here.

1024
01:16:31,240 --> 01:16:35,519
 Just in previous slide, mu2 in previous slide, sigma 1.

1025
01:16:35,519 --> 01:16:37,000
 This is determinant.

1026
01:16:37,000 --> 01:16:38,080
 It stops here.

1027
01:16:38,080 --> 01:16:40,640
 Square root.

1028
01:16:40,720 --> 01:16:43,200
 So for any given x, we can simply

1029
01:16:43,200 --> 01:16:47,440
 start to do this formula to calculate the px, gamma, omega

1030
01:16:47,440 --> 01:16:48,760
 1.

1031
01:16:48,760 --> 01:16:55,160
 Because, actually, the mu1, mu2, mu1, and the sigma 1

1032
01:16:55,160 --> 01:16:58,480
 are all estimated from the data.

1033
01:17:02,280 --> 01:17:07,720
 Similarly, for class 2, we can have the density function.

1034
01:17:07,760 --> 01:17:12,240
 So this is actually the square root

1035
01:17:12,240 --> 01:17:16,440
 of the determinant of sigma 2.

1036
01:17:16,440 --> 01:17:18,640
 Sigma here means sigma 1, sigma 2 is a capital

1037
01:17:18,640 --> 01:17:19,920
 letter of sigma.

1038
01:17:19,920 --> 01:17:21,600
 So it means current matrix.

1039
01:17:30,080 --> 01:17:32,320
 So the density function could be like this.

1040
01:17:32,320 --> 01:17:36,600
 So p1 could be just a joint.

1041
01:17:36,600 --> 01:17:39,040
 A joint probability of x and omega 1,

1042
01:17:39,040 --> 01:17:42,840
 which is the product of the density function

1043
01:17:42,840 --> 01:17:46,760
 with a prior probability of p omega 1.

1044
01:17:46,760 --> 01:17:51,840
 So in this example, we have 100 samples from class 1.

1045
01:17:51,840 --> 01:17:53,640
 100 samples from class 2.

1046
01:17:53,640 --> 01:17:56,840
 So the prior probability for the two classes,

1047
01:17:56,840 --> 01:18:01,280
 both equal, actually, half, right?

1048
01:18:01,280 --> 01:18:03,160
 See, from 5.

1049
01:18:03,160 --> 01:18:04,560
 So this is a prior probability.

1050
01:18:04,560 --> 01:18:08,760
 1 half, 1 half times the density function for class 1,

1051
01:18:08,760 --> 01:18:10,360
 density function for class 2.

1052
01:18:10,360 --> 01:18:12,800
 So this is a Gisler function.

1053
01:18:12,800 --> 01:18:16,200
 For any input x, for any sample,

1054
01:18:16,200 --> 01:18:20,320
 either it's a training sample or a testing sample, x,

1055
01:18:20,320 --> 01:18:21,440
 the vector is given, right?

1056
01:18:21,440 --> 01:18:22,920
 It's known.

1057
01:18:22,920 --> 01:18:26,120
 We can substitute the formula to calculate g1, x.

1058
01:18:26,120 --> 01:18:28,080
 And also, we can substitute the next formula,

1059
01:18:28,080 --> 01:18:30,080
 next ingredient, calculate g2, x.

1060
01:18:30,080 --> 01:18:34,320
 Then we can compare the two Gisler functions

1061
01:18:34,320 --> 01:18:35,719
 to see which one is bigger, right?

1062
01:18:35,719 --> 01:18:38,719
 Which one is greater?

1063
01:18:38,719 --> 01:18:43,080
 If g1 is greater, then we see the sample x.

1064
01:18:43,080 --> 01:18:46,639
 And it's a sample in class 1, omega 1.

1065
01:18:46,639 --> 01:18:50,759
 All the samples, g2 is greater, right?

1066
01:18:50,759 --> 01:18:53,400
 We send the sample to class 2.

1067
01:18:53,400 --> 01:18:56,719
 If the two, the same value, then we

1068
01:18:56,719 --> 01:18:59,639
 see that this sample is on the descent boundary.

1069
01:18:59,639 --> 01:19:01,639
 And it's 50-50%, right?

1070
01:19:01,640 --> 01:19:05,240
 We cannot decide 50-50.

1071
01:19:05,240 --> 01:19:11,080
 So this is the sample, it's on the descent boundary.

1072
01:19:11,080 --> 01:19:13,400
 Just now I showed you the sample examples, right?

1073
01:19:13,400 --> 01:19:16,760
 Actually, I then generated, actually, like $40,000 data

1074
01:19:16,760 --> 01:19:19,160
 point, $40,000.

1075
01:19:19,160 --> 01:19:21,960
 For usual data point x, I substitute the formula

1076
01:19:21,960 --> 01:19:24,520
 to calculate g1, x, calculate g2, x.

1077
01:19:24,520 --> 01:19:26,840
 And then obtain the corresponding class.

1078
01:19:26,840 --> 01:19:33,600
 And finally, actually, we get the regions for class 1 and class 2.

1079
01:19:33,600 --> 01:19:37,600
 One is the yellow region for one class,

1080
01:19:37,600 --> 01:19:41,360
 and the gray region for another class.

1081
01:19:41,360 --> 01:19:43,120
 OK.

1082
01:19:43,120 --> 01:19:46,280
 So then you can see a boundary in between, right?

1083
01:19:46,280 --> 01:19:50,800
 It's the boundary between the yellow and gray region.

1084
01:19:50,800 --> 01:19:54,040
 So this is the descent boundary.

1085
01:19:54,040 --> 01:19:56,920
 Any sample on the descent boundary

1086
01:19:56,920 --> 01:20:01,560
 will have the same g1 and g2x.

1087
01:20:01,560 --> 01:20:05,400
 So because the probability is 50% to 50%, right?

1088
01:20:05,400 --> 01:20:06,440
 You cannot decide.

1089
01:20:06,440 --> 01:20:08,200
 So this sample is on the descent boundary.

1090
01:20:12,960 --> 01:20:22,760
 So from this example, actually, we can clearly show the g1,

1091
01:20:22,760 --> 01:20:27,480
 g1, such a way, could classify the samples, actually,

1092
01:20:27,480 --> 01:20:28,280
 correctly, right?

1093
01:20:28,280 --> 01:20:30,680
 Of course, actually, we could have some errors.

1094
01:20:30,680 --> 01:20:33,840
 These errors, most likely, are autolyes.

1095
01:20:33,840 --> 01:20:36,600
 So in the last week, actually, we

1096
01:20:36,600 --> 01:20:37,760
 started autolyes, right?

1097
01:20:37,760 --> 01:20:41,440
 Autolyes in the value is actually very different

1098
01:20:41,440 --> 01:20:43,320
 from the normal values.

1099
01:20:43,320 --> 01:20:46,480
 So these are likely, likely autolyes.

1100
01:20:46,480 --> 01:20:54,000
 So for autolyes, normally, we classify them wrongly.

1101
01:20:54,000 --> 01:20:55,360
 OK.

1102
01:20:55,360 --> 01:20:58,000
 So this is the basic descent rule.

1103
01:20:58,000 --> 01:21:00,839
 Of course, I used as two examples,

1104
01:21:00,839 --> 01:21:02,679
 as the illustration, right?

1105
01:21:02,679 --> 01:21:07,000
 In practice, actually, the data is high dimensional.

1106
01:21:07,000 --> 01:21:12,240
 We know that data is a high-dent 3, and 400, 500.

1107
01:21:12,240 --> 01:21:13,440
 We cannot visualize data.

1108
01:21:13,440 --> 01:21:15,160
 It's very hard to visualize.

1109
01:21:15,160 --> 01:21:17,120
 Of course, we have other means we rely,

1110
01:21:17,120 --> 01:21:21,160
 but not directly we rely on data, even in the original space.

1111
01:21:21,160 --> 01:21:24,680
 So we cannot actually find the data dilution.

1112
01:21:24,680 --> 01:21:26,360
 But this is just for illustration, probably.

1113
01:21:26,360 --> 01:21:28,840
 Illustration, that means, in such a way,

1114
01:21:28,840 --> 01:21:31,280
 to design the classification algorithm,

1115
01:21:31,280 --> 01:21:33,680
 to design the different functions,

1116
01:21:33,680 --> 01:21:37,240
 and we classify the sample correctly.

1117
01:21:37,240 --> 01:21:39,840
 Based on this illustration, we should trust this method.

1118
01:21:40,280 --> 01:21:43,800
 OK.

1119
01:21:43,800 --> 01:21:50,240
 So this is the, actually, base decision theory, right?

1120
01:21:50,240 --> 01:21:52,280
 A base decision rule.

1121
01:21:52,280 --> 01:21:57,680
 And the key issue is the estimation of the density

1122
01:21:57,680 --> 01:21:59,440
 function.

1123
01:21:59,440 --> 01:22:03,320
 And here, we assume the following normal dilution.

1124
01:22:03,320 --> 01:22:06,000
 And then the key issue is the estimation

1125
01:22:06,000 --> 01:22:08,600
 of the mean vector parametric.

1126
01:22:08,600 --> 01:22:12,800
 And actually, the both estimate has a closed form.

1127
01:22:12,800 --> 01:22:15,640
 We have formula to calculate directly

1128
01:22:15,640 --> 01:22:21,560
 for the two parameter, the mean vector parametric.

1129
01:22:21,560 --> 01:22:25,440
 So then, actually, we look at another scenario.

1130
01:22:25,440 --> 01:22:27,760
 Just now, we see the data follow normal dilution.

1131
01:22:27,760 --> 01:22:31,320
 In practice, the data may follow normal dilution,

1132
01:22:31,320 --> 01:22:34,360
 but it's a multi-modal.

1133
01:22:34,440 --> 01:22:38,480
 In last week, actually, when I showed you

1134
01:22:38,480 --> 01:22:43,080
 some exploration tools, like he said, well,

1135
01:22:43,080 --> 01:22:46,519
 I mentioned that there are a few peaks, right?

1136
01:22:46,519 --> 01:22:50,519
 There's something like, they call multi-modal.

1137
01:22:50,519 --> 01:22:53,400
 Multi-modal, that means, actually,

1138
01:22:53,400 --> 01:22:58,639
 the data may come from multiple pre-gaussian functions.

1139
01:22:58,639 --> 01:22:59,599
 Gaussian functions.

1140
01:22:59,599 --> 01:23:00,280
 OK.

1141
01:23:00,280 --> 01:23:02,719
 So in such a scenario, if the data is

1142
01:23:02,720 --> 01:23:04,760
 from multiple gaussian functions,

1143
01:23:04,760 --> 01:23:08,480
 certainly, we should not use just one single gaussian function

1144
01:23:08,480 --> 01:23:13,040
 or normal function to model the distribution of the data.

1145
01:23:13,040 --> 01:23:16,120
 If they are from multiple actually gaussian functions,

1146
01:23:16,120 --> 01:23:18,520
 and then we should use multiple gaussian functions

1147
01:23:18,520 --> 01:23:21,160
 to construct a density function.

1148
01:23:21,160 --> 01:23:21,660
 OK.

1149
01:23:21,660 --> 01:23:23,040
 So this is one scenario.

1150
01:23:23,040 --> 01:23:25,560
 Another scenario is this.

1151
01:23:25,560 --> 01:23:28,440
 The data do not follow normal dilution.

1152
01:23:28,440 --> 01:23:30,360
 This is absolute dilution.

1153
01:23:30,360 --> 01:23:34,719
 It has been approved for any distribution

1154
01:23:34,719 --> 01:23:36,880
 if the data is continuous.

1155
01:23:36,880 --> 01:23:40,759
 We can use multiple normal density functions,

1156
01:23:40,759 --> 01:23:46,599
 multiple normal functions, to approximate the density function.

1157
01:23:46,599 --> 01:23:48,000
 If they follow normal density function,

1158
01:23:48,000 --> 01:23:50,280
 we use normal density function.

1159
01:23:50,280 --> 01:23:52,400
 If they don't follow, we can still

1160
01:23:52,400 --> 01:23:56,080
 use normal density function to approximate.

1161
01:23:56,080 --> 01:24:01,800
 But we need to use multiple density function

1162
01:24:01,800 --> 01:24:03,840
 to approximate the density function,

1163
01:24:03,840 --> 01:24:07,000
 multiple normal functions.

1164
01:24:07,000 --> 01:24:10,080
 Each of the normal functions has its own mean vector

1165
01:24:10,080 --> 01:24:12,840
 and correct matrix.

1166
01:24:12,840 --> 01:24:13,440
 OK.

1167
01:24:13,440 --> 01:24:18,600
 So this is the motivation behind the Gaussian mixture model.

1168
01:24:18,600 --> 01:24:23,519
 Mystery, that means the model is a mix of a few gaussian

1169
01:24:23,520 --> 01:24:28,360
 function, or a whole net gaussian function, or normal functions.

1170
01:24:28,360 --> 01:24:29,120
 OK.

1171
01:24:29,120 --> 01:24:33,440
 And this diagram just shows the data of class 2,

1172
01:24:33,440 --> 01:24:34,880
 the blue samples.

1173
01:24:34,880 --> 01:24:40,640
 Actually clearly the sample is from model 1

1174
01:24:40,640 --> 01:24:42,680
 actually density function.

1175
01:24:42,680 --> 01:24:45,720
 Actually it's from model 1 gaussian function.

1176
01:24:45,720 --> 01:24:49,160
 So it is from 2 gaussian functions.

1177
01:24:49,160 --> 01:24:52,920
 So for such a data, certainly we need

1178
01:24:52,960 --> 01:24:57,960
 to use the two normal functions to construct the density function.

1179
01:24:57,960 --> 01:25:01,320
 But if we don't know, of course you can use multiple,

1180
01:25:01,320 --> 01:25:05,000
 you can use multiple Gaussian functions

1181
01:25:05,000 --> 01:25:09,200
 to construct the density function.

1182
01:25:09,200 --> 01:25:10,120
 OK.

1183
01:25:10,120 --> 01:25:12,720
 So this is a Gaussian mixture model.

1184
01:25:12,720 --> 01:25:19,560
 So this is a Gaussian, this is like a PS omega, right?

1185
01:25:19,560 --> 01:25:21,160
 This is a density function, right?

1186
01:25:21,200 --> 01:25:23,120
 Class conditional density function.

1187
01:25:23,120 --> 01:25:25,120
 Class conditional probability density function.

1188
01:25:25,120 --> 01:25:31,840
 So now actually we have a summation of a few normal functions.

1189
01:25:31,840 --> 01:25:33,519
 M is the normal function, right?

1190
01:25:33,519 --> 01:25:34,559
 The normal function.

1191
01:25:34,559 --> 01:25:38,360
 And each function has its own mean vector correct matrix.

1192
01:25:38,360 --> 01:25:43,120
 So here we assume the function, the density function contains

1193
01:25:43,120 --> 01:25:46,720
 m normal functions.

1194
01:25:47,680 --> 01:25:55,200
 And so the Px is a weighted summation of this m Gaussian

1195
01:25:55,200 --> 01:25:56,480
 functions.

1196
01:25:56,480 --> 01:26:02,640
 Usually Gaussian function has a width r for i.

1197
01:26:02,640 --> 01:26:03,440
 OK.

1198
01:26:03,440 --> 01:26:08,720
 So this function is a summation of a few density functions.

1199
01:26:08,720 --> 01:26:10,760
 A few Gaussian density functions, right?

1200
01:26:10,760 --> 01:26:14,280
 This density function is a summation of a few Gaussian

1201
01:26:14,280 --> 01:26:17,800
 functions or normal functions.

1202
01:26:17,800 --> 01:26:19,960
 And the summation by no direct summation, right?

1203
01:26:19,960 --> 01:26:21,639
 It's a weighted summation.

1204
01:26:21,639 --> 01:26:24,880
 So that means each Gaussian function

1205
01:26:24,880 --> 01:26:27,880
 has a corresponding width denoted by r for.

1206
01:26:32,200 --> 01:26:37,519
 And so this r for summation of r for should be 1.

1207
01:26:37,519 --> 01:26:39,880
 Should be 1.

1208
01:26:39,880 --> 01:26:41,920
 So this is r for.

1209
01:26:41,920 --> 01:26:46,680
 And of course, if we assume a density function,

1210
01:26:46,680 --> 01:26:51,800
 and then how to estimate this Px omega,

1211
01:26:51,800 --> 01:26:58,040
 from the given sample x, the trinium sample, right?

1212
01:26:58,040 --> 01:26:59,360
 How to estimate the trinium?

1213
01:26:59,360 --> 01:27:02,640
 How to estimate?

1214
01:27:02,640 --> 01:27:09,120
 That means that we need to estimate mu 1, mu sigma 1,

1215
01:27:09,120 --> 01:27:13,559
 mu 2, sigma 2, and mu m, sigma i m.

1216
01:27:13,559 --> 01:27:16,640
 And also we also need to estimate r for 1, r for 2,

1217
01:27:16,640 --> 01:27:19,559
 and r for i m, right?

1218
01:27:19,559 --> 01:27:24,200
 So you ought to determine this density function, which

1219
01:27:24,200 --> 01:27:29,080
 is a mixture of a few Gaussian functions.

1220
01:27:29,080 --> 01:27:31,760
 So certainly this is much harder, right?

1221
01:27:31,760 --> 01:27:35,280
 It's much challenging than the previous scenario.

1222
01:27:35,280 --> 01:27:39,080
 We just have one single, actually normal function,

1223
01:27:39,080 --> 01:27:41,240
 to construct the density function.

1224
01:27:41,240 --> 01:27:42,759
 Then the mean vector and the current matrix

1225
01:27:42,759 --> 01:27:44,440
 could be easily obtained, right?

1226
01:27:44,440 --> 01:27:45,920
 But now we don't know.

1227
01:27:45,920 --> 01:27:47,880
 We don't know how many, right?

1228
01:27:47,880 --> 01:27:51,440
 We don't know which sample belongs to capulon number 1,

1229
01:27:51,440 --> 01:27:53,840
 which sample belongs to capulon number 2,

1230
01:27:53,840 --> 01:27:55,759
 Gaussian capulon number 2.

1231
01:27:55,759 --> 01:27:56,559
 We don't know.

1232
01:27:56,559 --> 01:28:01,400
 Then how to calculate the mu i, mu 1, mu sigma 1, mu 2,

1233
01:28:01,400 --> 01:28:02,360
 sigma 2?

1234
01:28:02,360 --> 01:28:06,440
 How to calculate the corresponding alpha 1, alpha 2?

1235
01:28:06,440 --> 01:28:07,040
 OK.

1236
01:28:07,040 --> 01:28:09,639
 Of course, actually, this is just for illustration purpose.

1237
01:28:09,639 --> 01:28:13,080
 We know this data belongs to this Gaussian function.

1238
01:28:13,080 --> 01:28:14,880
 This data belongs to this Gaussian function, right?

1239
01:28:14,880 --> 01:28:16,360
 But you practically just given data.

1240
01:28:16,360 --> 01:28:18,040
 You cannot see the data distribution.

1241
01:28:18,040 --> 01:28:20,960
 How do we know which sample belongs to which density

1242
01:28:20,960 --> 01:28:23,160
 function, which Gaussian function?

1243
01:28:23,160 --> 01:28:24,880
 You don't know.

1244
01:28:24,880 --> 01:28:27,519
 If you don't know how to estimate the mean vector

1245
01:28:27,519 --> 01:28:31,799
 and the current matrix, OK?

1246
01:28:32,599 --> 01:28:34,240
 So how to estimate?

1247
01:28:37,120 --> 01:28:40,920
 So here, we still need to use the so-called max likelihood

1248
01:28:40,920 --> 01:28:42,519
 method to estimate.

1249
01:28:42,519 --> 01:28:45,240
 We still need to use this method, OK?

1250
01:28:45,240 --> 01:28:46,719
 And here.

1251
01:28:53,360 --> 01:28:55,719
 So again, given this asset D, right?

1252
01:28:55,719 --> 01:29:00,000
 This D, this asset D. This asset for class 1.

1253
01:29:00,000 --> 01:29:01,639
 This asset for class 2.

1254
01:29:02,400 --> 01:29:05,280
 And because the two density functions are

1255
01:29:05,280 --> 01:29:06,720
 constructed separately, right?

1256
01:29:06,720 --> 01:29:12,160
 So here, the D just means the sample from the same class.

1257
01:29:12,160 --> 01:29:15,800
 So we can have the likelihood function, right?

1258
01:29:15,800 --> 01:29:18,120
 Which is the summation and multiplication

1259
01:29:18,120 --> 01:29:22,200
 of the many functions, right?

1260
01:29:22,200 --> 01:29:24,360
 Many probabilities.

1261
01:29:24,360 --> 01:29:27,560
 But after we take the logarithm of the equation,

1262
01:29:27,560 --> 01:29:30,720
 that becomes summation.

1263
01:29:30,760 --> 01:29:36,040
 You remember P here, actually, for each sample, right?

1264
01:29:36,040 --> 01:29:41,600
 So now it's X, R, I, mu, I, sigma, I.

1265
01:29:41,600 --> 01:29:43,200
 For each of the samples, right?

1266
01:29:43,200 --> 01:29:46,840
 And it is the density function.

1267
01:29:46,840 --> 01:29:50,560
 And it contains actually M Gaussian functions.

1268
01:29:53,200 --> 01:29:56,680
 So here, previously, we just for 2-clad, right?

1269
01:29:56,680 --> 01:30:00,680
 We just have mu1, mu2, sigma minus sigma 2, right?

1270
01:30:00,720 --> 01:30:03,160
 Now, even for 2-clad kind of problem,

1271
01:30:03,160 --> 01:30:05,440
 because actually for class 2, we have multiple

1272
01:30:05,440 --> 01:30:07,080
 actually Gaussian functions.

1273
01:30:07,080 --> 01:30:09,800
 And then, actually, we have more parameters to estimate,

1274
01:30:09,800 --> 01:30:14,400
 right now, and more parameters to estimate.

1275
01:30:14,400 --> 01:30:19,240
 But the principle of the same, we use the maximum likelihood

1276
01:30:19,240 --> 01:30:20,640
 method to estimate.

1277
01:30:20,640 --> 01:30:21,140
 OK.

1278
01:30:30,520 --> 01:30:33,640
 So here, we define actually one variable.

1279
01:30:33,640 --> 01:30:37,840
 And this OI, OI here means the Gaussian component O1.

1280
01:30:37,840 --> 01:30:42,960
 OK, O, O, O, OI, this is the Gaussian component I.

1281
01:30:42,960 --> 01:30:45,440
 So we have multiple Gaussian components, right?

1282
01:30:45,440 --> 01:30:49,360
 In this density function, we have M. OK.

1283
01:30:49,360 --> 01:30:52,519
 So each sample, because the sample, given sample, right,

1284
01:30:52,519 --> 01:30:55,240
 in this class, we don't know which Gaussian component

1285
01:30:55,240 --> 01:30:56,679
 is a belongs to.

1286
01:30:56,679 --> 01:30:57,179
 OK.

1287
01:30:57,179 --> 01:31:02,519
 So we assume this sample XK belonging to Gaussian component

1288
01:31:02,519 --> 01:31:03,519
 in the same class.

1289
01:31:03,519 --> 01:31:05,559
 All the Gaussian components are in the same class.

1290
01:31:05,559 --> 01:31:06,240
 OK.

1291
01:31:06,240 --> 01:31:12,120
 So XK belonging to the Gaussian component OI.

1292
01:31:12,120 --> 01:31:15,839
 We denote this probably as gram IK.

1293
01:31:15,839 --> 01:31:17,120
 Gram IK.

1294
01:31:17,120 --> 01:31:17,759
 OK.

1295
01:31:17,760 --> 01:31:20,440
 We denote this.

1296
01:31:20,440 --> 01:31:24,320
 And then, OK, how to calculate this gram IK,

1297
01:31:24,320 --> 01:31:28,840
 which is the probability of OI given XK.

1298
01:31:28,840 --> 01:31:34,960
 And here, again, we apply them the base theorem, right?

1299
01:31:34,960 --> 01:31:38,240
 So this is like a posterior probability, something like that.

1300
01:31:38,240 --> 01:31:41,960
 So this is the probability of OI given XK.

1301
01:31:41,960 --> 01:31:46,040
 So here, the numerator here is the John probability

1302
01:31:46,040 --> 01:31:52,960
 of SK and OI, right?

1303
01:31:52,960 --> 01:31:59,640
 So it's OI and SK times P, OI.

1304
01:31:59,640 --> 01:32:02,840
 This is similar to what we have used before, right?

1305
01:32:02,840 --> 01:32:06,920
 So this is the base theorem.

1306
01:32:06,920 --> 01:32:12,519
 And then, the denominator here is the total probability of X.

1307
01:32:12,519 --> 01:32:13,840
 So OK.

1308
01:32:13,840 --> 01:32:16,440
 So this is the gram IK.

1309
01:32:16,440 --> 01:32:19,320
 And so now, actually, we don't see one sample

1310
01:32:19,320 --> 01:32:23,320
 belonging to one specific class, actually, a component.

1311
01:32:23,320 --> 01:32:26,640
 Actually, for each sample, actually,

1312
01:32:26,640 --> 01:32:32,840
 it has a probability belonging to each of these m components,

1313
01:32:32,840 --> 01:32:37,280
 or Gaussian components, in this density function.

1314
01:32:37,280 --> 01:32:37,520
 OK.

1315
01:32:37,520 --> 01:32:40,040
 So this is actually the gram IK.

1316
01:32:40,040 --> 01:32:45,440
 The sample SK belongs to Gaussian component OI.

1317
01:32:45,440 --> 01:32:49,519
 This probability denotes the gram IK.

1318
01:32:49,519 --> 01:32:51,120
 OK.

1319
01:32:51,120 --> 01:32:53,160
 And then, actually, all of them

1320
01:32:53,160 --> 01:32:57,720
 follows the mass-malarly method, right?

1321
01:32:57,720 --> 01:33:01,040
 The threshold derivative to the fifth parameter,

1322
01:33:01,040 --> 01:33:02,000
 this year, right?

1323
01:33:02,000 --> 01:33:04,120
 Just follow the previous procedure.

1324
01:33:04,120 --> 01:33:06,960
 And finally, we can find this is the mu I.

1325
01:33:06,960 --> 01:33:08,320
 Mu I.

1326
01:33:08,320 --> 01:33:12,200
 Mu I, not the class I, right?

1327
01:33:12,200 --> 01:33:16,719
 This is a component I in this class.

1328
01:33:16,719 --> 01:33:17,219
 OK.

1329
01:33:17,219 --> 01:33:18,840
 Mu I.

1330
01:33:18,840 --> 01:33:23,480
 So we use all the samples, twice me.

1331
01:33:23,480 --> 01:33:23,980
 OK.

1332
01:33:23,980 --> 01:33:27,240
 But the usage of the sample belonging to this Gaussian

1333
01:33:27,240 --> 01:33:29,320
 component has a certain probability.

1334
01:33:29,320 --> 01:33:32,559
 So some of them, maybe, the probability is very small.

1335
01:33:32,559 --> 01:33:36,480
 That is a contribution, a bit, very small, very little,

1336
01:33:36,480 --> 01:33:39,440
 to the calculation of the mean value.

1337
01:33:39,440 --> 01:33:43,360
 So if the value, the gram IK belongs to this Gaussian component

1338
01:33:43,360 --> 01:33:47,200
 is zero, then that means it has no contribution.

1339
01:33:47,200 --> 01:33:54,639
 So here, it's a weighted summation of the SK.

1340
01:33:54,639 --> 01:33:56,160
 It's not a pre-progressional.

1341
01:33:56,160 --> 01:34:02,000
 Every SK has equal contribution to the mean vector, right?

1342
01:34:02,000 --> 01:34:05,120
 But here, because of the sample, it

1343
01:34:05,120 --> 01:34:08,320
 belongs to a certain Gaussian component

1344
01:34:08,320 --> 01:34:10,599
 with a certain probability.

1345
01:34:10,599 --> 01:34:12,760
 We use this probability as a weight

1346
01:34:12,760 --> 01:34:17,400
 to calculate the weight of the summation of all the samples

1347
01:34:17,400 --> 01:34:25,160
 as the mean weight of a Gaussian component, or Y.

1348
01:34:25,160 --> 01:34:25,760
 OK.

1349
01:34:25,760 --> 01:34:28,880
 So this is the weight of the summation of all the samples

1350
01:34:28,880 --> 01:34:30,400
 in this class, right?

1351
01:34:30,400 --> 01:34:31,360
 Weight of the summation.

1352
01:34:31,360 --> 01:34:34,880
 Of course, the sum of the samples could contribute a little,

1353
01:34:34,880 --> 01:34:36,680
 because of the gram IK.

1354
01:34:36,680 --> 01:34:38,960
 That means the probability belonging to this Gaussian

1355
01:34:38,960 --> 01:34:41,680
 component is very small.

1356
01:34:41,680 --> 01:34:42,280
 OK.

1357
01:34:42,280 --> 01:34:45,480
 So this is the mean vector.

1358
01:34:45,480 --> 01:34:48,320
 And then we can also treat it similarly

1359
01:34:48,320 --> 01:34:55,400
 to find the current matrix for Gaussian component OI.

1360
01:34:55,400 --> 01:34:57,000
 So look at this part.

1361
01:34:57,000 --> 01:35:00,440
 XK minus mu i, right?

1362
01:35:00,440 --> 01:35:07,400
 And XK minus mu i transpose, then K from 1 to n, right?

1363
01:35:07,400 --> 01:35:14,240
 So even if you ignore this term, then this is actually

1364
01:35:14,240 --> 01:35:16,720
 just the same as my pre-progress one, right?

1365
01:35:16,720 --> 01:35:18,560
 But now it's different.

1366
01:35:18,560 --> 01:35:20,800
 Each sample has a contribution.

1367
01:35:20,800 --> 01:35:23,280
 So this contribution is a weight.

1368
01:35:23,280 --> 01:35:24,840
 It's a weight.

1369
01:35:24,840 --> 01:35:29,720
 So this is the symbol I, the current matrix,

1370
01:35:29,720 --> 01:35:32,080
 for Gaussian component OI.

1371
01:35:32,080 --> 01:35:34,960
 So each of the samples has a contribution.

1372
01:35:34,960 --> 01:35:38,240
 This contribution is just a weight, right?

1373
01:35:38,240 --> 01:35:40,880
 This weight is just the probability of this sample

1374
01:35:40,880 --> 01:35:45,480
 belonging to this Gaussian component, OI.

1375
01:35:45,480 --> 01:35:49,040
 We use this weight to get the weight of the summation.

1376
01:35:49,040 --> 01:35:52,920
 So this is the symbol I.

1377
01:35:52,920 --> 01:35:57,960
 And then this is the alpha i, the weight, right?

1378
01:35:57,960 --> 01:36:00,400
 In the Gaussian mixture model.

1379
01:36:00,400 --> 01:36:04,040
 Alpha i is a weight for Gaussian component OI

1380
01:36:04,040 --> 01:36:09,840
 in the mixture model, which is the summation of gamma i k.

1381
01:36:09,840 --> 01:36:14,440
 Then the k belonging to OI, then divided by the n.

1382
01:36:14,440 --> 01:36:17,480
 OK.

1383
01:36:17,480 --> 01:36:22,440
 And actually, in this tree, we see that to calculate alpha i,

1384
01:36:22,440 --> 01:36:23,919
 if you look at this, right?

1385
01:36:23,919 --> 01:36:26,400
 If we know gamma i k, and then we

1386
01:36:26,400 --> 01:36:28,960
 got q calculated mu i, right?

1387
01:36:28,960 --> 01:36:29,759
 We have no problem.

1388
01:36:29,759 --> 01:36:33,400
 If we know gamma i k, we can calculate, OK?

1389
01:36:33,400 --> 01:36:35,719
 And if we know actually gamma i k,

1390
01:36:35,719 --> 01:36:40,480
 we can calculate actually the sigma i, right?

1391
01:36:40,480 --> 01:36:43,919
 Which is the current matrix, right?

1392
01:36:43,919 --> 01:36:48,160
 So the h here is gamma i k, OK?

1393
01:36:48,160 --> 01:36:52,280
 And also, if we know gamma i k, and we

1394
01:36:52,280 --> 01:36:55,000
 can calculate alpha i, OK?

1395
01:36:55,000 --> 01:36:55,960
 So that means the weight.

1396
01:36:55,960 --> 01:36:57,759
 For each of the Gaussian components,

1397
01:36:57,759 --> 01:36:59,400
 then we can calculate it, right?

1398
01:36:59,400 --> 01:37:01,679
 We can mean vector and current matrix.

1399
01:37:01,679 --> 01:37:04,320
 And also, for each of the Gaussian components,

1400
01:37:04,320 --> 01:37:08,200
 we have to calculate the weight, alpha i.

1401
01:37:08,200 --> 01:37:12,400
 So here, the main thing here is the gamma i k.

1402
01:37:12,400 --> 01:37:15,639
 As long as you know the gamma i k,

1403
01:37:15,639 --> 01:37:19,400
 the probability of a sample belonging to each of the Gaussian

1404
01:37:19,400 --> 01:37:20,759
 components.

1405
01:37:20,760 --> 01:37:22,280
 The gamma i k is known.

1406
01:37:22,280 --> 01:37:25,440
 And then the mean vector, the current matrix,

1407
01:37:25,440 --> 01:37:30,200
 and also the weight for each of the Gaussian components,

1408
01:37:30,200 --> 01:37:31,320
 are determined.

1409
01:37:31,320 --> 01:37:35,680
 Then the Gaussian-Mission model is determined.

1410
01:37:35,680 --> 01:37:41,200
 OK, so here, the k is to calculate gamma i k, right?

1411
01:37:41,200 --> 01:37:43,000
 So what is the gamma i k?

1412
01:37:43,000 --> 01:37:45,560
 If we go back to look at this formula, right?

1413
01:37:45,560 --> 01:37:57,440
 The gamma i k depends on x k, depends on alpha i,

1414
01:37:57,440 --> 01:37:59,040
 p x k o i, right?

1415
01:37:59,040 --> 01:38:04,160
 p x o i also depends on mu i and the sigma i.

1416
01:38:04,160 --> 01:38:09,400
 So in other words, the gamma i k, the mu i, the sigma i,

1417
01:38:09,400 --> 01:38:12,800
 the alpha i depends on gamma i k.

1418
01:38:12,800 --> 01:38:20,200
 But the gamma i k depends on mu i, sigma i, and the alpha i.

1419
01:38:20,200 --> 01:38:24,960
 So in other words, actually, we cannot have a closed form.

1420
01:38:24,960 --> 01:38:28,480
 We cannot have a closed form to find, actually,

1421
01:38:28,480 --> 01:38:31,280
 these parameters, right?

1422
01:38:31,280 --> 01:38:35,200
 Because actually, gamma i k is determined by others, right?

1423
01:38:35,200 --> 01:38:40,120
 And others are determined by gamma i k.

1424
01:38:40,120 --> 01:38:45,040
 So the parameter cannot be estimated in closed form.

1425
01:38:45,040 --> 01:38:46,920
 In closed form, that means we have a formula

1426
01:38:46,920 --> 01:38:48,240
 to calculate directly.

1427
01:38:48,240 --> 01:38:50,519
 We cannot have that, OK?

1428
01:38:50,519 --> 01:38:54,400
 So then how to solve this problem?

1429
01:38:54,400 --> 01:38:58,200
 Actually, we can use a so-called expectation-maximization

1430
01:38:58,200 --> 01:38:58,800
 method.

1431
01:38:58,800 --> 01:39:02,240
 This EM algorithm, actually, is also a commonly used algorithm

1432
01:39:02,240 --> 01:39:04,519
 in machine learning.

1433
01:39:04,519 --> 01:39:07,440
 EM, expectation-maximization.

1434
01:39:07,440 --> 01:39:10,200
 Actually, this algorithm contains two parts.

1435
01:39:10,200 --> 01:39:12,000
 One part, one step, two steps.

1436
01:39:12,000 --> 01:39:13,559
 One is called expectation.

1437
01:39:13,559 --> 01:39:15,519
 Another step is called maximization.

1438
01:39:15,519 --> 01:39:20,480
 So together, it's called expectation-maximization algorithm.

1439
01:39:20,480 --> 01:39:21,120
 OK.

1440
01:39:21,120 --> 01:39:23,759
 And actually, this algorithm is an iterative method.

1441
01:39:23,759 --> 01:39:26,759
 Because it cannot be solved.

1442
01:39:26,759 --> 01:39:29,839
 It cannot be estimated in a closed form.

1443
01:39:29,839 --> 01:39:33,919
 Then we can only use an iterative method to solve.

1444
01:39:33,920 --> 01:39:35,960
 It's iterative.

1445
01:39:35,960 --> 01:39:36,460
 OK.

1446
01:39:40,000 --> 01:39:42,840
 So how do you treat iterative?

1447
01:39:42,840 --> 01:39:45,680
 Normally, iterative method, now you say,

1448
01:39:45,680 --> 01:39:48,600
 now we need to start the formula initial step, right?

1449
01:39:48,600 --> 01:39:51,920
 We give the values for one set of parameters.

1450
01:39:51,920 --> 01:39:57,920
 For example, we give values for alpha i, mu i, and sigma i.

1451
01:39:57,920 --> 01:40:03,200
 And then based on this, we determine that the gamma i k.

1452
01:40:03,200 --> 01:40:07,000
 Then after that, actually, this is actually

1453
01:40:07,000 --> 01:40:09,720
 a normally called estimation step.

1454
01:40:09,720 --> 01:40:16,160
 Based on the gamma i k, then we calculate the mu i, sigma i,

1455
01:40:16,160 --> 01:40:17,800
 and alpha i.

1456
01:40:17,800 --> 01:40:21,240
 Then in the next step, based on the mu i, beta i,

1457
01:40:21,240 --> 01:40:24,280
 and the sigma i, gamma i, the alpha i,

1458
01:40:24,280 --> 01:40:26,880
 we calculate the mu, gamma i k.

1459
01:40:26,880 --> 01:40:28,880
 So we do this iteratively.

1460
01:40:28,880 --> 01:40:32,120
 At the beginning of the value, maybe I run my given.

1461
01:40:32,800 --> 01:40:39,000
 Step by step, actually, the value will converge.

1462
01:40:39,000 --> 01:40:41,120
 It will converge.

1463
01:40:41,120 --> 01:40:41,720
 OK.

1464
01:40:41,720 --> 01:40:43,880
 So this is the basic idea of the expectation

1465
01:40:43,880 --> 01:40:45,200
 and maximization method.

1466
01:40:45,200 --> 01:40:46,240
 It's a very commonly used.

1467
01:40:50,400 --> 01:40:53,400
 So the e-step, e-step, actually, estimates

1468
01:40:53,400 --> 01:40:59,040
 gamma i k for the given values of r, i, mu, i, sigma i.

1469
01:40:59,040 --> 01:41:00,680
 Because we see the formula, right?

1470
01:41:00,680 --> 01:41:05,480
 Gamma i k is a pskoi.

1471
01:41:05,480 --> 01:41:09,120
 O i contains the mu i, sigma i.

1472
01:41:09,120 --> 01:41:13,800
 So for given value of r, i, mu, i, sigma i,

1473
01:41:13,800 --> 01:41:15,200
 we calculate gamma i k.

1474
01:41:15,200 --> 01:41:18,200
 So this is called expectation step.

1475
01:41:18,200 --> 01:41:20,320
 Then next step is called maximization.

1476
01:41:20,320 --> 01:41:22,960
 After we get r for i k, we should

1477
01:41:22,960 --> 01:41:28,920
 find the values of r for i, mu, i, sigma i to maximize,

1478
01:41:28,920 --> 01:41:31,800
 actually, the loss function.

1479
01:41:31,800 --> 01:41:35,800
 So this step is called, actually, the maximization step.

1480
01:41:35,800 --> 01:41:38,600
 So these two steps, I use iteratively.

1481
01:41:38,600 --> 01:41:43,920
 Starting from expectation step, then we do the maximization.

1482
01:41:43,920 --> 01:41:47,680
 So after we estimate the r for i, mu, i, sigma i,

1483
01:41:47,680 --> 01:41:49,720
 we go back to the estimation step.

1484
01:41:49,720 --> 01:41:53,080
 Based on the new r for i, mu, i, sigma i

1485
01:41:53,080 --> 01:41:54,880
 to calculate new gamma i k.

1486
01:41:54,880 --> 01:41:56,800
 And then based on new gamma i k, we

1487
01:41:56,840 --> 01:41:59,280
 have a new r for i, mu, i, sigma i.

1488
01:41:59,280 --> 01:42:02,840
 So these two steps are implemented, right?

1489
01:42:02,840 --> 01:42:05,320
 It's iteratively.

1490
01:42:05,320 --> 01:42:07,920
 So this is the expectation maximization algorithm.

1491
01:42:12,920 --> 01:42:17,960
 So these are given the detailed steps of the algorithm.

1492
01:42:17,960 --> 01:42:22,800
 And so first step is called initialization.

1493
01:42:22,800 --> 01:42:23,560
 Initialization.

1494
01:42:24,080 --> 01:42:26,400
 OK.

1495
01:42:26,400 --> 01:42:29,840
 So we assume the step 0, the initial step, right?

1496
01:42:29,840 --> 01:42:32,840
 And then for r for i, mu, i, sigma i,

1497
01:42:32,840 --> 01:42:35,040
 and we should give them values, right,

1498
01:42:35,040 --> 01:42:39,000
 so that we can calculate gamma i k in the next step,

1499
01:42:39,000 --> 01:42:40,600
 in the expectation step.

1500
01:42:40,600 --> 01:42:44,480
 So how this way value are given in the initial step?

1501
01:42:44,480 --> 01:42:49,240
 Actually, these values, these are given randomly.

1502
01:42:49,240 --> 01:42:51,920
 In other words, we assign random values

1503
01:42:51,960 --> 01:42:56,280
 for this mu i, sigma i.

1504
01:42:56,280 --> 01:42:58,400
 We have some random values.

1505
01:42:58,400 --> 01:43:02,480
 But an r for i also give a random value.

1506
01:43:02,480 --> 01:43:06,760
 But we should make sure the summation of r or i should be 1,

1507
01:43:06,760 --> 01:43:09,760
 because this is the property of r for i, right?

1508
01:43:09,760 --> 01:43:14,120
 So you can assign some of the values of r, all right?

1509
01:43:14,120 --> 01:43:16,200
 Then finally, you need to make sure

1510
01:43:16,200 --> 01:43:19,720
 that the summation of the actually should be 1.

1511
01:43:22,880 --> 01:43:24,880
 So this is the initial step.

1512
01:43:24,880 --> 01:43:27,880
 So after we have the initial step,

1513
01:43:27,880 --> 01:43:28,840
 we have a foundation.

1514
01:43:28,840 --> 01:43:32,000
 We have basis to calculate gamma i k.

1515
01:43:32,000 --> 01:43:33,600
 So set the j equal to 1.

1516
01:43:33,600 --> 01:43:36,240
 Then we start the first step, expectation, right?

1517
01:43:36,240 --> 01:43:42,880
 Step based on the r for i, 0, r for i, mu i, 0, sigma i, 0,

1518
01:43:42,880 --> 01:43:46,840
 we calculate gamma i k 1, right?

1519
01:43:47,120 --> 01:43:55,720
 So this is the gamma i k, right?

1520
01:43:55,720 --> 01:43:58,840
 This is the gamma i k.

1521
01:43:58,840 --> 01:44:02,120
 SK, OI, so based on these values,

1522
01:44:02,120 --> 01:44:04,640
 given in the previous step, although it's random, right?

1523
01:44:04,640 --> 01:44:09,040
 But if it's given, so for any input, for any sample x,

1524
01:44:09,040 --> 01:44:10,040
 SK is known, right?

1525
01:44:10,040 --> 01:44:11,760
 SK will substitute.

1526
01:44:11,760 --> 01:44:15,480
 Then we can get this gamma i k.

1527
01:44:17,840 --> 01:44:19,040
 OK.

1528
01:44:19,040 --> 01:44:26,400
 So this is the maximization.

1529
01:44:26,400 --> 01:44:29,720
 After we get gamma i k, then the next step,

1530
01:44:29,720 --> 01:44:31,600
 we should calculate.

1531
01:44:31,600 --> 01:44:32,960
 We should calculate, right?

1532
01:44:32,960 --> 01:44:38,320
 So mu i, sigma i, and r i.

1533
01:44:38,320 --> 01:44:40,280
 So this is jamming the step, not the step.

1534
01:44:40,280 --> 01:44:44,080
 Now it's still in the first step, right?

1535
01:44:44,120 --> 01:44:47,800
 After the initialization, in the first step,

1536
01:44:47,800 --> 01:44:49,960
 we start from the expectation.

1537
01:44:49,960 --> 01:44:54,400
 And then we come to the maximization.

1538
01:44:54,400 --> 01:44:58,360
 So jam now equals 1.

1539
01:44:58,360 --> 01:45:02,240
 And then after that, we go back to step 2.

1540
01:45:02,240 --> 01:45:03,920
 Again, expectation, right?

1541
01:45:03,920 --> 01:45:06,000
 But now we need to calculate gamma i k

1542
01:45:06,000 --> 01:45:13,600
 based on the new estimate of the mu i, sigma i, and r for i.

1543
01:45:13,600 --> 01:45:18,000
 So then this pressure we are repeating for many times.

1544
01:45:18,000 --> 01:45:25,400
 Until the stopping criterion is satisfied.

1545
01:45:25,400 --> 01:45:32,160
 So the stopping criterion could be the number of iterations.

1546
01:45:32,160 --> 01:45:36,520
 You see, or based on iterations, if I iterate this

1547
01:45:36,520 --> 01:45:42,040
 pressure for 1,000 times, 1,000 iterations,

1548
01:45:42,120 --> 01:45:43,920
 then we can stop.

1549
01:45:43,920 --> 01:45:49,800
 Then we just run iteratively and perform the expectation step

1550
01:45:49,800 --> 01:45:53,120
 and also the maximization step.

1551
01:45:53,120 --> 01:46:02,640
 Many times until the method step is reached.

1552
01:46:02,640 --> 01:46:04,880
 So this is one criterion.

1553
01:46:04,880 --> 01:46:09,160
 Another criterion is that because at the beginning,

1554
01:46:09,160 --> 01:46:10,840
 we knew the solution is very bad, right?

1555
01:46:11,720 --> 01:46:13,240
 It's given randomly, right?

1556
01:46:13,240 --> 01:46:15,000
 So we would have seen a solution

1557
01:46:15,000 --> 01:46:18,320
 could convert to the optimal solution.

1558
01:46:18,320 --> 01:46:22,760
 And convert, that means after a certain step,

1559
01:46:22,760 --> 01:46:26,000
 the value should stabilize.

1560
01:46:26,000 --> 01:46:27,960
 Stabbed line.

1561
01:46:27,960 --> 01:46:29,760
 And even if there is a change, the change

1562
01:46:29,760 --> 01:46:33,520
 should be very small, a trivial change.

1563
01:46:33,520 --> 01:46:35,320
 So this is another criterion.

1564
01:46:35,320 --> 01:46:38,280
 If after a certain step, the change

1565
01:46:38,280 --> 01:46:46,280
 is of the parameters, including the mu i, sigma i, r for i,

1566
01:46:46,280 --> 01:46:51,280
 and also the gamma i k, all the parameters are stabilized.

1567
01:46:51,280 --> 01:46:53,639
 That means the change is very small from one step

1568
01:46:53,639 --> 01:46:55,280
 to another step.

1569
01:46:55,280 --> 01:46:57,920
 Then we can stop.

1570
01:46:57,920 --> 01:47:05,440
 So this is another criterion we can use in the training

1571
01:47:05,879 --> 01:47:09,480
 in the EM algorithm.

1572
01:47:09,480 --> 01:47:17,519
 So these are the two steps in the EM algorithm.

1573
01:47:17,519 --> 01:47:20,639
 These are very commonly used in machine learning.

1574
01:47:20,639 --> 01:47:26,400
 And so in particular, it isn't like a simple problem,

1575
01:47:26,400 --> 01:47:28,519
 because we just know it's a normal function.

1576
01:47:28,519 --> 01:47:30,879
 But if the function is very more complex,

1577
01:47:30,880 --> 01:47:34,640
 and then actually, normally we don't have a closed form

1578
01:47:34,640 --> 01:47:35,520
 solution.

1579
01:47:35,520 --> 01:47:38,080
 We don't have a closed form solution.

1580
01:47:38,080 --> 01:47:41,600
 And then we need to use iterating method.

1581
01:47:41,600 --> 01:47:45,280
 And then previously, you learned some organization method.

1582
01:47:45,280 --> 01:47:48,680
 We see a partial derivative of j, a partial derivative

1583
01:47:48,680 --> 01:47:52,960
 of the cost function with respect to the parameter,

1584
01:47:52,960 --> 01:47:54,800
 should be 0.

1585
01:47:54,800 --> 01:47:56,960
 Even if there's a loss function, cost function

1586
01:47:56,960 --> 01:47:58,840
 is very complicated.

1587
01:47:59,800 --> 01:48:02,600
 Even if you let this be 0, this equation

1588
01:48:02,600 --> 01:48:05,000
 is very hard to solve.

1589
01:48:05,000 --> 01:48:06,960
 Of course, this example is easy.

1590
01:48:06,960 --> 01:48:09,240
 It could be easier to solve.

1591
01:48:09,240 --> 01:48:13,360
 In such a scenario, we need to use the gradient descent method.

1592
01:48:13,360 --> 01:48:14,320
 Gradient descent.

1593
01:48:14,320 --> 01:48:16,920
 And then you all learned this before.

1594
01:48:16,920 --> 01:48:18,440
 You're an algorithm study, right?

1595
01:48:18,440 --> 01:48:19,920
 Gradient descent method.

1596
01:48:19,920 --> 01:48:23,000
 We always start from an initial position.

1597
01:48:23,000 --> 01:48:28,240
 Then we iteratively actually go to the optimal solution.

1598
01:48:28,280 --> 01:48:29,400
 So this is something like that.

1599
01:48:29,400 --> 01:48:30,920
 It's iterative.

1600
01:48:30,920 --> 01:48:32,679
 We don't have a closed form.

1601
01:48:32,679 --> 01:48:36,840
 But we can use the iterative way to find the solution.

1602
01:48:36,840 --> 01:48:39,480
 So this is the EM algorithm.

1603
01:48:39,480 --> 01:48:41,240
 Commonly used in machine learning.

1604
01:48:41,240 --> 01:48:45,320
 And in particular, long-ing, if the machine learning model

1605
01:48:45,320 --> 01:48:49,920
 develops in place, the parameter is hard to find.

1606
01:48:49,920 --> 01:48:51,840
 We don't have a closed form to estimate

1607
01:48:51,840 --> 01:48:54,320
 the parameters of the machine learning model.

1608
01:48:54,320 --> 01:48:57,759
 So when to turn to the state of expedition,

1609
01:48:57,759 --> 01:49:03,320
 expedition, it's really better to solve the solution.

1610
01:49:09,559 --> 01:49:11,240
 OK, maybe you have a break.

1611
01:49:11,240 --> 01:49:12,240
 Time is a break.

1612
01:49:12,240 --> 01:49:14,040
 After a break, I'll show you an example.

1613
01:50:24,320 --> 01:50:25,719
 I'll go down.

1614
01:50:25,719 --> 01:50:26,219
 Go down.

1615
01:50:54,320 --> 01:50:54,820
 OK.

1616
01:51:24,320 --> 01:51:24,820
 OK.

1617
01:51:54,320 --> 01:51:55,820
 OK.

1618
01:52:24,320 --> 01:52:25,820
 OK.

1619
01:52:54,320 --> 01:52:55,820
 OK.

1620
01:53:24,320 --> 01:53:25,820
 OK.

1621
01:53:54,320 --> 01:53:55,820
 OK.

1622
01:54:24,320 --> 01:54:25,820
 OK.

1623
01:54:54,320 --> 01:54:54,820
 OK.

1624
01:55:24,320 --> 01:55:25,820
 OK.

1625
01:55:54,320 --> 01:55:55,820
 OK.

1626
01:56:24,320 --> 01:56:24,820
 OK.

1627
01:56:54,320 --> 01:56:55,820
 OK.

1628
01:57:24,320 --> 01:57:24,820
 OK.

1629
01:57:54,320 --> 01:58:12,820
 OK.

1630
01:58:24,320 --> 01:58:25,320
 OK.

1631
01:58:54,320 --> 01:58:54,820
 OK.

1632
01:59:24,320 --> 01:59:24,820
 OK.

1633
01:59:54,320 --> 01:59:55,320
 OK.

1634
02:00:24,320 --> 02:00:25,320
 OK.

1635
02:00:54,320 --> 02:01:10,320
 OK.

1636
02:01:10,320 --> 02:01:12,880
 So now that we look at this example,

1637
02:01:12,880 --> 02:01:16,679
 and just now we have seen this in the example, right?

1638
02:01:16,679 --> 02:01:20,480
 And for class two, and certainly, the data

1639
02:01:20,480 --> 02:01:24,200
 is not from one single algorithm function.

1640
02:01:24,200 --> 02:01:26,080
 And actually, it is from two.

1641
02:01:26,080 --> 02:01:28,960
 Because actually, I generated the data from two algorithm

1642
02:01:28,960 --> 02:01:33,000
 functions for the sample in class two.

1643
02:01:33,000 --> 02:01:34,040
 OK.

1644
02:01:34,040 --> 02:01:37,160
 So now, of course, here we know, right?

1645
02:01:37,160 --> 02:01:38,480
 We know we assume we know.

1646
02:01:38,480 --> 02:01:42,920
 Or we know there are two components.

1647
02:01:42,920 --> 02:01:44,800
 There are two function functions.

1648
02:01:44,800 --> 02:01:50,000
 And for the data in the class two.

1649
02:01:50,000 --> 02:01:53,200
 And so in this function, Mr. Model,

1650
02:01:53,200 --> 02:01:56,320
 we should use actually two Gaussian components.

1651
02:01:56,320 --> 02:01:58,800
 So the m should be set to two, right?

1652
02:01:58,800 --> 02:01:59,760
 OK.

1653
02:01:59,760 --> 02:02:04,040
 And so now that we need to estimate,

1654
02:02:04,040 --> 02:02:06,960
 we need to estimate the mean vector and the current

1655
02:02:06,960 --> 02:02:10,400
 mark matrix for each of these two Gaussian components.

1656
02:02:10,400 --> 02:02:14,000
 And also we need to estimate the weightage, right,

1657
02:02:14,000 --> 02:02:16,559
 for this Gaussian component, for each of the two Gaussian

1658
02:02:16,559 --> 02:02:17,559
 components.

1659
02:02:17,559 --> 02:02:19,880
 That is r for 1, r for 2.

1660
02:02:19,880 --> 02:02:20,800
 OK.

1661
02:02:20,800 --> 02:02:24,760
 And so we need to use the so-called expectation

1662
02:02:24,760 --> 02:02:26,720
 maximization algorithm, right?

1663
02:02:26,720 --> 02:02:31,480
 And which is a iterative weight for the parameter estimation.

1664
02:02:31,480 --> 02:02:36,680
 And so in the first step, or before we start the expectation

1665
02:02:36,680 --> 02:02:40,600
 step, we need to assign random values

1666
02:02:40,600 --> 02:02:45,840
 onto the two mean vector, two mean two current matrix,

1667
02:02:45,840 --> 02:02:49,800
 and also the two weight, r for 1, r for 2.

1668
02:02:49,800 --> 02:02:52,040
 So we assume these are random values.

1669
02:02:52,040 --> 02:02:54,000
 So I use a mean two, right?

1670
02:02:54,000 --> 02:02:56,240
 Because this is for class two, right?

1671
02:02:56,240 --> 02:02:57,560
 So I use class two.

1672
02:02:57,560 --> 02:03:00,760
 This is the first component of class two, mean vector.

1673
02:03:00,760 --> 02:03:03,800
 And this is actually the second component of class two.

1674
02:03:03,800 --> 02:03:07,400
 So this is the mean vector for first Gaussian component

1675
02:03:07,400 --> 02:03:08,600
 of class two.

1676
02:03:08,600 --> 02:03:12,160
 This is the second Gaussian component, class two

1677
02:03:12,160 --> 02:03:14,240
 of class two, mean vector.

1678
02:03:14,240 --> 02:03:18,720
 So these are values I generate randomly, randomly.

1679
02:03:18,720 --> 02:03:23,680
 And these current matrix I also generate randomly.

1680
02:03:23,680 --> 02:03:25,600
 So this is an initial step, right?

1681
02:03:25,600 --> 02:03:27,480
 So we assume this is zero, right?

1682
02:03:27,480 --> 02:03:32,400
 This zero means the zero step, the initial step.

1683
02:03:32,400 --> 02:03:36,680
 And this two mean for class two.

1684
02:03:36,680 --> 02:03:39,680
 This one two mean for Gaussian component number one,

1685
02:03:39,680 --> 02:03:43,680
 Gaussian component number two, for class two.

1686
02:03:43,680 --> 02:03:46,200
 These are the r for 1, r for 2.

1687
02:03:46,200 --> 02:03:51,360
 Remember that summation of the two should be 1.

1688
02:03:51,360 --> 02:03:52,280
 OK.

1689
02:03:52,280 --> 02:03:55,320
 So these are the initial step.

1690
02:03:55,320 --> 02:03:59,360
 So after the initialization of these parameters,

1691
02:03:59,360 --> 02:04:02,440
 then we should start the iterative procedure, right?

1692
02:04:02,440 --> 02:04:05,200
 So the first actually the expectation step

1693
02:04:05,200 --> 02:04:10,120
 to estimate the gamma IK, the probability of each sample

1694
02:04:10,120 --> 02:04:14,920
 belonging to omega 1, or 1, belonging to the first component,

1695
02:04:14,920 --> 02:04:17,640
 also the probability to belong to the second Gaussian

1696
02:04:17,640 --> 02:04:20,160
 component.

1697
02:04:20,160 --> 02:04:22,360
 So the expectation step, right?

1698
02:04:22,360 --> 02:04:25,120
 And estimation step.

1699
02:04:25,120 --> 02:04:28,600
 So we have a formula to calculate, right?

1700
02:04:28,600 --> 02:04:32,320
 So actually after one thousand iterations,

1701
02:04:32,320 --> 02:04:37,000
 we assume actually the number of steps, number of iterations

1702
02:04:37,000 --> 02:04:38,840
 is set to one thousand.

1703
02:04:38,840 --> 02:04:42,160
 Then you repeat expectation and maximization,

1704
02:04:42,200 --> 02:04:45,360
 expectation is one thousand times.

1705
02:04:45,360 --> 02:04:48,880
 And so finally, we get this actually

1706
02:04:48,880 --> 02:04:53,240
 the estimate for the mean vector and the current measures.

1707
02:04:53,240 --> 02:04:58,720
 So the two mean vectors is after one thousand step, right?

1708
02:04:58,720 --> 02:05:02,760
 So this is 3.0992 minus 0.932.

1709
02:05:02,760 --> 02:05:05,559
 So this is the first with mean vector.

1710
02:05:05,559 --> 02:05:07,080
 This is the second mean vector, right?

1711
02:05:07,080 --> 02:05:08,599
 Second component.

1712
02:05:08,599 --> 02:05:10,280
 And the first actually the current matrix,

1713
02:05:10,280 --> 02:05:17,240
 the second current matrix, and also actually the width vector.

1714
02:05:17,240 --> 02:05:21,360
 Actually the width vector, so the width of r of 1, r of 2,

1715
02:05:21,360 --> 02:05:27,679
 actually so summation actually is 1.

1716
02:05:27,679 --> 02:05:29,400
 Summation is 1.

1717
02:05:29,400 --> 02:05:30,000
 OK.

1718
02:05:30,000 --> 02:05:32,240
 So this is like 49% something, right?

1719
02:05:32,240 --> 02:05:33,960
 49.5.

1720
02:05:33,960 --> 02:05:38,599
 And this is like 15.48%.

1721
02:05:38,600 --> 02:05:40,720
 So 50, 50 like, right?

1722
02:05:40,720 --> 02:05:42,520
 So OK.

1723
02:05:42,520 --> 02:05:43,880
 And so these are issues.

1724
02:05:50,360 --> 02:05:55,120
 So after we estimate the mean vector current

1725
02:05:55,120 --> 02:05:57,280
 measure for the two Gaussian components, right?

1726
02:05:57,280 --> 02:06:03,400
 Then we can construct actually the density function for class 2.

1727
02:06:03,400 --> 02:06:06,360
 Use the two Gaussian functions to construct

1728
02:06:06,400 --> 02:06:08,719
 the density function for class 2, right?

1729
02:06:08,719 --> 02:06:11,679
 So this is the density function of class 2, right?

1730
02:06:11,679 --> 02:06:14,679
 Continue to Gaussian component.

1731
02:06:14,679 --> 02:06:19,240
 And each of them has a width vector, has a width alpha,

1732
02:06:19,240 --> 02:06:22,839
 has a mean vector and current matrix.

1733
02:06:22,839 --> 02:06:23,400
 OK.

1734
02:06:23,400 --> 02:06:24,920
 So you can see there are two, right?

1735
02:06:24,920 --> 02:06:26,759
 So two Gaussian components.

1736
02:06:26,759 --> 02:06:36,160
 And each of them has its own mean vector and current matrix.

1737
02:06:37,160 --> 02:06:41,320
 And therefore class 1, I think for class 1,

1738
02:06:41,320 --> 02:06:45,760
 you just want Gaussian component, right?

1739
02:06:45,760 --> 02:06:48,800
 So we use all the sum of to estimate the mean vector,

1740
02:06:48,800 --> 02:06:51,519
 just average for all the samples.

1741
02:06:51,519 --> 02:06:53,920
 After we can estimate the current matrix.

1742
02:06:53,920 --> 02:06:59,320
 So this is the density function for class 1.

1743
02:06:59,320 --> 02:07:03,000
 So class 1 just has one Gaussian function.

1744
02:07:03,000 --> 02:07:05,000
 For class 2, we have two.

1745
02:07:05,000 --> 02:07:09,320
 Actually, class 2 is a Gaussian function, right?

1746
02:07:09,320 --> 02:07:11,760
 It's a width summation for two Gaussian functions.

1747
02:07:18,480 --> 02:07:21,880
 So these are the shows, the mean vector.

1748
02:07:21,880 --> 02:07:29,280
 And for the two, actually, component in class 2.

1749
02:07:29,280 --> 02:07:31,560
 Actually, from here, you can see this actually

1750
02:07:31,560 --> 02:07:33,760
 indeed provides a good estimate, right?

1751
02:07:33,760 --> 02:07:37,720
 For the two, actually, the mean vectors

1752
02:07:37,720 --> 02:07:40,280
 of the two Gaussian components, right?

1753
02:07:40,280 --> 02:07:42,360
 So this should be a good estimate, right?

1754
02:07:42,360 --> 02:07:44,160
 You just know roughly, you know,

1755
02:07:44,160 --> 02:07:45,640
 this is in the center part, right?

1756
02:07:45,640 --> 02:07:47,360
 Roughly this is in the center part.

1757
02:07:47,360 --> 02:07:52,040
 But actually, beginning of this values are randomly given.

1758
02:07:52,040 --> 02:07:54,160
 But actually, after many iterations, right?

1759
02:07:54,160 --> 02:07:56,680
 So they convert, actually, to the, indeed,

1760
02:07:56,680 --> 02:08:00,040
 to the true almost, and then close to the true values.

1761
02:08:00,360 --> 02:08:03,360
 OK.

1762
02:08:03,360 --> 02:08:03,960
 OK.

1763
02:08:03,960 --> 02:08:05,400
 So these are not the real one.

1764
02:08:05,400 --> 02:08:10,960
 It's just the mean vector for the class 1.

1765
02:08:10,960 --> 02:08:11,960
 OK.

1766
02:08:11,960 --> 02:08:17,840
 So now, actually, we see width of probability, right?

1767
02:08:17,840 --> 02:08:21,160
 Actually, we combine the probability

1768
02:08:21,160 --> 02:08:22,880
 and width of density function.

1769
02:08:22,880 --> 02:08:26,680
 And we can calculate the G-shape function, G1,

1770
02:08:26,680 --> 02:08:29,320
 actually, G2x, right?

1771
02:08:29,320 --> 02:08:35,759
 So in this question problem, example, class 1 has 100 samples.

1772
02:08:35,759 --> 02:08:38,280
 Class 2 has 200 samples.

1773
02:08:38,280 --> 02:08:41,040
 And then, actually, the probability for class 1

1774
02:08:41,040 --> 02:08:42,080
 is 1.

1775
02:08:42,080 --> 02:08:45,400
 Probability for class 2, probability, right?

1776
02:08:45,400 --> 02:08:48,960
 To the, becoming a 200 samples.

1777
02:08:48,960 --> 02:08:49,599
 OK.

1778
02:08:49,599 --> 02:08:52,679
 Then now, we use both the prior probability

1779
02:08:52,679 --> 02:08:54,160
 and density function.

1780
02:08:54,160 --> 02:08:59,639
 And actually, to actually estimate to obtain the,

1781
02:08:59,639 --> 02:09:01,760
 this, this function, G, right?

1782
02:09:01,760 --> 02:09:03,760
 OK.

1783
02:09:03,760 --> 02:09:06,680
 So this is G1x, right?

1784
02:09:06,680 --> 02:09:09,080
 P1x, 1.2, G2x.

1785
02:09:09,080 --> 02:09:11,840
 So we use the drunk probability of omega, right?

1786
02:09:11,840 --> 02:09:17,040
 And x, that's the, this one function, right?

1787
02:09:17,040 --> 02:09:19,800
 Now, G1x, we have G2x.

1788
02:09:19,800 --> 02:09:23,000
 So for any given x, for any actual x,

1789
02:09:23,000 --> 02:09:26,200
 either it's a training sample or testing sample, right?

1790
02:09:26,200 --> 02:09:28,400
 We can always substitute each of the formula

1791
02:09:28,400 --> 02:09:33,040
 into the equation to calculate G1x and G2x.

1792
02:09:33,040 --> 02:09:36,400
 And then, we can compare the size of the two values,

1793
02:09:36,400 --> 02:09:38,360
 two density functions, right?

1794
02:09:38,360 --> 02:09:42,560
 If G1x is greater, then we solve sample to class 1.

1795
02:09:42,560 --> 02:09:47,400
 If G2x is greater, we assign the sample to class 2, OK?

1796
02:09:47,400 --> 02:09:52,240
 So for this function, examples are generated

1797
02:09:52,360 --> 02:09:57,040
 at 40,000 data points in this range, from 1 to 4 to 6.

1798
02:09:57,040 --> 02:09:59,719
 We'll put x1x to the two features, OK?

1799
02:09:59,719 --> 02:10:05,040
 So for each of the samples, I substitute into the G1 and G2.

1800
02:10:05,040 --> 02:10:07,480
 And then, based on G1 and G2, finally,

1801
02:10:07,480 --> 02:10:11,559
 I get a class label, a class prediction, OK?

1802
02:10:11,559 --> 02:10:13,760
 So finally, this is a range, OK?

1803
02:10:13,760 --> 02:10:17,360
 For class 2, a yellow region.

1804
02:10:17,360 --> 02:10:21,639
 For class 1, the green region, OK?

1805
02:10:21,640 --> 02:10:25,840
 And so indeed, actually, from this example,

1806
02:10:25,840 --> 02:10:29,160
 I'm going to show you the Gaussian mixture model,

1807
02:10:29,160 --> 02:10:33,960
 actually build a good decision boundary, right?

1808
02:10:33,960 --> 02:10:37,520
 For the sample in the two classes, OK?

1809
02:10:40,520 --> 02:10:48,080
 So this is actually Gaussian mixture model used for class 2.

1810
02:10:48,080 --> 02:10:53,519
 Because class 2 contains more than one Gaussian component.

1811
02:10:53,519 --> 02:10:57,120
 OK, we use the EM, already, to estimate the value

1812
02:10:57,120 --> 02:10:58,480
 of the two Gaussian components.

1813
02:11:01,720 --> 02:11:03,960
 So these are shown, of course, in practice, right?

1814
02:11:03,960 --> 02:11:06,200
 Normally, we cannot see the data, right?

1815
02:11:06,200 --> 02:11:11,280
 We don't know how many Gaussian components.

1816
02:11:11,280 --> 02:11:13,160
 Normally, we always start from 1, right?

1817
02:11:13,160 --> 02:11:14,880
 We always assume that only 1.

1818
02:11:14,880 --> 02:11:17,080
 Normally, at least actually, at the first step,

1819
02:11:17,080 --> 02:11:18,720
 we assume there are 1, right?

1820
02:11:18,720 --> 02:11:20,200
 We try this method.

1821
02:11:20,200 --> 02:11:23,920
 We try it with just one Gaussian component for class 1,

1822
02:11:23,920 --> 02:11:25,960
 one Gaussian component for class 2.

1823
02:11:25,960 --> 02:11:29,760
 And then we build, actually, the, we construct the density

1824
02:11:29,760 --> 02:11:32,200
 function for each of the two classes.

1825
02:11:32,200 --> 02:11:35,519
 And then, based on density function and also the probability

1826
02:11:35,519 --> 02:11:39,680
 to construct the displacement function, which

1827
02:11:39,680 --> 02:11:41,400
 is a drunk probability, right?

1828
02:11:41,400 --> 02:11:44,960
 OK, so then we look at the result on the training,

1829
02:11:44,960 --> 02:11:46,960
 on the testing, OK?

1830
02:11:47,160 --> 02:11:50,600
 And so here, of course, we assume we don't know, right?

1831
02:11:50,600 --> 02:11:53,640
 For class 1, for class 1, we assume 1.

1832
02:11:53,640 --> 02:11:57,360
 For class 2, we also assume only one Gaussian component.

1833
02:11:57,360 --> 02:12:00,920
 OK, then, actually, for class 2, we

1834
02:12:00,920 --> 02:12:03,760
 estimate this is the mean vector,

1835
02:12:03,760 --> 02:12:06,760
 at least the current matrix.

1836
02:12:06,760 --> 02:12:11,880
 OK, and then, based on this, only one, the single Gaussian

1837
02:12:11,880 --> 02:12:16,120
 function to construct, actually, the density function

1838
02:12:16,240 --> 02:12:17,840
 for class 2.

1839
02:12:17,840 --> 02:12:24,599
 OK, and so this is the result, actually, when only one,

1840
02:12:24,599 --> 02:12:27,920
 actually, Gaussian function, Gaussian component

1841
02:12:27,920 --> 02:12:30,040
 is used for class 2.

1842
02:12:30,040 --> 02:12:33,519
 So this is the result.

1843
02:12:33,519 --> 02:12:36,120
 You can see the density of boundary is a bit different,

1844
02:12:36,120 --> 02:12:36,720
 right?

1845
02:12:36,720 --> 02:12:38,400
 So here, this is the density of boundary.

1846
02:12:41,120 --> 02:12:42,960
 OK, and this is the density of boundary.

1847
02:12:46,559 --> 02:12:48,240
 You can see the difference, right?

1848
02:12:48,240 --> 02:12:48,740
 OK.

1849
02:12:51,440 --> 02:12:53,440
 OK, so actually, I just mentioned,

1850
02:12:53,440 --> 02:12:56,440
 also, one of the students asked me during the break,

1851
02:12:56,440 --> 02:12:59,040
 actually, if we don't know m, how do you tell?

1852
02:12:59,040 --> 02:13:02,040
 In practice, we indeed don't know the m.

1853
02:13:02,040 --> 02:13:04,240
 Just now, in the previous slide, I already showed,

1854
02:13:04,240 --> 02:13:08,040
 if we don't know m, we assume 1, then we can see the result.

1855
02:13:08,040 --> 02:13:12,200
 So actually, in practice, you don't know, of course,

1856
02:13:12,200 --> 02:13:12,960
 you can try.

1857
02:13:12,960 --> 02:13:14,280
 We should try 1.

1858
02:13:14,280 --> 02:13:15,880
 1 can go first, right?

1859
02:13:15,880 --> 02:13:17,320
 Then we look at the performance.

1860
02:13:17,320 --> 02:13:19,680
 Then we see, oh, whether the performance is good or not,

1861
02:13:19,680 --> 02:13:20,360
 right?

1862
02:13:20,360 --> 02:13:22,840
 If actually, we see the performance of the testing data

1863
02:13:22,840 --> 02:13:27,680
 of our requirement, then we can just use 1.

1864
02:13:27,680 --> 02:13:30,000
 If actually, the result, actually,

1865
02:13:30,000 --> 02:13:33,000
 does not satisfy our requirement, OK,

1866
02:13:33,000 --> 02:13:36,400
 we want to have a better result, or actually, we

1867
02:13:36,400 --> 02:13:38,360
 want to have an optimal result.

1868
02:13:38,360 --> 02:13:40,000
 Then we should try 2, right?

1869
02:13:40,000 --> 02:13:40,800
 We should try 2.

1870
02:13:40,800 --> 02:13:41,680
 We should try 3.

1871
02:13:41,680 --> 02:13:43,080
 We should try more.

1872
02:13:43,080 --> 02:13:43,840
 OK.

1873
02:13:43,840 --> 02:13:46,280
 So here, we already tried 2, right?

1874
02:13:46,280 --> 02:13:47,400
 Actually, 2.

1875
02:13:47,400 --> 02:13:52,080
 Now we look at the model, too.

1876
02:13:52,080 --> 02:13:54,840
 We assume there are three.

1877
02:13:54,840 --> 02:13:55,840
 There are three.

1878
02:13:55,840 --> 02:13:56,480
 OK.

1879
02:13:56,480 --> 02:14:02,920
 So these are the iterative estimation

1880
02:14:02,920 --> 02:14:04,760
 using the E and all, right?

1881
02:14:04,760 --> 02:14:08,200
 So finally, we get the main vector for the three,

1882
02:14:08,200 --> 02:14:11,680
 actually, Gauchen-Kabone.

1883
02:14:11,680 --> 02:14:12,180
 OK.

1884
02:14:12,180 --> 02:14:14,140
 And see, this is only for class 2.

1885
02:14:14,140 --> 02:14:16,100
 I just pulled the sample in class 2.

1886
02:14:16,100 --> 02:14:18,180
 So this one, indeed, is here, right?

1887
02:14:18,180 --> 02:14:19,260
 Is it here?

1888
02:14:19,260 --> 02:14:21,820
 And then another one is here.

1889
02:14:21,820 --> 02:14:22,580
 Another one is here.

1890
02:14:22,580 --> 02:14:24,540
 The two is actually, main vector,

1891
02:14:24,540 --> 02:14:26,860
 close to each other.

1892
02:14:26,860 --> 02:14:29,740
 If there are, we assume there are two.

1893
02:14:29,740 --> 02:14:30,580
 OK.

1894
02:14:30,580 --> 02:14:34,100
 And particularly, we can see, actually,

1895
02:14:34,100 --> 02:14:38,220
 I think we should consider the value here for the R-class.

1896
02:14:38,220 --> 02:14:40,620
 The R-class.

1897
02:14:40,620 --> 02:14:46,860
 You can see the R-class value, R-class 1, is 0.0523.

1898
02:14:46,860 --> 02:14:49,260
 In other words, actually, this is actually R-class.

1899
02:14:49,260 --> 02:14:52,580
 Normally, it's weighted, right?

1900
02:14:52,580 --> 02:14:53,580
 Like 5.

1901
02:14:53,580 --> 02:14:56,460
 0.05, like 5%.

1902
02:14:56,460 --> 02:14:57,220
 OK.

1903
02:14:57,220 --> 02:15:01,059
 But look at other two Kaupo-net R-class values.

1904
02:15:01,059 --> 02:15:02,940
 1 is 0.05, right?

1905
02:15:02,940 --> 02:15:10,580
 And other 0.44, which are significantly greater than R-class 1.

1906
02:15:11,059 --> 02:15:14,340
 So this is the case, my three I used.

1907
02:15:14,340 --> 02:15:16,300
 Actually, from this example, we can see,

1908
02:15:16,300 --> 02:15:20,580
 oh, for one of the R-class, the weightage of that Kaupo-net

1909
02:15:20,580 --> 02:15:22,019
 is very small.

1910
02:15:22,019 --> 02:15:24,260
 Only 0.05, 5%.

1911
02:15:24,260 --> 02:15:26,380
 Because summation 2.0 should be 1, right?

1912
02:15:26,380 --> 02:15:28,620
 So here, among the three, this one

1913
02:15:28,620 --> 02:15:35,980
 is very significantly smaller than other two,

1914
02:15:35,980 --> 02:15:38,340
 significantly, right?

1915
02:15:38,380 --> 02:15:40,940
 Other 10 times of this weight.

1916
02:15:45,700 --> 02:15:48,660
 So from here, we can roughly judge how many Kaupo-net

1917
02:15:48,660 --> 02:15:49,780
 we should use, right?

1918
02:15:49,780 --> 02:15:52,460
 We already use.

1919
02:15:52,460 --> 02:15:54,940
 So now, actually, we can see another scenario.

1920
02:15:54,940 --> 02:15:58,420
 We assume there are five.

1921
02:15:58,420 --> 02:16:01,580
 So now we assume one or three, right?

1922
02:16:01,580 --> 02:16:03,060
 Now we assume five.

1923
02:16:03,060 --> 02:16:08,260
 And then these are the vectors, center vectors.

1924
02:16:08,260 --> 02:16:11,740
 So we see the mean vector for the five Kaupo-net.

1925
02:16:11,740 --> 02:16:12,980
 And then why is it here, right?

1926
02:16:12,980 --> 02:16:17,020
 In the center, and here, this one, the center, maybe

1927
02:16:17,020 --> 02:16:19,940
 this one just cover a few samples, right?

1928
02:16:19,940 --> 02:16:22,540
 Then we can have a mean vector of this.

1929
02:16:22,540 --> 02:16:25,180
 In the calculation of the mean vector,

1930
02:16:25,180 --> 02:16:27,340
 we use all the samples.

1931
02:16:27,340 --> 02:16:29,740
 But we have a gamma IK, right?

1932
02:16:29,740 --> 02:16:32,380
 Gamma IK, that means the sample K

1933
02:16:32,380 --> 02:16:35,100
 belong to this R-class, OK?

1934
02:16:35,100 --> 02:16:37,580
 And from here, we can see if the value,

1935
02:16:38,260 --> 02:16:39,820
 if the mean vector is here.

1936
02:16:39,820 --> 02:16:45,020
 Actually, this is most likely the gamma, the gamma value

1937
02:16:45,020 --> 02:16:46,420
 for the sample, right?

1938
02:16:46,420 --> 02:16:51,700
 Belong to this cluster, it's close to zero.

1939
02:16:51,700 --> 02:16:54,379
 Otherwise, actually, the center could not be here, right?

1940
02:16:54,379 --> 02:16:56,379
 Could not be actually here.

1941
02:16:56,379 --> 02:16:57,580
 Or it could not be here.

1942
02:16:57,580 --> 02:17:01,059
 Maybe only a few samples, actually, in this region

1943
02:17:01,059 --> 02:17:05,420
 have non-zero gamma IKs.

1944
02:17:05,420 --> 02:17:09,379
 But then finally, the location of the mean vector is here.

1945
02:17:09,379 --> 02:17:13,299
 And similarly, for this, this is where it's

1946
02:17:13,299 --> 02:17:14,460
 almost in the center, right?

1947
02:17:14,460 --> 02:17:16,100
 In the center.

1948
02:17:16,100 --> 02:17:20,059
 And this data is far away from the center,

1949
02:17:20,059 --> 02:17:23,020
 is near the boundary, right here.

1950
02:17:23,020 --> 02:17:29,180
 This means that most of the gammas IK, right?

1951
02:17:29,180 --> 02:17:30,540
 Close to zero.

1952
02:17:30,540 --> 02:17:33,299
 Only a few, actually.

1953
02:17:33,299 --> 02:17:34,420
 Yeah, if it's non-zero.

1954
02:17:34,420 --> 02:17:38,299
 Then finally, maybe only these samples have a non-zero gamma

1955
02:17:38,299 --> 02:17:39,139
 IK.

1956
02:17:39,139 --> 02:17:42,299
 Then finally, the average is located here.

1957
02:17:45,020 --> 02:17:48,340
 So these are the scenarios when five Gaussian components

1958
02:17:48,340 --> 02:17:50,180
 are used for class two.

1959
02:17:53,420 --> 02:17:58,180
 OK, then we look at the values of R5.

1960
02:17:58,180 --> 02:18:02,500
 So R5, 1, 0.4670.

1961
02:18:02,500 --> 02:18:07,460
 And then, actually, 0.03, very small, right?

1962
02:18:07,460 --> 02:18:12,500
 Then another 0.05, 0.02, and then another 0.04.

1963
02:18:12,500 --> 02:18:16,540
 You can see these values are 20 times of the small,

1964
02:18:16,540 --> 02:18:18,100
 smaller value, right?

1965
02:18:18,100 --> 02:18:20,420
 Or at least 10 times.

1966
02:18:20,420 --> 02:18:25,220
 So from these, actually, we can see only two components

1967
02:18:25,220 --> 02:18:27,820
 have significant greater values.

1968
02:18:28,580 --> 02:18:33,180
 So from these, probably, we can judge only two, actually,

1969
02:18:33,180 --> 02:18:35,820
 Gaussian components, which probably

1970
02:18:35,820 --> 02:18:37,820
 are the underlying data.

1971
02:18:37,820 --> 02:18:43,699
 OK, so we achieve, the R5 values provide information

1972
02:18:43,699 --> 02:18:53,379
 about the number of, actually, the number of, the number

1973
02:18:53,379 --> 02:18:56,860
 of Gaussian components.

1974
02:18:57,620 --> 02:18:59,460
 And in the data, or number of components,

1975
02:18:59,460 --> 02:19:04,460
 we should use to build the density function for that class.

1976
02:19:07,460 --> 02:19:09,500
 And of course, some of you ask, oh, in this question,

1977
02:19:09,500 --> 02:19:12,620
 alpha 1, and in that question, it's alpha 2.

1978
02:19:12,620 --> 02:19:13,860
 I think they don't matter.

1979
02:19:13,860 --> 02:19:15,900
 Finally, you just put that current component number 1,

1980
02:19:15,900 --> 02:19:16,860
 number 2.

1981
02:19:16,860 --> 02:19:18,340
 Number 1 is number 1.

1982
02:19:18,340 --> 02:19:20,340
 Or number 1, number 2, it doesn't really matter, right?

1983
02:19:20,340 --> 02:19:23,020
 You just finally all put them together, right?

1984
02:19:23,020 --> 02:19:24,860
 So you don't see, oh, in the previous example,

1985
02:19:24,860 --> 02:19:28,060
 number 1, number 2 or 3 have big value.

1986
02:19:28,060 --> 02:19:32,300
 But in this example, number 2, 3 have small value.

1987
02:19:32,300 --> 02:19:34,820
 Because initially, number 1, all the values

1988
02:19:34,820 --> 02:19:36,500
 are generally randomly, right?

1989
02:19:36,500 --> 02:19:38,060
 In the other step, OK?

1990
02:19:38,060 --> 02:19:42,020
 So which one is number 1, number 2, really doesn't matter.

1991
02:19:42,020 --> 02:19:45,860
 Because finally, it's summation of the two.

1992
02:19:45,860 --> 02:19:51,180
 So these are alpha, actually, provide information, right?

1993
02:19:51,180 --> 02:19:54,620
 Which can be used to determine the suitable number

1994
02:19:54,620 --> 02:19:59,340
 of component or Gaussian functions we can use,

1995
02:19:59,340 --> 02:20:02,260
 or we should use to construct the density function.

1996
02:20:08,260 --> 02:20:09,220
 OK.

1997
02:20:09,220 --> 02:20:11,260
 So now we look at this, right?

1998
02:20:11,260 --> 02:20:17,700
 So actually, in this case, I showed that in 4 class 2,

1999
02:20:17,700 --> 02:20:21,740
 for class 1, I used actually two Gaussian components.

2000
02:20:21,740 --> 02:20:28,020
 Although, no, the true scenario, that only one Gaussian

2001
02:20:28,020 --> 02:20:29,180
 component, right?

2002
02:20:29,180 --> 02:20:30,100
 For class 1.

2003
02:20:30,100 --> 02:20:31,340
 But here, we assume there are two.

2004
02:20:31,340 --> 02:20:34,300
 Because we have no idea how many we should use, right?

2005
02:20:34,300 --> 02:20:36,740
 We assume there are two for class 1.

2006
02:20:36,740 --> 02:20:39,700
 There are five for class 2.

2007
02:20:39,700 --> 02:20:42,820
 And then we build the density function.

2008
02:20:42,820 --> 02:20:46,380
 And finally, we have the density function.

2009
02:20:46,380 --> 02:20:48,859
 Compared with the previous designer boundary,

2010
02:20:48,859 --> 02:20:52,699
 certainly, the density boundary is much more complex, right?

2011
02:20:52,699 --> 02:20:55,859
 You see, zig-zag, not like previous smooth.

2012
02:20:55,859 --> 02:20:58,259
 Then I think that, right?

2013
02:20:58,259 --> 02:21:01,500
 So in one sense, this is more accurate.

2014
02:21:01,500 --> 02:21:06,619
 But you can see that, actually, we have a risk

2015
02:21:06,619 --> 02:21:11,420
 and to overfit the training data.

2016
02:21:11,420 --> 02:21:13,060
 So you can see this, right?

2017
02:21:13,060 --> 02:21:15,259
 You see that this is much more complex.

2018
02:21:15,340 --> 02:21:18,940
 And actually, you can see for the training data, maybe

2019
02:21:18,940 --> 02:21:20,820
 the error is significantly reduced.

2020
02:21:20,820 --> 02:21:23,260
 Actually, we can cut how many errors, right?

2021
02:21:23,260 --> 02:21:27,700
 So now, only this sample, this sample, actually,

2022
02:21:27,700 --> 02:21:28,700
 are misclassified.

2023
02:21:28,700 --> 02:21:32,100
 This should be in the green region, right?

2024
02:21:32,100 --> 02:21:33,260
 Now, they are yellow region.

2025
02:21:33,260 --> 02:21:35,220
 So this sample was misclassified.

2026
02:21:35,220 --> 02:21:36,540
 This is misclassified.

2027
02:21:36,540 --> 02:21:40,540
 Therefore, others, 1, 2, 3, 4, 5, 6, 7, 8.

2028
02:21:40,540 --> 02:21:44,260
 Maybe now, only 10 samples will be classified wrong.

2029
02:21:44,260 --> 02:21:50,980
 Previously, we have 14 or 15 samples that are misclassified.

2030
02:21:50,980 --> 02:21:54,180
 So certainly, by using more golden components,

2031
02:21:54,180 --> 02:22:00,140
 actually, we have a more complex decision boundary.

2032
02:22:00,140 --> 02:22:03,140
 And then we can have a better performance on the training

2033
02:22:03,140 --> 02:22:04,940
 data.

2034
02:22:04,940 --> 02:22:07,420
 But this does not mean, actually, that this is actually

2035
02:22:07,420 --> 02:22:08,940
 a better classified.

2036
02:22:08,940 --> 02:22:12,060
 This is a better, actually, density function.

2037
02:22:12,060 --> 02:22:19,340
 And because, actually, this has actually more complex or more

2038
02:22:19,340 --> 02:22:21,980
 golden components are used, we have a risk

2039
02:22:21,980 --> 02:22:25,420
 to overfit the training data, which

2040
02:22:25,420 --> 02:22:29,220
 will lead to non-backed generalization performance

2041
02:22:29,220 --> 02:22:33,539
 on the unseen testing data.

2042
02:22:33,539 --> 02:22:34,180
 OK.

2043
02:22:34,180 --> 02:22:38,420
 In particular, even the more, how much is more, right?

2044
02:22:38,420 --> 02:22:41,340
 We see 1 is suitable, 2 is suitable.

2045
02:22:41,340 --> 02:22:44,260
 If we use 3 or 5, maybe just actually,

2046
02:22:44,260 --> 02:22:46,780
 this is not very excessive.

2047
02:22:46,780 --> 02:22:48,500
 But even if the number is excessive,

2048
02:22:48,500 --> 02:22:50,940
 like the ordinary only 2 is sufficient,

2049
02:22:50,940 --> 02:22:54,660
 but you use 20, you use 30, you can imagine

2050
02:22:54,660 --> 02:22:57,740
 how complex could be the decision boundary.

2051
02:22:57,740 --> 02:22:59,380
 You can see the change, right?

2052
02:22:59,380 --> 02:23:00,900
 Usually, it's very smooth.

2053
02:23:00,900 --> 02:23:04,340
 If we use 2, actually, then actually, less smooth.

2054
02:23:04,340 --> 02:23:08,740
 Now, we use 5, 2 for class 1, 5 for class 2.

2055
02:23:08,740 --> 02:23:11,820
 You can see this decision boundary is more complex, right?

2056
02:23:11,820 --> 02:23:14,500
 Think about that.

2057
02:23:14,500 --> 02:23:16,100
 OK.

2058
02:23:16,100 --> 02:23:19,860
 So then in practice, how to determine?

2059
02:23:19,860 --> 02:23:20,580
 How to determine?

2060
02:23:20,580 --> 02:23:22,020
 We cannot see the data.

2061
02:23:22,020 --> 02:23:22,940
 We cannot see the data.

2062
02:23:22,940 --> 02:23:27,020
 We cannot know the decision boundary, how complex it is.

2063
02:23:27,020 --> 02:23:27,780
 How to determine?

2064
02:23:31,220 --> 02:23:31,940
 OK.

2065
02:23:31,940 --> 02:23:33,860
 And actually, in the last week, actually, when we see,

2066
02:23:33,860 --> 02:23:35,539
 we have some data, right?

2067
02:23:35,540 --> 02:23:39,740
 And the data normally are divided into two parts,

2068
02:23:39,740 --> 02:23:42,020
 by training and testing.

2069
02:23:42,020 --> 02:23:46,580
 And actually, sometimes, we divide the training,

2070
02:23:46,580 --> 02:23:47,740
 part of the field, part of the training

2071
02:23:47,740 --> 02:23:49,500
 as a validation data.

2072
02:23:49,500 --> 02:23:51,660
 So the data tree is basically divided

2073
02:23:51,660 --> 02:23:56,540
 into three parts, training, validation, and testing.

2074
02:23:56,540 --> 02:24:00,300
 And normally, the training could be 60% of data for training,

2075
02:24:00,300 --> 02:24:08,060
 and 20% for validation, and 20% for testing.

2076
02:24:08,060 --> 02:24:13,300
 Or 70% for training, 15% for validation,

2077
02:24:13,300 --> 02:24:16,420
 and 15% for testing.

2078
02:24:16,420 --> 02:24:18,779
 Then what's the usage for validation?

2079
02:24:18,779 --> 02:24:21,140
 How validation data could be used?

2080
02:24:21,140 --> 02:24:23,580
 Actually, I remember, actually, in the classes,

2081
02:24:23,580 --> 02:24:28,580
 I said validation data is used to determine

2082
02:24:28,580 --> 02:24:31,260
 the hyperparameters.

2083
02:24:31,260 --> 02:24:33,700
 So which is called hyperparameters?

2084
02:24:33,700 --> 02:24:35,340
 Hyperparameters cannot be directly

2085
02:24:35,340 --> 02:24:37,740
 estimated from the data.

2086
02:24:37,740 --> 02:24:42,860
 So in this example, mu i, sigma i, r i

2087
02:24:42,860 --> 02:24:45,580
 can be estimated from the data.

2088
02:24:45,580 --> 02:24:52,100
 But the number of components in the density function,

2089
02:24:52,100 --> 02:24:57,340
 this m, is a hyperparameter.

2090
02:24:57,380 --> 02:25:01,980
 So normally, we first set this hyperparameter, set this to 2,

2091
02:25:01,980 --> 02:25:06,380
 set this to 5, and then based on this set value,

2092
02:25:06,380 --> 02:25:15,820
 then we estimate sigma i, mu i, r i, sigma i, r i, mu i,

2093
02:25:15,820 --> 02:25:18,100
 into preset data.

2094
02:25:18,100 --> 02:25:20,380
 So this is a parameter called hyperparameter.

2095
02:25:20,380 --> 02:25:23,820
 Actually, we use validation data to determine

2096
02:25:23,820 --> 02:25:26,140
 this hyperparameter.

2097
02:25:26,140 --> 02:25:32,740
 In other words, we set m to 2, then we train the classifier.

2098
02:25:32,740 --> 02:25:35,019
 Then we look at the performance on the validation data.

2099
02:25:35,019 --> 02:25:39,539
 The validation data is not used for the parameter estimation.

2100
02:25:39,539 --> 02:25:41,859
 You look at performance.

2101
02:25:41,859 --> 02:25:44,500
 And then we set r to m to 3.

2102
02:25:44,500 --> 02:25:51,099
 Then we find the center vectors, the current matrix,

2103
02:25:51,099 --> 02:25:55,140
 the alphas for the three Gaussian components.

2104
02:25:55,140 --> 02:25:57,580
 And then we construct density function.

2105
02:25:57,580 --> 02:26:01,660
 And then after that, we perform classification

2106
02:26:01,660 --> 02:26:03,140
 on the validation data.

2107
02:26:03,140 --> 02:26:04,859
 Then we look at the performance.

2108
02:26:04,859 --> 02:26:08,980
 And then again, we look at the performance.

2109
02:26:08,980 --> 02:26:13,019
 m is set to 4, to 5, to more larger number.

2110
02:26:13,019 --> 02:26:17,779
 And finally, we find that we should use a value of m.

2111
02:26:17,779 --> 02:26:23,699
 That the performance on the validation data is the best.

2112
02:26:23,700 --> 02:26:31,460
 Normally, if you use less than necessary number of Gaussian

2113
02:26:31,460 --> 02:26:34,140
 components, the performance on the validation data

2114
02:26:34,140 --> 02:26:34,820
 could be better.

2115
02:26:34,820 --> 02:26:37,500
 Even on the train data could be better.

2116
02:26:37,500 --> 02:26:41,020
 If you use the excessive number of Gaussian components

2117
02:26:41,020 --> 02:26:46,860
 in the density function, then the performance on the validation

2118
02:26:46,860 --> 02:26:48,460
 data could be better.

2119
02:26:48,460 --> 02:26:53,980
 Because the model will affect the train data.

2120
02:26:53,980 --> 02:26:58,140
 So I should have a number of m could be determined

2121
02:26:58,140 --> 02:27:02,339
 based on the performance of the validation data.

2122
02:27:02,339 --> 02:27:07,820
 And the difference should be the number of m.

2123
02:27:07,820 --> 02:27:09,820
 So this is the determination of m.

2124
02:27:09,820 --> 02:27:11,580
 So these are observations.

2125
02:27:11,580 --> 02:27:15,820
 So when you set the number, even true is 2.

2126
02:27:15,820 --> 02:27:20,860
 If you use 3 or 4, that makes much difference.

2127
02:27:20,860 --> 02:27:23,580
 But if you use excessive, you know it's 2.

2128
02:27:23,580 --> 02:27:25,699
 If you use 20, use 10.

2129
02:27:25,699 --> 02:27:29,500
 But this excessive number of Gaussian components

2130
02:27:29,500 --> 02:27:33,660
 are used, the decision boundary becomes more complex,

2131
02:27:33,660 --> 02:27:36,340
 which represents the best performance on the train data,

2132
02:27:36,340 --> 02:27:40,420
 but the worst performance on the validation data.

2133
02:27:40,420 --> 02:27:43,660
 So this is an overfitting problem.

2134
02:27:43,660 --> 02:27:44,660
 Overfitting.

2135
02:27:52,660 --> 02:27:54,420
 OK, so why overfitting?

2136
02:27:54,420 --> 02:27:57,820
 Why do you do the overfitting?

2137
02:27:57,820 --> 02:27:59,060
 You can imagine.

2138
02:27:59,060 --> 02:28:02,740
 If you use a more Gaussian component,

2139
02:28:02,740 --> 02:28:09,060
 actually we could have more unknown parameters.

2140
02:28:09,060 --> 02:28:11,020
 We could have more unknown parameters.

2141
02:28:11,020 --> 02:28:13,539
 But the number of train assemblies is just limited.

2142
02:28:14,420 --> 02:28:16,940
 But actually when you use a more Gaussian component,

2143
02:28:16,940 --> 02:28:18,900
 then you have more examples.

2144
02:28:18,900 --> 02:28:21,700
 Then we could have overfitted data.

2145
02:28:21,700 --> 02:28:26,100
 I always want to actually use one example to illustrate

2146
02:28:26,100 --> 02:28:28,140
 what is overfitting.

2147
02:28:28,140 --> 02:28:31,540
 Just now, we have two parameters, a1 and a2.

2148
02:28:31,540 --> 02:28:34,300
 a1 plus a2 equals 1.

2149
02:28:34,300 --> 02:28:36,740
 We have two parameters, a1, a2.

2150
02:28:36,740 --> 02:28:39,060
 We have one equation.

2151
02:28:39,060 --> 02:28:40,980
 You have numerous solutions.

2152
02:28:40,980 --> 02:28:43,340
 You have numerous solutions.

2153
02:28:43,340 --> 02:28:47,540
 You always achieve, you set about the equation.

2154
02:28:47,540 --> 02:28:49,380
 You have numerous solutions.

2155
02:28:49,380 --> 02:28:51,140
 And actually, when the parameter estimation,

2156
02:28:51,140 --> 02:28:55,540
 normally one sample gives one equation.

2157
02:28:55,540 --> 02:28:59,780
 If the number of parameters is more than the number of equations,

2158
02:28:59,780 --> 02:29:06,300
 number of samples, then we could have numerous solutions.

2159
02:29:06,300 --> 02:29:07,980
 So that's the known overfitting.

2160
02:29:07,980 --> 02:29:10,900
 The number of, because we use a more component,

2161
02:29:10,940 --> 02:29:14,300
 we have more unknown parameters.

2162
02:29:14,300 --> 02:29:17,180
 And then probably the unknown number of parameters

2163
02:29:17,180 --> 02:29:22,380
 could be greater than the number of samples.

2164
02:29:22,380 --> 02:29:24,340
 Greater than the number of samples.

2165
02:29:24,340 --> 02:29:26,220
 So these are overfitting.

2166
02:29:26,220 --> 02:29:28,500
 So we can always link these two equations.

2167
02:29:28,500 --> 02:29:31,380
 a1 plus a2, one equation, we have two parameters.

2168
02:29:31,380 --> 02:29:33,980
 We can always satisfy this data.

2169
02:29:33,980 --> 02:29:36,060
 Always satisfy this train data.

2170
02:29:36,859 --> 02:29:39,220
 OK.

2171
02:29:39,220 --> 02:29:42,019
 So this is actually the one scenario.

2172
02:29:46,300 --> 02:29:49,740
 So another observation that the use of set number

2173
02:29:49,740 --> 02:29:53,539
 of Gaussian components, residing some small, actually,

2174
02:29:53,539 --> 02:29:55,260
 alpha values.

2175
02:29:55,260 --> 02:29:58,180
 So these are not probably the mean we use to determine

2176
02:29:58,180 --> 02:30:01,660
 the superman number of the Gaussian component.

2177
02:30:01,660 --> 02:30:05,019
 If alpha is too small, then it means that this alpha

2178
02:30:05,020 --> 02:30:10,100
 may only actually cover a few examples.

2179
02:30:10,100 --> 02:30:12,220
 So this contribution of this component

2180
02:30:12,220 --> 02:30:17,700
 is neglectable.

2181
02:30:17,700 --> 02:30:20,540
 So we can just ignore this.

2182
02:30:20,540 --> 02:30:23,820
 So based on alpha values, we have to determine

2183
02:30:23,820 --> 02:30:27,660
 suitable number of Gaussian components.

2184
02:30:27,660 --> 02:30:30,740
 But actually, in practice, no, actually, suitable number,

2185
02:30:30,740 --> 02:30:32,420
 what is suitable number?

2186
02:30:32,420 --> 02:30:33,980
 It could be an arrangement.

2187
02:30:33,980 --> 02:30:38,060
 So use 2, use 3, use 4.

2188
02:30:38,060 --> 02:30:40,060
 That will make much difference.

2189
02:30:40,060 --> 02:30:44,060
 So it's in an arrangement normally.

2190
02:30:44,060 --> 02:30:46,660
 So here it's called excessive number.

2191
02:30:46,660 --> 02:30:49,900
 Excessive, actually.

2192
02:30:49,900 --> 02:30:50,300
 OK.

2193
02:30:50,300 --> 02:30:56,460
 So this is the Gaussian-Mister model.

2194
02:30:56,460 --> 02:30:59,260
 So that's actually, I talked about another method.

2195
02:30:59,260 --> 02:31:01,740
 We call it naive-based.

2196
02:31:01,740 --> 02:31:04,180
 Naive-based.

2197
02:31:04,180 --> 02:31:07,100
 So it's a base theorem, a base design theory.

2198
02:31:07,100 --> 02:31:09,340
 Also based on probabilities.

2199
02:31:09,340 --> 02:31:11,500
 But actually, it's just called naive.

2200
02:31:11,500 --> 02:31:14,180
 Why is it called naive?

2201
02:31:14,180 --> 02:31:18,260
 And actually, we look at this base theorem.

2202
02:31:18,260 --> 02:31:20,860
 And this is the posterior probability, right?

2203
02:31:20,860 --> 02:31:25,260
 Probability of omega z and the given x.

2204
02:31:25,260 --> 02:31:29,460
 So if we look at the numerator part,

2205
02:31:29,460 --> 02:31:31,660
 the numerator part is a joint probability, right?

2206
02:31:31,660 --> 02:31:35,820
 px omega j, p omega j.

2207
02:31:35,820 --> 02:31:42,780
 And so here, actually, we assume the x is a vector.

2208
02:31:42,780 --> 02:31:48,020
 We use a multiple features or attributes.

2209
02:31:48,020 --> 02:31:52,660
 So now we look at the px omega j.

2210
02:31:52,660 --> 02:31:54,539
 And then you multiply it, right?

2211
02:31:54,539 --> 02:31:57,940
 It's a joint probability of x and omega j.

2212
02:31:57,940 --> 02:32:00,140
 So the joint probability is OK.

2213
02:32:00,140 --> 02:32:02,940
 And this x, we expand x.

2214
02:32:02,940 --> 02:32:07,300
 So this is x1, x2, and xd, that omega j.

2215
02:32:07,300 --> 02:32:11,940
 So here, actually, the joint probability of x1, x2, xd,

2216
02:32:11,940 --> 02:32:14,700
 omega j.

2217
02:32:14,700 --> 02:32:17,539
 And so now, actually, we can, again,

2218
02:32:17,539 --> 02:32:19,300
 we use this formula, right?

2219
02:32:19,300 --> 02:32:24,060
 We can use, actually, like x2, x3, xd, omega j as a condition.

2220
02:32:24,060 --> 02:32:25,580
 And then the probability x1.

2221
02:32:25,580 --> 02:32:30,980
 Then times this px2, x3, xd, omega j.

2222
02:32:30,980 --> 02:32:32,900
 So this is similar to this, right?

2223
02:32:32,900 --> 02:32:33,980
 So we have this formula.

2224
02:32:33,980 --> 02:32:40,380
 OK, now we can also, for this part, px2, x3, xd, omega j,

2225
02:32:40,380 --> 02:32:40,880
 right?

2226
02:32:40,880 --> 02:32:44,099
 Actually, this is also a joint probability of x2, xd,

2227
02:32:44,099 --> 02:32:45,420
 and omega j, right?

2228
02:32:45,420 --> 02:32:49,019
 So we can, this should be equal to the probability of x2

2229
02:32:49,060 --> 02:32:54,660
 given this condition, times the probability of this condition.

2230
02:32:54,660 --> 02:32:57,300
 And similarly, for x3, right?

2231
02:32:57,300 --> 02:32:59,260
 We can calculate this part, actually, for,

2232
02:32:59,260 --> 02:33:04,020
 should be x3 given the condition x4, xd, omega j,

2233
02:33:04,020 --> 02:33:05,980
 times the probability of this.

2234
02:33:05,980 --> 02:33:12,380
 So finally, we can expand this into some px1 and the condition

2235
02:33:12,380 --> 02:33:13,820
 x2, x3, xd.

2236
02:33:13,820 --> 02:33:18,460
 So px2 and condition x3, xd, times the condition px, actually,

2237
02:33:18,460 --> 02:33:20,860
 d, omega j.

2238
02:33:20,860 --> 02:33:22,940
 Then times the px, omega j.

2239
02:33:22,940 --> 02:33:24,820
 So this is just the expansion, right?

2240
02:33:24,820 --> 02:33:28,940
 The expansion of the joint probability of x and omega j.

2241
02:33:28,940 --> 02:33:31,220
 It can be into such a form.

2242
02:33:31,220 --> 02:33:32,020
 OK.

2243
02:33:32,020 --> 02:33:38,779
 So in a naive base, we assume all the features are independent.

2244
02:33:38,779 --> 02:33:40,740
 You remember in the last week, actually,

2245
02:33:40,740 --> 02:33:43,060
 now we see in this progression of the data, right?

2246
02:33:43,060 --> 02:33:47,340
 We see one of these flourishing is a scatter plot.

2247
02:33:47,340 --> 02:33:50,619
 So one variable feature against another feature, right?

2248
02:33:50,619 --> 02:33:52,220
 We plot the data division.

2249
02:33:52,220 --> 02:33:57,580
 So from this scatter plot, we can judge the relationship

2250
02:33:57,580 --> 02:34:00,580
 to the two features, the correlation, right?

2251
02:34:00,580 --> 02:34:03,820
 Sometimes the two features are uncorrelated.

2252
02:34:03,820 --> 02:34:04,939
 They are independent.

2253
02:34:04,939 --> 02:34:07,580
 But sometimes they are severely correlated.

2254
02:34:07,580 --> 02:34:09,340
 They are independent.

2255
02:34:09,340 --> 02:34:09,840
 OK.

2256
02:34:09,840 --> 02:34:11,779
 So this is the exploration in the last week, right?

2257
02:34:11,779 --> 02:34:12,939
 We explored this.

2258
02:34:12,940 --> 02:34:13,940
 The scatter plot.

2259
02:34:13,940 --> 02:34:14,540
 OK.

2260
02:34:14,540 --> 02:34:20,700
 So now we assume the data, the features, are interdependent.

2261
02:34:20,700 --> 02:34:21,380
 Interdependent.

2262
02:34:21,380 --> 02:34:27,940
 That means the conditional probability of x1, x2, x3.

2263
02:34:27,940 --> 02:34:33,900
 That means the probability of x1 is not affected by x2, x3, xd.

2264
02:34:33,900 --> 02:34:39,100
 The probability of x2 is not affected by x3, x4, and xd.

2265
02:34:39,100 --> 02:34:41,340
 Because we assume the data, the features

2266
02:34:41,340 --> 02:34:45,780
 are independent.

2267
02:34:45,780 --> 02:34:49,460
 So if they are independent, so we assume the features

2268
02:34:49,460 --> 02:34:50,780
 are independent, OK?

2269
02:34:50,780 --> 02:34:52,980
 Then this probability, we can simply

2270
02:34:52,980 --> 02:34:58,860
 even know this x1, just equal to x1, omega, j.

2271
02:34:58,860 --> 02:34:59,580
 OK.

2272
02:34:59,580 --> 02:35:04,060
 So because the features are independent, OK?

2273
02:35:04,060 --> 02:35:07,580
 So the x1 is not affected by x2, x2, x3.

2274
02:35:07,580 --> 02:35:13,060
 So this should be equal to px, omega, j.

2275
02:35:13,060 --> 02:35:15,980
 And similarly, actually for arbitrary features,

2276
02:35:15,980 --> 02:35:16,480
 right?

2277
02:35:16,480 --> 02:35:19,300
 xi, xi, we have this result.

2278
02:35:19,300 --> 02:35:22,180
 So now we summarize the substitute result,

2279
02:35:22,180 --> 02:35:24,620
 actually, for the general probability, right?

2280
02:35:24,620 --> 02:35:27,020
 Just now we see the general probability include,

2281
02:35:27,020 --> 02:35:29,100
 finally expanding to this part, right?

2282
02:35:29,100 --> 02:35:32,820
 But now, based on the assumption, actually, x1, x2, xd,

2283
02:35:32,820 --> 02:35:33,940
 are independent.

2284
02:35:33,940 --> 02:35:39,980
 Then this becomes actually x1, actually, and omega, j.

2285
02:35:39,980 --> 02:35:42,660
 This should be x2 and omega, j.

2286
02:35:42,660 --> 02:35:45,300
 So finally, so we have this result.

2287
02:35:48,980 --> 02:35:50,220
 OK.

2288
02:35:50,220 --> 02:35:52,980
 We have this result.

2289
02:35:52,980 --> 02:35:56,860
 So the general probability of a feature vector x and omega,

2290
02:35:56,860 --> 02:36:00,700
 j, actually, can be estimated based

2291
02:36:00,700 --> 02:36:05,060
 on the product of each individual feature, right?

2292
02:36:05,060 --> 02:36:08,620
 Each individual feature x1, we estimate the general probability

2293
02:36:08,620 --> 02:36:10,980
 with omega, j.

2294
02:36:10,980 --> 02:36:11,480
 OK.

2295
02:36:11,480 --> 02:36:12,940
 Then the modifications.

2296
02:36:16,260 --> 02:36:17,100
 OK.

2297
02:36:17,100 --> 02:36:20,540
 So when you want to estimate the general probability

2298
02:36:20,540 --> 02:36:23,740
 of the feature vector x and omega, j, OK?

2299
02:36:23,740 --> 02:36:28,140
 Based on the assumption of interdependence, right?

2300
02:36:28,140 --> 02:36:30,180
 Of all the features.

2301
02:36:30,180 --> 02:36:33,340
 Finally, actually, this general probability

2302
02:36:33,340 --> 02:36:36,780
 can be estimated based on this modification of each,

2303
02:36:36,780 --> 02:36:40,020
 general probability of each individual feature

2304
02:36:40,020 --> 02:36:43,700
 with actually the class of omega, j.

2305
02:36:46,580 --> 02:36:47,220
 OK.

2306
02:36:47,220 --> 02:36:51,940
 So this is the general probability based on the assumption.

2307
02:36:51,940 --> 02:36:57,060
 So now we start to this result into the base theorem,

2308
02:36:57,060 --> 02:36:59,180
 base theorem, right?

2309
02:36:59,180 --> 02:37:01,699
 The posterior probability, actually, in the numerator part,

2310
02:37:01,699 --> 02:37:04,820
 it's just the general probability of the feature vector x

2311
02:37:04,820 --> 02:37:07,500
 and the class omega, j.

2312
02:37:07,500 --> 02:37:09,260
 But based on the assumption, so this

2313
02:37:09,260 --> 02:37:15,660
 is reduced to the multiplication of each individual feature,

2314
02:37:15,660 --> 02:37:19,980
 general probability with the omega, j with the class.

2315
02:37:20,980 --> 02:37:21,480
 OK.

2316
02:37:25,779 --> 02:37:29,859
 Actually, then what's the advantage of this?

2317
02:37:29,859 --> 02:37:30,779
 What's the advantage?

2318
02:37:30,779 --> 02:37:32,820
 Actually, OK.

2319
02:37:32,820 --> 02:37:37,060
 So actually, the advantage is that for now,

2320
02:37:37,060 --> 02:37:39,900
 if you look at this, right?

2321
02:37:39,900 --> 02:37:43,460
 For each individual feature, if we assume it

2322
02:37:43,460 --> 02:37:49,180
 for a normal distribution, then how many unknown parameters?

2323
02:37:50,939 --> 02:37:53,300
 Two, why is the mean value not vector?

2324
02:37:53,300 --> 02:37:56,460
 Mean value, another is the mean, is the standard value

2325
02:37:56,460 --> 02:37:57,580
 of the variance.

2326
02:37:57,580 --> 02:38:01,140
 So for each feature, for each feature,

2327
02:38:01,140 --> 02:38:04,980
 or for each attribute, if we assume for a normal distribution,

2328
02:38:04,980 --> 02:38:07,340
 we have two parameters.

2329
02:38:07,340 --> 02:38:09,740
 But totally, we have d features.

2330
02:38:09,740 --> 02:38:14,580
 Then totally, we have a 2d parameters, 2d.

2331
02:38:14,580 --> 02:38:15,340
 2d, right?

2332
02:38:15,340 --> 02:38:16,859
 Only 2d.

2333
02:38:16,860 --> 02:38:20,580
 But even imagine, actually, now if we think about it,

2334
02:38:20,580 --> 02:38:23,380
 if we don't assume the unit dependence, right?

2335
02:38:23,380 --> 02:38:26,060
 So we have the current matrix.

2336
02:38:26,060 --> 02:38:27,740
 We have the mean vector.

2337
02:38:27,740 --> 02:38:30,500
 For mean vector, how many parameters?

2338
02:38:30,500 --> 02:38:32,540
 D. So vector, right?

2339
02:38:32,540 --> 02:38:34,060
 D parameters.

2340
02:38:34,060 --> 02:38:36,460
 But how about the current matrix?

2341
02:38:36,460 --> 02:38:40,140
 D times D, right?

2342
02:38:40,140 --> 02:38:42,900
 If we assume, actually, we have 100 features.

2343
02:38:42,900 --> 02:38:46,500
 Then 100 times 100 is 10,000.

2344
02:38:46,540 --> 02:38:49,100
 Then divide by 2, because the current matrix is symmetric.

2345
02:38:49,100 --> 02:38:51,260
 OK, divide by 2.

2346
02:38:51,260 --> 02:38:55,580
 So 5000 plus 100 for the mean vector.

2347
02:38:55,580 --> 02:38:59,100
 5100 parameters.

2348
02:38:59,100 --> 02:39:00,900
 We need to estimate the number of parameters

2349
02:39:00,900 --> 02:39:03,260
 from the training samples.

2350
02:39:03,260 --> 02:39:06,380
 But if we assume the interdependency, right?

2351
02:39:06,380 --> 02:39:10,100
 Between the features, each feature has two parameters.

2352
02:39:10,100 --> 02:39:15,220
 Totally, we have 200 unknown parameters.

2353
02:39:15,260 --> 02:39:17,580
 If we use the feature vector, right?

2354
02:39:17,580 --> 02:39:19,420
 5100.

2355
02:39:19,420 --> 02:39:22,420
 But now we just have 200.

2356
02:39:22,420 --> 02:39:25,140
 Actually, the number is significantly reduced.

2357
02:39:25,140 --> 02:39:27,260
 OK, so although in practice, they

2358
02:39:27,260 --> 02:39:29,699
 mean not necessarily true.

2359
02:39:29,699 --> 02:39:31,859
 This assumption for interdependence.

2360
02:39:31,859 --> 02:39:36,460
 OK, but actually, in practice, the result of the 9-bit

2361
02:39:36,460 --> 02:39:38,060
 is quite good.

2362
02:39:38,060 --> 02:39:39,779
 OK, so this is the reason.

2363
02:39:39,779 --> 02:39:43,140
 This 9-bit, I think in all the 12-boxes,

2364
02:39:43,180 --> 02:39:46,500
 we can find this actually, 9-bit, this method.

2365
02:39:46,500 --> 02:39:47,820
 It's very useful.

2366
02:39:47,820 --> 02:39:49,380
 Although the assumption, right?

2367
02:39:49,380 --> 02:39:50,740
 Actually, simple another problem.

2368
02:39:50,740 --> 02:39:52,460
 It seems that, actually, this assumption

2369
02:39:52,460 --> 02:39:57,260
 may be not violated in practice.

2370
02:39:57,260 --> 02:40:00,180
 But actually, the result is good in practice.

2371
02:40:00,180 --> 02:40:03,140
 So although we see that one is in principle,

2372
02:40:03,140 --> 02:40:05,260
 in theory, it's optimal.

2373
02:40:05,260 --> 02:40:07,859
 But because of the number of training samples is limited,

2374
02:40:07,859 --> 02:40:09,340
 we just have 1,000 data.

2375
02:40:09,340 --> 02:40:15,860
 But we have to estimate 5,100, actually, sample,

2376
02:40:15,860 --> 02:40:17,460
 the unknown parameter, right?

2377
02:40:17,460 --> 02:40:19,540
 This is just one example.

2378
02:40:19,540 --> 02:40:21,660
 If the number of features is 1,000,

2379
02:40:21,660 --> 02:40:23,380
 so this could be not.

2380
02:40:23,380 --> 02:40:29,300
 The explosion of the number of samples,

2381
02:40:29,300 --> 02:40:32,100
 the number of unknown parameters, right?

2382
02:40:32,100 --> 02:40:36,380
 So although in theory, you can estimate this.

2383
02:40:36,380 --> 02:40:38,340
 But actually, in practice, this value

2384
02:40:38,340 --> 02:40:41,420
 is quite far from the true value.

2385
02:40:41,420 --> 02:40:43,740
 Because the number of training samples is too small,

2386
02:40:43,740 --> 02:40:49,340
 compared with the number of unknown parameters.

2387
02:40:49,340 --> 02:40:54,700
 OK, so I think, yeah, that's maybe a little bit of this.

2388
02:40:54,700 --> 02:40:59,220
 Top of 9-bit classifier.

2389
02:40:59,220 --> 02:41:04,340
 OK, so last week, we said the features

2390
02:41:04,340 --> 02:41:07,460
 can be categorized into different categories, right?

2391
02:41:08,419 --> 02:41:10,980
 Quantitative, qualitative, quantitative like,

2392
02:41:10,980 --> 02:41:14,580
 also can be, let's say, continuous features and discrete.

2393
02:41:14,580 --> 02:41:16,220
 For continuous features, actually,

2394
02:41:16,220 --> 02:41:18,619
 we can assume the following number of user, right?

2395
02:41:18,619 --> 02:41:19,699
 We follow normal user.

2396
02:41:19,699 --> 02:41:22,699
 But for discrete features, we cannot assume the following

2397
02:41:22,699 --> 02:41:23,939
 normal user, right?

2398
02:41:23,939 --> 02:41:25,380
 For discrete, OK?

2399
02:41:25,380 --> 02:41:28,060
 So for the naive base here, OK?

2400
02:41:28,060 --> 02:41:33,259
 So for continuous features, we can have a Gaussian naive base.

2401
02:41:33,259 --> 02:41:36,259
 And then for discrete features, we certainly cannot

2402
02:41:36,260 --> 02:41:38,140
 use the Gaussian function.

2403
02:41:38,140 --> 02:41:39,900
 So what function we can use?

2404
02:41:39,900 --> 02:41:44,980
 We can have the Bernoulli naive base, where the data,

2405
02:41:44,980 --> 02:41:47,660
 the feature, just have two values.

2406
02:41:47,660 --> 02:41:54,100
 If agenda is a feature, male or female, just two values.

2407
02:41:54,100 --> 02:41:59,420
 Then I use the Bernoulli naive base.

2408
02:41:59,420 --> 02:42:01,900
 If the feature could have multiple values,

2409
02:42:01,900 --> 02:42:05,940
 but it's still discrete, we can use the so-called

2410
02:42:05,980 --> 02:42:10,780
 multinomial naive base, OK?

2411
02:42:10,780 --> 02:42:15,420
 So the second one, the Bernoulli naive and the

2412
02:42:15,420 --> 02:42:20,380
 multinomial naive base are all for discrete features,

2413
02:42:20,380 --> 02:42:23,820
 discrete, and Gaussian for continuous.

2414
02:42:23,820 --> 02:42:28,740
 So that is the reason why we should differentiate the feature.

2415
02:42:28,740 --> 02:42:31,260
 Before, actually, you apply the data to the machine

2416
02:42:31,260 --> 02:42:32,420
 system, right?

2417
02:42:32,420 --> 02:42:34,940
 So the learning algorithm, we should explore the data.

2418
02:42:34,940 --> 02:42:37,860
 We should identify the top of the data, right?

2419
02:42:37,860 --> 02:42:38,940
 Which feature is continuous?

2420
02:42:38,940 --> 02:42:42,860
 Which feature is discrete?

2421
02:42:42,860 --> 02:42:43,540
 OK.

2422
02:42:43,540 --> 02:42:45,900
 And then, you know, if you naive, we

2423
02:42:45,900 --> 02:42:48,340
 should use different methods, different ways

2424
02:42:48,340 --> 02:42:50,220
 to estimate the data functions.

2425
02:42:53,300 --> 02:42:53,900
 OK.

2426
02:42:53,900 --> 02:42:56,900
 I think that for the first one, I think

2427
02:42:56,900 --> 02:42:58,900
 there are no need to introduce the algorithm,

2428
02:42:58,900 --> 02:43:00,580
 because in the first case, we already

2429
02:43:00,580 --> 02:43:04,540
 introduced the univariate Gaussian function, right?

2430
02:43:05,020 --> 02:43:07,380
 So we no need for the first one.

2431
02:43:07,380 --> 02:43:11,060
 And for the second one, the one, we will introduce in the next

2432
02:43:11,060 --> 02:43:12,780
 week, OK?

2433
02:43:12,780 --> 02:43:14,780
 And today is the festival, all right?

2434
02:43:14,780 --> 02:43:18,060
 Probably, you guys are all not some event there, right?

2435
02:43:18,060 --> 02:43:19,660
 And no?

2436
02:43:19,660 --> 02:43:21,860
 Probably, I need a winner garden.

2437
02:43:21,860 --> 02:43:23,020
 Not there, OK.

2438
02:43:23,020 --> 02:43:27,340
 So we classed, actually, we classed earlier.

2439
02:43:27,340 --> 02:43:29,380
 And can go home now?

2440
02:43:29,380 --> 02:43:30,020
 Yeah.

2441
02:43:30,020 --> 02:43:30,740
 Thank you.

2442
02:43:30,740 --> 02:43:34,740
 APPLAUSE

2443
02:45:00,740 --> 02:45:02,740
 I

2444
02:45:31,680 --> 02:45:33,680
 Thank you

2445
02:45:49,580 --> 02:45:51,580
 You

2446
02:46:00,740 --> 02:46:02,740
 I'm not sure what he's talking about.

2447
02:46:02,740 --> 02:46:05,740
 He's talking about the hair that's being cut.

2448
02:46:05,740 --> 02:46:07,740
 I'm not sure what he's talking about.

2449
02:46:07,740 --> 02:46:09,740
 I don't know what he's talking about.

2450
02:46:09,740 --> 02:46:11,740
 I'm not sure what he's talking about.

2451
02:46:11,740 --> 02:46:13,740
 I'm not sure what he's talking about.

2452
02:46:13,740 --> 02:46:15,740
 I'm not sure what he's talking about.

2453
02:46:15,740 --> 02:46:17,740
 I'm not sure what he's talking about.

2454
02:46:17,740 --> 02:46:19,740
 I'm not sure what he's talking about.

2455
02:46:19,740 --> 02:46:21,740
 I'm not sure what he's talking about.

2456
02:46:21,740 --> 02:46:23,740
 I'm not sure what he's talking about.

2457
02:46:23,740 --> 02:46:25,740
 I'm not sure what he's talking about.

2458
02:46:25,740 --> 02:46:27,740
 I'm not sure what he's talking about.

2459
02:46:27,740 --> 02:46:29,740
 I'm not sure what he's talking about.

2460
02:46:29,740 --> 02:46:32,740
 He's talking about the hair that keeps getting cut.

2461
02:46:32,740 --> 02:46:34,740
 I'm not sure what he's talking about.

2462
02:46:34,740 --> 02:46:36,740
 I'm not sure what he's talking about.

2463
02:46:36,740 --> 02:46:39,740
 I'm not sure what goes around coming from that hair.

2464
02:46:39,740 --> 02:46:42,740
 When would you want one?

2465
02:46:42,740 --> 02:46:44,740
 That's time then?

2466
02:46:44,740 --> 02:46:46,740
 Or this?

2467
02:46:46,740 --> 02:46:48,740
 Where is the, where is the, where, where would the hair come?

2468
02:46:48,740 --> 02:46:54,740
 I'm not sure.

2469
02:46:54,740 --> 02:46:57,740
 I'm not sure what he's talking about.

2470
02:46:57,740 --> 02:47:03,340
 Doctor don't get into the test

2471
02:47:03,340 --> 02:47:05,900
 The only way!

2472
02:47:17,600 --> 02:47:20,320
 You look at this, no of this

2473
02:47:20,640 --> 02:47:21,880
 See that trend?

2474
02:47:22,480 --> 02:47:24,340
 I said that I be there

2475
02:47:26,440 --> 02:47:27,440
 Where are you going?

2476
02:47:28,660 --> 02:47:29,680
 Go home

2477
02:47:31,440 --> 02:47:32,580
 What plan do you have to make?

2478
02:47:33,340 --> 02:47:37,800
 Mom started to see him.

2479
02:47:37,800 --> 02:47:57,800
 Make her你的家

2480
02:49:57,800 --> 02:50:01,080
 because he rejected me, so I walked down here.

2481
02:50:04,359 --> 02:50:06,000
 So what's the time?

2482
02:50:06,000 --> 02:50:07,199
 8PM.

2483
02:50:08,480 --> 02:50:09,519
 Hi.

2484
02:50:09,519 --> 02:50:09,759
 Hi.

2485
02:50:09,759 --> 02:50:11,039
 Hi.

2486
02:50:11,039 --> 02:50:12,439
 It's afternoon now?

2487
02:50:12,439 --> 02:50:13,560
 Hm.

2488
02:50:13,599 --> 02:50:16,039
 Prepare for school.

2489
02:50:16,640 --> 02:50:18,279
 I'm tired of that.

2490
02:50:18,560 --> 02:50:20,359
 Speaking from my laptop.

2491
02:50:22,640 --> 02:50:24,640
 Are you sure?

2492
02:50:24,640 --> 02:50:25,519
 Hm.

2493
02:50:26,960 --> 02:50:27,759
 Hm.

2494
02:50:27,800 --> 02:50:29,800
 I'm not sure what you're talking about.

2495
02:50:29,800 --> 02:50:31,800
 What?

2496
02:50:31,800 --> 02:50:33,800
 I'm not sure what you're talking about.

2497
02:50:33,800 --> 02:50:35,800
 I'm just saying.

2498
02:50:35,800 --> 02:50:37,800
 Oh, no.

2499
02:50:41,800 --> 02:50:43,800
 You're not coming?

2500
02:50:43,800 --> 02:50:45,800
 Yeah, you're not coming.

2501
02:50:45,800 --> 02:50:47,800
 What?

2502
02:50:47,800 --> 02:50:49,800
 I'm just saying.

2503
02:50:49,800 --> 02:50:51,800
 I'm not going to...

2504
02:50:51,800 --> 02:50:53,800
 So, um...

2505
02:51:15,800 --> 02:51:17,800
 This is very big.

2506
02:51:21,800 --> 02:51:25,160
 oh

2507
02:51:25,160 --> 02:51:27,160
 Oh

2508
02:51:30,140 --> 02:51:32,140
 We

2509
02:51:44,640 --> 02:51:46,640
 Yeah

2510
02:51:55,160 --> 02:51:57,160
 I'm sorry, I'm sorry.

2511
02:51:57,160 --> 02:51:59,160
 I'm sorry.

2512
02:51:59,160 --> 02:52:01,160
 I'm sorry.

2513
02:52:01,160 --> 02:52:03,160
 I'm sorry.

2514
02:52:03,160 --> 02:52:05,160
 I'm sorry.

2515
02:52:05,160 --> 02:52:07,160
 I'm sorry.

2516
02:52:07,160 --> 02:52:09,160
 I'm sorry.

2517
02:52:09,160 --> 02:52:11,160
 I'm sorry.

2518
02:52:11,160 --> 02:52:13,160
 I'm sorry.

2519
02:52:13,160 --> 02:52:15,160
 I'm sorry.

2520
02:52:15,160 --> 02:52:17,160
 I'm sorry.

2521
02:52:17,160 --> 02:52:19,160
 I'm sorry.

2522
02:52:19,160 --> 02:52:21,160
 I'm sorry.

2523
02:52:21,160 --> 02:52:23,160
 I'm sorry.

2524
02:52:23,160 --> 02:52:25,160
 I'm already a 아 Ugh

2525
02:52:25,160 --> 02:52:27,160
 I'm already a are

2526
02:52:27,160 --> 02:52:29,160
 ouch

2527
02:52:29,160 --> 02:52:31,160
 haven't

2528
02:52:31,160 --> 02:52:39,160
 got

2529
02:52:39,160 --> 02:52:48,660
 the

2530
02:52:48,660 --> 02:52:50,660
 wait

2531
02:52:51,440 --> 02:53:00,280
 (*

2532
02:53:00,280 --> 02:53:14,520
 ...

2533
02:53:14,520 --> 02:53:16,520
 What is this?

2534
02:53:16,520 --> 02:53:18,520
 It's Chinese.

2535
02:53:18,520 --> 02:53:20,520
 What?

2536
02:53:24,520 --> 02:53:26,520
 It's Chinese.

2537
02:53:34,520 --> 02:53:36,520
 What is this?

2538
02:53:38,520 --> 02:53:40,520
 What?

2539
02:53:40,520 --> 02:53:42,520
 What?

2540
02:53:42,520 --> 02:53:46,520
 It hides luggage optimum on other side of it.

2541
02:53:48,520 --> 02:53:51,520
 Can you see it?

2542
02:53:51,520 --> 02:53:54,520
 You still can.

2543
02:54:00,520 --> 02:54:03,520
 Can you see it?

2544
02:54:05,520 --> 02:54:08,520
 No,

2545
02:55:38,520 --> 02:55:45,520
 We're here to find out what's going on.

2546
02:55:45,520 --> 02:55:52,520
 We're here to find out what's going on.

2547
02:55:52,520 --> 02:56:00,520
 We're here to find out what's going on.

2548
02:56:00,520 --> 02:56:10,520
 We're here to find out what's going on.

2549
02:56:10,520 --> 02:56:20,520
 We're here to find out what's going on.

2550
02:56:20,520 --> 02:56:30,520
 We're here to find out what's going on.

2551
02:56:30,520 --> 02:56:40,520
 We're here to find out what's going on.

2552
02:56:40,520 --> 02:56:50,520
 We're here to find out what's going on.

2553
02:56:50,520 --> 02:57:00,520
 We're here to find out what's going on.

2554
02:57:00,520 --> 02:57:10,520
 We're here to find out what's going on.

2555
02:57:10,520 --> 02:57:20,520
 We're here to find out what's going on.

2556
02:57:20,520 --> 02:57:30,520
 We're here to find out what's going on.

2557
02:57:30,520 --> 02:57:40,520
 We're here to find out what's going on.

2558
02:57:40,520 --> 02:57:50,520
 We're here to find out what's going on.

2559
02:57:50,520 --> 02:58:00,520
 We're here to find out what's going on.

2560
02:58:00,520 --> 02:58:10,520
 We're here to find out what's going on.

2561
02:58:10,520 --> 02:58:20,520
 We're here to find out what's going on.

2562
02:58:20,520 --> 02:58:30,520
 We're here to find out what's going on.

2563
02:58:30,520 --> 02:58:40,520
 We're here to find out what's going on.

2564
02:58:40,520 --> 02:58:50,520
 We're here to find out what's going on.

2565
02:58:50,520 --> 02:59:00,520
 We're here to find out what's going on.

2566
02:59:00,520 --> 02:59:02,520
 So what?

2567
02:59:02,520 --> 02:59:04,520
 No.

2568
02:59:04,520 --> 02:59:08,520
 You're a alarm.

2569
02:59:08,520 --> 02:59:12,520
 You're a alarm.

2570
02:59:12,520 --> 02:59:20,520
 You're a alarm.

2571
02:59:20,520 --> 02:59:22,520
 What else?

2572
02:59:22,520 --> 02:59:24,520
 No.

2573
02:59:24,520 --> 02:59:34,520
 You're a alarm.

2574
02:59:34,520 --> 02:59:44,520
 You're a alarm.

2575
02:59:44,520 --> 02:59:54,520
 You're a alarm.

2576
02:59:54,520 --> 03:00:04,520
 You're a alarm.

