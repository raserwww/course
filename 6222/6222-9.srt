1
00:00:00,000 --> 00:00:02,000
 you

2
00:01:00,000 --> 00:01:07,000
 are

3
00:01:07,000 --> 00:01:09,000
 .

4
00:01:13,000 --> 00:01:15,000
 Last week we studied the

5
00:01:18,000 --> 00:01:20,000
 traditional neural network, the

6
00:01:20,000 --> 00:01:22,000
 M.A.R.P.

7
00:01:22,000 --> 00:01:24,000
 and how powerful is the neural

8
00:01:24,000 --> 00:01:26,000
 network and what is the problem of

9
00:01:26,000 --> 00:01:28,000
 the traditional neural network.

10
00:01:28,000 --> 00:01:30,000
 And how the

11
00:01:30,000 --> 00:01:32,000
 C.A.N.

12
00:01:32,000 --> 00:01:34,000
 solve the problem of the

13
00:01:34,000 --> 00:01:36,000
 M.A.R.P.

14
00:01:36,000 --> 00:01:38,000
 basically it is

15
00:01:38,000 --> 00:01:40,000
 the overfitting

16
00:01:40,000 --> 00:01:42,000
 problem of the M.A.R.P.

17
00:01:42,000 --> 00:01:44,000
 .

18
00:01:44,000 --> 00:01:46,000
 So maybe just use a few words to

19
00:01:46,000 --> 00:01:48,000
 summarize the property of the

20
00:01:48,000 --> 00:01:50,000
 C.A.N.

21
00:01:50,000 --> 00:01:52,000
 and just one most

22
00:01:52,000 --> 00:01:54,000
 significant property of the C.A.N.

23
00:01:54,000 --> 00:01:56,000
 .

24
00:01:56,000 --> 00:01:58,000
 .

25
00:01:58,000 --> 00:02:00,000
 So the C.A.N.

26
00:02:00,000 --> 00:02:02,000
 is a

27
00:02:02,000 --> 00:02:04,000
 kind of a field or

28
00:02:04,000 --> 00:02:06,000
 convolution.

29
00:02:06,000 --> 00:02:08,000
 So the convolution

30
00:02:08,000 --> 00:02:10,000
 if the field is small

31
00:02:10,000 --> 00:02:12,000
 then the output

32
00:02:12,000 --> 00:02:14,000
 only depends on

33
00:02:14,000 --> 00:02:16,000
 the few input

34
00:02:16,000 --> 00:02:18,000
 no-coning.

35
00:02:18,000 --> 00:02:20,000
 Because if the field window is

36
00:02:20,000 --> 00:02:22,000
 small, output only depends on a few

37
00:02:22,000 --> 00:02:24,000
 inputs.

38
00:02:24,000 --> 00:02:26,000
 So the C.A.N.

39
00:02:26,000 --> 00:02:28,000
 is the

40
00:02:28,000 --> 00:02:30,000
 two determining the output

41
00:02:30,000 --> 00:02:32,000
 in one layer of the C.A.N.

42
00:02:32,000 --> 00:02:34,000
 .

43
00:02:34,000 --> 00:02:36,000
 So people always understand C.A.N.

44
00:02:36,000 --> 00:02:38,000
 in such a way.

45
00:02:38,000 --> 00:02:40,000
 This is a very

46
00:02:40,000 --> 00:02:42,000
 straightforward understanding

47
00:02:42,000 --> 00:02:44,000
 of the C.A.N.

48
00:02:44,000 --> 00:02:46,000
 .

49
00:02:46,000 --> 00:02:48,000
 So people always think C.A.N.

50
00:02:48,000 --> 00:02:50,000
 is some kind of the local feature.

51
00:02:50,000 --> 00:02:52,000
 So C.A.N.

52
00:02:52,000 --> 00:02:54,000
 is a very good property.

53
00:02:54,000 --> 00:02:56,000
 But in my

54
00:02:56,000 --> 00:02:58,000
 opinion this is not

55
00:02:58,000 --> 00:03:00,000
 the most important features

56
00:03:00,000 --> 00:03:02,000
 of the C.A.N.

57
00:03:02,000 --> 00:03:04,000
 The most important feature

58
00:03:04,000 --> 00:03:06,000
 of the C.A.N. is

59
00:03:06,000 --> 00:03:08,000
 one output

60
00:03:08,000 --> 00:03:10,000
 depends on a few inputs.

61
00:03:10,000 --> 00:03:12,000
 This is not very important.

62
00:03:12,000 --> 00:03:14,000
 The important sense is

63
00:03:14,000 --> 00:03:16,000
 the

64
00:03:16,000 --> 00:03:18,000
 relation between output and input

65
00:03:18,000 --> 00:03:20,000
 goes through a few weights.

66
00:03:20,000 --> 00:03:22,000
 The two weights

67
00:03:22,000 --> 00:03:24,000
 are used

68
00:03:24,000 --> 00:03:26,000
 for all output.

69
00:03:26,000 --> 00:03:28,000
 You see in that

70
00:03:28,000 --> 00:03:30,000
 field time process

71
00:03:30,000 --> 00:03:32,000
 the field

72
00:03:32,000 --> 00:03:34,000
 coefficient, the field window

73
00:03:34,000 --> 00:03:36,000
 we are sliding to all positions.

74
00:03:36,000 --> 00:03:38,000
 So that means

75
00:03:38,000 --> 00:03:40,000
 all output pictures

76
00:03:40,000 --> 00:03:42,000
 come from

77
00:03:42,000 --> 00:03:44,000
 same field coefficient.

78
00:03:44,000 --> 00:03:46,000
 So I think

79
00:03:46,000 --> 00:03:48,000
 this is the most important.

80
00:03:48,000 --> 00:03:50,000
 The most important feature

81
00:03:50,000 --> 00:03:52,000
 of the C.A.N.

82
00:03:52,000 --> 00:03:54,000
 is the

83
00:03:54,000 --> 00:03:56,000
 output.

84
00:03:56,000 --> 00:03:58,000
 So in the prediction process

85
00:03:58,000 --> 00:04:00,000
 or in the traditional

86
00:04:00,000 --> 00:04:02,000
 field image to get the output

87
00:04:02,000 --> 00:04:04,000
 this is not important

88
00:04:04,000 --> 00:04:06,000
 but in the machine learning

89
00:04:06,000 --> 00:04:08,000
 if we introduce this one

90
00:04:08,000 --> 00:04:10,000
 in the machine learning

91
00:04:10,000 --> 00:04:12,000
 then in the training phase

92
00:04:12,000 --> 00:04:14,000
 that means a same parameter

93
00:04:14,000 --> 00:04:16,000
 goes through all pictures

94
00:04:16,000 --> 00:04:18,000
 and the value

95
00:04:18,000 --> 00:04:20,000
 of the parameter is very reliable

96
00:04:20,000 --> 00:04:22,000
 robust because

97
00:04:22,000 --> 00:04:24,000
 it is determined

98
00:04:24,000 --> 00:04:26,000
 by all pictures

99
00:04:26,000 --> 00:04:28,000
 in the input.

100
00:04:28,000 --> 00:04:30,000
 So I think this is

101
00:04:30,000 --> 00:04:32,000
 the most significant

102
00:04:32,000 --> 00:04:34,000
 feature of the C.A.N.

103
00:04:34,000 --> 00:04:36,000
 Of course I haven't read this

104
00:04:36,000 --> 00:04:38,000
 statement from any paper

105
00:04:38,000 --> 00:04:40,000
 or any literature. This is what

106
00:04:40,000 --> 00:04:42,000
 my thought, my understanding

107
00:04:42,000 --> 00:04:44,000
 of the C.A.N.

108
00:04:44,000 --> 00:04:46,000
 because

109
00:04:46,000 --> 00:04:48,000
 I mean

110
00:04:48,000 --> 00:04:50,000
 particular parameter will go through

111
00:04:50,000 --> 00:04:52,000
 every position so all

112
00:04:52,000 --> 00:04:54,000
 pictures are used to determine

113
00:04:54,000 --> 00:04:56,000
 the value of the parameter.

114
00:04:56,000 --> 00:04:58,000
 So this is

115
00:04:58,000 --> 00:05:00,000
 most significant.

116
00:05:00,000 --> 00:05:02,000
 That means the same parameter

117
00:05:02,000 --> 00:05:04,000
 are applied to all

118
00:05:04,000 --> 00:05:06,000
 output of a layer

119
00:05:06,000 --> 00:05:08,000
 of the C.A.N.

120
00:05:08,000 --> 00:05:10,000
 Okay later we will further

121
00:05:10,000 --> 00:05:12,000
 utilize this one to study

122
00:05:12,000 --> 00:05:14,000
 how the

123
00:05:14,000 --> 00:05:16,000
 transformer works.

124
00:05:16,000 --> 00:05:18,000
 Okay.

125
00:05:18,000 --> 00:05:20,000
 Now before we

126
00:05:20,000 --> 00:05:22,000
 come to the transformer

127
00:05:22,000 --> 00:05:24,000
 we first

128
00:05:24,000 --> 00:05:26,000
 take some time

129
00:05:26,000 --> 00:05:28,000
 to study some extension

130
00:05:28,000 --> 00:05:30,000
 of the C.A.N.

131
00:05:30,000 --> 00:05:32,000
 because C.A.N. is very powerful

132
00:05:32,000 --> 00:05:34,000
 so it must be trained

133
00:05:34,000 --> 00:05:36,000
 by a large dataset.

134
00:05:36,000 --> 00:05:38,000
 So usually if

135
00:05:38,000 --> 00:05:40,000
 we want to solve the problem

136
00:05:40,000 --> 00:05:42,000
 we use a powerful C.A.N.

137
00:05:42,000 --> 00:05:44,000
 trained by a huge dataset.

138
00:05:44,000 --> 00:05:46,000
 Okay.

139
00:05:46,000 --> 00:05:48,000
 But the C.A.N. in nature

140
00:05:48,000 --> 00:05:50,000
 it is a general feature extractor.

141
00:05:50,000 --> 00:05:52,000
 Right? It extracts the feature

142
00:05:52,000 --> 00:05:54,000
 from the image. This is

143
00:05:54,000 --> 00:05:56,000
 a very general

144
00:05:56,000 --> 00:05:58,000
 extractor feature from the input.

145
00:05:58,000 --> 00:06:00,000
 If we use a huge dataset

146
00:06:00,000 --> 00:06:02,000
 to train it then

147
00:06:02,000 --> 00:06:04,000
 the extract feature will be

148
00:06:04,000 --> 00:06:06,000
 a general feature on

149
00:06:06,000 --> 00:06:08,000
 the huge dataset.

150
00:06:08,000 --> 00:06:10,000
 But how to utilize this powerful C.A.N.

151
00:06:10,000 --> 00:06:12,000
 to solve some

152
00:06:12,000 --> 00:06:14,000
 specific application problem?

153
00:06:14,000 --> 00:06:16,000
 Then

154
00:06:16,000 --> 00:06:18,000
 for specific application problem

155
00:06:18,000 --> 00:06:20,000
 we need to put some additional

156
00:06:20,000 --> 00:06:22,000
 components on top

157
00:06:22,000 --> 00:06:24,000
 of the C.A.N. so that

158
00:06:24,000 --> 00:06:26,000
 together the network

159
00:06:26,000 --> 00:06:28,000
 can perform a specific

160
00:06:28,000 --> 00:06:30,000
 task. Okay.

161
00:06:30,000 --> 00:06:32,000
 So for specific tasks we also

162
00:06:32,000 --> 00:06:34,000
 have a training dataset

163
00:06:34,000 --> 00:06:36,000
 but this dataset could be small

164
00:06:36,000 --> 00:06:38,000
 compared to such as

165
00:06:38,000 --> 00:06:40,000
 the image needs this huge dataset.

166
00:06:40,000 --> 00:06:42,000
 Right? But we need to adapt

167
00:06:42,000 --> 00:06:44,000
 the general C.A.N.

168
00:06:44,000 --> 00:06:46,000
 into specific tasks.

169
00:06:46,000 --> 00:06:48,000
 So here we

170
00:06:48,000 --> 00:06:50,000
 first introduce some examples

171
00:06:50,000 --> 00:06:52,000
 how to utilize the C.A.N. to solve

172
00:06:52,000 --> 00:06:54,000
 specific tasks.

173
00:06:54,000 --> 00:06:56,000
 Now the first one is

174
00:06:56,000 --> 00:06:58,000
 we utilize the C.A.N.

175
00:06:58,000 --> 00:07:00,000
 for the image segmentation.

176
00:07:00,000 --> 00:07:02,000
 Okay. Here we show an image

177
00:07:02,000 --> 00:07:04,000
 this one. Right? Then this is

178
00:07:04,000 --> 00:07:06,000
 a segmentation result.

179
00:07:06,000 --> 00:07:08,000
 Here with different C.A.N.

180
00:07:08,000 --> 00:07:10,000
 represent different object.

181
00:07:10,000 --> 00:07:12,000
 Sorry.

182
00:07:34,000 --> 00:07:36,000
 Okay.

183
00:07:52,000 --> 00:07:54,000
 Okay. This one.

184
00:07:54,000 --> 00:07:56,000
 This picture shows the segmentation result.

185
00:07:56,000 --> 00:07:58,000
 We use different color

186
00:07:58,000 --> 00:08:00,000
 indicate different

187
00:08:00,000 --> 00:08:02,000
 object be segmented

188
00:08:02,000 --> 00:08:04,000
 by the image.

189
00:08:04,000 --> 00:08:06,000
 So we have a segmentation result

190
00:08:06,000 --> 00:08:08,000
 which is the image

191
00:08:08,000 --> 00:08:10,000
 such as a person, the car,

192
00:08:10,000 --> 00:08:12,000
 the road, the side road,

193
00:08:12,000 --> 00:08:14,000
 the building, the sky.

194
00:08:14,000 --> 00:08:16,000
 And so on. Right?

195
00:08:16,000 --> 00:08:18,000
 So this shows the segmentation result.

196
00:08:18,000 --> 00:08:20,000
 From this segmentation result

197
00:08:20,000 --> 00:08:22,000
 we see that the so-called

198
00:08:22,000 --> 00:08:24,000
 semantic segmentation need

199
00:08:24,000 --> 00:08:26,000
 classify every picture

200
00:08:26,000 --> 00:08:28,000
 into different class.

201
00:08:28,000 --> 00:08:30,000
 Okay. Not classify

202
00:08:30,000 --> 00:08:32,000
 every picture into different class.

203
00:08:32,000 --> 00:08:34,000
 So for such

204
00:08:34,000 --> 00:08:36,000
 segmentation problem it will be

205
00:08:36,000 --> 00:08:38,000
 much

206
00:08:38,000 --> 00:08:40,000
 complicated than just

207
00:08:40,000 --> 00:08:42,000
 an image classification

208
00:08:42,000 --> 00:08:44,000
 problem. Image classification

209
00:08:44,000 --> 00:08:46,000
 given one image the output

210
00:08:46,000 --> 00:08:48,000
 of the network just one class.

211
00:08:48,000 --> 00:08:50,000
 But for the segmentation we need

212
00:08:50,000 --> 00:08:52,000
 classify this picture

213
00:08:52,000 --> 00:08:54,000
 into the person, this picture

214
00:08:54,000 --> 00:08:56,000
 into the road, this picture

215
00:08:56,000 --> 00:08:58,000
 into the building, and the other

216
00:08:58,000 --> 00:09:00,000
 part of the segmentation is

217
00:09:00,000 --> 00:09:02,000
 much more complex

218
00:09:02,000 --> 00:09:04,000
 than the image classification

219
00:09:04,000 --> 00:09:06,000
 or object recognition because

220
00:09:06,000 --> 00:09:08,000
 it need classify every

221
00:09:08,000 --> 00:09:10,000
 picture in the input image.

222
00:09:10,000 --> 00:09:12,000
 Now then how

223
00:09:12,000 --> 00:09:14,000
 to use the print trend

224
00:09:14,000 --> 00:09:16,000
 powerful thing to

225
00:09:16,000 --> 00:09:18,000
 solve this segmentation

226
00:09:18,000 --> 00:09:20,000
 task. Now

227
00:09:20,000 --> 00:09:22,000
 first of all we use one thing

228
00:09:22,000 --> 00:09:24,000
 and this blue color one

229
00:09:24,000 --> 00:09:26,000
 is a wrist

230
00:09:26,000 --> 00:09:28,000
 net 101.

231
00:09:28,000 --> 00:09:30,000
 It's a wrist net has

232
00:09:30,000 --> 00:09:32,000
 101 layers.

233
00:09:32,000 --> 00:09:34,000
 Okay. You such a very

234
00:09:34,000 --> 00:09:36,000
 powerful CNN. This CNN

235
00:09:36,000 --> 00:09:38,000
 is print trend by the image

236
00:09:38,000 --> 00:09:40,000
 net. Okay. So the

237
00:09:40,000 --> 00:09:42,000
 performance of this CNN for

238
00:09:42,000 --> 00:09:44,000
 the image classification is

239
00:09:44,000 --> 00:09:46,000
 very excellent.

240
00:09:46,000 --> 00:09:48,000
 It's already print trend.

241
00:09:48,000 --> 00:09:50,000
 Now after

242
00:09:50,000 --> 00:09:52,000
 we use this powerful

243
00:09:52,000 --> 00:09:54,000
 CNN as a backbone. If we

244
00:09:54,000 --> 00:09:56,000
 want it to do the

245
00:09:56,000 --> 00:09:58,000
 image segmentation

246
00:09:58,000 --> 00:10:00,000
 so what additional

247
00:10:00,000 --> 00:10:02,000
 sense we need to do?

248
00:10:02,000 --> 00:10:04,000
 Okay. So first

249
00:10:04,000 --> 00:10:06,000
 of all we

250
00:10:06,000 --> 00:10:08,000
 study the structure of this

251
00:10:08,000 --> 00:10:10,000
 print trend and wrist net 101.

252
00:10:10,000 --> 00:10:12,000
 So the input image

253
00:10:12,000 --> 00:10:14,000
 is 500 by

254
00:10:14,000 --> 00:10:16,000
 500.

255
00:10:16,000 --> 00:10:18,000
 3 channel RGB. And then

256
00:10:18,000 --> 00:10:20,000
 after many convolutional layer

257
00:10:20,000 --> 00:10:22,000
 and pooling layer at the last

258
00:10:22,000 --> 00:10:24,000
 convolution layer the output

259
00:10:24,000 --> 00:10:26,000
 that becomes to 16 by

260
00:10:26,000 --> 00:10:28,000
 16 with

261
00:10:28,000 --> 00:10:30,000
 2,000 channels.

262
00:10:30,000 --> 00:10:32,000
 Okay. So this is the

263
00:10:32,000 --> 00:10:34,000
 output of the CNN and of course

264
00:10:34,000 --> 00:10:36,000
 last convolution layer

265
00:10:36,000 --> 00:10:38,000
 of the wrist net

266
00:10:38,000 --> 00:10:40,000
 101.

267
00:10:40,000 --> 00:10:42,000
 Now the wrist

268
00:10:42,000 --> 00:10:44,000
 net is

269
00:10:44,000 --> 00:10:46,000
 all

270
00:10:46,000 --> 00:10:48,000
 parameter is trend by image

271
00:10:48,000 --> 00:10:50,000
 net. The image net

272
00:10:50,000 --> 00:10:52,000
 is classifying every image

273
00:10:52,000 --> 00:10:54,000
 into different class.

274
00:10:54,000 --> 00:10:56,000
 Okay. So after

275
00:10:56,000 --> 00:10:58,000
 this last layer of the

276
00:10:58,000 --> 00:11:00,000
 CNN and then all this

277
00:11:00,000 --> 00:11:02,000
 value are used as the feature

278
00:11:02,000 --> 00:11:04,000
 to do the classification.

279
00:11:04,000 --> 00:11:06,000
 Then to classify the whole

280
00:11:06,000 --> 00:11:08,000
 input into one of the

281
00:11:08,000 --> 00:11:10,000
 into one of the class.

282
00:11:10,000 --> 00:11:12,000
 Right. So here we see

283
00:11:12,000 --> 00:11:14,000
 that the final classification

284
00:11:14,000 --> 00:11:16,000
 still utilize some

285
00:11:16,000 --> 00:11:18,000
 spatial information of the input

286
00:11:18,000 --> 00:11:20,000
 image because the last

287
00:11:20,000 --> 00:11:22,000
 layer the feature map is

288
00:11:22,000 --> 00:11:24,000
 16 by 16.

289
00:11:24,000 --> 00:11:26,000
 So we have

290
00:11:26,000 --> 00:11:28,000
 similar to image size

291
00:11:28,000 --> 00:11:30,000
 of the 16 by 16.

292
00:11:30,000 --> 00:11:32,000
 We still utilize this 16

293
00:11:32,000 --> 00:11:34,000
 by 16 features

294
00:11:34,000 --> 00:11:36,000
 together to perform

295
00:11:36,000 --> 00:11:38,000
 the classification.

296
00:11:38,000 --> 00:11:40,000
 Now because of this training

297
00:11:40,000 --> 00:11:42,000
 in case that means the

298
00:11:42,000 --> 00:11:44,000
 16 by 16

299
00:11:44,000 --> 00:11:46,000
 metrics

300
00:11:46,000 --> 00:11:48,000
 each element in this

301
00:11:48,000 --> 00:11:50,000
 metrics will be different

302
00:11:50,000 --> 00:11:52,000
 because they are trained

303
00:11:52,000 --> 00:11:54,000
 together to make a

304
00:11:54,000 --> 00:11:56,000
 decision to do the classification.

305
00:11:56,000 --> 00:11:58,000
 Right. Okay.

306
00:11:58,000 --> 00:12:00,000
 But for the segmentation

307
00:12:00,000 --> 00:12:02,000
 we need to classify

308
00:12:02,000 --> 00:12:04,000
 every pixel even

309
00:12:04,000 --> 00:12:06,000
 in this last

310
00:12:06,000 --> 00:12:08,000
 even in this

311
00:12:08,000 --> 00:12:10,000
 sorry.

312
00:12:16,000 --> 00:12:18,000
 Okay.

313
00:12:18,000 --> 00:12:20,000
 Sorry.

314
00:12:20,000 --> 00:12:22,000
 Yes.

315
00:12:24,000 --> 00:12:26,000
 Okay. Even in

316
00:12:26,000 --> 00:12:28,000
 this last layer

317
00:12:28,000 --> 00:12:30,000
 we need to classify every pixel

318
00:12:30,000 --> 00:12:32,000
 into different class.

319
00:12:32,000 --> 00:12:34,000
 So but the output

320
00:12:34,000 --> 00:12:36,000
 is trained

321
00:12:36,000 --> 00:12:38,000
 not

322
00:12:38,000 --> 00:12:40,000
 for each individual one

323
00:12:40,000 --> 00:12:42,000
 to determine its class

324
00:12:42,000 --> 00:12:44,000
 but together to determine its

325
00:12:44,000 --> 00:12:46,000
 class. Okay.

326
00:12:46,000 --> 00:12:48,000
 Now we understand that if we

327
00:12:48,000 --> 00:12:50,000
 want to we are classify

328
00:12:50,000 --> 00:12:52,000
 a pixel for example we

329
00:12:52,000 --> 00:12:54,000
 cannot just look at this pixel

330
00:12:54,000 --> 00:12:56,000
 we should look at its

331
00:12:56,000 --> 00:12:58,000
 surroundings so that

332
00:12:58,000 --> 00:13:00,000
 this is some kind of the context

333
00:13:00,000 --> 00:13:02,000
 of this pixel.

334
00:13:02,000 --> 00:13:04,000
 So because of that if we

335
00:13:04,000 --> 00:13:06,000
 utilize this in the print transition

336
00:13:06,000 --> 00:13:08,000
 directly to do the

337
00:13:08,000 --> 00:13:10,000
 segmentation that means to

338
00:13:10,000 --> 00:13:12,000
 classify every element in

339
00:13:12,000 --> 00:13:14,000
 this special map

340
00:13:14,000 --> 00:13:16,000
 16 by 16 it will perform

341
00:13:16,000 --> 00:13:18,000
 poorly because the print

342
00:13:18,000 --> 00:13:20,000
 training is not such

343
00:13:20,000 --> 00:13:22,000
 a purpose. Okay.

344
00:13:22,000 --> 00:13:24,000
 Well in this

345
00:13:24,000 --> 00:13:26,000
 application we need to classify

346
00:13:26,000 --> 00:13:28,000
 every pixel independently

347
00:13:28,000 --> 00:13:30,000
 into different class. So

348
00:13:30,000 --> 00:13:32,000
 every pixel one pixel

349
00:13:32,000 --> 00:13:34,000
 unknown may not

350
00:13:34,000 --> 00:13:36,000
 contain sufficient information

351
00:13:36,000 --> 00:13:38,000
 to determine this what

352
00:13:38,000 --> 00:13:40,000
 is this pixel reliably.

353
00:13:40,000 --> 00:13:42,000
 So we need to build up

354
00:13:42,000 --> 00:13:44,000
 so called context that means

355
00:13:44,000 --> 00:13:46,000
 to classify each of this

356
00:13:46,000 --> 00:13:48,000
 16 by 16 pixel we need

357
00:13:48,000 --> 00:13:50,000
 to look at more information

358
00:13:50,000 --> 00:13:52,000
 its context or its

359
00:13:52,000 --> 00:13:54,000
 surroundings. So

360
00:13:54,000 --> 00:13:56,000
 to apply this print

361
00:13:56,000 --> 00:13:58,000
 transition into the segmentation

362
00:13:58,000 --> 00:14:00,000
 we need to build up another

363
00:14:00,000 --> 00:14:02,000
 a few layers

364
00:14:02,000 --> 00:14:04,000
 here CCA context

365
00:14:04,000 --> 00:14:06,000
 to further to do the

366
00:14:06,000 --> 00:14:08,000
 convolution

367
00:14:08,000 --> 00:14:10,000
 based on this 16 by 16

368
00:14:10,000 --> 00:14:12,000
 further

369
00:14:12,000 --> 00:14:14,000
 a few layers in this convolution

370
00:14:14,000 --> 00:14:16,000
 so that we can connect

371
00:14:16,000 --> 00:14:18,000
 the information of one

372
00:14:18,000 --> 00:14:20,000
 position not just from this position

373
00:14:20,000 --> 00:14:22,000
 also it is surroundings.

374
00:14:22,000 --> 00:14:24,000
 And this connection of these

375
00:14:24,000 --> 00:14:26,000
 surroundings to train its weights

376
00:14:26,000 --> 00:14:28,000
 is used our segmentation

377
00:14:28,000 --> 00:14:30,000
 data set.

378
00:14:30,000 --> 00:14:32,000
 The ground truth is

379
00:14:32,000 --> 00:14:34,000
 the segmentation is different from

380
00:14:34,000 --> 00:14:36,000
 the previous the

381
00:14:36,000 --> 00:14:38,000
 segmentation data set.

382
00:14:38,000 --> 00:14:40,000
 The ground truth is just

383
00:14:40,000 --> 00:14:42,000
 the whole image is one class.

384
00:14:42,000 --> 00:14:44,000
 Now here we

385
00:14:44,000 --> 00:14:46,000
 we are training this CCA

386
00:14:46,000 --> 00:14:48,000
 this context is

387
00:14:48,000 --> 00:14:50,000
 we are utilizing our segmentation

388
00:14:50,000 --> 00:14:52,000
 data set. The ground truth are

389
00:14:52,000 --> 00:14:54,000
 different in the segmentation

390
00:14:54,000 --> 00:14:56,000
 training data set we have the ground

391
00:14:56,000 --> 00:14:58,000
 truth different pixel is

392
00:14:58,000 --> 00:15:00,000
 different class.

393
00:15:00,000 --> 00:15:02,000
 So here we need to build

394
00:15:02,000 --> 00:15:04,000
 up our context

395
00:15:04,000 --> 00:15:06,000
 to look at this pixel

396
00:15:06,000 --> 00:15:08,000
 not just look at this pixel

397
00:15:08,000 --> 00:15:10,000
 look at more surroundings

398
00:15:24,000 --> 00:15:26,000
 okay we need

399
00:15:26,000 --> 00:15:28,000
 to classify one pixel

400
00:15:28,000 --> 00:15:30,000
 we need to look at more

401
00:15:30,000 --> 00:15:32,000
 surrounding information to determine

402
00:15:32,000 --> 00:15:33,840
 which is a class of one pixel.

403
00:15:33,840 --> 00:15:37,440
 So we need a so-called context information

404
00:15:37,440 --> 00:15:40,360
 to get a better, renewable classification

405
00:15:40,360 --> 00:15:43,440
 for individual pixels.

406
00:15:43,440 --> 00:15:45,240
 Now, this is one site.

407
00:15:45,240 --> 00:15:49,960
 Another site is the image.

408
00:15:49,960 --> 00:15:53,800
 Object could have a large scale, could have a small scale.

409
00:15:53,800 --> 00:15:57,200
 For example, for this image, this person is large scale

410
00:15:57,200 --> 00:15:59,360
 because it's closer to the camera,

411
00:15:59,360 --> 00:16:01,560
 whereas the car is very small

412
00:16:01,560 --> 00:16:04,920
 because it is far away from the camera.

413
00:16:04,920 --> 00:16:08,000
 Some class, it can be small scale.

414
00:16:08,000 --> 00:16:10,239
 Some class, it can be large scale.

415
00:16:10,239 --> 00:16:14,000
 So for the classification, we cannot just

416
00:16:14,000 --> 00:16:17,479
 utilize the final feature to do the classification.

417
00:16:17,479 --> 00:16:20,760
 We also need some small-scale features.

418
00:16:20,760 --> 00:16:24,880
 That means the output of the convolution layer

419
00:16:24,880 --> 00:16:28,680
 in the earlier stage, in the earlier layers,

420
00:16:28,680 --> 00:16:31,479
 to do the classification.

421
00:16:31,480 --> 00:16:37,480
 Because we could have a very small scale of the object.

422
00:16:37,480 --> 00:16:42,440
 And also, along the edge, we need to look at a small scale

423
00:16:42,440 --> 00:16:44,880
 closely to do the classification.

424
00:16:44,880 --> 00:16:49,960
 If we use a large area, somehow it will average the information.

425
00:16:49,960 --> 00:16:56,600
 So it cannot specify to determine accurately the edge

426
00:16:56,600 --> 00:16:58,120
 location.

427
00:16:58,120 --> 00:17:02,480
 So to do the segmentation, we need to classify,

428
00:17:02,480 --> 00:17:07,119
 apply the classifier, not just in the last layer of the feature.

429
00:17:07,119 --> 00:17:11,240
 We also need to utilize the feature in the previous layers.

430
00:17:11,240 --> 00:17:15,839
 So all these features will come to a classification classifier.

431
00:17:15,839 --> 00:17:18,240
 Of course, the classifier is a very simple classifier,

432
00:17:18,240 --> 00:17:20,680
 just a linear classifier.

433
00:17:20,680 --> 00:17:24,680
 Then the classification result should merge together

434
00:17:24,680 --> 00:17:26,000
 to make a decision.

435
00:17:26,000 --> 00:17:28,319
 What is the class?

436
00:17:28,319 --> 00:17:31,400
 So this layer is called skip layer,

437
00:17:31,400 --> 00:17:38,200
 because it's from the earlier stage, skip to the last result

438
00:17:38,200 --> 00:17:39,400
 with a classification.

439
00:17:39,400 --> 00:17:43,480
 So this classification layer is called skip layers.

440
00:17:43,480 --> 00:17:47,840
 So we need to combine the classification of multiple

441
00:17:47,840 --> 00:17:51,120
 scale of the features, then merge them together

442
00:17:51,120 --> 00:17:54,160
 to make a decision.

443
00:17:54,160 --> 00:18:01,280
 So this is so-called skip layers to do that classification.

444
00:18:01,280 --> 00:18:05,160
 And also, even we use lower layer of the feature

445
00:18:05,160 --> 00:18:07,280
 to directly do the classification,

446
00:18:07,280 --> 00:18:09,240
 and then merge them together.

447
00:18:09,240 --> 00:18:14,040
 So usually, we cannot use the very known layer,

448
00:18:14,040 --> 00:18:16,120
 because it is very noisy.

449
00:18:16,120 --> 00:18:20,440
 So we usually use the higher layer and the middle layer

450
00:18:20,440 --> 00:18:23,640
 feature to do the classification, then merge them together.

451
00:18:23,640 --> 00:18:26,920
 So the lower layer is very noisy.

452
00:18:26,920 --> 00:18:29,680
 If we use it to the classification,

453
00:18:29,680 --> 00:18:32,240
 it may cause some problem.

454
00:18:32,240 --> 00:18:37,040
 So the context information, built up context information

455
00:18:37,040 --> 00:18:41,320
 by these additional layers of the convolution,

456
00:18:41,320 --> 00:18:46,720
 and the merge of the classification

457
00:18:46,720 --> 00:18:52,680
 of multiple different layers to get the segmentation result.

458
00:18:52,680 --> 00:18:57,840
 This is already proposed before this work.

459
00:18:57,840 --> 00:19:02,280
 But then in one of my work, what is our contribution?

460
00:19:02,280 --> 00:19:06,960
 This work published in 3VPR 2018 as an aura paper,

461
00:19:06,960 --> 00:19:10,200
 and then later further published in a transaction

462
00:19:10,200 --> 00:19:12,760
 and image processing.

463
00:19:12,760 --> 00:19:18,320
 Now our contribution here is we make this context layer.

464
00:19:18,320 --> 00:19:22,520
 Context construction is not a simple convolution.

465
00:19:22,520 --> 00:19:30,320
 We try to build up our context as a contrast context.

466
00:19:30,320 --> 00:19:32,840
 Later I will elaborate that.

467
00:19:32,840 --> 00:19:38,840
 And the second contribution is we don't just simply sum

468
00:19:38,840 --> 00:19:42,080
 all classification score together.

469
00:19:42,080 --> 00:19:48,720
 We build up a gate to actively combine them together.

470
00:19:48,720 --> 00:19:54,200
 Because for some positions, we should mainly

471
00:19:54,200 --> 00:19:56,120
 use large scale of the feature.

472
00:19:56,120 --> 00:19:59,800
 For some other pictures, we may put more weightage

473
00:19:59,800 --> 00:20:01,680
 on a small scale of the feature.

474
00:20:01,680 --> 00:20:05,680
 Depends on the position is in the large scale of the object

475
00:20:05,680 --> 00:20:08,160
 or small scale of the object.

476
00:20:08,160 --> 00:20:14,200
 So here we built up the gate to control the fusion

477
00:20:14,200 --> 00:20:17,120
 of the classification score.

478
00:20:17,120 --> 00:20:19,320
 Now the third contribution of this work

479
00:20:19,320 --> 00:20:22,879
 is we utilize a very known layer of the feature.

480
00:20:22,879 --> 00:20:27,000
 But we only utilize it around the age,

481
00:20:27,000 --> 00:20:30,919
 around the boundary of the different object.

482
00:20:30,919 --> 00:20:35,439
 So we try to identify a position is close to the boundary

483
00:20:35,439 --> 00:20:37,600
 or far away to the boundary.

484
00:20:37,600 --> 00:20:42,439
 Then we utilize a very lower layer of the information

485
00:20:42,439 --> 00:20:46,760
 to work close to the boundary so that we

486
00:20:46,760 --> 00:20:52,040
 can very specific to determine the position of the boundary

487
00:20:52,040 --> 00:20:53,879
 more accurately.

488
00:20:53,879 --> 00:20:57,120
 OK, so this is the three contributions in this work.

489
00:20:57,120 --> 00:21:00,520
 Now I will go through the ideas of these three.

490
00:21:00,520 --> 00:21:04,760
 The first one is so-called we have the context,

491
00:21:04,760 --> 00:21:09,360
 but we call it the contrasted local feature.

492
00:21:09,360 --> 00:21:11,200
 Now how to build up the context?

493
00:21:11,200 --> 00:21:15,040
 For example, the last layer of the ResNet 101

494
00:21:15,040 --> 00:21:18,000
 is the 16 by 16 map.

495
00:21:18,000 --> 00:21:22,000
 So we want to connect information of larger area

496
00:21:22,000 --> 00:21:26,040
 as the context of the center pictures.

497
00:21:26,040 --> 00:21:29,399
 Before our work, others work is also

498
00:21:29,399 --> 00:21:32,639
 apply the convolution then to next layer,

499
00:21:32,639 --> 00:21:33,920
 then convolution to next layer.

500
00:21:33,920 --> 00:21:38,680
 Because convolution will connect information

501
00:21:38,680 --> 00:21:40,840
 in a window to the output.

502
00:21:40,840 --> 00:21:44,440
 So we were further to apply the convolution, of course,

503
00:21:44,480 --> 00:21:47,000
 trained by the segmentation data set

504
00:21:47,000 --> 00:21:50,040
 and not the imaginary data set.

505
00:21:50,040 --> 00:21:54,640
 But what in my opinion is if we further do the convolution,

506
00:21:54,640 --> 00:22:00,440
 although the output will connect the larger area of the information

507
00:22:00,440 --> 00:22:04,360
 as the output, but if the area is too large,

508
00:22:04,360 --> 00:22:06,800
 it will blur the information.

509
00:22:06,800 --> 00:22:11,960
 Because convolution anyway is a weighted average.

510
00:22:11,960 --> 00:22:15,760
 So if we weighted average of the picture of a larger area,

511
00:22:15,760 --> 00:22:18,240
 then as one output, then the output

512
00:22:18,240 --> 00:22:21,880
 is just some kind of the blurred information.

513
00:22:21,880 --> 00:22:26,320
 So this may cause some negative effect.

514
00:22:26,320 --> 00:22:31,000
 Now then how to connect the information of the context?

515
00:22:31,000 --> 00:22:34,160
 We do the contrast.

516
00:22:34,160 --> 00:22:38,320
 Given the input feature map, to produce the output feature map,

517
00:22:38,320 --> 00:22:40,760
 we apply two convolution.

518
00:22:40,760 --> 00:22:44,640
 One convolution is with a very small window.

519
00:22:44,640 --> 00:22:51,480
 For this 3 by 3, contains just a focus on the local area.

520
00:22:51,480 --> 00:22:53,160
 Get a convolution result.

521
00:22:53,160 --> 00:22:57,360
 And then we use a dynamic convolution, also nine pictures,

522
00:22:57,360 --> 00:23:00,920
 but it cover large area.

523
00:23:00,920 --> 00:23:03,160
 Then this is a convolution result.

524
00:23:03,160 --> 00:23:08,120
 Then the final result will be the difference between these two.

525
00:23:08,159 --> 00:23:12,879
 So here I show the difference between these two convolution.

526
00:23:12,879 --> 00:23:15,959
 So what is the purpose of this difference?

527
00:23:15,959 --> 00:23:19,439
 This difference means that the output

528
00:23:19,439 --> 00:23:24,399
 is focused on the information of this specific position.

529
00:23:24,399 --> 00:23:30,040
 But we compile this information with its surroundings

530
00:23:30,040 --> 00:23:34,600
 to see what is the difference.

531
00:23:34,600 --> 00:23:35,959
 OK, this is a contrast.

532
00:23:35,959 --> 00:23:37,840
 It's a difference.

533
00:23:37,840 --> 00:23:42,040
 So this is also simulating our human.

534
00:23:42,040 --> 00:23:46,639
 If our human want to identify what is it at one position,

535
00:23:46,639 --> 00:23:49,240
 we cannot just look at this one.

536
00:23:49,240 --> 00:23:53,120
 We must also look at some surrounding areas.

537
00:23:53,120 --> 00:23:56,560
 But the information at this one and the surrounding

538
00:23:56,560 --> 00:23:57,760
 are different.

539
00:23:57,760 --> 00:23:59,560
 We use the surrounding information

540
00:23:59,560 --> 00:24:03,919
 is just to do the comparison, compile,

541
00:24:04,440 --> 00:24:08,120
 what is the difference between this central information

542
00:24:08,120 --> 00:24:10,280
 and its surroundings.

543
00:24:10,280 --> 00:24:14,640
 This difference will help us to determine what is it

544
00:24:14,640 --> 00:24:16,680
 at the center point.

545
00:24:16,680 --> 00:24:21,880
 OK, so we build up this module, this block,

546
00:24:21,880 --> 00:24:25,480
 to call this context contrast local feature.

547
00:24:25,480 --> 00:24:27,720
 Because basically it is a local feature,

548
00:24:27,720 --> 00:24:32,840
 but it compiles with the surroundings

549
00:24:32,840 --> 00:24:35,840
 of these local positions.

550
00:24:35,840 --> 00:24:40,720
 So basically we also build up a few layers of this one.

551
00:24:40,720 --> 00:24:45,360
 And then all these features are used for the classification,

552
00:24:45,360 --> 00:24:49,160
 go to the fusion or classification scores.

553
00:24:49,160 --> 00:24:52,240
 OK, so this is one idea of this contribution.

554
00:24:52,240 --> 00:24:56,080
 Another contribution is so-called gauge sum.

555
00:24:56,080 --> 00:24:58,960
 As I mentioned that to do the segmentation,

556
00:24:58,960 --> 00:25:03,280
 we need to classify the feature of the convolution feature,

557
00:25:03,280 --> 00:25:05,080
 not just from one layer.

558
00:25:05,080 --> 00:25:08,879
 We need to classify the convolution feature

559
00:25:08,879 --> 00:25:11,800
 in many different layers.

560
00:25:11,800 --> 00:25:16,639
 Earlier layer is a small scale of the feature.

561
00:25:16,639 --> 00:25:20,480
 The high layer of the convolution has a larger scale.

562
00:25:20,480 --> 00:25:22,840
 Because after many convolutions, it

563
00:25:22,840 --> 00:25:26,120
 will connect a large area of the information.

564
00:25:26,120 --> 00:25:29,280
 So the feature is a larger scale.

565
00:25:29,280 --> 00:25:33,679
 Now here, this blue one is the backbone,

566
00:25:33,679 --> 00:25:37,199
 is the same as recently when 0.1.

567
00:25:37,199 --> 00:25:41,600
 So we utilize the feature map of different layers

568
00:25:41,600 --> 00:25:47,120
 to go to the classifier, then get the classification score

569
00:25:47,120 --> 00:25:50,360
 based on the different layer of the feature.

570
00:25:50,360 --> 00:25:52,520
 And then after that, at the end, is

571
00:25:52,520 --> 00:25:55,159
 a sum merge of all this classification

572
00:25:55,159 --> 00:25:59,120
 score to produce a final classification score.

573
00:25:59,120 --> 00:26:02,679
 But here, we don't just simply sum them together.

574
00:26:02,679 --> 00:26:08,919
 We produce a weight, so-called in the gauge fusion.

575
00:26:08,919 --> 00:26:11,120
 We produce a weight.

576
00:26:11,120 --> 00:26:15,000
 Then to make all this different, this classification score

577
00:26:15,000 --> 00:26:18,959
 from different layer, weight sum together

578
00:26:18,960 --> 00:26:23,440
 to produce a final classification score.

579
00:26:23,440 --> 00:26:27,600
 Now, mathematically, basically, it is just the weight sum.

580
00:26:27,600 --> 00:26:30,080
 This is a classification score.

581
00:26:30,080 --> 00:26:32,760
 This G is this weight.

582
00:26:32,760 --> 00:26:34,000
 OK.

583
00:26:34,000 --> 00:26:37,560
 Now, this is just a weight sum.

584
00:26:37,560 --> 00:26:40,360
 So why I call it a gauge sum?

585
00:26:40,360 --> 00:26:44,640
 Because in traditional way, how to determine the weights

586
00:26:44,640 --> 00:26:46,440
 in the weight sum?

587
00:26:46,440 --> 00:26:50,320
 Some approach, some application, we

588
00:26:50,320 --> 00:26:53,360
 may pre-define what is the weights.

589
00:26:53,360 --> 00:26:57,080
 Then to do the weight sum, some other approach

590
00:26:57,080 --> 00:27:02,720
 also try to determine this weights by the training data set.

591
00:27:02,720 --> 00:27:06,640
 That means the weight can be learned from the training data.

592
00:27:06,640 --> 00:27:07,840
 Right?

593
00:27:07,840 --> 00:27:10,720
 OK, so it will adapt to the training data.

594
00:27:10,720 --> 00:27:12,960
 So then to do the weight sum.

595
00:27:12,960 --> 00:27:18,680
 But here, this weight in this network is not pre-defined,

596
00:27:18,680 --> 00:27:23,160
 is also not determined by the training data.

597
00:27:23,160 --> 00:27:26,800
 It is generated by the test data.

598
00:27:26,800 --> 00:27:33,360
 OK, so because we have a network in the prediction process,

599
00:27:33,360 --> 00:27:37,480
 in the inference process, the input, the test image comes.

600
00:27:37,480 --> 00:27:43,600
 Then this test image comes, generates this convolution

601
00:27:43,600 --> 00:27:46,800
 and their output, then goes through this network

602
00:27:46,800 --> 00:27:50,880
 to generate what is the weights.

603
00:27:50,880 --> 00:27:53,760
 So the weights come from the test data,

604
00:27:53,760 --> 00:27:59,280
 not determined by the training data.

605
00:27:59,280 --> 00:28:03,840
 But this network, the permit of this network

606
00:28:03,840 --> 00:28:06,560
 is trained by the training data.

607
00:28:06,560 --> 00:28:10,760
 OK, but the real weights is generated by the test data.

608
00:28:10,760 --> 00:28:15,560
 So because of that, we call this is a gate to sum.

609
00:28:15,560 --> 00:28:18,560
 Now, how is this network?

610
00:28:18,560 --> 00:28:24,080
 OK, now, we have seen and has many layers.

611
00:28:24,080 --> 00:28:27,320
 Each layer will produce multiple feature maps.

612
00:28:27,320 --> 00:28:30,120
 And all these feature maps will come together

613
00:28:30,120 --> 00:28:31,679
 to do the classification.

614
00:28:31,679 --> 00:28:32,639
 Right?

615
00:28:32,640 --> 00:28:36,760
 Now, because one layer, each layer will produce

616
00:28:36,760 --> 00:28:40,720
 many feature maps, then we just use a convolution layer,

617
00:28:40,720 --> 00:28:48,520
 one by one convolution, to merge all map into one map.

618
00:28:48,520 --> 00:28:51,840
 OK, then one layer has only one map.

619
00:28:51,840 --> 00:28:56,760
 One map, we call this map as information map.

620
00:28:56,760 --> 00:28:59,720
 It contains information of this layer.

621
00:29:00,280 --> 00:29:07,760
 We convert multiple layers into just a single map

622
00:29:07,760 --> 00:29:09,640
 into just one single map.

623
00:29:09,640 --> 00:29:13,920
 This map is information map.

624
00:29:13,920 --> 00:29:16,440
 Now, from this information map, then we

625
00:29:16,440 --> 00:29:19,280
 use this information map of different layer

626
00:29:19,280 --> 00:29:27,640
 to generate the weights of this layer.

627
00:29:27,640 --> 00:29:32,280
 Now, but this information map, the information is unbalanced

628
00:29:32,280 --> 00:29:37,920
 because in the convolution network, in any network,

629
00:29:37,920 --> 00:29:41,280
 the information is from input to the output.

630
00:29:41,280 --> 00:29:44,360
 So the higher layer information is

631
00:29:44,360 --> 00:29:47,600
 generated by lower layer information.

632
00:29:47,600 --> 00:29:50,040
 So that means the higher layer information

633
00:29:50,040 --> 00:29:56,200
 contains a summary of the information of the lower layers.

634
00:29:56,200 --> 00:29:59,720
 But the lower layer has no information.

635
00:29:59,720 --> 00:30:04,240
 How is the higher layer summarize its information?

636
00:30:04,240 --> 00:30:06,920
 Become information in the higher layer.

637
00:30:06,920 --> 00:30:12,800
 So anyway, the information is transferred from one direction,

638
00:30:12,800 --> 00:30:16,280
 from no layer to higher layer.

639
00:30:16,280 --> 00:30:18,720
 To balance this information, then we

640
00:30:18,720 --> 00:30:23,080
 utilize a simple R to make the information of the higher

641
00:30:23,080 --> 00:30:28,240
 layer information map go to the no-neighborhood algorithm

642
00:30:28,240 --> 00:30:33,240
 to produce the final feature information map.

643
00:30:33,240 --> 00:30:36,639
 So this information will be more balanced

644
00:30:36,639 --> 00:30:39,560
 because the information now, this information,

645
00:30:39,560 --> 00:30:42,879
 is the two direction, or have two direction transfer

646
00:30:42,879 --> 00:30:44,800
 of the information.

647
00:30:44,800 --> 00:30:47,480
 So the information of a different layer,

648
00:30:47,480 --> 00:30:53,040
 the information will be balanced after we go through R.

649
00:30:53,920 --> 00:30:55,760
 Now then after that, of course, we

650
00:30:55,760 --> 00:30:58,280
 need some kind of the normalization.

651
00:30:58,280 --> 00:31:03,240
 And then we produce the weights based on this map.

652
00:31:03,240 --> 00:31:07,560
 OK, the weights is also a map because every pixel,

653
00:31:07,560 --> 00:31:09,159
 we have different cospheres.

654
00:31:09,159 --> 00:31:10,879
 We have different cospheres score.

655
00:31:10,879 --> 00:31:14,560
 Then we need to merge them together.

656
00:31:14,560 --> 00:31:19,120
 So why these weights cannot be generated by the training data?

657
00:31:19,120 --> 00:31:21,920
 Because for the segmentation, the object

658
00:31:21,920 --> 00:31:24,400
 for different test data are totally different.

659
00:31:24,400 --> 00:31:28,600
 Even a same person, for this test image,

660
00:31:28,600 --> 00:31:30,000
 it can be this position.

661
00:31:30,000 --> 00:31:34,080
 For other test images, it can be totally other position.

662
00:31:34,080 --> 00:31:37,640
 So the weights cannot be determined by the training data.

663
00:31:37,640 --> 00:31:41,680
 Must be generated by the specific training data

664
00:31:41,680 --> 00:31:48,920
 then used in the classification of pixels of this test data.

665
00:31:48,920 --> 00:31:51,080
 Now this is our second contribution.

666
00:31:51,080 --> 00:31:56,560
 Another one is so-called boundary reframing.

667
00:31:56,560 --> 00:32:00,840
 From here, we can see that in this network structure,

668
00:32:00,840 --> 00:32:03,919
 we see that from this classification score,

669
00:32:03,919 --> 00:32:05,600
 we merge them together.

670
00:32:05,600 --> 00:32:11,240
 We already have some roughly segmentation result.

671
00:32:11,240 --> 00:32:13,960
 And then from the roughly segmentation result,

672
00:32:13,960 --> 00:32:18,159
 we can generate a boundary of this test image.

673
00:32:18,159 --> 00:32:20,840
 Now this generated boundary, then

674
00:32:20,840 --> 00:32:25,800
 we can utilize it to guide how to utilize the lower neighbor

675
00:32:25,800 --> 00:32:26,760
 feature.

676
00:32:26,760 --> 00:32:30,679
 We can put the lower neighbor feature higher weight

677
00:32:30,679 --> 00:32:33,439
 for the position close to the boundary

678
00:32:33,439 --> 00:32:39,159
 and the small weights for the feature far away from the boundary.

679
00:32:39,159 --> 00:32:40,879
 This is an idea.

680
00:32:40,879 --> 00:32:44,080
 The idea is also not difficult to understand.

681
00:32:44,080 --> 00:32:49,360
 So basically, given an image from the higher neighbor feature

682
00:32:49,360 --> 00:32:51,879
 and a middle neighbor feature classification,

683
00:32:51,879 --> 00:32:57,639
 we can roughly get a rough segmentation result.

684
00:32:57,639 --> 00:32:59,879
 Then based on the rough segmentation result,

685
00:32:59,879 --> 00:33:04,399
 we can produce a boundary.

686
00:33:04,399 --> 00:33:09,479
 This boundary, of course, is just one pixel line

687
00:33:09,479 --> 00:33:13,040
 to form this boundary.

688
00:33:13,040 --> 00:33:15,800
 Now from this boundary, we can always

689
00:33:15,800 --> 00:33:20,360
 go through a smooth field to convert this hard boundary

690
00:33:20,360 --> 00:33:23,000
 into a soft boundary.

691
00:33:23,000 --> 00:33:26,200
 This one is a hard boundary because the boundary

692
00:33:26,200 --> 00:33:27,480
 pixel is 1.

693
00:33:27,480 --> 00:33:29,560
 Not a boundary is 0.

694
00:33:29,560 --> 00:33:30,720
 It's a hard boundary.

695
00:33:30,720 --> 00:33:33,800
 So we convert this into a soft boundary.

696
00:33:33,800 --> 00:33:39,200
 That means the pixel value is directly from 0, increased to 1,

697
00:33:39,200 --> 00:33:41,879
 and then decreased to 0.

698
00:33:41,879 --> 00:33:43,720
 The soft boundary.

699
00:33:44,240 --> 00:33:45,520
 We easily understand.

700
00:33:45,520 --> 00:33:48,840
 Go through a smooth field such as Gaussian field,

701
00:33:48,840 --> 00:33:54,800
 then we can blur this boundary into a soft boundary.

702
00:33:54,800 --> 00:33:59,720
 Here, we use a Gaussian field to produce a soft boundary.

703
00:33:59,720 --> 00:34:04,920
 But if we utilize the Gaussian field, the Gaussian field,

704
00:34:04,920 --> 00:34:10,560
 if we choose a Gaussian field, we choose a Gaussian PDF

705
00:34:10,560 --> 00:34:14,560
 as the impulse response of the field.

706
00:34:14,560 --> 00:34:18,120
 Then if we choose this small standard deviation,

707
00:34:18,120 --> 00:34:21,080
 then the Gaussian function is still like that.

708
00:34:21,080 --> 00:34:23,159
 If we choose a large standard deviation,

709
00:34:23,159 --> 00:34:26,000
 then it is more flat.

710
00:34:26,000 --> 00:34:31,719
 Now if we look at a line closer to the boundary,

711
00:34:31,719 --> 00:34:34,560
 if we take a line closer to the boundary,

712
00:34:34,560 --> 00:34:40,120
 then the hard boundary is just one impulse.

713
00:34:40,159 --> 00:34:43,359
 Only at the pixel of the boundary, it is one.

714
00:34:43,359 --> 00:34:46,600
 All other peaks are not on the boundary, all are 0.

715
00:34:46,600 --> 00:34:49,120
 So it is an impulse.

716
00:34:49,120 --> 00:34:54,759
 Now if we apply a Gaussian field to smooth it,

717
00:34:54,759 --> 00:34:58,560
 then one impulse will become to this Gaussian function.

718
00:34:58,560 --> 00:35:01,680
 Because we know an impulse can walk with any function,

719
00:35:01,680 --> 00:35:03,880
 it's this function.

720
00:35:03,880 --> 00:35:09,080
 Now if we take different standard deviation of the Gaussian

721
00:35:09,080 --> 00:35:13,279
 filter, then we can produce different output.

722
00:35:13,279 --> 00:35:16,160
 Now from this different output, we

723
00:35:16,160 --> 00:35:21,319
 can see that for a position close to this impulse,

724
00:35:21,319 --> 00:35:26,000
 close to a boundary, this function has a large value.

725
00:35:26,000 --> 00:35:30,720
 Well, this filter has an output way of small.

726
00:35:30,720 --> 00:35:36,080
 But for the position far away from the boundary,

727
00:35:36,080 --> 00:35:39,480
 it could be this function has a larger value,

728
00:35:39,480 --> 00:35:43,880
 but this function has a very small value.

729
00:35:43,880 --> 00:35:48,560
 So from here, we can see we can use

730
00:35:48,560 --> 00:35:53,520
 these three different outputs to weight

731
00:35:53,520 --> 00:36:01,160
 different layer of the very known level of the features.

732
00:36:01,160 --> 00:36:05,080
 So we can see close to the boundary,

733
00:36:05,080 --> 00:36:07,880
 the small scale has a large weight.

734
00:36:07,880 --> 00:36:10,319
 Far away from the boundary, the small scale

735
00:36:10,319 --> 00:36:11,920
 has very small weights.

736
00:36:11,920 --> 00:36:16,279
 Well, from this output, we can see far away from the boundary,

737
00:36:16,279 --> 00:36:21,120
 the weights will be higher than other weights.

738
00:36:21,120 --> 00:36:25,840
 So this smooth output with different Gaussian

739
00:36:25,840 --> 00:36:27,880
 filter, different standard deviation

740
00:36:27,880 --> 00:36:32,080
 are used to weight the classification score

741
00:36:32,080 --> 00:36:35,799
 from different layer, then merge them together

742
00:36:35,799 --> 00:36:41,240
 to produce the final classification score.

743
00:36:41,240 --> 00:36:47,720
 So this is so with that, it can improve the boundary.

744
00:36:47,720 --> 00:36:51,799
 I just introduced the idea, not show the result.

745
00:36:51,799 --> 00:37:01,240
 So basically, this paper in the CVPR aura paper,

746
00:37:01,279 --> 00:37:09,759
 2018 now it received more than 300 of the citations.

747
00:37:09,759 --> 00:37:12,399
 OK, next example, we will show how

748
00:37:12,399 --> 00:37:17,560
 to change the convolution from the fixed kernel

749
00:37:17,560 --> 00:37:25,439
 into a variable kernel and why it is necessary.

750
00:37:25,439 --> 00:37:29,560
 Now here, we show an image here.

751
00:37:29,560 --> 00:37:32,240
 This is the ground truth of the segmentation.

752
00:37:32,240 --> 00:37:35,560
 We have the train, we have the lake, we have the sky,

753
00:37:35,560 --> 00:37:40,600
 we have the mountain, we have the land,

754
00:37:40,600 --> 00:37:44,000
 the different cars, we classify every pixel

755
00:37:44,000 --> 00:37:44,880
 into different cars.

756
00:37:44,880 --> 00:37:49,600
 So this is the segmentation result or the ground truth.

757
00:37:49,600 --> 00:37:51,920
 Now from here, we can see, for example,

758
00:37:51,920 --> 00:37:55,920
 for pixel A and the pixel B, one pixel,

759
00:37:55,920 --> 00:38:00,480
 the segmentation network should be very reliable,

760
00:38:00,480 --> 00:38:04,240
 classify it as the lake pixel.

761
00:38:04,240 --> 00:38:07,960
 Well, this pixel, we should classify it

762
00:38:07,960 --> 00:38:11,320
 a pixel of a train.

763
00:38:11,320 --> 00:38:16,160
 How can make the pixel A and the pixel B different?

764
00:38:16,160 --> 00:38:20,000
 Obviously, we need to produce the feature vector

765
00:38:20,000 --> 00:38:24,560
 that the feature vector at the pixel A and the pixel B,

766
00:38:24,560 --> 00:38:27,840
 it will be very different.

767
00:38:27,840 --> 00:38:31,600
 So that this is a pixel of the lake,

768
00:38:31,600 --> 00:38:34,880
 this is a pixel of a train.

769
00:38:34,880 --> 00:38:39,880
 Then how to reliably classify this pixel as a train?

770
00:38:39,880 --> 00:38:43,440
 We cannot just use the information at this point.

771
00:38:43,440 --> 00:38:47,720
 We should use information of a large area together

772
00:38:47,720 --> 00:38:51,640
 to determine this pixel is a train pixel.

773
00:38:51,640 --> 00:38:54,839
 If you just use one local information,

774
00:38:54,839 --> 00:38:59,920
 it is hardly to decide it is a belong to a train.

775
00:38:59,920 --> 00:39:03,440
 We should look at a larger area.

776
00:39:03,440 --> 00:39:08,920
 OK, now if we use a convolution, maybe

777
00:39:08,920 --> 00:39:13,600
 layers of the convolution to produce the effective color

778
00:39:13,600 --> 00:39:17,480
 size is larger, for example, this example.

779
00:39:17,480 --> 00:39:21,040
 We should show this large convolution kernel

780
00:39:21,040 --> 00:39:25,880
 to get the output of the pixel A and output of the pixel B.

781
00:39:25,880 --> 00:39:27,160
 What happened?

782
00:39:27,160 --> 00:39:32,680
 We will see that the information in this pink color window

783
00:39:32,680 --> 00:39:35,280
 and the yellow color window, the information

784
00:39:35,280 --> 00:39:40,320
 are almost the same because you use a large area.

785
00:39:40,320 --> 00:39:45,640
 We have a large area of these two windows are overlapped.

786
00:39:45,640 --> 00:39:49,839
 So this will affect the discrimination

787
00:39:49,839 --> 00:39:54,720
 between pixel A and pixel B. But if we

788
00:39:54,720 --> 00:39:58,319
 use a small area to connect information,

789
00:39:58,319 --> 00:40:06,440
 then we cannot reliably make sure this pixel B is a train.

790
00:40:06,440 --> 00:40:08,000
 We need to use a large area.

791
00:40:08,000 --> 00:40:11,319
 But to use a large area, it could average the information

792
00:40:11,319 --> 00:40:15,120
 so that the different class, the convolution result,

793
00:40:15,120 --> 00:40:19,839
 are almost the same because we have much common pixels

794
00:40:19,839 --> 00:40:23,160
 as the input to produce the output.

795
00:40:23,160 --> 00:40:25,520
 So this is the problem.

796
00:40:25,520 --> 00:40:27,680
 Now how to solve this problem?

797
00:40:27,680 --> 00:40:31,240
 We should try to do a convolution

798
00:40:31,240 --> 00:40:34,319
 to produce a feature vector of the pixel A,

799
00:40:34,319 --> 00:40:40,040
 use the information of the neck and surround the neck.

800
00:40:40,040 --> 00:40:44,319
 Now the final feature vector to represent this as a pixel

801
00:40:44,320 --> 00:40:45,640
 of the train.

802
00:40:45,640 --> 00:40:48,960
 We should use the information of the pixels

803
00:40:48,960 --> 00:40:54,160
 or pixels of the train and the surroundings of the train.

804
00:40:54,160 --> 00:40:56,400
 It is meant.

805
00:40:56,400 --> 00:41:00,640
 So if it's possible, we do the convolution,

806
00:41:00,640 --> 00:41:04,280
 the effective convolution kernel of the pixel A

807
00:41:04,280 --> 00:41:06,080
 is this red color.

808
00:41:06,080 --> 00:41:09,680
 And if we do the convolution to produce the output feature

809
00:41:09,680 --> 00:41:14,399
 vector of the pixel B is this yellow color,

810
00:41:14,399 --> 00:41:18,640
 then the output of the pixel A and the pixel B

811
00:41:18,640 --> 00:41:22,359
 were significantly different.

812
00:41:22,359 --> 00:41:25,480
 OK, different from this case.

813
00:41:25,480 --> 00:41:29,960
 And also we connect a large number of the pixel information.

814
00:41:29,960 --> 00:41:33,440
 We can reliably determine this point

815
00:41:33,440 --> 00:41:38,359
 is a point of the train, not a neck.

816
00:41:38,360 --> 00:41:42,440
 Now then how to make our convolution

817
00:41:42,440 --> 00:41:46,960
 use the kernel with different size and different shape?

818
00:41:46,960 --> 00:41:51,360
 Then we need to produce a so-called filter mask

819
00:41:51,360 --> 00:41:57,480
 to mask to put different weightage at different pixels

820
00:41:57,480 --> 00:42:01,800
 for the convolution of a single pixel.

821
00:42:01,800 --> 00:42:03,880
 So how to produce that?

822
00:42:03,880 --> 00:42:06,440
 I will skip the ideas.

823
00:42:06,440 --> 00:42:10,200
 But basically it is a pired convolution.

824
00:42:10,200 --> 00:42:15,800
 So given a print train to get an output feature map,

825
00:42:15,800 --> 00:42:21,120
 we further go through a large size of the convolution.

826
00:42:21,120 --> 00:42:24,520
 So we can connect information of the large area.

827
00:42:24,520 --> 00:42:30,040
 But this convolution result will be multiplied by a mask.

828
00:42:30,040 --> 00:42:35,320
 This mask is where we determine the filter convolution

829
00:42:35,320 --> 00:42:38,000
 kernel shape and size.

830
00:42:38,000 --> 00:42:40,240
 But how to produce this mask?

831
00:42:40,240 --> 00:42:43,400
 To produce a mask of one pixel, we

832
00:42:43,400 --> 00:42:46,960
 try to do a pired convolution.

833
00:42:46,960 --> 00:42:49,400
 The pired convolution is the difference

834
00:42:49,400 --> 00:42:51,280
 of the convolution of this center

835
00:42:51,280 --> 00:42:56,600
 pixel with all possible other pixels in the surroundings.

836
00:42:56,600 --> 00:42:59,280
 Now after the pired convolution, we

837
00:42:59,280 --> 00:43:02,960
 can get the information, correlation information.

838
00:43:02,960 --> 00:43:07,160
 That means for one pixel, we want to have a mask.

839
00:43:07,160 --> 00:43:10,360
 We try to get the information how

840
00:43:10,360 --> 00:43:14,640
 this pixel is correlated with other pixels.

841
00:43:14,640 --> 00:43:19,440
 For other pixels, it has strong correlation to this one.

842
00:43:19,440 --> 00:43:21,600
 Then the weight will be larger.

843
00:43:21,600 --> 00:43:24,680
 For some pixels, it is a weakly correlated

844
00:43:24,680 --> 00:43:26,120
 with this center pixel.

845
00:43:26,120 --> 00:43:28,760
 Then the weight will be small.

846
00:43:28,760 --> 00:43:31,280
 So we will produce a mask.

847
00:43:31,280 --> 00:43:35,640
 Then we use this mask to multiply to the convolution.

848
00:43:35,640 --> 00:43:40,240
 Then we will effectively to shape the convolution kernel

849
00:43:40,240 --> 00:43:42,720
 size and shape.

850
00:43:42,720 --> 00:43:44,280
 OK.

851
00:43:44,280 --> 00:43:46,480
 Here we use the pired convolution.

852
00:43:46,480 --> 00:43:52,080
 Every time we only use two small convolution,

853
00:43:52,080 --> 00:43:57,000
 one is a center position, another is a different position.

854
00:43:57,000 --> 00:43:59,800
 Then we compile these two convolution results

855
00:43:59,800 --> 00:44:04,200
 to get the correlation between different pixels.

856
00:44:04,200 --> 00:44:07,040
 Now next one, we will show a result.

857
00:44:07,040 --> 00:44:10,920
 What is this so-called shape mask?

858
00:44:10,920 --> 00:44:11,840
 OK.

859
00:44:11,840 --> 00:44:13,880
 This is an input image.

860
00:44:13,880 --> 00:44:22,880
 And this is a generated shape mask of a pixel in this area.

861
00:44:22,880 --> 00:44:29,160
 And this is a generated shape mask of a pixel in this area.

862
00:44:29,160 --> 00:44:32,080
 Because the shape mask is just weighted, right?

863
00:44:32,080 --> 00:44:33,920
 So we reduce the size.

864
00:44:33,920 --> 00:44:37,399
 So in fact, this image is the size

865
00:44:37,399 --> 00:44:40,319
 is similar to the whole image.

866
00:44:40,319 --> 00:44:41,319
 OK.

867
00:44:41,319 --> 00:44:47,080
 Now from here, we can see if this pired convolution

868
00:44:47,080 --> 00:44:50,399
 correctly generates a shape mask.

869
00:44:50,399 --> 00:44:57,600
 Because to do the convolution to extract the feature of the pixel

870
00:44:57,600 --> 00:45:00,000
 around here, we do the convolution

871
00:45:00,000 --> 00:45:03,759
 to collect all the information of all the pixels

872
00:45:03,759 --> 00:45:05,040
 in the large area.

873
00:45:05,040 --> 00:45:06,920
 But we put the weightage.

874
00:45:06,920 --> 00:45:11,120
 The pixels in this area is this car

875
00:45:11,120 --> 00:45:16,799
 where it has very high weightage because it's bright.

876
00:45:16,799 --> 00:45:21,279
 And the pixel on the road is also relatively high weightage.

877
00:45:21,279 --> 00:45:24,680
 Where the pixel of the grass and the cow, the weightage

878
00:45:24,680 --> 00:45:27,080
 is very small.

879
00:45:27,080 --> 00:45:32,840
 So this is a generated shape mask for convolution

880
00:45:32,840 --> 00:45:37,680
 of these pixels within this window.

881
00:45:37,680 --> 00:45:38,240
 OK.

882
00:45:38,240 --> 00:45:45,920
 This shape mask, in fact, represents the whole image size.

883
00:45:45,920 --> 00:45:51,160
 Now another shape mask shows that this is a shape mask

884
00:45:51,160 --> 00:45:57,040
 of to do the convolution, gets the output of the pixels

885
00:45:57,360 --> 00:46:00,080
 in this small window.

886
00:46:00,080 --> 00:46:04,120
 This is a pixel from the cow.

887
00:46:04,120 --> 00:46:08,000
 So we then connect the information of the cow

888
00:46:08,000 --> 00:46:13,840
 and other cows where it gets the high venues.

889
00:46:13,840 --> 00:46:17,279
 And even grass has high venues.

890
00:46:17,279 --> 00:46:17,840
 OK.

891
00:46:17,840 --> 00:46:22,680
 Where the road and the car is not correlated with the cow,

892
00:46:22,680 --> 00:46:27,520
 then in the shape mask, the venue is very small.

893
00:46:27,520 --> 00:46:28,120
 OK.

894
00:46:28,120 --> 00:46:31,640
 Similarly for other examples, I will not go through this example.

895
00:46:31,640 --> 00:46:34,560
 Just use this example to elaborate

896
00:46:34,560 --> 00:46:38,600
 what is the meaning of this shape mask.

897
00:46:38,600 --> 00:46:41,080
 Now from this shape mask, we can say

898
00:46:41,080 --> 00:46:43,720
 that if you have some experience in deep learning,

899
00:46:43,720 --> 00:46:48,319
 you should know this map is very similar to the so-called

900
00:46:48,319 --> 00:46:51,240
 attention map.

901
00:46:51,399 --> 00:46:54,240
 And attention map, in fact, the transformer

902
00:46:54,240 --> 00:46:57,959
 is fully utilized the attention.

903
00:46:57,959 --> 00:46:58,640
 OK.

904
00:46:58,640 --> 00:47:02,720
 So before the transformer, people walk on the scene

905
00:47:02,720 --> 00:47:08,439
 already notice that we need some kind of attention.

906
00:47:08,439 --> 00:47:11,359
 Well, this attention, we produce this attention

907
00:47:11,359 --> 00:47:13,959
 using the PyRWise convolution.

908
00:47:13,959 --> 00:47:14,200
 OK.

909
00:47:14,200 --> 00:47:18,759
 Use two pixels at two different positions.

910
00:47:18,760 --> 00:47:23,480
 What is the compare the convolution result

911
00:47:23,480 --> 00:47:26,760
 to get the attention result?

912
00:47:26,760 --> 00:47:30,560
 This is quite similar to the attention

913
00:47:30,560 --> 00:47:32,600
 module in the transformer.

914
00:47:32,600 --> 00:47:35,280
 So later I will elaborate that.

915
00:47:35,280 --> 00:47:37,120
 OK.

916
00:47:37,120 --> 00:47:39,800
 Now another contribution of this work

917
00:47:39,800 --> 00:47:43,160
 is called neighboring denoising.

918
00:47:43,160 --> 00:47:46,800
 In the segmentation, somehow we have

919
00:47:46,840 --> 00:47:50,760
 two different kinds of the arrow.

920
00:47:50,760 --> 00:47:57,400
 One arrow is somehow is the so-called high level

921
00:47:57,400 --> 00:47:58,400
 of the arrow.

922
00:47:58,400 --> 00:48:01,280
 It is a classifying arrow uses the feature

923
00:48:01,280 --> 00:48:05,120
 of the higher level of the convolution.

924
00:48:05,120 --> 00:48:07,360
 So because the higher level of the convolution,

925
00:48:07,360 --> 00:48:11,800
 the feature map corrects a large scale of the features.

926
00:48:11,800 --> 00:48:21,360
 So that most likely the classification class is correct.

927
00:48:21,360 --> 00:48:26,680
 We can identify what is different class in this input image.

928
00:48:26,680 --> 00:48:31,680
 But we can only identify the class center

929
00:48:31,680 --> 00:48:38,600
 where on the boundary of the class, it may make some arrow.

930
00:48:38,600 --> 00:48:43,680
 So this is one kind of the classification arrow.

931
00:48:43,680 --> 00:48:46,000
 Another kind of the arrow is called

932
00:48:46,000 --> 00:48:47,600
 the known level of the arrow.

933
00:48:47,600 --> 00:48:50,400
 Known level of the arrow is the classification result

934
00:48:50,400 --> 00:48:53,000
 may introduce many different class

935
00:48:53,000 --> 00:48:56,440
 never appear in your input image.

936
00:48:56,440 --> 00:48:59,720
 It comes from the very noise input.

937
00:48:59,720 --> 00:49:00,520
 OK.

938
00:49:00,520 --> 00:49:05,839
 So how is this two types of the classification arrow?

939
00:49:05,839 --> 00:49:08,480
 For example, here we show examples.

940
00:49:08,480 --> 00:49:14,840
 In usually in one input image, we want to segment it.

941
00:49:14,840 --> 00:49:17,920
 It may contain a few different objects.

942
00:49:17,920 --> 00:49:21,600
 So the number of the class could be just five or seven,

943
00:49:21,600 --> 00:49:24,480
 a few different objects.

944
00:49:24,480 --> 00:49:24,720
 OK.

945
00:49:24,720 --> 00:49:27,840
 This is for one single input image.

946
00:49:27,840 --> 00:49:29,680
 We want to do the segmentation.

947
00:49:29,680 --> 00:49:33,360
 We have only a few particular class.

948
00:49:33,360 --> 00:49:37,600
 But your network is trained by a data set.

949
00:49:37,640 --> 00:49:40,839
 A data set may contain many images.

950
00:49:40,839 --> 00:49:48,240
 Then together we may have 500 different class inside

951
00:49:48,240 --> 00:49:52,680
 of your class training data set.

952
00:49:52,680 --> 00:49:55,799
 But for one single particular input image

953
00:49:55,799 --> 00:50:01,240
 to do the segmentation, it contains only a few class.

954
00:50:01,240 --> 00:50:03,279
 Then we do an experiment.

955
00:50:03,280 --> 00:50:09,960
 Show that if we use the feature map of the convolution

956
00:50:09,960 --> 00:50:12,640
 as the feature to do the classification,

957
00:50:12,640 --> 00:50:17,520
 if we use a higher layer of the convolution map

958
00:50:17,520 --> 00:50:19,480
 as the feature to do the classification,

959
00:50:19,480 --> 00:50:22,120
 then the final classification result

960
00:50:22,120 --> 00:50:25,000
 will produce just five class.

961
00:50:25,000 --> 00:50:30,240
 This five class is the two class in the input image.

962
00:50:30,479 --> 00:50:34,200
 All together, it only produce five different class.

963
00:50:34,200 --> 00:50:38,959
 But the class may be run at the edge, at the boundary,

964
00:50:38,959 --> 00:50:43,240
 between two different objects.

965
00:50:43,240 --> 00:50:47,520
 But it will not produce a very noise class.

966
00:50:47,520 --> 00:50:50,479
 Now, the class pigeon result of the middle layer

967
00:50:50,479 --> 00:50:55,439
 or lower layer of the convolution map

968
00:50:55,439 --> 00:50:57,600
 as the classification feature, then

969
00:50:57,640 --> 00:51:00,560
 will produce a large number of the class.

970
00:51:00,560 --> 00:51:05,120
 Many class is never shown in this input image.

971
00:51:05,120 --> 00:51:06,480
 OK.

972
00:51:06,480 --> 00:51:09,040
 So then how to solve this problem?

973
00:51:09,040 --> 00:51:13,480
 We somehow utilize the so-called enabling denoising.

974
00:51:13,480 --> 00:51:19,520
 That means in this network, in the convolution network,

975
00:51:19,520 --> 00:51:25,360
 we use different layer of the feature map as the classification

976
00:51:25,360 --> 00:51:27,560
 feature to do the classification.

977
00:51:27,560 --> 00:51:32,560
 The result of the classification score of the different layer

978
00:51:32,560 --> 00:51:33,799
 will be different.

979
00:51:33,799 --> 00:51:35,040
 Right?

980
00:51:35,040 --> 00:51:36,440
 OK.

981
00:51:36,440 --> 00:51:40,799
 The last layer, the classification result,

982
00:51:40,799 --> 00:51:46,279
 will be mostly correct within the two class level.

983
00:51:46,279 --> 00:51:49,240
 But for some pictures, it could be wrong.

984
00:51:49,240 --> 00:51:52,600
 But it will not introduce the class

985
00:51:52,600 --> 00:51:55,320
 never appear in the input image.

986
00:51:55,320 --> 00:51:55,880
 OK.

987
00:51:55,880 --> 00:52:02,000
 So we can utilize the classification score of the higher

988
00:52:02,000 --> 00:52:08,160
 layer after some conversion to remove some class

989
00:52:08,160 --> 00:52:11,120
 or reduce the classification score of the lower

990
00:52:11,120 --> 00:52:13,120
 layer classification result.

991
00:52:13,120 --> 00:52:18,520
 For such class, it will not appear in the higher layer.

992
00:52:18,520 --> 00:52:23,680
 So for the class not appear in this result,

993
00:52:23,680 --> 00:52:26,720
 the classification score will be reduced.

994
00:52:26,720 --> 00:52:30,359
 Well, for class appear at the same layer,

995
00:52:30,359 --> 00:52:33,080
 we increase the weightage.

996
00:52:33,080 --> 00:52:37,560
 So that means we use the high level of the classification

997
00:52:37,560 --> 00:52:42,160
 score to de-depress the classification score of the

998
00:52:42,160 --> 00:52:46,759
 lower layer that the class is not

999
00:52:46,759 --> 00:52:49,080
 appear in the higher layer.

1000
00:52:49,080 --> 00:52:52,279
 So we can do such way from the higher layer

1001
00:52:52,280 --> 00:52:56,840
 to lower layer at the end to produce a final classification

1002
00:52:56,840 --> 00:52:57,840
 score.

1003
00:52:57,840 --> 00:53:02,440
 So this one, the basic idea is use the higher layer of the

1004
00:53:02,440 --> 00:53:07,760
 feature classification score to somehow remove the noise

1005
00:53:07,760 --> 00:53:11,040
 of the class key result generated from the lower

1006
00:53:11,040 --> 00:53:15,520
 layer of the classification result.

1007
00:53:18,920 --> 00:53:19,280
 OK.

1008
00:53:19,280 --> 00:53:25,200
 So this is another example to show how to use print

1009
00:53:25,200 --> 00:53:31,280
 and C&N general C&N for some specific application.

1010
00:53:35,040 --> 00:53:35,560
 OK.

1011
00:53:35,560 --> 00:53:40,360
 Now, in fact, here, as I mentioned that, in the work

1012
00:53:40,360 --> 00:53:45,120
 of the C&N, we already generate some idea to produce

1013
00:53:45,120 --> 00:53:47,280
 the attention map.

1014
00:53:47,280 --> 00:53:51,120
 The attention map, what means attention map, for example,

1015
00:53:51,120 --> 00:53:57,800
 this part is highly correlated.

1016
00:53:57,800 --> 00:53:59,800
 So the attention will be high.

1017
00:53:59,800 --> 00:54:04,280
 Well, the grass is not correlated to the car, so the

1018
00:54:04,280 --> 00:54:07,880
 attention in the grass is very low.

1019
00:54:07,880 --> 00:54:11,480
 So the correlation is also some kind of the attention.

1020
00:54:11,480 --> 00:54:14,360
 Just use different name.

1021
00:54:14,400 --> 00:54:14,680
 OK.

1022
00:54:14,680 --> 00:54:19,600
 Now, we will study the so-called transformer.

1023
00:54:19,600 --> 00:54:22,520
 So why people study the transformer?

1024
00:54:22,520 --> 00:54:23,640
 OK.

1025
00:54:23,640 --> 00:54:29,200
 Transformer is invented by a group of the young researchers

1026
00:54:29,200 --> 00:54:30,640
 in Google.

1027
00:54:30,640 --> 00:54:34,720
 Publish a paper in CWPR called is a transformer.

1028
00:54:34,720 --> 00:54:34,960
 OK.

1029
00:54:34,960 --> 00:54:38,640
 So first, we understand what is a transformer.

1030
00:54:38,640 --> 00:54:41,800
 Transformer is a neural network architecture that will

1031
00:54:41,800 --> 00:54:46,280
 transform an input sequence into an output sequence.

1032
00:54:46,280 --> 00:54:46,840
 OK.

1033
00:54:46,840 --> 00:54:51,440
 Such a sequence can be a speech, can be text, or a time

1034
00:54:51,440 --> 00:54:52,680
 series.

1035
00:54:52,680 --> 00:54:53,200
 OK.

1036
00:54:53,200 --> 00:54:56,200
 So originally, the transformer is to handle a

1037
00:54:56,200 --> 00:54:58,800
 sequence of the information.

1038
00:54:58,800 --> 00:54:59,280
 OK.

1039
00:54:59,280 --> 00:55:03,760
 So this is basically the original basic structure of

1040
00:55:03,760 --> 00:55:06,480
 the transformer.

1041
00:55:06,480 --> 00:55:13,560
 Now, we know to handle a sequence of the information.

1042
00:55:13,560 --> 00:55:18,040
 We can use the so-called recurrent neural network, or

1043
00:55:18,040 --> 00:55:18,680
 RNN.

1044
00:55:18,680 --> 00:55:23,200
 The further development of the RNN is the AROHTM.

1045
00:55:23,200 --> 00:55:29,360
 This is somehow enhanced structure of the RNN.

1046
00:55:29,360 --> 00:55:31,560
 So we have AROHTM.

1047
00:55:31,560 --> 00:55:39,080
 But the transformer outperforms them because RNN and

1048
00:55:39,080 --> 00:55:43,880
 AROHTM, the information processing is one by one.

1049
00:55:43,880 --> 00:55:44,480
 OK.

1050
00:55:44,480 --> 00:55:51,960
 So it's from the beginning to the end to process this

1051
00:55:51,960 --> 00:55:53,640
 information.

1052
00:55:53,640 --> 00:55:58,680
 But in the transformer, we take all sequence information

1053
00:55:58,680 --> 00:56:02,839
 parallel together into the network to produce the

1054
00:56:02,839 --> 00:56:04,200
 output.

1055
00:56:04,200 --> 00:56:04,720
 OK.

1056
00:56:04,720 --> 00:56:07,960
 So this is the difference between the transformer and the

1057
00:56:07,960 --> 00:56:10,359
 previous RNN and AROHTM.

1058
00:56:14,240 --> 00:56:19,080
 Now, it is developed originally for NARP, natural

1059
00:56:19,080 --> 00:56:24,080
 language processing to process the text information.

1060
00:56:24,080 --> 00:56:27,000
 Now, the chat GPD is very powerful.

1061
00:56:27,000 --> 00:56:31,600
 The basic element of the chat GPD is transformer, or

1062
00:56:31,600 --> 00:56:33,440
 come from the transformer.

1063
00:56:33,440 --> 00:56:34,040
 OK.

1064
00:56:34,040 --> 00:56:37,600
 So originally, it comes from the NARP.

1065
00:56:37,600 --> 00:56:41,760
 But now, it also becomes a very powerful structure, even

1066
00:56:41,760 --> 00:56:44,000
 for the computation problem.

1067
00:56:44,000 --> 00:56:44,240
 OK.

1068
00:56:44,240 --> 00:56:46,960
 So much powerful as CNN.

1069
00:56:50,840 --> 00:56:55,640
 Now, transformer gets its powers because it's used a

1070
00:56:55,640 --> 00:56:57,160
 tension module.

1071
00:56:57,160 --> 00:56:57,799
 OK.

1072
00:56:57,799 --> 00:57:02,279
 It captures a relation between different parts of the

1073
00:57:02,279 --> 00:57:03,680
 input.

1074
00:57:03,680 --> 00:57:03,960
 OK.

1075
00:57:03,960 --> 00:57:06,879
 So then, how to understand the transformer?

1076
00:57:06,879 --> 00:57:10,920
 So after the transformer shows such a very excellent

1077
00:57:10,920 --> 00:57:13,120
 performance, much better than CNN.

1078
00:57:13,120 --> 00:57:16,440
 Then people try to understand what is the transformer, what

1079
00:57:16,440 --> 00:57:19,520
 is the difference between transformer and the CNN.

1080
00:57:19,520 --> 00:57:24,640
 Then people think about CNN is somehow has no quality.

1081
00:57:24,640 --> 00:57:28,480
 CNN output is only captured at local information.

1082
00:57:28,480 --> 00:57:31,680
 But the transformer is global information.

1083
00:57:31,680 --> 00:57:32,359
 OK.

1084
00:57:32,359 --> 00:57:35,400
 Because the output of the transformer is captured

1085
00:57:35,400 --> 00:57:37,560
 from all input.

1086
00:57:37,560 --> 00:57:43,279
 So because the transformer is global, so it will outperform

1087
00:57:43,279 --> 00:57:48,120
 much better than the local information

1088
00:57:48,120 --> 00:57:51,560
 shown in the CNN.

1089
00:57:51,560 --> 00:57:55,560
 But in my personal opinion, this understanding is not

1090
00:57:55,560 --> 00:57:56,520
 correct.

1091
00:57:56,520 --> 00:57:59,000
 It's somehow problem.

1092
00:57:59,000 --> 00:58:04,200
 Now, if we want to capture the global information, the

1093
00:58:04,200 --> 00:58:09,240
 traditional NARP is really connect the whole global

1094
00:58:09,240 --> 00:58:10,680
 information.

1095
00:58:10,680 --> 00:58:14,440
 In the traditional neural network, each output is

1096
00:58:14,440 --> 00:58:17,480
 determined by all input.

1097
00:58:17,480 --> 00:58:20,160
 It is fully connected.

1098
00:58:20,160 --> 00:58:24,720
 So the traditional NARP is really get a global

1099
00:58:24,720 --> 00:58:26,440
 information.

1100
00:58:26,440 --> 00:58:32,000
 But why is this traditional NARP perform so poorly?

1101
00:58:32,000 --> 00:58:34,600
 It has a very severe problem.

1102
00:58:34,600 --> 00:58:38,160
 And the problem is solved by CNN.

1103
00:58:38,160 --> 00:58:41,440
 We come from the global information to a local

1104
00:58:41,440 --> 00:58:42,440
 information.

1105
00:58:42,440 --> 00:58:45,160
 The CNN field size is small.

1106
00:58:45,160 --> 00:58:46,680
 It capture a local information.

1107
00:58:46,680 --> 00:58:50,919
 So the local information in the CNN perform much, much,

1108
00:58:50,919 --> 00:58:55,120
 much better than NARP global information.

1109
00:58:55,120 --> 00:58:58,560
 But why we come back to the global information in the

1110
00:58:58,560 --> 00:59:01,359
 transformer?

1111
00:59:01,359 --> 00:59:01,680
 OK.

1112
00:59:01,680 --> 00:59:05,520
 So this superficially understanding is not

1113
00:59:05,520 --> 00:59:07,080
 strictly correct.

1114
00:59:07,080 --> 00:59:12,160
 So this is why here I show this global and the local is

1115
00:59:12,160 --> 00:59:16,200
 the transformer will capture the relationship not global

1116
00:59:16,200 --> 00:59:17,520
 in relationship.

1117
00:59:17,520 --> 00:59:23,160
 But it capture the relationship of each token with

1118
00:59:23,160 --> 00:59:25,839
 every other tokens.

1119
00:59:25,839 --> 00:59:29,319
 It's not all tokens.

1120
00:59:29,319 --> 00:59:29,839
 OK.

1121
00:59:29,839 --> 00:59:34,960
 So here we understand what is the difference between the

1122
00:59:34,960 --> 00:59:38,240
 CNN and the transformer.

1123
00:59:38,240 --> 00:59:41,720
 First, CNN field size is small.

1124
00:59:41,720 --> 00:59:44,919
 When they are of the convolution, really the output

1125
00:59:44,920 --> 00:59:47,920
 only capture a small local information.

1126
00:59:47,920 --> 00:59:53,280
 But the CNN has several times hundreds or

1127
00:59:53,280 --> 00:59:55,120
 thousands of the NARPs.

1128
00:59:55,120 --> 00:59:59,920
 Then after we do many convolution, the effective

1129
00:59:59,920 --> 01:00:02,280
 kernel size is very large.

1130
01:00:02,280 --> 01:00:06,360
 Because the first layer, 3 by 3 convolution, then the output

1131
01:00:06,360 --> 01:00:09,360
 of the 3 by 3 convolution come to next convolution of the

1132
01:00:09,360 --> 01:00:10,400
 3 by 3 window.

1133
01:00:10,400 --> 01:00:14,080
 Then the output of the second layer, it capture the

1134
01:00:14,080 --> 01:00:19,600
 information of the 5 by 5 window in the input.

1135
01:00:19,600 --> 01:00:23,040
 Now if we increase the size of the convolution layer, then

1136
01:00:23,040 --> 01:00:27,440
 the output of the convolution, in fact, capture a very large

1137
01:00:27,440 --> 01:00:31,600
 area could be the whole image.

1138
01:00:31,600 --> 01:00:37,799
 Because we have hundreds of the NARPs.

1139
01:00:37,799 --> 01:00:42,040
 So the CNN only capture the local information is not

1140
01:00:42,040 --> 01:00:45,279
 strictly correct.

1141
01:00:45,279 --> 01:00:48,920
 Because the higher layer of the output of the CNN can

1142
01:00:48,920 --> 01:00:54,320
 capture information of a very large area.

1143
01:00:54,320 --> 01:00:58,040
 But if the CNN, we only use the high

1144
01:00:58,040 --> 01:00:59,200
 level of the feature.

1145
01:00:59,200 --> 01:01:01,400
 For example, in the segmentation, we cannot just

1146
01:01:01,400 --> 01:01:02,640
 use the higher level feature.

1147
01:01:02,640 --> 01:01:06,360
 We must use the earlier layer of the feature together to do

1148
01:01:06,360 --> 01:01:07,920
 the classification.

1149
01:01:07,920 --> 01:01:11,560
 If we only use the high level of the feature, it capture a

1150
01:01:11,560 --> 01:01:15,440
 large area of the information, but it capture all the

1151
01:01:15,440 --> 01:01:19,400
 pictures in this large area, then the output will somehow

1152
01:01:19,400 --> 01:01:22,440
 blur the information.

1153
01:01:22,440 --> 01:01:26,560
 So it has some kind of the average effect.

1154
01:01:26,560 --> 01:01:29,200
 Because all convolution is just weight

1155
01:01:29,200 --> 01:01:31,920
 the average of input.

1156
01:01:31,920 --> 01:01:36,640
 So it will blur the information and somehow average the

1157
01:01:36,640 --> 01:01:40,560
 information into not so discriminative.

1158
01:01:40,560 --> 01:01:47,440
 Now, in the transformer, it is not used or large area to

1159
01:01:47,440 --> 01:01:49,240
 produce the output.

1160
01:01:49,240 --> 01:01:56,040
 It compute one picture with each of other point, not all

1161
01:01:56,040 --> 01:01:58,080
 together.

1162
01:01:58,080 --> 01:01:59,880
 So this is the difference.

1163
01:01:59,880 --> 01:02:04,000
 So the transformer here is to capture the relation of each

1164
01:02:04,000 --> 01:02:10,520
 word or token with every other.

1165
01:02:11,520 --> 01:02:13,560
 Not all other.

1166
01:02:13,560 --> 01:02:16,240
 So this is a difference.

1167
01:02:16,240 --> 01:02:20,080
 Now, another sense is the original paper, the first

1168
01:02:20,080 --> 01:02:22,920
 paper of the transformer.

1169
01:02:22,920 --> 01:02:28,520
 There also are a group of the young researchers in Google.

1170
01:02:28,520 --> 01:02:35,279
 The title of this paper is Attention Is All You Need.

1171
01:02:35,279 --> 01:02:38,200
 This is the title of this paper.

1172
01:02:38,200 --> 01:02:40,640
 Attention is all you need.

1173
01:02:40,640 --> 01:02:42,720
 Here I will discuss.

1174
01:02:42,720 --> 01:02:47,680
 Really, attention is all we need.

1175
01:02:47,680 --> 01:02:51,520
 It appears to be this paper present this transformer

1176
01:02:51,520 --> 01:02:56,720
 first time, try to make the transformer totally different

1177
01:02:56,720 --> 01:02:57,640
 from the thing.

1178
01:02:57,640 --> 01:03:00,759
 Nothing to do with the thing.

1179
01:03:00,759 --> 01:03:01,839
 OK.

1180
01:03:01,839 --> 01:03:06,279
 But after I study the transformer, I will say the

1181
01:03:06,280 --> 01:03:11,480
 transformer is also very closely to the CNN.

1182
01:03:11,480 --> 01:03:11,760
 OK.

1183
01:03:11,760 --> 01:03:16,840
 Later, I will show you how it close to the CNN.

1184
01:03:16,840 --> 01:03:21,080
 Now, the rest part we will study how this transformer

1185
01:03:21,080 --> 01:03:22,720
 works.

1186
01:03:22,720 --> 01:03:26,280
 And the attention is really everything.

1187
01:03:26,280 --> 01:03:30,520
 And the transformer has any relation to CNN.

1188
01:03:30,520 --> 01:03:33,360
 So can we simply understand that the transformer is

1189
01:03:33,360 --> 01:03:37,840
 global and CNN is just the local so that the transformer

1190
01:03:37,840 --> 01:03:43,040
 out of the CNN is not such a simple reason.

1191
01:03:43,040 --> 01:03:43,640
 OK.

1192
01:03:43,640 --> 01:03:47,560
 So we will study these issues.

1193
01:03:47,560 --> 01:03:54,360
 Now, before we further study, I would like to mention one

1194
01:03:54,360 --> 01:03:57,080
 very interesting sense.

1195
01:03:57,080 --> 01:04:01,560
 So you know, just last week I read the newspaper,

1196
01:04:01,560 --> 01:04:04,520
 Hinton received the Nobel Prize.

1197
01:04:04,520 --> 01:04:05,600
 Right?

1198
01:04:05,600 --> 01:04:07,799
 Now, Hinton received the Nobel Prize

1199
01:04:07,799 --> 01:04:11,680
 because of the two contributions.

1200
01:04:11,680 --> 01:04:17,400
 One is Hinton invented the back propagation in 1980s.

1201
01:04:17,400 --> 01:04:17,920
 OK.

1202
01:04:17,920 --> 01:04:22,880
 This back propagation greatly brings neural network research

1203
01:04:22,880 --> 01:04:24,840
 into a very high peak.

1204
01:04:24,840 --> 01:04:26,520
 Just the back propagation.

1205
01:04:26,520 --> 01:04:29,680
 Although back propagation is just a simple gradient

1206
01:04:29,680 --> 01:04:30,880
 decent method.

1207
01:04:31,240 --> 01:04:33,880
 We use the chance to compute the gradient.

1208
01:04:33,880 --> 01:04:34,280
 That's all.

1209
01:04:34,280 --> 01:04:35,760
 This is the back propagation.

1210
01:04:35,760 --> 01:04:36,920
 OK.

1211
01:04:36,920 --> 01:04:40,240
 The second most significant contribution of the Hinton

1212
01:04:40,240 --> 01:04:42,280
 is CNN.

1213
01:04:42,280 --> 01:04:45,920
 Hinton is the first person together with his student

1214
01:04:45,920 --> 01:04:50,640
 to build up the CNN then to achieve the image recognition

1215
01:04:50,640 --> 01:04:53,040
 of very high performance.

1216
01:04:53,040 --> 01:04:57,600
 Significantly higher than the performance before that.

1217
01:04:57,600 --> 01:04:59,640
 Use a CNN.

1218
01:04:59,640 --> 01:05:00,080
 OK.

1219
01:05:00,080 --> 01:05:03,759
 So this is the two contribution of the Hinton.

1220
01:05:03,759 --> 01:05:07,720
 Now, Hinton received the Nobel Prize.

1221
01:05:07,720 --> 01:05:10,600
 Now, because in fact, the Hinton,

1222
01:05:10,600 --> 01:05:14,960
 after invented the CNN, then many big companies

1223
01:05:14,960 --> 01:05:18,240
 show that CNN performs so powerful.

1224
01:05:18,240 --> 01:05:18,840
 Right?

1225
01:05:18,840 --> 01:05:25,799
 So try to get Hinton join the company.

1226
01:05:25,799 --> 01:05:28,400
 So many companies, big companies,

1227
01:05:28,400 --> 01:05:32,320
 try to ask Hinton to join the company.

1228
01:05:32,320 --> 01:05:36,280
 So at the end, Hinton joined Google

1229
01:05:36,280 --> 01:05:39,480
 as the vice president of Google.

1230
01:05:39,480 --> 01:05:41,440
 Of course, the salary is 10 times

1231
01:05:41,440 --> 01:05:45,640
 as a professor in University of Toronto.

1232
01:05:45,640 --> 01:05:52,200
 Now, he worked in Google several years or more than 10 years.

1233
01:05:52,200 --> 01:05:55,200
 I don't know what is the exact years

1234
01:05:55,200 --> 01:05:58,640
 as the VP vice president.

1235
01:05:58,640 --> 01:06:00,520
 Now, two years ago or one years ago,

1236
01:06:00,520 --> 01:06:02,240
 I cannot remember.

1237
01:06:02,240 --> 01:06:10,160
 Hinton left Google, come back to the University of Toronto,

1238
01:06:10,160 --> 01:06:13,359
 somehow left Google.

1239
01:06:13,359 --> 01:06:18,240
 But what is the reason that Hinton left Google?

1240
01:06:18,240 --> 01:06:21,439
 I'm not quite sure.

1241
01:06:21,439 --> 01:06:25,040
 Can you guess why Hinton left Google?

1242
01:06:25,080 --> 01:06:31,520
 Hinton himself claims that he's very somehow disappointed.

1243
01:06:31,520 --> 01:06:36,040
 He thinks about the AI can do very bad things.

1244
01:06:36,040 --> 01:06:36,600
 OK?

1245
01:06:36,600 --> 01:06:39,200
 Now, the AI is so powerful, right?

1246
01:06:39,200 --> 01:06:41,720
 Can be cheaper than our human.

1247
01:06:41,720 --> 01:06:45,000
 So in future, AI could do a very bad thing.

1248
01:06:45,000 --> 01:06:49,680
 So he don't want further work on the AI.

1249
01:06:49,680 --> 01:06:52,440
 This is a reason given by Hinton himself.

1250
01:06:52,440 --> 01:06:55,120
 So he don't want to do the AI anymore.

1251
01:06:55,120 --> 01:06:59,760
 So he left Google, come back to the University of Toronto.

1252
01:06:59,760 --> 01:07:02,400
 But I have doubt this is a real reason.

1253
01:07:05,400 --> 01:07:08,360
 Because it does not make sense that AI

1254
01:07:08,360 --> 01:07:12,200
 can be cheaper than the human.

1255
01:07:12,200 --> 01:07:16,880
 In my opinion, AI can never exceed our human's brain.

1256
01:07:16,880 --> 01:07:18,800
 OK?

1257
01:07:18,800 --> 01:07:25,760
 I doubt this is a real reason of Hinton left Google.

1258
01:07:25,760 --> 01:07:29,560
 But there's one sense in Google.

1259
01:07:29,560 --> 01:07:32,720
 Because before the transformer, the thing

1260
01:07:32,720 --> 01:07:38,120
 is the most powerful network structure.

1261
01:07:38,120 --> 01:07:43,960
 But then a group of the young researchers also in Google

1262
01:07:43,960 --> 01:07:46,760
 invent the transformer.

1263
01:07:46,800 --> 01:07:49,240
 And then people see that transformer

1264
01:07:49,240 --> 01:07:52,360
 is much more powerful than CNN.

1265
01:07:52,360 --> 01:07:57,200
 Now, all this larger model of the AI, large model,

1266
01:07:57,200 --> 01:08:02,840
 is built up by transformer, not the CNN.

1267
01:08:02,840 --> 01:08:03,480
 OK?

1268
01:08:03,480 --> 01:08:07,480
 So you can think about what is the reason this Hinton

1269
01:08:07,480 --> 01:08:08,600
 left Google.

1270
01:08:08,600 --> 01:08:11,080
 But Hinton received the Nobel Prize.

1271
01:08:11,080 --> 01:08:12,600
 OK?

1272
01:08:12,600 --> 01:08:15,080
 OK, now we will study what is a transformer.

1273
01:08:15,080 --> 01:08:18,840
 It is so powerful, right?

1274
01:08:18,840 --> 01:08:24,640
 Now, this is the basic component of the transformer.

1275
01:08:24,640 --> 01:08:32,080
 Basically, transformer has encoders and decoders.

1276
01:08:32,080 --> 01:08:37,720
 OK, encoders and decoders are two fundamental components

1277
01:08:37,720 --> 01:08:39,840
 in the transformer.

1278
01:08:39,840 --> 01:08:45,000
 OK, input, sequence, input into that decoder.

1279
01:08:45,920 --> 01:08:46,600
 Sorry.

1280
01:08:46,600 --> 01:08:52,080
 Then after the encoders process the input information,

1281
01:08:52,080 --> 01:08:56,279
 the output of the encoder come to decoder.

1282
01:08:56,279 --> 01:09:02,279
 Then the decoder output the final features.

1283
01:09:02,279 --> 01:09:06,279
 Then we just use a simple classifier, linear classifier,

1284
01:09:06,279 --> 01:09:08,600
 to get the output.

1285
01:09:08,600 --> 01:09:13,520
 So this is the basic structure of the transformer.

1286
01:09:13,520 --> 01:09:15,960
 Because the transformer first proposed

1287
01:09:15,960 --> 01:09:19,120
 to solve the problem of the NLP.

1288
01:09:19,120 --> 01:09:22,840
 So the input is a sequence of the word.

1289
01:09:22,840 --> 01:09:26,840
 It could be a sentence or a whole paragraph.

1290
01:09:26,840 --> 01:09:30,880
 Anyway, it consists of many different words, right?

1291
01:09:30,880 --> 01:09:32,400
 OK, it's a sequence.

1292
01:09:32,400 --> 01:09:33,120
 OK.

1293
01:09:33,120 --> 01:09:37,600
 Now, in this sequence we can define each word as a token.

1294
01:09:37,600 --> 01:09:40,160
 OK, so we have a number of the tokens

1295
01:09:40,160 --> 01:09:45,200
 parallel input into the encoder of the transformer.

1296
01:09:45,200 --> 01:09:49,960
 And then the decoder will output a larger sequence.

1297
01:09:49,960 --> 01:09:54,599
 This sequence could be the translation of the input

1298
01:09:54,599 --> 01:09:57,120
 sequence into different language.

1299
01:09:57,120 --> 01:09:59,800
 It could be answer the question.

1300
01:09:59,800 --> 01:10:01,360
 The input is a question.

1301
01:10:01,360 --> 01:10:04,080
 The output is the answer.

1302
01:10:04,080 --> 01:10:08,680
 OK, depends on how you train these transformers.

1303
01:10:08,680 --> 01:10:09,599
 OK.

1304
01:10:09,600 --> 01:10:13,000
 So this is the basic structure of the transformer.

1305
01:10:13,000 --> 01:10:15,840
 Now, then what is encoders and decoders?

1306
01:10:15,840 --> 01:10:18,920
 Here is encoders is ace, right?

1307
01:10:18,920 --> 01:10:25,920
 So this encoders and decoders contains a stack of the encoder.

1308
01:10:25,920 --> 01:10:29,040
 And here a stack of the decoder.

1309
01:10:29,040 --> 01:10:33,560
 So we have many encoders linked together.

1310
01:10:33,560 --> 01:10:35,520
 We have many decoders.

1311
01:10:35,520 --> 01:10:36,640
 OK.

1312
01:10:36,640 --> 01:10:42,000
 This number of the encoder could be 6, 10, or several times,

1313
01:10:42,000 --> 01:10:43,880
 or even 100.

1314
01:10:43,880 --> 01:10:45,320
 OK, many encoders.

1315
01:10:45,320 --> 01:10:48,880
 Here also many decoders.

1316
01:10:48,880 --> 01:10:52,200
 And each encoder, the network structure

1317
01:10:52,200 --> 01:10:55,400
 are identical, exactly same.

1318
01:10:55,400 --> 01:10:56,680
 No difference.

1319
01:10:56,680 --> 01:10:59,800
 For the decoder, it's also all these decoders.

1320
01:10:59,800 --> 01:11:02,840
 The structure are exactly same.

1321
01:11:02,840 --> 01:11:03,640
 OK.

1322
01:11:03,640 --> 01:11:09,440
 So we can study the transformer, just study one encoder

1323
01:11:09,440 --> 01:11:10,920
 and one decoder, right?

1324
01:11:10,920 --> 01:11:14,600
 Because all structures are identical same.

1325
01:11:14,600 --> 01:11:17,640
 So we have encoders and decoders.

1326
01:11:17,640 --> 01:11:20,840
 Well, we have multiple encoders.

1327
01:11:20,840 --> 01:11:28,120
 It's the last encoder output where input into each decoder.

1328
01:11:28,120 --> 01:11:28,960
 OK.

1329
01:11:28,960 --> 01:11:32,320
 Then together this decoder produce

1330
01:11:32,320 --> 01:11:35,200
 the final feature vector and then

1331
01:11:35,200 --> 01:11:41,200
 to do the classification to produce output sequence.

1332
01:11:41,200 --> 01:11:43,440
 The final one is also a class field.

1333
01:11:43,440 --> 01:11:46,880
 For example, in the NLP, if it's a class field,

1334
01:11:46,880 --> 01:11:50,240
 then each word is one class.

1335
01:11:50,240 --> 01:11:53,519
 If a language has one million different words,

1336
01:11:53,519 --> 01:11:56,840
 then we have one million different class, right?

1337
01:11:56,840 --> 01:12:01,040
 So to determine what is the word, it's just to classify.

1338
01:12:01,040 --> 01:12:04,720
 This input should be not which class, right?

1339
01:12:04,720 --> 01:12:07,480
 OK.

1340
01:12:07,480 --> 01:12:11,880
 Now, first then from here we can see, right?

1341
01:12:11,880 --> 01:12:16,960
 This is encoders and decoders meaning, but all are identical.

1342
01:12:16,960 --> 01:12:20,840
 So here we only study the structure of one encoder

1343
01:12:20,840 --> 01:12:23,320
 and one decoder.

1344
01:12:23,320 --> 01:12:25,600
 OK.

1345
01:12:25,640 --> 01:12:30,800
 Now, this is show a structure of encoder.

1346
01:12:30,800 --> 01:12:34,560
 Here show a structure of the decoder.

1347
01:12:34,560 --> 01:12:39,320
 So here we can see what is the component of the encoder

1348
01:12:39,320 --> 01:12:41,200
 in the transformer.

1349
01:12:41,200 --> 01:12:45,560
 First input come, we need to convert the words

1350
01:12:45,560 --> 01:12:50,560
 into a feature vector of each word, a different vector

1351
01:12:50,560 --> 01:12:52,440
 to represent the input.

1352
01:12:52,480 --> 01:12:56,559
 This is called embedding, embedding the natural words

1353
01:12:56,559 --> 01:12:58,320
 into a numeric value, right?

1354
01:12:58,320 --> 01:13:00,000
 This is common sense.

1355
01:13:00,000 --> 01:13:04,559
 But here the transformer utilize the so-called position

1356
01:13:04,559 --> 01:13:07,559
 encoding to sum together with this one.

1357
01:13:07,559 --> 01:13:09,599
 This is not important.

1358
01:13:09,599 --> 01:13:11,000
 We first ignore that.

1359
01:13:11,000 --> 01:13:15,519
 But anyway, the input in terms of the numeric representation

1360
01:13:15,519 --> 01:13:19,320
 of the input, we are input into encoder.

1361
01:13:19,320 --> 01:13:21,639
 So what's the component of the encoder?

1362
01:13:21,640 --> 01:13:26,880
 The component encoder has first self-attention.

1363
01:13:26,880 --> 01:13:28,840
 It performs the attention.

1364
01:13:28,840 --> 01:13:32,960
 Then after that, just do some kind of normalization

1365
01:13:32,960 --> 01:13:37,800
 and then go through a feed forward network

1366
01:13:37,800 --> 01:13:40,120
 and then go through a normalization.

1367
01:13:40,120 --> 01:13:40,760
 That's all.

1368
01:13:40,760 --> 01:13:42,320
 We can see from here.

1369
01:13:42,320 --> 01:13:49,320
 An encoder, the key part is two component, attention

1370
01:13:49,320 --> 01:13:53,960
 and feed forward network.

1371
01:13:53,960 --> 01:13:57,759
 This feed forward network could be two-nayer, three-nayer,

1372
01:13:57,759 --> 01:14:01,360
 just a few-nayer of the neural network.

1373
01:14:01,360 --> 01:14:03,280
 OK?

1374
01:14:03,280 --> 01:14:05,599
 Now then what is a decoder?

1375
01:14:05,599 --> 01:14:07,320
 In the structure of the decoder, we

1376
01:14:07,320 --> 01:14:11,880
 see that the input come to also to do an attention.

1377
01:14:11,880 --> 01:14:17,200
 This attention is exactly same as the attention in the encoder

1378
01:14:17,200 --> 01:14:20,800
 because both call self-attention because it's just

1379
01:14:20,800 --> 01:14:25,160
 to pay attention of the same input.

1380
01:14:25,160 --> 01:14:29,000
 And then same as the encoder, decoder, go through after

1381
01:14:29,000 --> 01:14:32,000
 attention, go through a normalization.

1382
01:14:32,000 --> 01:14:36,920
 Then after normalization, now this is a neural for decoder.

1383
01:14:36,920 --> 01:14:40,639
 It has a so-called cross-attention or encoder,

1384
01:14:40,639 --> 01:14:44,960
 decoder, attention because if you receive the input from

1385
01:14:44,960 --> 01:14:49,280
 decoder and the output from the encoder to do the cross

1386
01:14:49,280 --> 01:14:51,200
 attention, OK?

1387
01:14:51,200 --> 01:14:53,360
 But basically it is attention.

1388
01:14:53,360 --> 01:14:57,840
 Only the input are different from the self-attention.

1389
01:14:57,840 --> 01:15:01,160
 Now after this attention, go through a normalization,

1390
01:15:01,160 --> 01:15:07,200
 then go through a feed forward network similar to same as

1391
01:15:07,200 --> 01:15:08,520
 the encoder.

1392
01:15:08,520 --> 01:15:12,720
 Then get the linear cross-fee, get the output.

1393
01:15:15,160 --> 01:15:18,800
 Now from here we can see even we have the encoder and the

1394
01:15:18,800 --> 01:15:21,480
 decoder, they are very similar.

1395
01:15:21,480 --> 01:15:26,280
 The key part of this encoder and the decoder are attention

1396
01:15:26,280 --> 01:15:29,200
 and a feed forward network.

1397
01:15:29,200 --> 01:15:31,920
 OK.

1398
01:15:31,920 --> 01:15:34,240
 As the cross-attention, encoder, decoder,

1399
01:15:34,240 --> 01:15:36,000
 attention, and the self-attention, it's only

1400
01:15:36,000 --> 01:15:38,480
 input of the attention are different.

1401
01:15:38,480 --> 01:15:43,400
 The structure or the method to do the attention are exactly

1402
01:15:43,400 --> 01:15:44,600
 same.

1403
01:15:44,600 --> 01:15:50,720
 OK, so this is structure of the transformer of one encoder

1404
01:15:50,720 --> 01:15:55,400
 and one decoder in the transformer.

1405
01:15:55,400 --> 01:15:57,800
 We know the transformer just now we mentioned that it's

1406
01:15:57,800 --> 01:16:02,120
 multiple encoder and a multiple decoder.

1407
01:16:02,120 --> 01:16:10,280
 Now for one encoder, before we go to study specifically one

1408
01:16:10,280 --> 01:16:14,040
 encoder and one decoder, we first

1409
01:16:14,040 --> 01:16:17,200
 want to see how the transformer works.

1410
01:16:17,200 --> 01:16:19,160
 Now here I show an example.

1411
01:16:19,160 --> 01:16:23,760
 This transformer is utilized to do the translation.

1412
01:16:23,760 --> 01:16:28,680
 One sentence in one language, I believe this Spanish language,

1413
01:16:28,680 --> 01:16:32,280
 goes to the transformer to get the output in the English.

1414
01:16:32,280 --> 01:16:38,360
 So it transfer, translate the Spanish into English.

1415
01:16:38,360 --> 01:16:41,960
 OK, so how the transformer works?

1416
01:16:41,960 --> 01:16:44,680
 First, if we want to do the translation,

1417
01:16:44,680 --> 01:16:49,480
 we put a whole sentence input encoder.

1418
01:16:49,480 --> 01:16:50,360
 OK.

1419
01:16:50,360 --> 01:16:54,160
 Each in a sentence, each word is a token.

1420
01:16:54,160 --> 01:16:56,920
 It's a different word, a different token.

1421
01:16:56,920 --> 01:16:59,200
 OK, then this word go to encoder.

1422
01:16:59,200 --> 01:17:01,560
 Of course, we can have many, many encoder.

1423
01:17:01,560 --> 01:17:05,520
 Then get the output of the master encoder.

1424
01:17:05,520 --> 01:17:09,280
 Then we get this is output of the encoder.

1425
01:17:09,280 --> 01:17:11,240
 Now then the output of the encoder,

1426
01:17:11,240 --> 01:17:16,719
 we are input into the decoder, all different decoders.

1427
01:17:16,719 --> 01:17:23,800
 But all words, all word token in the input go to the encoder

1428
01:17:23,800 --> 01:17:27,759
 and then go to the decoder, only produce one word

1429
01:17:27,759 --> 01:17:34,080
 of the output, or one token in the output.

1430
01:17:34,080 --> 01:17:35,040
 OK.

1431
01:17:35,040 --> 01:17:38,440
 And then what is the next word?

1432
01:17:38,440 --> 01:17:41,919
 The next word is the first output of the word

1433
01:17:41,919 --> 01:17:46,559
 come back as the input of the decoder together

1434
01:17:46,559 --> 01:17:52,400
 with this encode output, same as before.

1435
01:17:52,400 --> 01:17:56,400
 OK, then go to the decoder, produce a second word.

1436
01:17:56,400 --> 01:17:58,759
 Then the second word come here.

1437
01:17:58,759 --> 01:18:01,839
 The first word and the second word together

1438
01:18:01,839 --> 01:18:05,480
 with the encoder output go to the encoder

1439
01:18:05,480 --> 01:18:09,240
 to output the third word.

1440
01:18:09,240 --> 01:18:16,240
 So anyway, this is a process to generate a sentence word

1441
01:18:16,240 --> 01:18:18,360
 by word.

1442
01:18:18,360 --> 01:18:24,959
 Each output word determined by all input words

1443
01:18:24,959 --> 01:18:30,160
 and the previous output word in this decoder.

1444
01:18:30,200 --> 01:18:36,599
 OK, so this is the working process of the transformer.

1445
01:18:36,599 --> 01:18:41,840
 Then after break, we will first study the insight of the encoder

1446
01:18:41,840 --> 01:18:46,920
 and the decoder, the how attention works, OK?

1447
01:18:46,920 --> 01:18:50,519
 What is this feed forward network?

1448
01:18:50,519 --> 01:18:53,599
 OK, so we can have 20 minutes of break now.

1449
01:19:30,160 --> 01:19:36,660
 Maybe 180.

1450
01:19:39,160 --> 01:19:43,240
 Then

1451
01:19:48,440 --> 01:19:52,760
 that is the feed source of the encoder.

1452
01:19:52,760 --> 01:19:58,760
 How many different times across hook or a block problem

1453
01:19:58,760 --> 01:19:59,760
 five.

1454
01:20:03,760 --> 01:20:04,760
 Ah, yes, yes, yes.

1455
01:20:04,760 --> 01:20:07,760
 So, what do you think is a combination?

1456
01:20:07,760 --> 01:20:09,760
 Ah, so there's no fusion.

1457
01:20:09,760 --> 01:20:12,760
 Yes, there's no fusion.

1458
01:20:12,760 --> 01:20:14,760
 So, it's like,

1459
01:20:17,760 --> 01:20:19,760
 Oh, that means that the way it is,

1460
01:20:19,760 --> 01:20:21,760
 it's a combination.

1461
01:20:21,760 --> 01:20:22,760
 But it's just,

1462
01:20:22,760 --> 01:20:26,760
 it's a kind of a combination.

1463
01:20:27,760 --> 01:20:29,760
 The use keeps in the way.

1464
01:20:29,760 --> 01:20:32,760
 But attention seems to have some correlation in way

1465
01:20:32,760 --> 01:20:36,760
 to find their correlation or not.

1466
01:20:36,760 --> 01:20:39,760
 But why we say the clouds,

1467
01:20:39,760 --> 01:20:44,760
 clouds and clouds are correlations.

1468
01:20:44,760 --> 01:20:48,760
 The correlation is some kind of a stocky thing.

1469
01:20:48,760 --> 01:20:51,760
 It is a terming by the trend.

1470
01:20:51,760 --> 01:20:53,760
 Because in the trend,

1471
01:20:53,760 --> 01:20:55,760
 many image,

1472
01:20:55,760 --> 01:20:57,760
 as an image,

1473
01:20:57,760 --> 01:20:59,760
 the case of the cloud,

1474
01:20:59,760 --> 01:21:01,760
 although some image may not be accurate,

1475
01:21:01,760 --> 01:21:03,760
 but most image,

1476
01:21:03,760 --> 01:21:07,760
 the cloud and the cloud always are accurate.

1477
01:21:07,760 --> 01:21:10,760
 So, in the trend process,

1478
01:21:10,760 --> 01:21:12,760
 things are established.

1479
01:21:12,760 --> 01:21:14,760
 Ah,

1480
01:21:14,760 --> 01:21:16,760
 ah,

1481
01:21:16,760 --> 01:21:18,760
 take training data.

1482
01:21:18,760 --> 01:21:20,760
 It's a mistake.

1483
01:21:21,760 --> 01:21:24,760
 So, the question we have from the last question,

1484
01:21:24,760 --> 01:21:27,760
 is the first one is here.

1485
01:21:27,760 --> 01:21:30,760
 Here is the question about the surrounding context.

1486
01:21:30,760 --> 01:21:34,760
 Why is it taking its place?

1487
01:21:34,760 --> 01:21:37,760
 Or is it going to return to a place?

1488
01:21:37,760 --> 01:21:39,760
 It's okay to return to a place.

1489
01:21:39,760 --> 01:21:41,760
 This is not just a good one.

1490
01:21:41,760 --> 01:21:43,760
 This is also a good one.

1491
01:21:43,760 --> 01:21:44,760
 Right?

1492
01:21:44,760 --> 01:21:46,760
 It's just that it's just a small point

1493
01:21:46,760 --> 01:21:48,760
 that it's going to put on the spot.

1494
01:21:48,760 --> 01:21:49,760
 Right.

1495
01:21:49,760 --> 01:21:54,760
 It's just that we want to make it a larger size area.

1496
01:21:54,760 --> 01:21:57,760
 We still want to just use the 90s.

1497
01:21:57,760 --> 01:21:59,760
 So, these 9 points,

1498
01:21:59,760 --> 01:22:02,760
 do you find the effect better in time?

1499
01:22:02,760 --> 01:22:03,760
 No, no, no.

1500
01:22:03,760 --> 01:22:05,760
 It's not going to use more effects.

1501
01:22:05,760 --> 01:22:07,760
 But if there are more,

1502
01:22:07,760 --> 01:22:10,760
 it's different from the other calculations.

1503
01:22:10,760 --> 01:22:12,760
 We want to make each calculation

1504
01:22:12,760 --> 01:22:14,760
 the same as the other.

1505
01:22:14,760 --> 01:22:16,760
 So, these are the 9 points of the calculation.

1506
01:22:16,760 --> 01:22:18,760
 Let's get it.

1507
01:22:20,760 --> 01:22:22,760
 This is to make the last question

1508
01:22:22,760 --> 01:22:25,760
 the information we are going to talk about.

1509
01:22:25,760 --> 01:22:28,760
 Okay, this is not the information.

1510
01:22:28,760 --> 01:22:30,760
 The reason why we are talking about the information

1511
01:22:30,760 --> 01:22:35,760
 is because the information is coming from this side

1512
01:22:35,760 --> 01:22:37,760
 and from this side to this side.

1513
01:22:37,760 --> 01:22:40,760
 So, this information is generated

1514
01:22:40,760 --> 01:22:42,760
 by these information.

1515
01:22:42,760 --> 01:22:45,760
 But this information has nothing to do with this information.

1516
01:22:45,760 --> 01:22:48,760
 This information doesn't know what this side is.

1517
01:22:48,760 --> 01:22:50,760
 What is this?

1518
01:22:50,760 --> 01:22:52,760
 But this information is generated from this side.

1519
01:22:52,760 --> 01:22:57,760
 So, if we need to create a different position,

1520
01:22:57,760 --> 01:22:59,760
 then we need to make different layers.

1521
01:22:59,760 --> 01:23:01,760
 We need to do comparison.

1522
01:23:01,760 --> 01:23:03,760
 If we want to do comparison,

1523
01:23:03,760 --> 01:23:06,760
 then their information is comparable

1524
01:23:06,760 --> 01:23:09,760
 and has a more coherent effect.

1525
01:23:09,760 --> 01:23:11,760
 But if they create this information

1526
01:23:11,760 --> 01:23:13,760
 from one point to another,

1527
01:23:13,760 --> 01:23:16,760
 then the comparison is not so balanced.

1528
01:23:16,760 --> 01:23:18,760
 So, we use a network

1529
01:23:18,760 --> 01:23:20,760
 to make this layer of information

1530
01:23:20,760 --> 01:23:22,760
 more consistent.

1531
01:23:24,760 --> 01:23:27,760
 The next question is about the other one.

1532
01:23:27,760 --> 01:23:31,760
 I don't understand how to

1533
01:23:31,760 --> 01:23:34,760
 explain the information of these two groups

1534
01:23:34,760 --> 01:23:36,760
 in this video.

1535
01:23:36,760 --> 01:23:40,760
 Okay, this is the part

1536
01:23:40,760 --> 01:23:44,760
 where we create this mask.

1537
01:23:44,760 --> 01:23:47,760
 When we make a comparison,

1538
01:23:47,760 --> 01:23:50,760
 each pixel has a mask.

1539
01:23:50,760 --> 01:23:52,760
 The common one is the mask.

1540
01:23:52,760 --> 01:23:55,760
 The next one is the mask.

1541
01:23:55,760 --> 01:23:57,760
 The other one is the mask.

1542
01:23:57,760 --> 01:24:00,760
 The second one is the mask.

1543
01:24:00,760 --> 01:24:02,760
 Each pixel has a mask.

1544
01:24:02,760 --> 01:24:04,760
 This mask is equivalent to

1545
01:24:04,760 --> 01:24:06,760
 the image of all the images.

1546
01:24:06,760 --> 01:24:08,760
 Okay?

1547
01:24:08,760 --> 01:24:10,760
 This is the output of this pixel.

1548
01:24:10,760 --> 01:24:13,760
 The resolution is the image of all the images.

1549
01:24:13,760 --> 01:24:17,760
 But this picture is equivalent to the image of the year.

1550
01:24:17,760 --> 01:24:20,760
 I don't use this picture.

1551
01:24:20,760 --> 01:24:22,760
 I only use this icon.

1552
01:24:22,760 --> 01:24:24,760
 How do I make this mask?

1553
01:24:24,760 --> 01:24:26,760
 I have already made a comparison.

1554
01:24:26,760 --> 01:24:28,760
 This is the image of the year.

1555
01:24:28,760 --> 01:24:30,760
 This is the special image.

1556
01:24:30,760 --> 01:24:34,760
 How do we make these two things

1557
01:24:34,760 --> 01:24:38,760
 into the same image?

1558
01:24:38,760 --> 01:24:40,760
 This picture is equivalent to

1559
01:24:40,760 --> 01:24:43,760
 the image of this picture.

1560
01:24:43,760 --> 01:24:47,760
 I need a mask to make this video.

1561
01:24:47,760 --> 01:24:51,760
 Of course, the next one is the image of the year.

1562
01:24:51,760 --> 01:24:56,760
 The next one is the image of the year.

1563
01:24:56,760 --> 01:24:58,760
 This is another mask.

1564
01:24:58,760 --> 01:25:00,760
 This one is the image of the year.

1565
01:25:00,760 --> 01:25:03,760
 The other one is the image of the year.

1566
01:25:03,760 --> 01:25:09,360
 This part is where the masks are placed together.

1567
01:25:09,360 --> 01:25:11,760
 What about the other part?

1568
01:25:11,760 --> 01:25:15,760
 We don't have enough time to talk about this part.

1569
01:25:15,760 --> 01:25:21,760
 What about the other two masks?

1570
01:25:21,760 --> 01:25:23,760
 What?

1571
01:25:23,760 --> 01:25:26,760
 There are many masks here.

1572
01:25:26,760 --> 01:25:28,760
 There are many masks here.

1573
01:25:28,760 --> 01:25:32,760
 This is the mask that is shown.

1574
01:25:32,760 --> 01:25:38,760
 H times W is the size of this mask.

1575
01:25:38,760 --> 01:25:42,760
 So each one has a mask.

1576
01:25:42,760 --> 01:25:44,760
 This is the mask.

1577
01:25:44,760 --> 01:25:46,760
 This is the mask.

1578
01:25:46,760 --> 01:25:50,760
 This is the mask.

1579
01:25:50,760 --> 01:25:54,760
 What do you think is the best way to bring the masks?

1580
01:25:55,760 --> 01:25:59,760
 What is the best way to bring the masks?

1581
01:25:59,760 --> 01:26:02,760
 I think it's the best way to bring the masks.

1582
01:26:02,760 --> 01:26:05,760
 You should choose the first one.

1583
01:26:05,760 --> 01:26:07,760
 This is the best way to bring the masks.

1584
01:26:07,760 --> 01:26:10,760
 I haven't chosen the right one.

1585
01:26:10,760 --> 01:26:17,760
 But normally, the best way to bring the masks is to bring the masks.

1586
01:26:17,760 --> 01:26:20,760
 I think it's the best way to bring the masks.

1587
01:26:20,760 --> 01:26:24,760
 I haven't chosen the right one.

1588
01:26:24,760 --> 01:26:26,760
 I don't know what to do next.

1589
01:26:26,760 --> 01:26:29,760
 I want to hear from my parents.

1590
01:26:29,760 --> 01:26:32,760
 I want to see if I can bring the next one.

1591
01:26:32,760 --> 01:26:34,760
 I don't have any other choice.

1592
01:26:34,760 --> 01:26:37,760
 I'll talk about it next time.

1593
01:26:50,760 --> 01:26:52,760
 I'll talk about it next time.

1594
01:27:20,760 --> 01:27:22,760
 I'll talk about it next time.

1595
01:27:50,760 --> 01:27:52,760
 I'll talk about it next time.

1596
01:28:20,760 --> 01:28:22,760
 I'll talk about it next time.

1597
01:28:50,760 --> 01:28:52,760
 I'll talk about it next time.

1598
01:29:20,760 --> 01:29:22,760
 I'll talk about it next time.

1599
01:29:50,760 --> 01:29:52,760
 I'll talk about it next time.

1600
01:30:20,760 --> 01:30:22,760
 I'll talk about it next time.

1601
01:30:22,760 --> 01:30:24,760
 I'll talk about it next time.

1602
01:30:24,760 --> 01:30:26,760
 I'll talk about it next time.

1603
01:30:26,760 --> 01:30:28,760
 I'll talk about it next time.

1604
01:30:28,760 --> 01:30:30,760
 I'll talk about it next time.

1605
01:30:30,760 --> 01:30:32,760
 I'll talk about it next time.

1606
01:30:32,760 --> 01:30:34,760
 I'll talk about it next time.

1607
01:30:34,760 --> 01:30:36,760
 I'll talk about it next time.

1608
01:30:36,760 --> 01:30:38,760
 I'll talk about it next time.

1609
01:30:38,760 --> 01:30:40,760
 I'll talk about it next time.

1610
01:30:40,760 --> 01:30:42,760
 I'll talk about it next time.

1611
01:30:42,760 --> 01:30:44,760
 I'll talk about it next time.

1612
01:30:44,760 --> 01:30:46,760
 I'll talk about it next time.

1613
01:30:46,760 --> 01:30:48,760
 I'll talk about it next time.

1614
01:30:48,760 --> 01:30:50,760
 I'll talk about it next time.

1615
01:30:50,760 --> 01:30:52,760
 I'll talk about it next time.

1616
01:30:52,760 --> 01:30:54,760
 I'll talk about it next time.

1617
01:30:54,760 --> 01:30:56,760
 I'll talk about it next time.

1618
01:30:56,760 --> 01:30:58,760
 I'll talk about it next time.

1619
01:30:58,760 --> 01:31:00,760
 I'll talk about it next time.

1620
01:31:00,760 --> 01:31:02,760
 I'll talk about it next time.

1621
01:31:02,760 --> 01:31:04,760
 I'll talk about it next time.

1622
01:31:04,760 --> 01:31:06,760
 I'll talk about it next time.

1623
01:31:06,760 --> 01:31:08,760
 I'll talk about it next time.

1624
01:31:08,760 --> 01:31:10,760
 I'll talk about it next time.

1625
01:31:10,760 --> 01:31:12,760
 I'll talk about it next time.

1626
01:31:12,760 --> 01:31:14,760
 I'll talk about it next time.

1627
01:31:14,760 --> 01:31:16,760
 I'll talk about it next time.

1628
01:31:16,760 --> 01:31:18,760
 It is closely each other

1629
01:31:18,760 --> 01:31:20,760
 for similar words,

1630
01:31:20,760 --> 01:31:21,760
 and far,

1631
01:31:21,760 --> 01:31:24,760
 and the distance of the

1632
01:31:24,760 --> 01:31:26,760
 embitting vector is larger

1633
01:31:26,760 --> 01:31:28,760
 for the words that

1634
01:31:28,760 --> 01:31:30,760
 are totally different.

1635
01:31:30,760 --> 01:31:32,760
 Or quite different.

1636
01:31:32,760 --> 01:31:34,760
 Somehow, we can have

1637
01:31:34,760 --> 01:31:36,760
 a better

1638
01:31:36,760 --> 01:31:38,760
 embedding to present

1639
01:31:38,760 --> 01:31:40,760
 a word by

1640
01:31:40,760 --> 01:31:42,760
 numeric vector.

1641
01:31:42,760 --> 01:31:44,760
 So, usually

1642
01:31:44,760 --> 01:31:49,760
 So this embedding of the word we need 212 dimensional vector.

1643
01:31:50,760 --> 01:31:52,680
 Okay, just different words.

1644
01:31:52,680 --> 01:31:54,520
 This vector are different.

1645
01:31:54,520 --> 01:31:57,820
 Okay, so this is the embedding, right?

1646
01:31:58,880 --> 01:32:00,440
 Now in the transformer,

1647
01:32:00,440 --> 01:32:04,120
 it is particularly have a so-called position encoding,

1648
01:32:06,160 --> 01:32:08,400
 because the different words show

1649
01:32:08,400 --> 01:32:11,640
 at a different position in the input, right?

1650
01:32:11,640 --> 01:32:15,960
 So we will encode the position into a vector.

1651
01:32:15,960 --> 01:32:19,200
 And then this position vector, for example,

1652
01:32:19,200 --> 01:32:22,000
 position one, position two, position three,

1653
01:32:22,000 --> 01:32:24,560
 they are different vector.

1654
01:32:24,560 --> 01:32:29,160
 Then this vector will sum into each word

1655
01:32:30,120 --> 01:32:33,240
 to get the final vector.

1656
01:32:34,880 --> 01:32:39,040
 Now this position embedding is very specific

1657
01:32:39,040 --> 01:32:41,280
 for the transformer, okay?

1658
01:32:42,120 --> 01:32:44,040
 Then I may ask the question,

1659
01:32:44,040 --> 01:32:47,320
 why we need the position embedding?

1660
01:32:48,480 --> 01:32:51,920
 In the input, different word,

1661
01:32:51,920 --> 01:32:55,320
 we have a vector, numeric vector,

1662
01:32:55,320 --> 01:32:58,360
 they already at a different position.

1663
01:32:58,360 --> 01:33:02,960
 Their position is always unique in all process.

1664
01:33:02,960 --> 01:33:06,280
 The first order, always in the first place,

1665
01:33:06,280 --> 01:33:09,080
 the second order is always in the second place.

1666
01:33:09,960 --> 01:33:13,120
 So the information of the position of different words

1667
01:33:13,120 --> 01:33:17,400
 is very clear show at the position, right?

1668
01:33:17,400 --> 01:33:22,200
 So why we need to code the one, two, three, four,

1669
01:33:22,200 --> 01:33:25,760
 sum into this numeric vector?

1670
01:33:25,760 --> 01:33:26,600
 Why?

1671
01:33:29,080 --> 01:33:30,920
 Try to find it,

1672
01:33:30,920 --> 01:33:34,660
 because I cannot find the reason why.

1673
01:33:35,660 --> 01:33:40,099
 I also think this problem by myself

1674
01:33:40,099 --> 01:33:43,099
 of why we need this position

1675
01:33:43,099 --> 01:33:45,420
 and the sum that this information of a position

1676
01:33:45,420 --> 01:33:47,220
 into this word.

1677
01:33:47,220 --> 01:33:50,980
 Because the position of this word is clearly show

1678
01:33:50,980 --> 01:33:54,380
 the word appear at a different position, right?

1679
01:33:54,380 --> 01:33:56,540
 Okay, but in the transformer,

1680
01:33:56,540 --> 01:34:01,540
 somehow use this position embedding, okay?

1681
01:34:02,540 --> 01:34:07,100
 I haven't found the reason in the literature,

1682
01:34:07,100 --> 01:34:10,740
 in the paper, in the information, right?

1683
01:34:10,740 --> 01:34:14,500
 But I figure out some reasons by myself, okay?

1684
01:34:14,500 --> 01:34:19,380
 I don't want to illustrate what is this reason

1685
01:34:19,380 --> 01:34:22,060
 you can think about by yourself, okay?

1686
01:34:22,060 --> 01:34:24,620
 Why we need this position embedding?

1687
01:34:24,620 --> 01:34:27,700
 But anyway, after position embedding,

1688
01:34:27,700 --> 01:34:31,460
 we still have one vector for one word,

1689
01:34:31,460 --> 01:34:33,220
 or one token, right?

1690
01:34:34,220 --> 01:34:36,100
 And then in the transformer,

1691
01:34:36,100 --> 01:34:38,460
 it is different from traditionally,

1692
01:34:38,460 --> 01:34:43,460
 one input vector is always, we use a column vector, okay?

1693
01:34:43,900 --> 01:34:48,180
 Here, in the transformer, one word is a vector,

1694
01:34:48,180 --> 01:34:51,340
 it's a 512 dimensional vector.

1695
01:34:51,340 --> 01:34:54,940
 We put this vector into a row vector.

1696
01:34:55,820 --> 01:35:00,700
 One row vector represent one word, or one token.

1697
01:35:00,700 --> 01:35:04,820
 Then all token together, a sentence or a paragraph

1698
01:35:04,820 --> 01:35:09,820
 of the text will form together a matrix, okay?

1699
01:35:10,019 --> 01:35:12,700
 So keep this in your mind.

1700
01:35:12,700 --> 01:35:17,700
 In this matrix, the input to the transformer is a matrix.

1701
01:35:18,019 --> 01:35:22,580
 In this matrix, one row is one token,

1702
01:35:22,580 --> 01:35:25,740
 is a numeric value of a vector of one token.

1703
01:35:25,740 --> 01:35:29,860
 Different row is different token, okay?

1704
01:35:29,900 --> 01:35:31,900
 Then we have the matrix.

1705
01:35:31,900 --> 01:35:36,660
 This is input to the encoder and also decoder.

1706
01:35:38,339 --> 01:35:42,259
 Now, after we have this embedding, right?

1707
01:35:42,259 --> 01:35:44,660
 Anyway, the input data is a matrix.

1708
01:35:44,660 --> 01:35:48,980
 Then we can make this matrix go to encoder.

1709
01:35:52,660 --> 01:35:57,660
 And this is the structure of the encoder.

1710
01:35:57,820 --> 01:36:01,340
 From this structure, we can see this matrix

1711
01:36:01,340 --> 01:36:04,460
 come to the attention module.

1712
01:36:04,460 --> 01:36:08,500
 Then the output of the attention will be,

1713
01:36:08,500 --> 01:36:12,540
 also a matrix has same size as the input,

1714
01:36:12,540 --> 01:36:14,180
 exactly same size.

1715
01:36:14,180 --> 01:36:18,820
 Then each module have the residue connection.

1716
01:36:18,820 --> 01:36:21,340
 Residue connection, we know if you study

1717
01:36:21,340 --> 01:36:23,340
 what is the wrist net, right?

1718
01:36:23,340 --> 01:36:25,460
 If you have some experience in deep learning,

1719
01:36:25,460 --> 01:36:27,860
 you must know what is a wrist net.

1720
01:36:27,860 --> 01:36:32,860
 wrist net is also a very significant advance

1721
01:36:34,260 --> 01:36:36,780
 in the development of the thing.

1722
01:36:36,780 --> 01:36:41,260
 Okay, wrist net has some kind of the residue connection.

1723
01:36:41,260 --> 01:36:44,940
 So all module has the residue connection,

1724
01:36:44,940 --> 01:36:49,940
 includes the decoder here, we ignore it.

1725
01:36:50,340 --> 01:36:55,340
 This just show the residue connection, okay?

1726
01:36:55,940 --> 01:36:58,460
 Now, this residue connection

1727
01:36:58,460 --> 01:37:02,820
 or basically borrow from the same wrist net, right?

1728
01:37:02,820 --> 01:37:03,660
 Okay.

1729
01:37:06,300 --> 01:37:08,620
 Now, and then in this module,

1730
01:37:08,620 --> 01:37:13,300
 the layer normalization is nothing to elaborate.

1731
01:37:13,300 --> 01:37:15,620
 It's just do some kind of the normalization.

1732
01:37:15,620 --> 01:37:19,860
 We know after some process, we need some normalization, right?

1733
01:37:19,860 --> 01:37:23,340
 So the two key components in the encoder

1734
01:37:23,340 --> 01:37:28,340
 and also decoder is attention and feed forward network.

1735
01:37:29,220 --> 01:37:34,220
 Feed forward network is just a few layers of the network.

1736
01:37:34,300 --> 01:37:37,220
 But in the paper, it mentioned that

1737
01:37:37,220 --> 01:37:41,540
 this feed forward network is point wise network.

1738
01:37:42,860 --> 01:37:47,860
 Okay, so you can think about what means this point wise.

1739
01:37:48,580 --> 01:37:53,580
 Okay, so it's a point wise feed forward network.

1740
01:37:55,099 --> 01:37:57,740
 It's just a couple of the million layers

1741
01:37:57,740 --> 01:38:00,940
 separated by a new function, that's all.

1742
01:38:00,940 --> 01:38:05,740
 But try to find out what means the point wise.

1743
01:38:05,740 --> 01:38:08,460
 Okay, later I will elaborate that

1744
01:38:08,460 --> 01:38:13,019
 because many people somehow ignore this one.

1745
01:38:13,019 --> 01:38:15,380
 Even some paper, published paper,

1746
01:38:15,420 --> 01:38:19,300
 just simply change the point wise feed forward

1747
01:38:19,300 --> 01:38:23,260
 and this feed forward network, change it into MRP.

1748
01:38:24,780 --> 01:38:28,300
 This is totally wrong, it is not MRP.

1749
01:38:28,300 --> 01:38:32,260
 Okay, even the VIT vision transformer

1750
01:38:32,260 --> 01:38:35,540
 is the first paper published in CELIPR

1751
01:38:35,540 --> 01:38:38,820
 to utilize the transformer into computer vision

1752
01:38:38,820 --> 01:38:42,500
 called VIT vision transformer.

1753
01:38:43,460 --> 01:38:48,460
 In this paper, it's running show that this module is MRP.

1754
01:38:51,060 --> 01:38:52,700
 It's totally wrong.

1755
01:38:52,700 --> 01:38:54,020
 Okay, later I will show that.

1756
01:38:54,020 --> 01:38:57,180
 What means the point wise feed forward network.

1757
01:38:58,060 --> 01:39:03,060
 Now, next we will focus on what is this attention?

1758
01:39:03,300 --> 01:39:06,700
 Because attention, the first paper of the transformer

1759
01:39:06,700 --> 01:39:09,900
 claims that attention is all you need.

1760
01:39:09,900 --> 01:39:12,460
 Right, so what is this attention?

1761
01:39:12,460 --> 01:39:16,260
 Now, the attention is a module of the attention

1762
01:39:16,260 --> 01:39:19,940
 has three input, okay, it's called

1763
01:39:21,060 --> 01:39:26,060
 query key and the value or QKV, okay.

1764
01:39:27,540 --> 01:39:32,540
 All these three input come from a same input.

1765
01:39:33,020 --> 01:39:36,300
 That means input is a matrix, right?

1766
01:39:36,300 --> 01:39:40,420
 These metrics will produce three different copy

1767
01:39:41,300 --> 01:39:43,620
 and these three different copy we call

1768
01:39:43,620 --> 01:39:46,900
 is a query key and the value.

1769
01:39:46,900 --> 01:39:51,100
 How to produce three copy from one matrix?

1770
01:39:51,100 --> 01:39:53,780
 Produce three metrics, very simple.

1771
01:39:53,780 --> 01:39:58,140
 We just multiply this matrix by another winning matrix,

1772
01:39:58,140 --> 01:40:01,700
 then produce one matrix, multiply different metrics,

1773
01:40:01,700 --> 01:40:04,700
 produce another matrix, not all.

1774
01:40:04,700 --> 01:40:08,420
 Okay, for example, if here show this matrix,

1775
01:40:08,420 --> 01:40:13,420
 we have four tokens, each token has three dimensional

1776
01:40:13,860 --> 01:40:16,060
 vector, right, of course in practice,

1777
01:40:16,060 --> 01:40:18,020
 it is hundreds of the dimension.

1778
01:40:18,020 --> 01:40:19,980
 This is just for illustration, right.

1779
01:40:19,980 --> 01:40:24,340
 So we have this matrix, data matrix, four by three.

1780
01:40:24,340 --> 01:40:26,980
 Now, if this four by three matrix multiplied

1781
01:40:26,980 --> 01:40:30,140
 by three by three matrix, what you result?

1782
01:40:30,140 --> 01:40:33,620
 It's a lot of four by three matrix, right.

1783
01:40:33,620 --> 01:40:37,100
 Now, if this weakening matrix W are different,

1784
01:40:37,100 --> 01:40:38,620
 then of course this multiplication

1785
01:40:38,620 --> 01:40:40,740
 will produce different metrics.

1786
01:40:40,740 --> 01:40:45,740
 Okay, so one input matrix, we multiply by a projection

1787
01:40:46,420 --> 01:40:49,900
 matrix, three by three matrix, then we'll produce

1788
01:40:49,900 --> 01:40:51,520
 three different copy.

1789
01:40:52,620 --> 01:40:56,060
 These three by three matrix is,

1790
01:40:56,060 --> 01:40:58,660
 all element in this weakening matrix

1791
01:40:58,660 --> 01:41:02,020
 is learned from the training data.

1792
01:41:02,020 --> 01:41:04,100
 Okay, we learn from the training data

1793
01:41:04,100 --> 01:41:08,140
 to get this weakening matrix, three by three matrix,

1794
01:41:08,140 --> 01:41:11,420
 then to learn three different weakening matrix,

1795
01:41:11,420 --> 01:41:15,100
 then multiply by two the input data,

1796
01:41:15,100 --> 01:41:17,540
 then produce three copies.

1797
01:41:17,540 --> 01:41:21,540
 Okay, now these three copies will perform

1798
01:41:21,540 --> 01:41:23,940
 the so-called attention.

1799
01:41:23,940 --> 01:41:27,740
 It is called self-attention because these three copies

1800
01:41:27,740 --> 01:41:31,420
 come from a same input matrix, right.

1801
01:41:31,420 --> 01:41:33,420
 So it is called self-attention.

1802
01:41:34,900 --> 01:41:37,540
 Now, anyway, after this attention,

1803
01:41:37,540 --> 01:41:40,540
 and then go to the feed-forward network,

1804
01:41:40,540 --> 01:41:44,540
 and then the output of the master encoder,

1805
01:41:44,540 --> 01:41:49,540
 where output come here, where produce two copies.

1806
01:41:51,140 --> 01:41:54,500
 Okay, save us key and the value.

1807
01:41:54,500 --> 01:41:58,780
 And the output of the previous self-attention

1808
01:41:58,780 --> 01:42:02,020
 from the decoder is take us the query.

1809
01:42:02,020 --> 01:42:05,940
 Then these three perform attention exactly same

1810
01:42:05,940 --> 01:42:09,580
 as the self-attention, but it's called cross-attention

1811
01:42:09,580 --> 01:42:13,540
 or encoder-decoder-attention because this come

1812
01:42:13,540 --> 01:42:17,620
 from the two different input matrix, right.

1813
01:42:17,620 --> 01:42:22,140
 Okay, and the decoder input to the decoder

1814
01:42:22,140 --> 01:42:24,980
 also perform the self-attention exactly same

1815
01:42:24,980 --> 01:42:27,740
 as the encoder, no difference.

1816
01:42:27,740 --> 01:42:31,300
 Okay, so now after we understand that,

1817
01:42:31,300 --> 01:42:35,060
 we will see how this attention performs.

1818
01:42:35,060 --> 01:42:40,060
 But this attention will show in one encoder,

1819
01:42:40,260 --> 01:42:43,220
 we have attention at feed-forward, right.

1820
01:42:43,220 --> 01:42:46,860
 We have multiple attention or multiple encoder

1821
01:42:46,860 --> 01:42:50,060
 or a stack of the encoders, right.

1822
01:42:50,060 --> 01:42:54,500
 But one encoder in fact is multiple attention.

1823
01:42:54,500 --> 01:42:57,620
 It's called multiple attention heads.

1824
01:42:58,580 --> 01:43:00,980
 What is multiple attention heads?

1825
01:43:00,980 --> 01:43:05,059
 So as I mentioned that one to perform one attention,

1826
01:43:05,059 --> 01:43:10,059
 we need three metrics to perform attention.

1827
01:43:10,700 --> 01:43:14,179
 But in one attention unit, in fact in the transformer,

1828
01:43:14,179 --> 01:43:17,540
 we have mainly these attention,

1829
01:43:17,540 --> 01:43:22,540
 but all these attention comes from a same metrics,

1830
01:43:23,700 --> 01:43:25,620
 data metrics.

1831
01:43:25,620 --> 01:43:28,940
 As I mentioned that a data metrics can multiply

1832
01:43:28,940 --> 01:43:32,900
 by weighting metrics produced three different copy.

1833
01:43:32,900 --> 01:43:36,919
 Of course, we can produce 30 different copy.

1834
01:43:38,059 --> 01:43:40,179
 Then this 30 different copy,

1835
01:43:41,940 --> 01:43:44,900
 every three of them to perform the attention,

1836
01:43:44,900 --> 01:43:48,099
 a lot of three of them to perform attention and so on.

1837
01:43:48,099 --> 01:43:50,299
 We can perform many, many attention.

1838
01:43:51,940 --> 01:43:55,320
 You can design how many attention you want to perform.

1839
01:43:56,320 --> 01:44:01,480
 Then each attention unit will produce a same four by three

1840
01:44:01,480 --> 01:44:04,960
 metrics, same as the input, original input.

1841
01:44:04,960 --> 01:44:09,480
 Then all this output of the attention will merge together

1842
01:44:09,480 --> 01:44:12,719
 to form just a three by four metrics,

1843
01:44:14,120 --> 01:44:18,679
 a four by three metrics, right, same as the input, okay.

1844
01:44:18,679 --> 01:44:23,679
 So this is a process of one attention.

1845
01:44:24,480 --> 01:44:28,160
 It is called multiple attention heads

1846
01:44:28,160 --> 01:44:32,160
 because it's parallel to perform many attention.

1847
01:44:33,560 --> 01:44:36,320
 The difference of this different attention

1848
01:44:36,320 --> 01:44:39,240
 is just the different projection metrics

1849
01:44:39,240 --> 01:44:43,120
 to project the input data into different copies.

1850
01:44:44,680 --> 01:44:49,680
 The each attention, the working in each attention

1851
01:44:50,480 --> 01:44:52,960
 is exactly same.

1852
01:44:54,680 --> 01:44:58,840
 Then we will see how this attention comes.

1853
01:45:00,080 --> 01:45:04,640
 Okay, as I mentioned that here I need to repeat again, okay.

1854
01:45:04,640 --> 01:45:07,680
 The input words, right, this example show

1855
01:45:07,680 --> 01:45:10,320
 we have four tokens, okay.

1856
01:45:10,320 --> 01:45:13,200
 This four tokens after the embedding

1857
01:45:13,200 --> 01:45:17,680
 becomes to four numeric vector.

1858
01:45:17,680 --> 01:45:20,800
 Then the four words, each word is a vector

1859
01:45:20,800 --> 01:45:23,120
 with each word four metrics.

1860
01:45:23,120 --> 01:45:26,720
 So in this example, the metrics are four by three metrics

1861
01:45:26,720 --> 01:45:30,320
 indicated we have four words or four tokens.

1862
01:45:30,320 --> 01:45:33,760
 Each token is a three dimensional vector.

1863
01:45:33,760 --> 01:45:35,519
 Again, this is just for illustration.

1864
01:45:35,519 --> 01:45:40,519
 The vector is hundreds of the dimension, okay.

1865
01:45:41,960 --> 01:45:44,440
 Okay, so anyway, it is a matrix, right.

1866
01:45:44,440 --> 01:45:47,720
 Now, these metrics we can project

1867
01:45:47,720 --> 01:45:50,800
 to three different subspace.

1868
01:45:50,800 --> 01:45:54,680
 So called subspace is just multiplied by three by three

1869
01:45:54,680 --> 01:45:58,520
 metrics then produce, this modification

1870
01:45:58,520 --> 01:46:00,680
 we'll produce as four by three metrics,

1871
01:46:00,680 --> 01:46:03,440
 different three by three metrics multiplied to the input

1872
01:46:03,440 --> 01:46:05,600
 then produce different copy.

1873
01:46:05,600 --> 01:46:08,680
 Now, then this three copy is called

1874
01:46:09,680 --> 01:46:12,240
 query, key and the value.

1875
01:46:13,240 --> 01:46:16,520
 Now, how these three metrics perform

1876
01:46:16,520 --> 01:46:18,440
 the so-called attention?

1877
01:46:19,440 --> 01:46:22,400
 Now, the three metrics first,

1878
01:46:22,400 --> 01:46:24,719
 the three metrics here, first the shield

1879
01:46:24,719 --> 01:46:27,759
 because this is very important in fact.

1880
01:46:27,759 --> 01:46:31,480
 Now, if, although the input is a metrics

1881
01:46:31,480 --> 01:46:36,080
 but we can use one, x one to indicate

1882
01:46:36,080 --> 01:46:40,040
 one row vector, right.

1883
01:46:40,040 --> 01:46:42,280
 Because one row is one word.

1884
01:46:42,280 --> 01:46:45,759
 So if we have four words, then we have four rows.

1885
01:46:45,760 --> 01:46:48,680
 So we use x one to indicate the first row,

1886
01:46:48,680 --> 01:46:50,760
 x two indicates the second row.

1887
01:46:50,760 --> 01:46:54,800
 So anyway, the x is the input metrics.

1888
01:46:54,800 --> 01:46:58,880
 This input metrics multiplied by three different

1889
01:46:58,880 --> 01:47:03,880
 projection metrics will produce three different output.

1890
01:47:03,920 --> 01:47:08,920
 QKV come from the same x, just different

1891
01:47:10,240 --> 01:47:15,240
 this waiting metrics to get QKV, three different metrics.

1892
01:47:16,720 --> 01:47:19,560
 Okay, this show here, right.

1893
01:47:19,560 --> 01:47:24,560
 This data metrics multiplied by this three projection metrics

1894
01:47:24,640 --> 01:47:27,400
 produce three copy of the metrics.

1895
01:47:27,400 --> 01:47:28,240
 Okay.

1896
01:47:30,160 --> 01:47:32,680
 Now, then from these three metrics,

1897
01:47:32,680 --> 01:47:34,880
 we perform the attention.

1898
01:47:34,880 --> 01:47:36,360
 What is attention?

1899
01:47:36,360 --> 01:47:40,200
 The attention is just one metrics Q

1900
01:47:40,200 --> 01:47:43,400
 multiplied by metrics K transpose.

1901
01:47:43,400 --> 01:47:44,800
 Of course, it must transpose.

1902
01:47:44,840 --> 01:47:46,960
 Otherwise it cannot multiply together

1903
01:47:46,960 --> 01:47:49,000
 because number of the column in Q

1904
01:47:49,000 --> 01:47:51,600
 must be equals to number of the,

1905
01:47:51,600 --> 01:47:53,080
 number of the column in Q

1906
01:47:53,080 --> 01:47:56,520
 must equals to number of the rows in this,

1907
01:47:56,520 --> 01:47:57,920
 in this second metrics.

1908
01:47:57,920 --> 01:48:00,400
 So we need to transpose K.

1909
01:48:00,400 --> 01:48:04,200
 Then we multiply them together to produce the R.

1910
01:48:05,680 --> 01:48:09,960
 Now, the dimension of the R must be four by four,

1911
01:48:09,960 --> 01:48:12,240
 not four by three, right.

1912
01:48:12,240 --> 01:48:16,200
 Three is a number of the element in one word.

1913
01:48:16,200 --> 01:48:19,280
 Four is number of the words we have.

1914
01:48:19,280 --> 01:48:23,080
 Okay, so we get a four by four metrics R.

1915
01:48:24,920 --> 01:48:29,040
 Now, of course this is just a metrics multiplication.

1916
01:48:29,040 --> 01:48:34,040
 No more than that, but we can still look into

1917
01:48:34,519 --> 01:48:38,179
 what is the element in this four by four metrics.

1918
01:48:39,060 --> 01:48:43,420
 Okay, so from the element of this R metrics,

1919
01:48:43,420 --> 01:48:48,420
 we can see each element is the product of the two words

1920
01:48:49,100 --> 01:48:51,500
 or two token, okay.

1921
01:48:51,500 --> 01:48:55,100
 Q one multiplied by all key,

1922
01:48:55,100 --> 01:48:57,700
 then Q two multiplied by all key,

1923
01:48:57,700 --> 01:49:00,940
 Q three multiplied by all key, right.

1924
01:49:00,940 --> 01:49:05,860
 Okay, so we have such metrics as R.

1925
01:49:05,860 --> 01:49:09,860
 Now, each element in this R is a multiplication

1926
01:49:09,860 --> 01:49:14,500
 of the two words, multiplication of the two vectors.

1927
01:49:14,500 --> 01:49:19,099
 The result will show the similarity of this two vector

1928
01:49:19,980 --> 01:49:23,400
 or the correlation of this two vector.

1929
01:49:24,940 --> 01:49:27,860
 You see, you can recall what is the definition

1930
01:49:27,860 --> 01:49:30,019
 of the correlation, what is the definition

1931
01:49:30,019 --> 01:49:32,460
 of a covariance matrix.

1932
01:49:32,460 --> 01:49:36,420
 It's just a multiplication of one element

1933
01:49:36,420 --> 01:49:39,420
 multiplied each of other elements

1934
01:49:39,420 --> 01:49:42,740
 to get the element in our covariance matrix.

1935
01:49:42,740 --> 01:49:45,540
 Covariance matrix will show the correlation

1936
01:49:45,540 --> 01:49:48,180
 between different components, right.

1937
01:49:48,180 --> 01:49:52,940
 So this attention is not very novel,

1938
01:49:52,940 --> 01:49:56,300
 it's just very similar to the concept

1939
01:49:56,300 --> 01:50:01,100
 of the covariance matrix or correlation matrix

1940
01:50:01,100 --> 01:50:04,300
 because we should understand this thought product

1941
01:50:04,300 --> 01:50:07,940
 of the two vector or one vector in row

1942
01:50:07,940 --> 01:50:11,460
 multiply another vector in column, produce one number.

1943
01:50:11,460 --> 01:50:16,460
 This number will show how these two vector are related

1944
01:50:16,940 --> 01:50:21,940
 or correlated or the attention is larger or no.

1945
01:50:22,740 --> 01:50:26,380
 This more, the same meaning, okay.

1946
01:50:26,380 --> 01:50:30,680
 Just to take the correlation between different words.

1947
01:50:32,100 --> 01:50:35,940
 Okay, so after we have this matrix,

1948
01:50:35,940 --> 01:50:39,340
 we call it some kind of the attention matrix, right.

1949
01:50:39,340 --> 01:50:42,380
 R is a Q times K transpose.

1950
01:50:42,380 --> 01:50:46,780
 Then we need to do some kind of the normalization, okay.

1951
01:50:46,780 --> 01:50:49,580
 Or I will skip this normalization,

1952
01:50:49,580 --> 01:50:51,700
 but anyway it's just divided by D,

1953
01:50:51,700 --> 01:50:55,140
 KD is the number of the dimension of the feature

1954
01:50:55,140 --> 01:50:57,940
 of this data vector.

1955
01:50:57,940 --> 01:50:59,860
 So this is just a normalization.

1956
01:50:59,860 --> 01:51:04,219
 Then go to a softer matrix is also kind of the normalization.

1957
01:51:04,219 --> 01:51:07,339
 Anyway, after we multiply two vector,

1958
01:51:07,339 --> 01:51:11,900
 two matrix to produce a similar as a covariance matrix, right.

1959
01:51:11,900 --> 01:51:15,219
 We need to do some kind of the normalization.

1960
01:51:16,139 --> 01:51:21,139
 Then this normalized matrix show the correlation

1961
01:51:22,580 --> 01:51:27,580
 between Q and K, then multiply to the third matrix,

1962
01:51:28,420 --> 01:51:32,180
 way to produce a matrix Z.

1963
01:51:33,340 --> 01:51:36,780
 Now a four by four matrix multiplied by four by three

1964
01:51:36,780 --> 01:51:40,220
 matrix produce a four by three matrix Z.

1965
01:51:41,180 --> 01:51:46,059
 So this Z is the output of the attention unit.

1966
01:51:46,059 --> 01:51:51,059
 Okay, the input is the X produce three copies Q, K, V,

1967
01:51:51,059 --> 01:51:53,860
 then produce a Z as the output.

1968
01:51:54,820 --> 01:51:59,259
 This Z and the input X has identical size,

1969
01:51:59,259 --> 01:52:02,700
 same shape four by three matrix.

1970
01:52:06,339 --> 01:52:08,940
 Now that's all, this is the attention.

1971
01:52:10,860 --> 01:52:14,139
 Okay, so now if you try to figure out

1972
01:52:15,059 --> 01:52:18,019
 what is the meaning of this attention,

1973
01:52:18,019 --> 01:52:20,420
 how this attention performs,

1974
01:52:21,420 --> 01:52:26,420
 we will take one examples, what is the output Z?

1975
01:52:27,660 --> 01:52:31,900
 Now as I mentioned that output Z matrix is a matrix,

1976
01:52:31,900 --> 01:52:34,540
 is also four by three matrix.

1977
01:52:34,540 --> 01:52:38,180
 It is same one row is one token.

1978
01:52:39,260 --> 01:52:41,380
 Different row is different token.

1979
01:52:41,380 --> 01:52:45,260
 Okay, different column is different feature for this token.

1980
01:52:46,380 --> 01:52:48,300
 Okay, same as the input.

1981
01:52:48,300 --> 01:52:52,660
 Okay, now in this matrix if we pick one row,

1982
01:52:52,660 --> 01:52:57,260
 one token for example Z four is the fourth word for example.

1983
01:52:58,140 --> 01:53:02,660
 This fourth word, how to produce the output of the fourth word?

1984
01:53:04,580 --> 01:53:09,140
 This fourth word by mathematical expression is shown here.

1985
01:53:09,140 --> 01:53:10,980
 From the matrix multiplication,

1986
01:53:10,980 --> 01:53:12,620
 you can very easily figure out,

1987
01:53:12,620 --> 01:53:15,940
 I will not, I will skip how this one multiplied this one,

1988
01:53:15,940 --> 01:53:17,820
 then produce this expression, right?

1989
01:53:18,500 --> 01:53:22,300
 This is just an enlium algebra matrix multiplication.

1990
01:53:22,300 --> 01:53:26,700
 Now, but from this expression what we can get,

1991
01:53:28,700 --> 01:53:33,700
 an output word or token Z four is a combination

1992
01:53:35,260 --> 01:53:37,580
 of all input words,

1993
01:53:37,580 --> 01:53:41,220
 V one, V two, V three and V four.

1994
01:53:41,220 --> 01:53:42,060
 Is a,

1995
01:53:43,980 --> 01:53:46,540
 weight combination.

1996
01:53:46,540 --> 01:53:49,180
 Why is the weight combination of the V one, V three,

1997
01:53:49,180 --> 01:53:51,380
 V one, V two, V three, V four,

1998
01:53:51,380 --> 01:53:56,220
 not Q one, Q, not K one, K two, K three, K four.

1999
01:53:56,220 --> 01:54:01,220
 Because I believe for this is the wrong,

2000
01:54:02,180 --> 01:54:03,900
 it should be transposed, right?

2001
01:54:03,900 --> 01:54:06,420
 Here K should be transposed.

2002
01:54:06,420 --> 01:54:10,180
 So somehow I'm lazy, I just copy this from some,

2003
01:54:10,180 --> 01:54:14,500
 some information this should be transposed, right?

2004
01:54:14,500 --> 01:54:19,500
 Okay, now because this Q times K already produce,

2005
01:54:22,700 --> 01:54:25,900
 somehow the meaning are different, it's a transpose.

2006
01:54:25,900 --> 01:54:28,780
 After the transpose, we have one vector multiplied

2007
01:54:28,780 --> 01:54:31,460
 the vector produce just one value.

2008
01:54:31,460 --> 01:54:35,220
 The value in this Q times K here,

2009
01:54:35,220 --> 01:54:40,220
 in fact it shows a correlation between Q and K, right?

2010
01:54:40,380 --> 01:54:43,820
 This is one vector multiplied by one, it's just one value.

2011
01:54:43,820 --> 01:54:48,740
 So this is just the weight of the V.

2012
01:54:50,139 --> 01:54:55,139
 Okay, so now we can see the output,

2013
01:54:56,340 --> 01:55:00,540
 the first word of the output is a linear combination

2014
01:55:00,540 --> 01:55:04,259
 of all words in terms of the V.

2015
01:55:04,259 --> 01:55:08,259
 But how to weight, this is a different way together

2016
01:55:08,259 --> 01:55:12,780
 to produce the new, the new word.

2017
01:55:12,780 --> 01:55:15,059
 Because it's a fourth word,

2018
01:55:15,059 --> 01:55:19,540
 where to indicate it is related to the four.

2019
01:55:19,540 --> 01:55:23,820
 We have Q four, Q four, Q four, Q four, right?

2020
01:55:25,219 --> 01:55:28,780
 Okay, the query is the four,

2021
01:55:28,780 --> 01:55:31,900
 is the fourth word of the query, okay?

2022
01:55:31,900 --> 01:55:36,900
 Well, the key and the value we use from one up to four.

2023
01:55:37,900 --> 01:55:42,900
 But the usage of the K and the V are different, okay?

2024
01:55:44,259 --> 01:55:49,059
 We call this V is the value, it's really the word, okay?

2025
01:55:49,059 --> 01:55:53,900
 Where K is the key, it's a key of the V,

2026
01:55:53,900 --> 01:55:56,860
 it's somehow representation of the V.

2027
01:55:57,900 --> 01:56:02,900
 So we want to use the Q to retrieve the information

2028
01:56:03,780 --> 01:56:05,420
 from the V.

2029
01:56:05,420 --> 01:56:10,420
 We don't know which of the four V four should be used

2030
01:56:11,420 --> 01:56:14,139
 to produce the output Z four.

2031
01:56:14,139 --> 01:56:17,460
 Then we combine all this V together

2032
01:56:17,460 --> 01:56:20,059
 but put different weightage.

2033
01:56:20,059 --> 01:56:25,139
 Well, if the V is represented by K, right?

2034
01:56:25,139 --> 01:56:29,580
 If this K and Q four is highly coordinated,

2035
01:56:29,580 --> 01:56:32,700
 then the weight will be high.

2036
01:56:32,700 --> 01:56:36,580
 If, for example, the V four,

2037
01:56:36,580 --> 01:56:39,099
 the representation of the V four is K four,

2038
01:56:39,099 --> 01:56:44,099
 it is not very well related to this query,

2039
01:56:48,019 --> 01:56:49,980
 the fourth word of the query,

2040
01:56:49,980 --> 01:56:52,860
 then this weight value will be very low.

2041
01:56:54,099 --> 01:56:59,099
 Okay, so we can see the first multiplication K and V

2042
01:56:59,100 --> 01:57:04,020
 will establish the correlation between different words

2043
01:57:04,020 --> 01:57:09,020
 and then use this correlation as the weight to each word

2044
01:57:09,180 --> 01:57:13,020
 to produce a new representation of a word.

2045
01:57:14,660 --> 01:57:19,060
 Okay, so this is the meaning of this attention, okay?

2046
01:57:20,620 --> 01:57:24,500
 So the meaning of this attention is output,

2047
01:57:24,500 --> 01:57:28,500
 each word of the output is a linear weight combination

2048
01:57:28,500 --> 01:57:32,700
 of all input, but the weights is determined

2049
01:57:32,700 --> 01:57:37,700
 by how other words are coordinated to the fourth word.

2050
01:57:39,540 --> 01:57:43,260
 Okay, for example, if the fourth word is closely

2051
01:57:43,260 --> 01:57:44,820
 coordinated to the fourth word,

2052
01:57:44,820 --> 01:57:46,940
 then of course this weight is high,

2053
01:57:46,940 --> 01:57:49,780
 after normalization is one,

2054
01:57:49,780 --> 01:57:52,540
 but the fourth word can also coordinate it

2055
01:57:52,540 --> 01:57:54,340
 by some other word.

2056
01:57:54,340 --> 01:57:58,220
 Then we also use the information of some other word

2057
01:57:58,220 --> 01:58:02,580
 to get some into the fourth word.

2058
01:58:02,580 --> 01:58:07,580
 So after one attention, one word, for example here,

2059
01:58:07,620 --> 01:58:12,620
 is a fourth word contains information of all other words,

2060
01:58:13,980 --> 01:58:18,740
 but based on how other words are coordinated

2061
01:58:18,740 --> 01:58:20,980
 to the fourth word.

2062
01:58:20,980 --> 01:58:25,980
 Okay, so this is based on my understanding,

2063
01:58:26,419 --> 01:58:29,179
 the role of this so-called attention.

2064
01:58:32,059 --> 01:58:33,980
 Now it's pretty clear, right?

2065
01:58:33,980 --> 01:58:37,419
 The fourth word is a linear combination of all words.

2066
01:58:37,419 --> 01:58:40,620
 If we take weight as the word, okay?

2067
01:58:40,620 --> 01:58:42,059
 Different way are different words,

2068
01:58:42,059 --> 01:58:45,540
 but the different word has different weightage.

2069
01:58:45,540 --> 01:58:49,540
 The weights is determined by how the other words

2070
01:58:49,540 --> 01:58:51,980
 are coordinated to this fourth word,

2071
01:58:51,980 --> 01:58:55,099
 because we have k and the correlation between k1

2072
01:58:55,140 --> 01:59:00,140
 and k4, k2 and k4, k3 and k4, k4 and k4.

2073
01:59:02,060 --> 01:59:05,880
 All are coordination of all other words to the fourth word.

2074
01:59:05,880 --> 01:59:10,880
 This correlation are used as the weight to other words,

2075
01:59:11,460 --> 01:59:14,620
 then combine them together to produce

2076
01:59:14,620 --> 01:59:18,300
 new representation of the fourth word.

2077
01:59:19,300 --> 01:59:20,740
 Right?

2078
01:59:20,740 --> 01:59:23,740
 Now this is the role of the attention.

2079
01:59:23,740 --> 01:59:26,420
 This attention is used in the self-attention,

2080
01:59:26,420 --> 01:59:31,420
 cross-attention, and used in the encoder and decoder.

2081
01:59:32,580 --> 01:59:35,340
 Exactly identical use this formula.

2082
01:59:35,340 --> 01:59:36,500
 No difference.

2083
01:59:40,500 --> 01:59:45,500
 Okay, now here, of course this is a self-attention, right?

2084
01:59:45,980 --> 01:59:48,139
 The cross-attention, as I mentioned,

2085
01:59:48,139 --> 01:59:50,900
 that the mathematical formula are exact same.

2086
01:59:50,900 --> 01:59:54,740
 The only difference is for the cross-attention,

2087
01:59:56,179 --> 01:59:58,860
 the key come from the decoder,

2088
01:59:58,860 --> 02:00:02,620
 the previous stage of the decoder,

2089
02:00:02,620 --> 02:00:06,059
 our self-attention of the input of the decoder.

2090
02:00:06,059 --> 02:00:09,059
 Then the cross-connection is keys come from here,

2091
02:00:09,059 --> 02:00:14,059
 but value of key come from the output of the encoder.

2092
02:00:21,379 --> 02:00:26,379
 Okay, so then in the decoder,

2093
02:00:26,460 --> 02:00:28,620
 why it is working in this way?

2094
02:00:31,099 --> 02:00:34,339
 We want to produce the output.

2095
02:00:34,339 --> 02:00:37,780
 It is generated from the input, right?

2096
02:00:37,780 --> 02:00:42,780
 So the input, final decoder output is take us the value,

2097
02:00:43,780 --> 02:00:46,219
 the basis of the information.

2098
02:00:46,219 --> 02:00:50,139
 Based on this information, we produce the output.

2099
02:00:50,140 --> 02:00:53,780
 But this information is come from different words.

2100
02:00:53,780 --> 02:00:58,220
 Each word we have a representation as the word,

2101
02:00:58,220 --> 02:01:00,140
 so we have the key.

2102
02:01:00,140 --> 02:01:04,720
 Then what output we want to have?

2103
02:01:04,720 --> 02:01:09,720
 We use the previous output as the query to see,

2104
02:01:10,060 --> 02:01:13,900
 we have this query, we want to retrieve some information

2105
02:01:13,900 --> 02:01:17,700
 from the input to produce new output.

2106
02:01:17,700 --> 02:01:22,700
 Okay, so this is a rule of the query, key and the value.

2107
02:01:24,500 --> 02:01:25,340
 Okay.

2108
02:01:27,780 --> 02:01:31,460
 Now this is the attention,

2109
02:01:31,460 --> 02:01:33,900
 cross-attention in the decoder.

2110
02:01:33,900 --> 02:01:38,300
 Okay, but the mathematical formula is exactly same as here.

2111
02:01:38,300 --> 02:01:43,179
 After we produce, we determine what is the key,

2112
02:01:43,179 --> 02:01:46,860
 key and the way, these three metrics.

2113
02:01:46,860 --> 02:01:51,339
 Then the operation inside of this one

2114
02:01:51,339 --> 02:01:54,179
 identical to the self-attention,

2115
02:01:54,179 --> 02:01:59,179
 also identical to the attention in the encoder.

2116
02:02:00,780 --> 02:02:02,380
 They are all identical.

2117
02:02:06,639 --> 02:02:11,639
 Okay, now if we further see that for the decoder,

2118
02:02:12,780 --> 02:02:15,019
 we have two attention, right?

2119
02:02:15,060 --> 02:02:18,020
 Self-attention and cross-attention.

2120
02:02:18,020 --> 02:02:22,100
 Okay, somehow you can also study each element,

2121
02:02:22,100 --> 02:02:27,100
 what is each element to try to find out

2122
02:02:27,660 --> 02:02:30,460
 the meaning of this attention,

2123
02:02:30,460 --> 02:02:34,700
 what we try to get.

2124
02:02:34,700 --> 02:02:38,700
 For example, this cross-attention is a final output, right?

2125
02:02:38,700 --> 02:02:41,500
 Of course, we have much stages, right?

2126
02:02:41,500 --> 02:02:46,500
 Okay, so here the input is this one,

2127
02:02:46,940 --> 02:02:49,260
 output is this one, right?

2128
02:02:50,580 --> 02:02:55,580
 So here one word of the output come from the,

2129
02:02:55,580 --> 02:03:00,580
 all words of the input because it's a V1, V2, V3, V4.

2130
02:03:01,140 --> 02:03:06,140
 Okay, this is, V1, V2, V3, V4 come from this different words.

2131
02:03:07,140 --> 02:03:12,140
 So we use this, the first word as the key,

2132
02:03:12,380 --> 02:03:16,940
 as the query to see how this word

2133
02:03:16,940 --> 02:03:21,260
 coordinated to the first word, second word, third word,

2134
02:03:21,260 --> 02:03:24,300
 and fourth word as the weight.

2135
02:03:26,140 --> 02:03:28,340
 Use their coordination as the weight

2136
02:03:28,340 --> 02:03:33,340
 to weight this four words to produce one word here.

2137
02:03:36,540 --> 02:03:40,900
 This query word is this one.

2138
02:03:40,900 --> 02:03:45,900
 Anyway, before that here,

2139
02:03:45,900 --> 02:03:51,900
 I can clearly show how to produce one output of the attention, right?

2140
02:03:52,020 --> 02:03:55,980
 It's the same applied to the cross-attention.

2141
02:03:55,980 --> 02:03:58,700
 So you can also see the difference

2142
02:03:58,700 --> 02:04:02,540
 between the self-attention and the cross-attention,

2143
02:04:02,540 --> 02:04:07,540
 just if I forget input is here,

2144
02:04:08,980 --> 02:04:11,340
 here is input, here is output, right?

2145
02:04:11,340 --> 02:04:14,740
 If this one, then one output word come

2146
02:04:14,740 --> 02:04:19,740
 is one, come from one column, okay?

2147
02:04:20,820 --> 02:04:22,340
 Then, no.

2148
02:04:26,019 --> 02:04:30,540
 I somehow get confused to elaborate this matrix.

2149
02:04:30,540 --> 02:04:34,060
 But anyway, you better use this one

2150
02:04:34,060 --> 02:04:39,060
 to understand the meaning of what is output of the attention,

2151
02:04:39,100 --> 02:04:39,940
 okay?

2152
02:04:44,940 --> 02:04:48,980
 Now, we summarize this transformer

2153
02:04:48,980 --> 02:04:53,340
 and also to figure out what is the unique feature

2154
02:04:53,340 --> 02:04:55,340
 of this transformer.

2155
02:04:55,340 --> 02:05:00,340
 Now, this shows how this transformer works, right?

2156
02:05:00,540 --> 02:05:02,860
 Okay, so here is the input.

2157
02:05:02,860 --> 02:05:07,580
 This is your application of the answer, the question.

2158
02:05:07,580 --> 02:05:10,900
 The input is a question, hey, how are you?

2159
02:05:10,900 --> 02:05:14,820
 We have four words or four tokens, okay?

2160
02:05:14,820 --> 02:05:19,140
 These four words and all words of the input

2161
02:05:19,140 --> 02:05:23,780
 apparently come to the input into the transformer.

2162
02:05:23,780 --> 02:05:27,900
 Then transformer after encoder and then decoder

2163
02:05:27,900 --> 02:05:31,980
 will produce one word in the output.

2164
02:05:31,980 --> 02:05:35,780
 Then this word come to the input of the decoder

2165
02:05:35,780 --> 02:05:40,580
 together with the output of the encoder

2166
02:05:40,580 --> 02:05:43,580
 produce a second word, then come to the input,

2167
02:05:43,580 --> 02:05:46,099
 produce the third word and so on.

2168
02:05:46,099 --> 02:05:51,099
 So this is a working procedure of the transformer, okay?

2169
02:05:52,719 --> 02:05:55,740
 Now, this is a diagram of the transformer.

2170
02:05:55,740 --> 02:05:59,260
 I just capture this one from the first paper

2171
02:05:59,260 --> 02:06:01,059
 of the transformer, okay?

2172
02:06:01,059 --> 02:06:04,420
 In the CVPR published, the author is a group

2173
02:06:04,420 --> 02:06:06,460
 of the young researcher of the Google.

2174
02:06:07,660 --> 02:06:11,860
 Okay, now this is a diagram shown in this paper.

2175
02:06:11,860 --> 02:06:16,260
 Okay, show the whole diagram of the transformer.

2176
02:06:16,260 --> 02:06:21,260
 Here, show the transformer contains encoder and decoder.

2177
02:06:21,380 --> 02:06:24,300
 Right?

2178
02:06:24,300 --> 02:06:27,340
 But the encoder is times n.

2179
02:06:27,340 --> 02:06:32,140
 So that means we can have many encoder linked together.

2180
02:06:32,140 --> 02:06:34,780
 A stack of encoders.

2181
02:06:34,780 --> 02:06:39,020
 Here is just show one encoder, okay?

2182
02:06:39,020 --> 02:06:41,660
 The real transformer can have many encoder.

2183
02:06:41,660 --> 02:06:44,380
 How many depends on your design, right?

2184
02:06:44,380 --> 02:06:47,620
 You want to put how many elements

2185
02:06:47,620 --> 02:06:51,540
 but all module are identical, structure are identical.

2186
02:06:53,300 --> 02:06:58,300
 Now, in one encoder, it's called multi-hate attention.

2187
02:06:58,700 --> 02:07:01,860
 Okay, so the attention is we will produce

2188
02:07:01,860 --> 02:07:05,820
 many, many attentions, then produce many output,

2189
02:07:05,820 --> 02:07:09,700
 then after that we merge them together into one output.

2190
02:07:09,700 --> 02:07:13,620
 Okay, this is a multi-hate attention.

2191
02:07:13,620 --> 02:07:18,420
 Now, for each attention, to perform each attention

2192
02:07:18,420 --> 02:07:23,420
 is one input go to a projection to project

2193
02:07:23,740 --> 02:07:28,580
 to three different subspace or three different space.

2194
02:07:28,580 --> 02:07:33,380
 Okay, use three different projection metrics.

2195
02:07:33,380 --> 02:07:36,980
 So one metrics generate three copies

2196
02:07:36,980 --> 02:07:39,660
 of the same size of the metrics.

2197
02:07:39,660 --> 02:07:44,660
 Then this three metrics is called query key value.

2198
02:07:45,300 --> 02:07:48,620
 And then this three perform attention.

2199
02:07:49,900 --> 02:07:54,900
 The output of the attention is a metrics same as input.

2200
02:07:55,300 --> 02:07:58,059
 Okay, for example, four by three metrics.

2201
02:07:58,059 --> 02:08:01,780
 Then of course go to some kind of the normalization.

2202
02:08:01,780 --> 02:08:03,380
 Okay, I skip this normalization,

2203
02:08:03,380 --> 02:08:05,340
 it does not make much role.

2204
02:08:05,340 --> 02:08:08,180
 Anyway, we need this normalization, right?

2205
02:08:08,180 --> 02:08:12,220
 Then after that we go to a feed forward network.

2206
02:08:13,260 --> 02:08:15,620
 Then normalization, that's all.

2207
02:08:17,100 --> 02:08:21,100
 Output of one multi-hate attention.

2208
02:08:21,100 --> 02:08:23,540
 This is one encoder, right?

2209
02:08:24,780 --> 02:08:28,660
 Now, for the decoder, okay, we have two attention.

2210
02:08:28,660 --> 02:08:33,140
 One is a self attention, it is output of the previous

2211
02:08:33,140 --> 02:08:34,700
 output as an input.

2212
02:08:34,700 --> 02:08:39,700
 In the training phase, it is target output.

2213
02:08:39,740 --> 02:08:41,620
 It is a ground source, okay?

2214
02:08:41,620 --> 02:08:44,059
 But in the prediction process,

2215
02:08:44,059 --> 02:08:47,580
 we use the previous output as input here.

2216
02:08:47,580 --> 02:08:51,660
 But in the training, for example, in the translation,

2217
02:08:51,660 --> 02:08:55,059
 this is an input sentence, then this is the output sentence.

2218
02:08:55,059 --> 02:08:58,420
 We know the output sentence in the training, right?

2219
02:08:58,420 --> 02:09:02,700
 So we have the input in the encoder in the training.

2220
02:09:02,700 --> 02:09:05,660
 Then in the prediction, this one is unknown

2221
02:09:05,660 --> 02:09:08,820
 because this is the output, the true output, right?

2222
02:09:08,820 --> 02:09:13,120
 So we use the previous output link come to here.

2223
02:09:13,120 --> 02:09:18,120
 But anyway, the decoder has also a self attention.

2224
02:09:18,320 --> 02:09:21,540
 The module exactly same as the encoder.

2225
02:09:21,540 --> 02:09:23,540
 The decoder has one self attention

2226
02:09:23,540 --> 02:09:27,620
 and then come to cross attention.

2227
02:09:28,500 --> 02:09:32,660
 The structure or the procedure of the cross attention

2228
02:09:32,660 --> 02:09:35,140
 is same as the self attention only,

2229
02:09:35,140 --> 02:09:38,620
 that three copy of the input are different.

2230
02:09:38,620 --> 02:09:41,740
 It come from, one is come from the output

2231
02:09:41,740 --> 02:09:43,760
 of the previous self attention,

2232
02:09:43,760 --> 02:09:48,420
 the two come from the output of encoder.

2233
02:09:48,420 --> 02:09:53,420
 Okay, then after that is one feed forward network.

2234
02:09:54,260 --> 02:09:58,020
 Then produce the final feature vector.

2235
02:09:58,020 --> 02:10:01,180
 Then we go to a very simple linear classifier

2236
02:10:01,180 --> 02:10:02,620
 to get the final output.

2237
02:10:03,540 --> 02:10:06,940
 Now this is a whole structure of the transformer.

2238
02:10:06,940 --> 02:10:10,700
 This is what this picture I just copied from the paper.

2239
02:10:11,500 --> 02:10:15,300
 Attention is all you need, the title of the paper.

2240
02:10:17,820 --> 02:10:21,060
 Now from here we can see the paper

2241
02:10:22,060 --> 02:10:26,060
 represent the structure of the transformer.

2242
02:10:26,060 --> 02:10:29,580
 It's really all attention, right?

2243
02:10:29,580 --> 02:10:34,140
 Besides the attention, we also have a feed forward network.

2244
02:10:34,140 --> 02:10:37,860
 Feed forward layers here, show here.

2245
02:10:37,860 --> 02:10:42,860
 Okay, now here attention is, we must admit,

2246
02:10:44,820 --> 02:10:48,560
 attention is a core component in the transformer.

2247
02:10:49,560 --> 02:10:54,000
 But attention is people already use this attention

2248
02:10:54,000 --> 02:10:57,720
 in the same to put some attention

2249
02:10:57,720 --> 02:11:00,840
 after the same and so on, right?

2250
02:11:00,840 --> 02:11:05,840
 But the transformer is systematically use the attention

2251
02:11:06,000 --> 02:11:10,160
 and repeatedly use attention many many times.

2252
02:11:10,160 --> 02:11:12,960
 This is only difference between the attention

2253
02:11:12,960 --> 02:11:17,120
 in the transformer and the attention of some other works

2254
02:11:17,120 --> 02:11:22,120
 in the same structure before the transformer.

2255
02:11:23,880 --> 02:11:27,800
 Transformer, I already emphasize this,

2256
02:11:27,800 --> 02:11:30,519
 it use attention many times.

2257
02:11:30,519 --> 02:11:34,320
 It systematically use the attention, okay?

2258
02:11:34,320 --> 02:11:38,840
 But the attention itself is not a new one, right?

2259
02:11:38,840 --> 02:11:41,920
 The attention in many other application will already

2260
02:11:41,920 --> 02:11:46,640
 use the idea or similar concept of this attention.

2261
02:11:46,640 --> 02:11:50,040
 Attention is just to try to capture the similarity

2262
02:11:50,040 --> 02:11:52,040
 or correlation, right?

2263
02:11:53,680 --> 02:11:56,040
 Okay, so of course in the transformer,

2264
02:11:56,040 --> 02:11:59,080
 the attention is core component.

2265
02:11:59,080 --> 02:12:03,820
 But although it's a core component in the transformer,

2266
02:12:03,820 --> 02:12:07,440
 the attention itself is three metrics

2267
02:12:07,440 --> 02:12:10,440
 in the input multiplied together.

2268
02:12:10,440 --> 02:12:12,120
 This is attention basically

2269
02:12:13,120 --> 02:12:18,120
 process of the attention, right?

2270
02:12:19,240 --> 02:12:24,240
 So attention itself is not a learnable module.

2271
02:12:26,599 --> 02:12:29,640
 Attention there's no machine learning

2272
02:12:29,640 --> 02:12:33,440
 inside of that, compute the attention, right?

2273
02:12:33,440 --> 02:12:38,440
 So then what are the machine learnable parameter

2274
02:12:39,440 --> 02:12:41,360
 in the transformer?

2275
02:12:43,120 --> 02:12:48,120
 And then how are this learned parameter applied

2276
02:12:49,080 --> 02:12:50,320
 in the transformer?

2277
02:12:51,280 --> 02:12:54,780
 Are transformer any connection to the CNN?

2278
02:12:54,780 --> 02:12:59,759
 From this presentation of the structure of the transformer,

2279
02:12:59,759 --> 02:13:04,200
 it appears to be really nothing to do with the CNN, right?

2280
02:13:04,200 --> 02:13:08,400
 It just computes the attention and attention again.

2281
02:13:08,559 --> 02:13:11,599
 Many times and then of course we have the feed forward

2282
02:13:11,599 --> 02:13:14,320
 network, but feed forward network,

2283
02:13:14,320 --> 02:13:17,639
 you can use the name feed forward network,

2284
02:13:17,639 --> 02:13:20,040
 never use the name CNN.

2285
02:13:20,960 --> 02:13:23,080
 It just a feed forward.

2286
02:13:23,080 --> 02:13:25,519
 It just put one another word,

2287
02:13:25,519 --> 02:13:28,679
 it's a point wise feed forward network.

2288
02:13:30,320 --> 02:13:34,839
 Now, if we really study where is a learnable part,

2289
02:13:34,839 --> 02:13:37,960
 is learn from the training where we see

2290
02:13:37,960 --> 02:13:41,200
 the learnable part is this read corner,

2291
02:13:41,200 --> 02:13:43,920
 is this weakening matrix,

2292
02:13:43,920 --> 02:13:48,440
 is learned from the training data, okay?

2293
02:13:48,440 --> 02:13:51,880
 And this read corner matrix,

2294
02:13:51,880 --> 02:13:55,600
 one is used in this feed forward network,

2295
02:13:55,600 --> 02:14:00,600
 another is used to from one input produce three copies.

2296
02:14:02,280 --> 02:14:05,080
 In fact, it could produce 30 copies

2297
02:14:05,080 --> 02:14:07,820
 because it's a much hate attention.

2298
02:14:07,820 --> 02:14:10,460
 It's not just a producer three copies

2299
02:14:10,460 --> 02:14:13,540
 to perform one attention,

2300
02:14:13,540 --> 02:14:18,219
 it's to perform many, many attention,

2301
02:14:18,219 --> 02:14:21,460
 but after that it merge them together.

2302
02:14:22,740 --> 02:14:27,740
 Okay, so anyway, the basic process

2303
02:14:27,900 --> 02:14:32,299
 of how to use the learnable parameter is here.

2304
02:14:32,299 --> 02:14:37,019
 W, Q, W, K, W, V and W, one, two, three.

2305
02:14:37,020 --> 02:14:42,020
 All these metrics are learned from the training data.

2306
02:14:43,780 --> 02:14:47,180
 Then how these metrics are utilized

2307
02:14:47,180 --> 02:14:50,660
 to process the information,

2308
02:14:50,660 --> 02:14:53,460
 the input information is X, right?

2309
02:14:54,800 --> 02:14:58,180
 Okay, from this metrics matrix,

2310
02:14:58,180 --> 02:15:00,060
 what we can observe,

2311
02:15:01,980 --> 02:15:06,980
 X is a matrix, okay?

2312
02:15:06,980 --> 02:15:11,980
 One row is one word, different row is different word, okay?

2313
02:15:12,940 --> 02:15:17,379
 So this metrics X produce a metrics Q,

2314
02:15:17,379 --> 02:15:19,580
 this is the same size of the X,

2315
02:15:19,580 --> 02:15:22,740
 so it is just a copy of the X.

2316
02:15:22,740 --> 02:15:27,740
 Of course, different, we have different copies.

2317
02:15:28,820 --> 02:15:31,019
 Then this Q is also metrics,

2318
02:15:31,019 --> 02:15:34,860
 is also a word one, word two, word three, word four, right?

2319
02:15:34,860 --> 02:15:38,500
 We have Q one, Q two, Q three, Q four.

2320
02:15:38,500 --> 02:15:43,339
 Now, if we say X is an input sentence,

2321
02:15:43,339 --> 02:15:46,099
 the output is a Q is a sentence,

2322
02:15:46,099 --> 02:15:48,679
 then what is a word one?

2323
02:15:50,380 --> 02:15:52,299
 From this metrics multiplication,

2324
02:15:52,299 --> 02:15:57,299
 we see the word one Q one is the X one multiplied by WQ.

2325
02:16:01,299 --> 02:16:02,139
 Right?

2326
02:16:02,140 --> 02:16:05,420
 So you understand how the metrics multiplication

2327
02:16:05,420 --> 02:16:08,980
 to perform the first row of the Q.

2328
02:16:10,220 --> 02:16:14,300
 The first row of the Q is only determined

2329
02:16:14,300 --> 02:16:19,300
 by first row of the X and all parameter in this W, right?

2330
02:16:22,140 --> 02:16:24,620
 Then the second row of the Q only depends

2331
02:16:24,620 --> 02:16:29,100
 on the second input X, second row of the X.

2332
02:16:29,100 --> 02:16:32,020
 Nothing to do with the other row of the X.

2333
02:16:33,540 --> 02:16:36,740
 Now, this operation, what is this operation?

2334
02:16:37,780 --> 02:16:41,420
 If we say the different words in the image

2335
02:16:41,420 --> 02:16:45,340
 is different pixel, okay, or a map, right?

2336
02:16:45,340 --> 02:16:49,220
 If we say the vector X one is a different channel,

2337
02:16:49,220 --> 02:16:54,220
 different column in one word is different channel

2338
02:16:55,300 --> 02:16:57,059
 in a feature map,

2339
02:16:58,019 --> 02:17:01,459
 then how use this X to produce Q?

2340
02:17:02,500 --> 02:17:07,500
 We use the same metrics applied to all different eggs.

2341
02:17:09,740 --> 02:17:12,779
 Now, in terms of the different word,

2342
02:17:12,779 --> 02:17:14,539
 this is a convolution,

2343
02:17:14,539 --> 02:17:18,299
 it's just a single point of the convolution

2344
02:17:18,299 --> 02:17:20,500
 because the same parameter,

2345
02:17:20,500 --> 02:17:24,019
 learned parameter are going through all positions

2346
02:17:25,020 --> 02:17:27,020
 to produce different output.

2347
02:17:29,260 --> 02:17:33,060
 In my opinion, this is a convolution.

2348
02:17:33,060 --> 02:17:36,700
 The effect is the same as the convolution.

2349
02:17:36,700 --> 02:17:39,900
 That means the W is determined,

2350
02:17:39,900 --> 02:17:44,420
 is learned from all X one, X two, X three, X four,

2351
02:17:44,420 --> 02:17:48,260
 not just from one X something.

2352
02:17:48,260 --> 02:17:52,860
 All words are used to determine this W

2353
02:17:52,860 --> 02:17:56,700
 and the same W, same learned parameter

2354
02:17:56,700 --> 02:17:59,820
 are applied to all different words.

2355
02:18:00,980 --> 02:18:03,740
 If you try to understand what is this metrics

2356
02:18:03,740 --> 02:18:06,660
 of operation, you know that, right?

2357
02:18:06,660 --> 02:18:10,220
 So, all this linear projection,

2358
02:18:10,220 --> 02:18:15,220
 this is has same effect of the convolution.

2359
02:18:15,380 --> 02:18:17,660
 As I mentioned that in the beginning,

2360
02:18:17,660 --> 02:18:22,300
 the most significant feature of the convolution

2361
02:18:22,299 --> 02:18:24,539
 use the convolution in the machine learning

2362
02:18:24,539 --> 02:18:28,259
 is the parameter is go through all input.

2363
02:18:28,259 --> 02:18:32,699
 All input are used to determine a same parameter.

2364
02:18:33,939 --> 02:18:38,859
 This is also true in the transformer.

2365
02:18:38,859 --> 02:18:43,859
 How to learn this parameter W, KW, KW, W.

2366
02:18:44,700 --> 02:18:49,700
 Okay, so this is strongly linked with the convolution.

2367
02:18:50,980 --> 02:18:55,980
 Convolution does not mean it's just a local window.

2368
02:18:55,980 --> 02:18:58,460
 The convolution, the most important thing

2369
02:18:58,460 --> 02:19:01,020
 is the convolution is this window is moving

2370
02:19:01,020 --> 02:19:06,020
 to all positions in the training phase.

2371
02:19:07,220 --> 02:19:08,060
 Okay.

2372
02:19:08,500 --> 02:19:09,340
 Okay.

2373
02:19:11,779 --> 02:19:16,779
 Now, the next known parameter is W1, W2, W3, and so on,

2374
02:19:19,219 --> 02:19:22,859
 used in this feed forward network.

2375
02:19:22,859 --> 02:19:27,459
 The feed forward network, we here show an example

2376
02:19:27,459 --> 02:19:31,900
 of the three layers of the feed forward network.

2377
02:19:31,900 --> 02:19:36,900
 Now, in the transformer, it is just X multiply W1,

2378
02:19:36,900 --> 02:19:40,219
 go to a renew function, then multiply W2,

2379
02:19:40,219 --> 02:19:42,699
 go to a renew function, then multiply W3,

2380
02:19:42,699 --> 02:19:44,539
 go to a renew function.

2381
02:19:44,539 --> 02:19:49,539
 From here, we can see this X multiply by W1

2382
02:19:49,699 --> 02:19:54,300
 or the output multiply W2, same as this structure, right?

2383
02:19:55,580 --> 02:20:00,580
 We can see from here is also same parameter W1

2384
02:20:01,539 --> 02:20:04,619
 goes through all different words.

2385
02:20:05,620 --> 02:20:10,620
 And the same as this Q, one output words only depends

2386
02:20:11,060 --> 02:20:16,060
 on this input words, nothing to do with other input words.

2387
02:20:17,500 --> 02:20:18,420
 Right?

2388
02:20:18,420 --> 02:20:21,540
 So this is why there are some literature called

2389
02:20:21,540 --> 02:20:25,940
 this network as the point wise feed forward network.

2390
02:20:27,300 --> 02:20:31,420
 So in fact, this network is equivalent

2391
02:20:31,420 --> 02:20:34,500
 to convolutional network.

2392
02:20:34,500 --> 02:20:37,660
 Well, the convolution is the theater goes

2393
02:20:37,660 --> 02:20:39,620
 through all different words.

2394
02:20:39,620 --> 02:20:43,540
 The convolution in the different words dimension,

2395
02:20:43,540 --> 02:20:45,500
 it is just one point.

2396
02:20:45,500 --> 02:20:50,500
 Similar to image recognition, it's a one by one convolution.

2397
02:20:51,980 --> 02:20:53,180
 It's just one point.

2398
02:20:53,180 --> 02:20:55,500
 The size window is not three by three,

2399
02:20:55,500 --> 02:20:59,220
 it's just one by one, just one pixel, okay?

2400
02:20:59,220 --> 02:21:03,820
 But this one point goes through all positions of the pixels.

2401
02:21:03,820 --> 02:21:07,460
 But one point is we have many multiple channel, right?

2402
02:21:07,460 --> 02:21:10,300
 So this one point is just for the theater,

2403
02:21:10,300 --> 02:21:13,900
 it's not just one width, we have a weighting vector.

2404
02:21:13,900 --> 02:21:17,660
 But weighting vector is weight different channel, okay?

2405
02:21:17,660 --> 02:21:19,859
 Not a different spatial position.

2406
02:21:19,859 --> 02:21:23,699
 So here, this W, different W is just,

2407
02:21:23,699 --> 02:21:27,859
 the value of the W matrix is just weight different,

2408
02:21:29,420 --> 02:21:34,420
 compliant of a word, but all words use the same W, okay?

2409
02:21:37,980 --> 02:21:42,300
 So I believe this is also very strongly related

2410
02:21:42,300 --> 02:21:44,619
 to convolution CNN.

2411
02:21:48,859 --> 02:21:52,939
 All parameter learned from the training data

2412
02:21:52,939 --> 02:21:56,640
 is in a way same as the CNN.

2413
02:21:56,640 --> 02:21:59,840
 We move the parameter in all different positions.

2414
02:21:59,840 --> 02:22:02,240
 Here, we move this learned parameter

2415
02:22:02,240 --> 02:22:05,000
 in all different token.

2416
02:22:05,000 --> 02:22:07,920
 We are running through all different token

2417
02:22:07,920 --> 02:22:10,519
 to produce one parameter.

2418
02:22:10,519 --> 02:22:15,519
 And the learned one parameter are used in all input token.

2419
02:22:18,039 --> 02:22:21,080
 Now, from this matrix multiplication,

2420
02:22:21,080 --> 02:22:24,640
 it is not difficult to figure out that, right?

2421
02:22:25,640 --> 02:22:28,599
 So I don't believe in the transformer attention

2422
02:22:28,599 --> 02:22:32,080
 is everything, it's all you need.

2423
02:22:32,080 --> 02:22:37,080
 All learned parameter is learned in a way similar to CNN.

2424
02:22:38,800 --> 02:22:39,640
 Okay.

2425
02:22:41,000 --> 02:22:46,000
 Now, because the transformer is first used in the NARP,

2426
02:22:46,439 --> 02:22:48,400
 the text processing.

2427
02:22:48,400 --> 02:22:51,240
 So the transformer show very powerful.

2428
02:22:51,240 --> 02:22:56,240
 Before the transformer, the NARP is much behind

2429
02:22:56,720 --> 02:22:58,199
 the computation.

2430
02:22:58,199 --> 02:23:01,039
 The performance of the NARP is very poor.

2431
02:23:01,039 --> 02:23:03,720
 Compared to computation, computation you see

2432
02:23:03,720 --> 02:23:06,199
 and the performance is already very good.

2433
02:23:06,199 --> 02:23:08,880
 The NARP performance is very bad.

2434
02:23:08,880 --> 02:23:11,720
 So not widely used in the practice.

2435
02:23:12,600 --> 02:23:16,800
 But because of this transformer,

2436
02:23:16,800 --> 02:23:19,920
 the performance of the NARP jumped

2437
02:23:19,920 --> 02:23:23,320
 to very highly performed.

2438
02:23:23,320 --> 02:23:28,320
 So now the NARP, I believe, outperform computation.

2439
02:23:28,480 --> 02:23:30,640
 For example, chat GPT, right?

2440
02:23:30,640 --> 02:23:34,000
 It's so powerful that the computation

2441
02:23:34,000 --> 02:23:36,320
 because of this transformer.

2442
02:23:36,320 --> 02:23:38,720
 Now, because the transformer is so powerful,

2443
02:23:38,720 --> 02:23:43,080
 so excellent to perform in the process, the text data,

2444
02:23:43,080 --> 02:23:47,200
 then people try to convert this transformer

2445
02:23:47,200 --> 02:23:49,320
 into the computer vision.

2446
02:23:49,320 --> 02:23:54,160
 Okay, so we have the VIT, the vision transformer.

2447
02:23:55,800 --> 02:23:59,000
 So the first person that converts it

2448
02:23:59,000 --> 02:24:04,000
 call this structure as the VIT vision transformer.

2449
02:24:04,960 --> 02:24:07,480
 So how to do this conversion?

2450
02:24:07,480 --> 02:24:09,200
 So it is very simple, right?

2451
02:24:09,200 --> 02:24:12,320
 We have given an image, we partition the image

2452
02:24:12,320 --> 02:24:14,640
 into different patch.

2453
02:24:14,640 --> 02:24:17,200
 Okay, then for example, we partition an image

2454
02:24:17,200 --> 02:24:20,200
 into three by three patch.

2455
02:24:20,200 --> 02:24:22,320
 Then we have nine patches, right?

2456
02:24:22,320 --> 02:24:27,200
 Then this nine patches, each patch will serve as one token.

2457
02:24:27,200 --> 02:24:32,200
 Where different picture in one patch is different component

2458
02:24:33,120 --> 02:24:38,120
 as the numeric vector description of this patch.

2459
02:24:40,080 --> 02:24:45,080
 Okay, so the picture within a patch is a vector

2460
02:24:46,320 --> 02:24:50,280
 of this token, different patch is different token.

2461
02:24:51,360 --> 02:24:56,360
 So after such a conversion, we can input this image

2462
02:24:56,440 --> 02:24:59,160
 into a transformer, right?

2463
02:25:03,720 --> 02:25:07,480
 Now, the first application to apply this transformer

2464
02:25:07,480 --> 02:25:10,640
 into the computer vision problem is VIT,

2465
02:25:10,640 --> 02:25:14,800
 is applied to do the image classification,

2466
02:25:14,800 --> 02:25:17,439
 to classify the image into different class.

2467
02:25:17,439 --> 02:25:19,800
 For example, the image is an image of the bird

2468
02:25:19,800 --> 02:25:22,840
 or image of the bar, image of the car and so on.

2469
02:25:22,840 --> 02:25:27,840
 Okay, so in this application, we only need the encoder.

2470
02:25:28,600 --> 02:25:33,600
 We don't need the decoder because it's a discriminative

2471
02:25:34,600 --> 02:25:39,640
 network, not the generative network.

2472
02:25:39,640 --> 02:25:44,080
 Only the generative network, we need some decoder.

2473
02:25:46,040 --> 02:25:48,680
 Okay, now this is a VIT.

2474
02:25:48,680 --> 02:25:50,920
 This paper of course is very important

2475
02:25:50,920 --> 02:25:55,000
 because it's the first work to use the transformer

2476
02:25:55,000 --> 02:25:57,360
 in the computer vision, right?

2477
02:25:58,520 --> 02:26:02,600
 But this paper although is very famous,

2478
02:26:04,160 --> 02:26:07,920
 it shows this transformer structure here.

2479
02:26:07,920 --> 02:26:12,920
 Okay, show this network, feed forward network

2480
02:26:12,920 --> 02:26:15,560
 to name it as MRP.

2481
02:26:15,560 --> 02:26:18,240
 This of course is incorrect.

2482
02:26:19,440 --> 02:26:22,000
 As I mentioned that this feed forward

2483
02:26:22,000 --> 02:26:25,520
 is point wise feed forward, okay?

2484
02:26:26,560 --> 02:26:31,560
 Output of this network of one token is only determined

2485
02:26:31,560 --> 02:26:35,000
 by one token of the input, not all token.

2486
02:26:35,840 --> 02:26:39,600
 Okay, obviously this is not the MRP, right?

2487
02:26:39,600 --> 02:26:44,600
 Okay, in the encoder module of the transformer.

2488
02:26:50,519 --> 02:26:54,240
 It should be a point wise feed forward network.

2489
02:26:55,359 --> 02:26:59,640
 As I mentioned, explained just now,

2490
02:26:59,640 --> 02:27:03,599
 in fact this network is a convolution network.

2491
02:27:03,599 --> 02:27:06,039
 We can treat it as a convolutional network.

2492
02:27:09,119 --> 02:27:13,880
 Treat each word, each token are similar as peaks.

2493
02:27:13,880 --> 02:27:17,439
 Okay, different components to describe a word

2494
02:27:17,439 --> 02:27:21,359
 is different channel in the feature map.

2495
02:27:21,359 --> 02:27:25,400
 Then it is just a convolution, a one by one convolution, right?

2496
02:27:26,240 --> 02:27:31,240
 Okay, so this is of course the VIT is a first work

2497
02:27:32,840 --> 02:27:35,440
 to apply the transformer in the computer vision.

2498
02:27:35,440 --> 02:27:39,000
 It may not be very optimum, but anyway,

2499
02:27:39,000 --> 02:27:43,400
 it already shows that the performance could be

2500
02:27:43,400 --> 02:27:45,800
 much better than CNN.

2501
02:27:45,800 --> 02:27:50,080
 So the vision transformer outperforms the state of the art

2502
02:27:50,080 --> 02:27:55,080
 of CNN in multiple benchmark data set in the computation.

2503
02:27:57,200 --> 02:28:01,120
 So this shows how powerful is this transformer.

2504
02:28:03,000 --> 02:28:06,800
 So while the CNN has already approved track record

2505
02:28:06,800 --> 02:28:10,039
 in various computation problem to handle a large scale

2506
02:28:10,039 --> 02:28:13,840
 of the data set efficiently, but the vision transformer

2507
02:28:13,840 --> 02:28:16,960
 offers the advantage in the scenario that

2508
02:28:17,960 --> 02:28:22,919
 somehow we need global dependency information.

2509
02:28:23,960 --> 02:28:28,039
 Now I put a question mark in this so-called global.

2510
02:28:29,039 --> 02:28:33,560
 Now from the transformer, we say that it capture

2511
02:28:33,560 --> 02:28:37,759
 the relation between each pair of the word.

2512
02:28:37,759 --> 02:28:41,080
 That means it can capture of the correlation

2513
02:28:41,080 --> 02:28:44,779
 of the two parts even far away, right?

2514
02:28:44,780 --> 02:28:45,980
 But it only

2515
02:28:47,940 --> 02:28:52,300
 generates the attention between one part

2516
02:28:52,300 --> 02:28:54,980
 and each of other part.

2517
02:28:54,980 --> 02:28:59,980
 It never connects all information together, right?

2518
02:29:00,900 --> 02:29:05,540
 So we cannot say it's a global information.

2519
02:29:05,540 --> 02:29:10,540
 It's a pile-wise correlation, but all possible pile

2520
02:29:11,540 --> 02:29:15,380
 is computed by the transformer.

2521
02:29:16,780 --> 02:29:19,740
 Okay, so this is very important because in the computation

2522
02:29:19,740 --> 02:29:23,820
 maybe one part of the information could be closely

2523
02:29:23,820 --> 02:29:28,420
 correlated to some other part, but not correlated

2524
02:29:28,420 --> 02:29:31,260
 to all globally, right?

2525
02:29:31,260 --> 02:29:34,300
 But it can be closely correlated to other part,

2526
02:29:34,300 --> 02:29:36,820
 could be far away from this part.

2527
02:29:37,820 --> 02:29:42,140
 So the thing really is difficult to capture

2528
02:29:42,140 --> 02:29:45,380
 this kind of information, but the transformer

2529
02:29:45,380 --> 02:29:50,380
 will compute the attention between each pile of the token

2530
02:29:51,500 --> 02:29:55,580
 to solve the problem to get the relation

2531
02:29:55,580 --> 02:29:58,180
 between the two parts in the image.

2532
02:29:58,180 --> 02:30:01,699
 It could be far away, not closely together.

2533
02:30:02,619 --> 02:30:06,580
 So I believe this is a very significant feature

2534
02:30:06,580 --> 02:30:09,539
 in the transformer.

2535
02:30:09,539 --> 02:30:13,140
 Not just simply say transformer is global,

2536
02:30:14,100 --> 02:30:16,260
 Cn is just a local.

2537
02:30:19,420 --> 02:30:22,580
 Okay, now, lastly, I will show some of my work

2538
02:30:22,580 --> 02:30:24,500
 use the transformer, okay?

2539
02:30:24,500 --> 02:30:28,060
 In the earlier stage, of course, all this segmentation work

2540
02:30:28,060 --> 02:30:31,740
 I use the Cn, but now because the transformer

2541
02:30:31,740 --> 02:30:36,380
 is so powerful, then I also utilize the transformer

2542
02:30:36,380 --> 02:30:38,380
 in the image segmentation.

2543
02:30:38,380 --> 02:30:43,099
 We use this one in the so-called referring image segmentation.

2544
02:30:43,099 --> 02:30:46,699
 We build up a similar as a transformer,

2545
02:30:46,699 --> 02:30:50,820
 we call it the Weihung language transformer, VLT,

2546
02:30:50,820 --> 02:30:54,660
 because this network structure will handle

2547
02:30:54,660 --> 02:30:58,420
 the two information, image and text,

2548
02:30:58,420 --> 02:31:01,500
 or a language information and image information.

2549
02:31:02,820 --> 02:31:05,820
 Now, this network will perform referring segmentation

2550
02:31:05,820 --> 02:31:10,100
 that means given an image and given instruction

2551
02:31:10,100 --> 02:31:15,100
 in the language to segment the part of the image

2552
02:31:15,940 --> 02:31:19,340
 following the instruction of this language.

2553
02:31:19,340 --> 02:31:21,180
 For example, given this image, right?

2554
02:31:21,180 --> 02:31:24,300
 But the language instruct us to segment

2555
02:31:24,300 --> 02:31:27,980
 the small elephant on the left.

2556
02:31:27,980 --> 02:31:32,980
 Then the final result is owning segment out this elephant

2557
02:31:33,980 --> 02:31:36,660
 where other elephant is not segment out

2558
02:31:36,660 --> 02:31:41,660
 because the language instruction is small elephant

2559
02:31:42,060 --> 02:31:43,699
 on the left.

2560
02:31:43,699 --> 02:31:47,300
 Okay, this is so-called the referring segmentation.

2561
02:31:47,300 --> 02:31:50,340
 Okay, so this network meet handle two types

2562
02:31:50,340 --> 02:31:54,539
 of the information, image and the text.

2563
02:31:54,539 --> 02:31:58,000
 So it is called Weihung language transformer.

2564
02:31:58,000 --> 02:32:00,939
 Now, basically the structure we show here,

2565
02:32:00,980 --> 02:32:05,980
 we have image encoder to extract the feature of the image.

2566
02:32:06,540 --> 02:32:09,220
 This one we can use the CNN, right?

2567
02:32:09,220 --> 02:32:12,860
 Then we, from the text information,

2568
02:32:12,860 --> 02:32:17,700
 we need to get a feature vector, okay, by the embedding.

2569
02:32:17,700 --> 02:32:21,580
 Then after that, we go through some few process,

2570
02:32:21,580 --> 02:32:25,220
 then take a transformer, the encoder and the decoder

2571
02:32:25,220 --> 02:32:27,980
 of the transformer where the input of the encoder

2572
02:32:27,980 --> 02:32:29,860
 is the image feature.

2573
02:32:29,900 --> 02:32:33,940
 We take the feature of the language feature as the query

2574
02:32:33,940 --> 02:32:37,220
 to query the image feature and then at the end

2575
02:32:37,220 --> 02:32:39,420
 to produce the segmentation result.

2576
02:32:40,540 --> 02:32:45,540
 And here from original tokens from the language,

2577
02:32:45,900 --> 02:32:50,900
 we need somehow convert this token into a token

2578
02:32:50,900 --> 02:32:53,580
 closely related to the image.

2579
02:32:53,580 --> 02:32:57,140
 So we have the so-called query generation.

2580
02:32:57,140 --> 02:33:00,980
 We generate the query different from the original

2581
02:33:00,980 --> 02:33:04,900
 language tokens by compiler,

2582
02:33:04,900 --> 02:33:09,300
 compiler this different language token with the image token

2583
02:33:09,300 --> 02:33:11,619
 to generate a new queries.

2584
02:33:11,619 --> 02:33:13,740
 So here I show this module.

2585
02:33:13,740 --> 02:33:15,539
 I will not go through the details,

2586
02:33:15,539 --> 02:33:18,980
 but anyway, after that, we take the query

2587
02:33:18,980 --> 02:33:20,859
 from the language information,

2588
02:33:20,859 --> 02:33:25,260
 but convert it into related to the image

2589
02:33:25,260 --> 02:33:29,180
 as a query to query the image feature as a venue

2590
02:33:29,180 --> 02:33:34,180
 and key and as a key and a venue.

2591
02:33:35,500 --> 02:33:39,780
 The language as a query, then go to the decoder,

2592
02:33:39,780 --> 02:33:43,660
 then to produce the response to each query.

2593
02:33:43,660 --> 02:33:46,300
 And then all this response to each query

2594
02:33:46,300 --> 02:33:49,420
 we are figure out the segmented result

2595
02:33:49,420 --> 02:33:53,180
 where satisfy the instruction of this query.

2596
02:33:54,140 --> 02:33:54,980
 Okay.

2597
02:33:57,020 --> 02:34:02,020
 So this work is published in ICC way in 2021

2598
02:34:03,140 --> 02:34:07,220
 and also published in Parmi in last year.

2599
02:34:07,220 --> 02:34:10,980
 So these two papers, the ICC one already have two,

2600
02:34:10,980 --> 02:34:14,500
 more than 200 of citation this Parmi just published

2601
02:34:14,500 --> 02:34:18,460
 in last year, the citation is over 100,

2602
02:34:18,460 --> 02:34:20,980
 more than 100 of citation is in this one.

2603
02:34:21,980 --> 02:34:22,820
 Okay.

2604
02:34:22,820 --> 02:34:27,420
 So here I show example of how to apply the transformer

2605
02:34:27,420 --> 02:34:29,539
 in the computation problem.

2606
02:34:31,820 --> 02:34:32,660
 Okay.

2607
02:34:32,660 --> 02:34:35,260
 At the end, we will get the conclusion

2608
02:34:35,260 --> 02:34:38,300
 of all this deep learning part.

2609
02:34:39,500 --> 02:34:43,820
 So first AI now is very hot, right?

2610
02:34:43,820 --> 02:34:44,580
 It's booming.

2611
02:34:44,580 --> 02:34:48,980
 So we know this industry and the great impact

2612
02:34:48,980 --> 02:34:53,060
 of the artificial intelligence to us.

2613
02:34:54,060 --> 02:34:59,060
 But AI is booming because of the deep learning.

2614
02:34:59,180 --> 02:35:01,660
 Without deep learning, there's no AI.

2615
02:35:01,660 --> 02:35:03,580
 If there's no deep learning,

2616
02:35:03,580 --> 02:35:08,580
 then the standard of the AI is the same as 30 years ago.

2617
02:35:11,699 --> 02:35:12,859
 Okay.

2618
02:35:12,859 --> 02:35:17,859
 Now this whole AI is boost by the deep learning.

2619
02:35:18,100 --> 02:35:22,980
 Now the deep learning is becomes too very powerful

2620
02:35:22,980 --> 02:35:26,340
 because of the convolutional neural network.

2621
02:35:26,340 --> 02:35:31,220
 Convolutional neural network bring the deep learning.

2622
02:35:31,220 --> 02:35:32,060
 Okay.

2623
02:35:33,020 --> 02:35:34,180
 That's the CNN.

2624
02:35:35,060 --> 02:35:39,540
 Now CNN in last week we already studied the CNN.

2625
02:35:39,540 --> 02:35:40,540
 What is CNN?

2626
02:35:40,540 --> 02:35:43,940
 CNN solves the problem of the traditional neural network,

2627
02:35:43,940 --> 02:35:48,940
 M-A-R-P because CNN is a strongly recognized neural network.

2628
02:35:50,380 --> 02:35:51,900
 It has no co-attention,

2629
02:35:51,900 --> 02:35:55,300
 but the most important sense is CNN.

2630
02:35:55,300 --> 02:35:57,820
 It overcomes the overfitting problem.

2631
02:35:57,820 --> 02:36:02,820
 The parameter is learned from every single part of the image.

2632
02:36:03,580 --> 02:36:08,580
 Not just one part of the image determine one parameter.

2633
02:36:09,300 --> 02:36:14,300
 Other part of the image are used to determine other parameter.

2634
02:36:15,460 --> 02:36:17,740
 Each single parameter is determined

2635
02:36:17,740 --> 02:36:20,220
 by all part of the input image.

2636
02:36:21,380 --> 02:36:26,380
 Go by the convolution process in the learning phase.

2637
02:36:27,860 --> 02:36:28,700
 All right.

2638
02:36:28,700 --> 02:36:32,980
 So this is a key revolution of the CNN.

2639
02:36:34,900 --> 02:36:37,260
 But of course CNN has a limitation

2640
02:36:37,260 --> 02:36:41,420
 because it captures attention or similarity

2641
02:36:41,420 --> 02:36:43,940
 in terms of the filter window.

2642
02:36:43,940 --> 02:36:44,780
 Okay.

2643
02:36:44,780 --> 02:36:47,860
 So it will capture somehow how a window,

2644
02:36:47,860 --> 02:36:52,140
 this window small, then it can just capture local information.

2645
02:36:52,140 --> 02:36:54,380
 If the convolution window is larger,

2646
02:36:54,380 --> 02:36:59,380
 then we have too much of the information average together.

2647
02:36:59,380 --> 02:37:02,180
 It will also cause some problem.

2648
02:37:02,180 --> 02:37:06,180
 But this limitation is solved by the transformer

2649
02:37:06,180 --> 02:37:11,180
 because transformer captures attention pile wise.

2650
02:37:11,620 --> 02:37:12,460
 Okay.

2651
02:37:12,460 --> 02:37:16,140
 So it can capture attention between two parts

2652
02:37:16,140 --> 02:37:19,100
 is very far away,

2653
02:37:19,100 --> 02:37:24,100
 but it just captures the attention of the two small part.

2654
02:37:24,580 --> 02:37:25,420
 Okay.

2655
02:37:26,300 --> 02:37:29,300
 So this transformer,

2656
02:37:29,300 --> 02:37:30,900
 this property of the transformer

2657
02:37:30,900 --> 02:37:33,500
 overcome the limitation of the CNN.

2658
02:37:34,380 --> 02:37:39,380
 So the most critical machine learning part

2659
02:37:40,820 --> 02:37:45,420
 is not just simply make the machine learn from the data.

2660
02:37:45,420 --> 02:37:49,940
 We must use our human knowledge to guide the machine

2661
02:37:49,940 --> 02:37:52,300
 how to learn from the data,

2662
02:37:53,700 --> 02:37:56,700
 to design the network structure,

2663
02:37:56,700 --> 02:37:59,140
 to design different component,

2664
02:37:59,220 --> 02:38:04,220
 then apply this one in the machine learning from the data.

2665
02:38:07,340 --> 02:38:08,820
 So this guy,

2666
02:38:08,820 --> 02:38:11,699
 use human knowledge to guide the machine learning

2667
02:38:11,699 --> 02:38:16,539
 is also called some kind of the regularization.

2668
02:38:16,539 --> 02:38:18,660
 So this is why I, in my opinion,

2669
02:38:18,660 --> 02:38:21,619
 the regularization is utilize the human knowledge

2670
02:38:21,619 --> 02:38:23,900
 to guide the machine to learn from the data.

2671
02:38:23,900 --> 02:38:28,900
 This is the most critical part in the machine learning.

2672
02:38:29,939 --> 02:38:34,939
 So machine learning is just simply make machine to learn.

2673
02:38:35,340 --> 02:38:39,019
 This use our human knowledge to design the way

2674
02:38:39,019 --> 02:38:40,939
 the machine to learn from the data.

2675
02:38:40,939 --> 02:38:44,060
 This is really the center part

2676
02:38:44,060 --> 02:38:46,180
 of the machine learning and the computation.

2677
02:38:47,380 --> 02:38:51,900
 Now, if we take this opinion,

2678
02:38:51,900 --> 02:38:53,539
 the real good machine learning

2679
02:38:53,539 --> 02:38:56,820
 is not because the machine very powerful learn from data

2680
02:38:56,820 --> 02:39:00,420
 because our human can design architecture,

2681
02:39:00,420 --> 02:39:03,779
 design structure to make the machine

2682
02:39:03,779 --> 02:39:06,500
 where learn from the data.

2683
02:39:06,500 --> 02:39:09,140
 If in this perspective,

2684
02:39:09,140 --> 02:39:12,060
 many people scare the AI,

2685
02:39:12,060 --> 02:39:17,060
 future AI can outperform our human,

2686
02:39:17,500 --> 02:39:19,500
 then we cannot control the AI.

2687
02:39:19,500 --> 02:39:22,779
 Human cannot control the AI.

2688
02:39:22,779 --> 02:39:25,660
 In my opinion, this is nonsense.

2689
02:39:25,660 --> 02:39:29,340
 AI can never outperform our human

2690
02:39:29,340 --> 02:39:32,500
 because AI is always controlled by our human.

2691
02:39:33,619 --> 02:39:38,619
 Okay, so this is my opinion from artificial intelligence.

2692
02:39:39,740 --> 02:39:41,460
 Okay, then with this way,

2693
02:39:41,460 --> 02:39:46,460
 I complete my part of this course.

2694
02:39:47,100 --> 02:39:48,260
 Okay, so next week,

2695
02:39:48,260 --> 02:39:50,460
 we will have the new lecture come here

2696
02:39:50,460 --> 02:39:55,300
 to introduce the 3D and the video part of the computation.

2697
02:39:56,140 --> 02:39:58,240
 Okay, thank you very much.

2698
02:39:58,240 --> 02:39:59,580
 Thank you.

2699
02:40:25,660 --> 02:40:27,660
 Thank you.

2700
02:40:55,660 --> 02:40:57,660
 Thank you.

2701
02:41:25,660 --> 02:41:27,660
 Thank you.

2702
02:41:55,660 --> 02:41:57,660
 Thank you.

2703
02:42:25,660 --> 02:42:27,660
 Thank you.

2704
02:42:55,660 --> 02:42:57,660
 Thank you.

2705
02:43:25,660 --> 02:43:27,660
 Thank you.

2706
02:43:55,660 --> 02:43:57,660
 Thank you.

2707
02:44:25,660 --> 02:44:27,660
 Thank you.

2708
02:44:55,660 --> 02:44:57,660
 Thank you.

2709
02:45:25,660 --> 02:45:27,660
 Thank you.

2710
02:45:55,660 --> 02:45:57,660
 Thank you.

2711
02:46:25,660 --> 02:46:27,660
 Thank you.

2712
02:46:55,660 --> 02:46:57,660
 Thank you.

2713
02:47:25,660 --> 02:47:27,660
 Thank you.

2714
02:47:55,660 --> 02:47:57,660
 Thank you.

2715
02:48:25,660 --> 02:48:26,660
 Thank you.

2716
02:48:55,660 --> 02:48:57,660
 Thank you.

2717
02:49:25,660 --> 02:49:27,660
 Thank you.

2718
02:49:55,660 --> 02:49:57,660
 Thank you.

2719
02:50:25,660 --> 02:50:27,660
 Thank you.

2720
02:50:55,660 --> 02:50:57,660
 Thank you.

2721
02:51:25,660 --> 02:51:27,660
 Thank you.

2722
02:51:55,660 --> 02:51:57,660
 Thank you.

2723
02:52:25,660 --> 02:52:27,660
 Thank you.

2724
02:52:55,660 --> 02:52:57,660
 Thank you.

2725
02:53:25,660 --> 02:53:26,660
 Thank you.

2726
02:53:55,660 --> 02:53:57,660
 Thank you.

2727
02:54:25,660 --> 02:54:27,660
 Thank you.

2728
02:54:55,660 --> 02:54:57,660
 Thank you.

2729
02:55:25,660 --> 02:55:27,660
 Thank you.

2730
02:55:55,660 --> 02:55:57,660
 Thank you.

2731
02:56:25,660 --> 02:56:27,660
 Thank you.

2732
02:56:55,660 --> 02:56:57,660
 Thank you.

2733
02:57:25,660 --> 02:57:27,660
 Thank you.

2734
02:57:55,660 --> 02:57:57,660
 Thank you.

2735
02:58:25,660 --> 02:58:27,660
 Thank you.

2736
02:58:55,660 --> 02:58:57,660
 Thank you.

2737
02:59:25,660 --> 02:59:27,660
 Thank you.

2738
02:59:55,660 --> 02:59:57,660
 Thank you.

