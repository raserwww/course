1
00:00:00,000 --> 00:00:02,000
 you

2
00:00:30,000 --> 00:00:32,000
 you

3
00:01:00,000 --> 00:01:02,000
 you

4
00:01:30,000 --> 00:01:32,000
 you

5
00:02:00,000 --> 00:02:02,000
 you

6
00:02:30,000 --> 00:02:32,000
 you

7
00:03:00,000 --> 00:03:02,000
 you

8
00:03:30,000 --> 00:03:32,000
 you

9
00:04:30,000 --> 00:04:36,000
 you

10
00:04:40,000 --> 00:04:42,000
 .

11
00:04:44,000 --> 00:04:46,000
 Okay, good evening everyone.

12
00:04:48,000 --> 00:04:50,000
 So this project appears to be not work.

13
00:04:52,000 --> 00:04:54,000
 So anyway, I think these two are

14
00:04:56,000 --> 00:04:58,000
 sufficient for you to see the notes.

15
00:05:00,000 --> 00:05:02,000
 So we will start with the

16
00:05:04,000 --> 00:05:06,000
 first one, the

17
00:05:08,000 --> 00:05:10,000
 deep machine learning based on the

18
00:05:12,000 --> 00:05:14,000
 convolutional neural network.

19
00:05:16,000 --> 00:05:18,000
 So first we will study what is the neural

20
00:05:20,000 --> 00:05:22,000
 network. So we first introduce the network

21
00:05:24,000 --> 00:05:26,000
 structure and a neural

22
00:05:26,000 --> 00:05:28,000
 model.

23
00:05:30,000 --> 00:05:32,000
 So we will start with the

24
00:05:34,000 --> 00:05:36,000
 neural network. Then we will study

25
00:05:38,000 --> 00:05:40,000
 much more perceptome, it is much more neural network.

26
00:05:42,000 --> 00:05:44,000
 So then we will introduce the

27
00:05:46,000 --> 00:05:48,000
 convolutional neural network to understand

28
00:05:50,000 --> 00:05:52,000
 what is the deep learning.

29
00:05:52,000 --> 00:05:54,000
 So we will start with the

30
00:05:56,000 --> 00:05:58,000
 neural network.

31
00:06:00,000 --> 00:06:02,000
 So we first show a network structure

32
00:06:04,000 --> 00:06:06,000
 and a neural model. The left side

33
00:06:08,000 --> 00:06:10,000
 show a network structure.

34
00:06:12,000 --> 00:06:14,000
 So from this network structure it is pretty

35
00:06:16,000 --> 00:06:18,000
 clear what is the output in terms of the input.

36
00:06:18,000 --> 00:06:20,000
 So the output starts with the

37
00:06:22,000 --> 00:06:24,000
 output and a neural

38
00:06:26,000 --> 00:06:28,000
 network is done by a weight sum together

39
00:06:30,000 --> 00:06:32,000
 . And then we get the output of one

40
00:06:34,000 --> 00:06:36,000
 neurons and in the network structure

41
00:06:38,000 --> 00:06:40,000
 all neurons have exactly the same function

42
00:06:42,000 --> 00:06:44,000
 or same structure. So here

43
00:06:44,000 --> 00:06:49,000
 output of a neuron is weighted by the parameters,

44
00:06:49,000 --> 00:06:53,160
 then sum together, then this submission result

45
00:06:53,160 --> 00:06:57,080
 will compile with bias or choice,

46
00:06:57,080 --> 00:07:00,160
 then go through activation function,

47
00:07:00,160 --> 00:07:01,800
 then get the output.

48
00:07:01,800 --> 00:07:05,480
 So this model is applied to all neurons.

49
00:07:05,480 --> 00:07:08,880
 So all neurons has exactly same model.

50
00:07:08,880 --> 00:07:11,800
 Of course, the input will be different, right?

51
00:07:11,800 --> 00:07:15,800
 But of course, the parameter value are different,

52
00:07:15,800 --> 00:07:20,320
 but the model are identical to all neurons.

53
00:07:20,320 --> 00:07:21,480
 Okay.

54
00:07:21,480 --> 00:07:25,560
 So this is a graphically show a network structure

55
00:07:25,560 --> 00:07:29,000
 and a model of a single neuron.

56
00:07:29,000 --> 00:07:32,520
 Mathematically, it is even simpler to say

57
00:07:32,520 --> 00:07:37,520
 what is the neural network in terms of the input and output.

58
00:07:37,760 --> 00:07:41,000
 So from the previous slides, it is pretty clear.

59
00:07:41,000 --> 00:07:45,440
 So for all input xj, j from one to p, for example,

60
00:07:45,440 --> 00:07:49,880
 we have input p component in the input vectors, right?

61
00:07:49,880 --> 00:07:54,600
 All component of the input vectors are weighted sum together.

62
00:07:54,600 --> 00:07:59,600
 And then this submission result will compile with the choice

63
00:07:59,800 --> 00:08:01,640
 of all called bias.

64
00:08:01,640 --> 00:08:03,040
 We have different name, right?

65
00:08:03,040 --> 00:08:07,600
 Anyway, minus constant, then go through activation function,

66
00:08:07,600 --> 00:08:08,880
 then get the output.

67
00:08:09,840 --> 00:08:12,400
 This is applied to all neurons.

68
00:08:12,400 --> 00:08:13,240
 Okay.

69
00:08:14,480 --> 00:08:19,480
 Now, if we want to further work on this neural network,

70
00:08:21,000 --> 00:08:24,400
 sometimes we need express the neural network

71
00:08:24,400 --> 00:08:26,400
 in simpler form, right?

72
00:08:26,400 --> 00:08:31,400
 To simplify the expression, we can integrate this trisor

73
00:08:31,560 --> 00:08:34,880
 or this bias into this submission.

74
00:08:34,880 --> 00:08:35,960
 This is very easy.

75
00:08:35,960 --> 00:08:40,480
 For example, we suppose we have a constant component x0,

76
00:08:40,480 --> 00:08:42,280
 it is a constant one.

77
00:08:42,280 --> 00:08:47,280
 Then this trisor is similar as this weight as a minus theta.

78
00:08:47,440 --> 00:08:51,640
 So if we're doing so, then we can integrate

79
00:08:51,640 --> 00:08:55,480
 this minus trisor into this submission, okay?

80
00:08:55,480 --> 00:08:59,200
 To make the expression simpler.

81
00:08:59,200 --> 00:09:03,360
 So this is just to simplify the mathematical equation

82
00:09:03,360 --> 00:09:07,720
 so that we have the, for all neurons, right?

83
00:09:07,720 --> 00:09:12,240
 The output is just if a function if the input

84
00:09:12,240 --> 00:09:16,920
 of this function is just weight sum of all inputs.

85
00:09:16,920 --> 00:09:17,760
 That's all.

86
00:09:17,760 --> 00:09:21,040
 Okay, of course, although we can use this model,

87
00:09:21,040 --> 00:09:23,320
 don't forget the trisor.

88
00:09:23,320 --> 00:09:26,640
 Okay, this is very important for all neural networks.

89
00:09:26,640 --> 00:09:31,640
 If without the trisor, it will have a lot of limitations.

90
00:09:32,640 --> 00:09:37,640
 Because to make a decision, we know the Bayes class field

91
00:09:37,840 --> 00:09:39,840
 from the Gaussian assumption, right?

92
00:09:39,840 --> 00:09:42,280
 Nuclear distance, Mach-Henry distance,

93
00:09:42,280 --> 00:09:45,439
 all together must, together we have a trisor,

94
00:09:45,439 --> 00:09:47,920
 we have a constant, right?

95
00:09:47,920 --> 00:09:52,840
 Without this constant, then somehow the function

96
00:09:52,840 --> 00:09:54,319
 is greatly limited.

97
00:09:54,319 --> 00:09:59,120
 So never forget this constant, this trisor, okay?

98
00:09:59,120 --> 00:10:01,520
 Although we express a neural network

99
00:10:01,520 --> 00:10:05,360
 is just the weight sum of all inputs.

100
00:10:06,320 --> 00:10:11,320
 Okay, so this is W0 with a constant input minus one,

101
00:10:12,880 --> 00:10:17,880
 include in the input so that we can simplify the expression.

102
00:10:19,160 --> 00:10:22,480
 Okay, now for the activation function,

103
00:10:22,480 --> 00:10:25,600
 we have a few activation functions

104
00:10:25,600 --> 00:10:28,360
 in the development of the neural network.

105
00:10:28,360 --> 00:10:32,680
 In the earlier stage, we use the so-called binary function,

106
00:10:32,680 --> 00:10:36,280
 zero or one, that means if the input of this activation

107
00:10:36,280 --> 00:10:39,600
 function is larger than zero, then its output is one,

108
00:10:39,600 --> 00:10:41,560
 if smaller than zero is zero.

109
00:10:41,560 --> 00:10:45,000
 So this is just a step function, right?

110
00:10:45,000 --> 00:10:48,920
 So I guess why people utilize this function

111
00:10:48,920 --> 00:10:52,560
 as the activation function is because usually we use

112
00:10:52,560 --> 00:10:55,120
 this neural network to do the classification.

113
00:10:55,120 --> 00:10:58,680
 For the classification, what we need is class label.

114
00:10:58,680 --> 00:11:03,320
 So we need clearly what the output is zero or one.

115
00:11:03,320 --> 00:11:07,040
 One is this class, zero is not this class, right?

116
00:11:07,040 --> 00:11:11,760
 So because of that, somehow to make the output

117
00:11:11,760 --> 00:11:15,760
 be pretty clear, then we utilize this so-called

118
00:11:15,760 --> 00:11:17,400
 binary function.

119
00:11:17,400 --> 00:11:20,400
 Okay, but this binary function is very limited.

120
00:11:20,400 --> 00:11:25,319
 So later, we almost all neural network does not

121
00:11:25,319 --> 00:11:27,640
 utilize this binary function.

122
00:11:27,640 --> 00:11:31,560
 Okay, now a slightly improvement of this so-called

123
00:11:31,560 --> 00:11:34,480
 binary function is a piecewise linear functions.

124
00:11:34,480 --> 00:11:38,040
 It is a binary function, but in the middle part,

125
00:11:38,040 --> 00:11:42,040
 we introduce a linear function, fv equals 2v.

126
00:11:43,480 --> 00:11:46,720
 Now the very important activation function is the next two.

127
00:11:46,720 --> 00:11:49,319
 One is the so-called sigmoid function.

128
00:11:49,320 --> 00:11:52,560
 The definition of the sigmoid function is here one,

129
00:11:52,560 --> 00:11:55,600
 over one plus the exponential function,

130
00:11:55,600 --> 00:11:58,960
 and the index of the exponential function is minus av.

131
00:11:58,960 --> 00:12:02,440
 v is input, a is parameter.

132
00:12:02,440 --> 00:12:05,600
 Because this function is a smooth function, okay?

133
00:12:05,600 --> 00:12:08,520
 It's differentiable everywhere.

134
00:12:08,520 --> 00:12:13,520
 So this is a nice property of this so-called

135
00:12:14,520 --> 00:12:15,960
 sigmoid functions.

136
00:12:16,960 --> 00:12:20,400
 Now the sigmoid function, although it's differentiable

137
00:12:20,400 --> 00:12:24,240
 everywhere, it is nice to derive the learning rule,

138
00:12:24,240 --> 00:12:28,040
 because in the learning, we need to compute the gradient.

139
00:12:28,040 --> 00:12:32,040
 We need to take the derivative of the functions.

140
00:12:32,040 --> 00:12:37,040
 So this is a sigmoid function, it's very nice for

141
00:12:37,800 --> 00:12:39,200
 this application.

142
00:12:39,200 --> 00:12:43,960
 But this sigmoid function could have some limitations

143
00:12:43,960 --> 00:12:48,600
 because it is somehow close to this binary function.

144
00:12:48,600 --> 00:12:51,040
 Okay, so this is why in the deep learning,

145
00:12:51,040 --> 00:12:55,640
 C and N and the transformer, now all activation functions

146
00:12:55,640 --> 00:12:59,440
 utilize the so-called renewal functions.

147
00:12:59,440 --> 00:13:02,480
 So the renewal function, the mathematical definition,

148
00:13:02,480 --> 00:13:06,800
 is the maximum value of zero or input v.

149
00:13:06,800 --> 00:13:11,800
 So basically if v is larger than zero, then it is a v.

150
00:13:11,959 --> 00:13:15,479
 If v is smaller than zero, then it is zero.

151
00:13:15,479 --> 00:13:17,359
 So basically it's this one.

152
00:13:17,359 --> 00:13:21,839
 So this function in fact has, although it's very simple,

153
00:13:21,839 --> 00:13:24,599
 but it has very nice properties.

154
00:13:24,599 --> 00:13:28,760
 All deep learning utilize these activation functions.

155
00:13:28,760 --> 00:13:32,479
 Because later we will learn that for

156
00:13:32,479 --> 00:13:35,040
 multi-layer of the neural network,

157
00:13:35,040 --> 00:13:38,400
 the activation function must be nonlinear.

158
00:13:38,400 --> 00:13:41,480
 If it is a linear, then the multi-layer is just

159
00:13:41,480 --> 00:13:43,400
 equivalent to a single layer.

160
00:13:44,400 --> 00:13:46,199
 So it must be a nonlinear.

161
00:13:46,199 --> 00:13:49,760
 Well, this nonlinear function, I believe,

162
00:13:49,760 --> 00:13:52,760
 is the simplest nonlinear function.

163
00:13:52,760 --> 00:13:56,720
 So for example, if we plot all this activation function,

164
00:13:56,720 --> 00:13:58,360
 we will see this graph.

165
00:14:00,079 --> 00:14:01,000
 It is a linear, right?

166
00:14:01,000 --> 00:14:04,840
 The first one is a binary function, okay, zero or one.

167
00:14:04,840 --> 00:14:08,199
 The second one is somehow modified binary function,

168
00:14:08,200 --> 00:14:11,200
 where so-called piecewise linear function here

169
00:14:11,200 --> 00:14:15,880
 is a part of the linear functions, right?

170
00:14:15,880 --> 00:14:18,200
 Now this is a single mode function.

171
00:14:18,200 --> 00:14:21,840
 The single mode function is a smooth curve.

172
00:14:21,840 --> 00:14:26,840
 We can get the derivative or it is a differentiate

173
00:14:26,920 --> 00:14:31,680
 everywhere, every place we can get its derivative.

174
00:14:31,680 --> 00:14:36,680
 Well, this node is determined by this parameter a.

175
00:14:37,680 --> 00:14:41,599
 Okay, obviously, if we take a large value of the a,

176
00:14:41,599 --> 00:14:46,599
 then we have somehow a sharp jump.

177
00:14:47,079 --> 00:14:48,560
 Then the single mode function

178
00:14:48,560 --> 00:14:51,099
 we are close to the binary function.

179
00:14:51,099 --> 00:14:55,120
 Okay, if we take this parameter a small value,

180
00:14:55,120 --> 00:14:57,160
 then it is more flat.

181
00:14:57,160 --> 00:15:02,160
 Then it can be almost a linear function, right?

182
00:15:02,439 --> 00:15:04,040
 So this is a single mode function.

183
00:15:04,040 --> 00:15:07,400
 But the most important sense is a single mode function

184
00:15:07,400 --> 00:15:09,079
 is a continuous function.

185
00:15:09,079 --> 00:15:10,480
 It's a smooth function.

186
00:15:10,480 --> 00:15:14,120
 It is a differentiable everywhere, okay?

187
00:15:14,120 --> 00:15:16,199
 Now we come to a renewal function.

188
00:15:16,199 --> 00:15:19,280
 Now in all deep learning, we use the renewal function.

189
00:15:19,280 --> 00:15:21,520
 From this mathematical expression to plot,

190
00:15:21,520 --> 00:15:26,000
 it is quite clear it is zero or way, right?

191
00:15:26,000 --> 00:15:27,719
 If way is equals to way.

192
00:15:28,680 --> 00:15:32,199
 If way is smaller than zero, then the output is zero.

193
00:15:33,200 --> 00:15:38,200
 So this renewal function has a very nice property.

194
00:15:40,600 --> 00:15:43,360
 First of all, I cannot imagine we can have

195
00:15:44,560 --> 00:15:48,560
 a larger nonlinear function can be simpler than this one.

196
00:15:49,440 --> 00:15:51,920
 Okay, it's a simplest nonlinear function.

197
00:15:51,920 --> 00:15:56,920
 And it is a linear function if the input is positive.

198
00:15:56,920 --> 00:15:59,040
 Now if input is a negative,

199
00:15:59,040 --> 00:16:02,000
 then output is a constant zero.

200
00:16:02,000 --> 00:16:04,320
 So because of that, all deep learning,

201
00:16:04,320 --> 00:16:07,240
 the output is nonnegative.

202
00:16:07,240 --> 00:16:10,480
 Now because of it is a nonnegative,

203
00:16:10,480 --> 00:16:14,760
 then we can always based on the value of the output

204
00:16:14,760 --> 00:16:19,320
 to determine if this neuron is a significant

205
00:16:19,320 --> 00:16:22,360
 to the network or not.

206
00:16:22,360 --> 00:16:24,400
 Okay, if it is close to zero,

207
00:16:24,400 --> 00:16:27,160
 then this neuron is insignificant.

208
00:16:27,160 --> 00:16:32,160
 That's not take much lower in the whole network, right?

209
00:16:33,120 --> 00:16:36,000
 But if it is not as always zero,

210
00:16:36,000 --> 00:16:41,000
 then we cannot use this simple way to identify a neuron

211
00:16:41,400 --> 00:16:43,760
 is a significant contribution

212
00:16:43,760 --> 00:16:47,199
 or insignificant contribution, right?

213
00:16:47,199 --> 00:16:50,600
 And also if the input is nonnegative,

214
00:16:50,600 --> 00:16:55,120
 is input of this nonlinear function is positive,

215
00:16:55,120 --> 00:16:57,880
 then it is just a linear function.

216
00:16:57,880 --> 00:17:01,600
 Okay, linear and nonlinear has significant difference

217
00:17:01,600 --> 00:17:03,160
 because a linear function,

218
00:17:03,160 --> 00:17:06,280
 it's very easy to handle it.

219
00:17:06,280 --> 00:17:09,599
 So this is some nice property of the RELU functions.

220
00:17:10,560 --> 00:17:12,359
 Of course, we have,

221
00:17:12,359 --> 00:17:16,560
 we always have a huge number of the researchers, right?

222
00:17:16,560 --> 00:17:20,520
 Even this function is first utilized in the deep learning.

223
00:17:20,520 --> 00:17:23,119
 Some researchers try to improve it.

224
00:17:23,119 --> 00:17:28,119
 For example, utilize some kind of another small slope

225
00:17:28,439 --> 00:17:31,479
 in the negative way and negative range

226
00:17:31,479 --> 00:17:35,760
 to produce another modified RELU function,

227
00:17:35,760 --> 00:17:38,159
 but this does not make any sense.

228
00:17:38,159 --> 00:17:41,600
 In my opinion, this zero or always zero

229
00:17:41,600 --> 00:17:43,439
 and a linear function,

230
00:17:43,439 --> 00:17:47,719
 this is a very nice property for the deep learning.

231
00:17:47,719 --> 00:17:50,320
 This simple nonlinear function

232
00:17:51,200 --> 00:17:54,600
 enables the network can work

233
00:17:54,600 --> 00:17:59,159
 in very large number of the layers.

234
00:17:59,159 --> 00:18:00,399
 So we have no problem

235
00:18:00,399 --> 00:18:03,280
 because the activation function is very simple.

236
00:18:03,280 --> 00:18:08,280
 Okay, now anyway, this is some activation functions.

237
00:18:08,879 --> 00:18:12,360
 Now, then we will study this neural network.

238
00:18:12,360 --> 00:18:13,840
 Okay, first we will see,

239
00:18:14,919 --> 00:18:16,840
 given a network structure,

240
00:18:16,840 --> 00:18:20,959
 how to express the output in terms of the input.

241
00:18:20,959 --> 00:18:23,520
 So this is a first step, right?

242
00:18:23,520 --> 00:18:27,639
 So here, we use some symbol

243
00:18:27,639 --> 00:18:30,199
 to indicate the input and the output.

244
00:18:30,199 --> 00:18:32,959
 Input is X, of course,

245
00:18:32,959 --> 00:18:37,240
 usually we can arrange all inputs into a vector, right?

246
00:18:37,240 --> 00:18:39,639
 So input is X is a vector.

247
00:18:39,639 --> 00:18:44,639
 The component X, Y, X2 and X and output is also a vector,

248
00:18:44,640 --> 00:18:48,760
 so Y, okay, we have Y1 up to Yc, okay?

249
00:18:48,760 --> 00:18:53,280
 Then from the mathematical model of the neural network

250
00:18:53,280 --> 00:18:55,240
 or the model of the neuron,

251
00:18:55,240 --> 00:18:59,800
 we know that the output of one neuron

252
00:18:59,800 --> 00:19:04,280
 is the linear, which the sum of all inputs,

253
00:19:04,280 --> 00:19:08,760
 then goes through an activation function, right?

254
00:19:08,760 --> 00:19:12,680
 So if it's a weighted sum of the all input,

255
00:19:12,680 --> 00:19:15,080
 then we can express this weighted sum

256
00:19:15,080 --> 00:19:19,280
 as the multiplication of one weighted vector

257
00:19:19,280 --> 00:19:23,960
 contains all weights linked to a single neuron

258
00:19:23,960 --> 00:19:27,920
 multiplied by input, okay, vector.

259
00:19:27,920 --> 00:19:32,600
 Then we get the weighted summation, okay?

260
00:19:32,600 --> 00:19:35,560
 But all this vector of the,

261
00:19:37,360 --> 00:19:42,160
 a vector can only contain all weights of one neurons, right?

262
00:19:42,160 --> 00:19:45,760
 Because one neurons has multiple input weights sum,

263
00:19:45,760 --> 00:19:48,520
 so we need multiple weights.

264
00:19:48,520 --> 00:19:52,680
 All these multiple weights contain in one vectors,

265
00:19:52,680 --> 00:19:56,200
 but we have multiple neurons, apparently, right?

266
00:19:56,200 --> 00:19:58,440
 So we have multiple these vectors,

267
00:19:58,440 --> 00:20:00,920
 so we can put all these weighting vector

268
00:20:00,920 --> 00:20:05,480
 each for each neuron together into a matrix.

269
00:20:05,480 --> 00:20:10,480
 To this matrix contains all parameter of one nails,

270
00:20:11,360 --> 00:20:13,360
 our neural network, right?

271
00:20:13,360 --> 00:20:15,680
 We can put it into a matrix.

272
00:20:16,520 --> 00:20:18,520
 So if we put a matrix,

273
00:20:18,520 --> 00:20:23,520
 then all output is a matrix multiplied by input vectors,

274
00:20:25,360 --> 00:20:26,200
 okay?

275
00:20:26,200 --> 00:20:30,440
 So similarly, we can understand if we put all weights

276
00:20:30,440 --> 00:20:34,200
 in a matrix, what is the dimension of this matrix?

277
00:20:35,080 --> 00:20:38,040
 If for this layer of the neural network,

278
00:20:38,040 --> 00:20:42,040
 we have input, un-input, and output is D,

279
00:20:42,040 --> 00:20:47,040
 then this matrix must be M by D matrix, right?

280
00:20:47,040 --> 00:20:49,120
 M by D matrix.

281
00:20:49,120 --> 00:20:51,639
 Similarly for the second layer,

282
00:20:51,639 --> 00:20:56,639
 if input is D, output is C, C number of the output,

283
00:20:57,040 --> 00:21:02,040
 then the matrix V that contain all parameter

284
00:21:02,360 --> 00:21:07,360
 in this second layer must be D by C matrix.

285
00:21:08,159 --> 00:21:09,000
 All right?

286
00:21:10,159 --> 00:21:13,760
 Okay, so we have input, we have weighting matrix,

287
00:21:13,760 --> 00:21:16,159
 then we have output in the hidden layer,

288
00:21:16,159 --> 00:21:19,120
 in the first layer is Z, okay?

289
00:21:19,120 --> 00:21:22,720
 Then we have the weighting matrix of the second layer,

290
00:21:22,720 --> 00:21:26,760
 then produce the final output Y, okay?

291
00:21:26,760 --> 00:21:31,120
 Now, this is an expression from input and output,

292
00:21:31,120 --> 00:21:34,120
 but for training, for machine learning,

293
00:21:34,120 --> 00:21:37,360
 for this network learn from the training data,

294
00:21:37,360 --> 00:21:42,360
 for each training samples, we must have ground tools,

295
00:21:43,479 --> 00:21:46,639
 or the desired output, T, here,

296
00:21:47,840 --> 00:21:51,879
 in the initial phase of the neural network,

297
00:21:51,879 --> 00:21:53,919
 we call it a teacher, okay?

298
00:21:53,919 --> 00:21:58,919
 So it is a T, anyway, this is ground tools of the Y, okay?

299
00:21:59,159 --> 00:22:02,000
 So that is the ground tools of the training sample.

300
00:22:02,000 --> 00:22:05,000
 Given a training sample, we know the input,

301
00:22:05,000 --> 00:22:09,160
 and for this input, what is output is T, okay?

302
00:22:09,160 --> 00:22:12,320
 Given a network, the real output Y

303
00:22:12,320 --> 00:22:14,320
 may be different from the T.

304
00:22:14,320 --> 00:22:17,800
 Then the machine learning is to learn the parameter W

305
00:22:17,800 --> 00:22:22,000
 and the way to make this Y close to this T.

306
00:22:22,000 --> 00:22:26,200
 Then this is a training process, right?

307
00:22:26,200 --> 00:22:31,200
 So this is all variables and parameters

308
00:22:31,360 --> 00:22:34,640
 of a multi-layer neural network,

309
00:22:34,640 --> 00:22:38,200
 or we call it M-A-R-P, multi-layer perceptons.

310
00:22:38,200 --> 00:22:42,120
 Of course, here, we just show a two-layer network.

311
00:22:44,480 --> 00:22:49,480
 Okay, now, then mathematically, to express output

312
00:22:49,960 --> 00:22:53,240
 and the input, we can use three different ways.

313
00:22:53,240 --> 00:22:57,800
 We can use the scanner way, or vector way, or matrix way.

314
00:22:57,800 --> 00:23:02,800
 The scanner way is each, every variable is just one,

315
00:23:03,040 --> 00:23:06,440
 one venue, one venue is just one single variable.

316
00:23:06,440 --> 00:23:08,919
 Then we can use this scanner way

317
00:23:08,919 --> 00:23:11,840
 to express the output and input.

318
00:23:11,840 --> 00:23:15,919
 For example, all input, although input is a vector, right?

319
00:23:15,919 --> 00:23:18,600
 But a vector contains meaning component.

320
00:23:18,600 --> 00:23:22,000
 So each component are weighted by the weights

321
00:23:22,000 --> 00:23:24,639
 of the first layer, then sum together.

322
00:23:25,640 --> 00:23:30,640
 Then goes activation function to produce the output

323
00:23:31,000 --> 00:23:34,040
 of one single neuron, right?

324
00:23:34,040 --> 00:23:37,920
 So all this variable are scanner variable.

325
00:23:37,920 --> 00:23:39,360
 Scanner variable.

326
00:23:39,360 --> 00:23:42,360
 Of course, you may say we should start from zero

327
00:23:42,360 --> 00:23:44,280
 to include bias.

328
00:23:44,280 --> 00:23:46,560
 This is just to simplify it.

329
00:23:46,560 --> 00:23:50,200
 We can also put the bias as i equals to one,

330
00:23:50,200 --> 00:23:54,280
 the real signal x, the real input start from the two

331
00:23:54,560 --> 00:23:56,720
 and right, so no problem.

332
00:23:56,720 --> 00:24:01,720
 Okay, so anyway, this one, we use all scanner variable

333
00:24:03,040 --> 00:24:07,960
 and the parameter from the input to produce the output.

334
00:24:07,960 --> 00:24:09,520
 Now, this is pretty clear.

335
00:24:09,520 --> 00:24:12,680
 This is come from the structure of the neural network.

336
00:24:12,680 --> 00:24:17,680
 And this weighted sum, we should be able to know

337
00:24:17,800 --> 00:24:20,600
 that this weighted sum is in fact,

338
00:24:20,600 --> 00:24:24,080
 the multiplication of the two vector, right?

339
00:24:24,080 --> 00:24:27,879
 Okay, so we already started so many content.

340
00:24:27,879 --> 00:24:31,280
 You should be able to immediately know

341
00:24:31,280 --> 00:24:34,199
 that the multiplication of the two vector

342
00:24:34,199 --> 00:24:37,439
 is a weighted sum, is this one, right?

343
00:24:37,439 --> 00:24:42,080
 So if we represent the output in terms of the vector

344
00:24:42,080 --> 00:24:46,480
 multiplication, then go through activation function.

345
00:24:46,480 --> 00:24:51,399
 So this is a vector way to represent the input vector

346
00:24:51,399 --> 00:24:53,280
 to a single output.

347
00:24:54,360 --> 00:24:57,280
 Of one neuron.

348
00:24:57,280 --> 00:25:02,280
 But if we want to have, this is a first layer,

349
00:25:03,000 --> 00:25:06,000
 then the second layer for each of the output

350
00:25:06,000 --> 00:25:09,960
 of the first layer, then further linear combine them together

351
00:25:09,960 --> 00:25:14,960
 goes activation function, then produce a single output

352
00:25:15,360 --> 00:25:17,360
 of the second layer, right?

353
00:25:17,360 --> 00:25:20,560
 Here, this is j from one to D.

354
00:25:20,560 --> 00:25:23,840
 This one only produce a single j, right?

355
00:25:23,840 --> 00:25:26,159
 Similarly, for the second layer,

356
00:25:26,159 --> 00:25:29,840
 we can also use multiplication of the two vectors

357
00:25:29,840 --> 00:25:33,159
 to represent this weighted summation.

358
00:25:34,360 --> 00:25:37,360
 Okay, now this is a vector way,

359
00:25:37,360 --> 00:25:40,919
 but use a vector as the input can only produce

360
00:25:40,919 --> 00:25:43,040
 a single output, right?

361
00:25:43,040 --> 00:25:46,520
 Now if we want to express all output,

362
00:25:46,520 --> 00:25:49,360
 then this output will be a vector,

363
00:25:49,360 --> 00:25:53,480
 then this vector becomes a matrix.

364
00:25:53,480 --> 00:25:58,480
 So all that form a vector will be this matrix

365
00:25:58,520 --> 00:26:03,120
 multiplied by x, go through an activation function.

366
00:26:03,120 --> 00:26:06,520
 This one will be a vector of the z.

367
00:26:07,560 --> 00:26:09,160
 Okay.

368
00:26:09,160 --> 00:26:14,160
 Then a lot of matrix goes through activation function,

369
00:26:14,600 --> 00:26:18,960
 then produce a vector of the final output

370
00:26:18,960 --> 00:26:23,120
 for two neural network.

371
00:26:23,120 --> 00:26:24,719
 Okay, so from here we can see,

372
00:26:24,719 --> 00:26:28,159
 we can express the output in terms of the input

373
00:26:28,159 --> 00:26:29,199
 of two layers.

374
00:26:29,199 --> 00:26:31,320
 Of course, we can also do three layers

375
00:26:31,320 --> 00:26:33,719
 or more than three layers, right?

376
00:26:33,719 --> 00:26:37,040
 Based on the scanner way, this black color,

377
00:26:37,040 --> 00:26:40,159
 or vector way in this blue color,

378
00:26:40,159 --> 00:26:43,040
 or a matrix way in this green color.

379
00:26:46,399 --> 00:26:50,600
 Now which way we use depends on

380
00:26:50,600 --> 00:26:55,120
 which way is more convenient for our work, right?

381
00:26:55,120 --> 00:26:57,520
 They are equivalent to all these different way

382
00:26:57,520 --> 00:26:58,560
 are equivalent.

383
00:27:00,560 --> 00:27:05,560
 Okay, now we first study it,

384
00:27:06,040 --> 00:27:09,679
 what is the role of this activation function?

385
00:27:09,679 --> 00:27:12,879
 If the activation function is a linear function,

386
00:27:12,879 --> 00:27:17,159
 for example, if r equals to r, then what happened?

387
00:27:17,200 --> 00:27:21,640
 Here we use a matrix way to see the effect.

388
00:27:21,640 --> 00:27:25,040
 Now, if the activation function is a linear function,

389
00:27:25,040 --> 00:27:27,360
 we can very easily say, we get this one.

390
00:27:27,360 --> 00:27:29,640
 Even a linear function,

391
00:27:29,640 --> 00:27:32,680
 if r equals to a constant c times r,

392
00:27:32,680 --> 00:27:36,480
 then all this c can put in the beginning, right?

393
00:27:36,480 --> 00:27:41,080
 So to simplify it, we just use if r equals to r,

394
00:27:41,080 --> 00:27:46,080
 anyway it indicates a linear activation function.

395
00:27:46,360 --> 00:27:48,639
 So if it's a linear activation function,

396
00:27:48,639 --> 00:27:51,320
 then this a4 can disappear.

397
00:27:51,320 --> 00:27:53,520
 So we get this expression.

398
00:27:53,520 --> 00:27:57,439
 From this expression, we can see the v transpose

399
00:27:57,439 --> 00:28:02,439
 times w transpose can be just one matrix, right?

400
00:28:02,800 --> 00:28:05,040
 Maybe a matrix, one matrix.

401
00:28:05,040 --> 00:28:08,159
 Well, for this two layers of the neural network,

402
00:28:08,159 --> 00:28:10,199
 we have two matrix.

403
00:28:10,199 --> 00:28:11,040
 Okay.

404
00:28:11,040 --> 00:28:14,720
 Because all matrix are known from the training data,

405
00:28:14,720 --> 00:28:17,600
 and then from the training data,

406
00:28:17,600 --> 00:28:20,280
 then to utilize this learned parameter

407
00:28:20,280 --> 00:28:24,520
 to compute the output given an input.

408
00:28:24,520 --> 00:28:28,080
 But two matrix, if you learn from the training data,

409
00:28:28,080 --> 00:28:32,640
 is equivalent to just one single matrix.

410
00:28:32,640 --> 00:28:36,240
 So that means it is the same as just a single layer

411
00:28:36,240 --> 00:28:37,760
 of the neural network.

412
00:28:38,800 --> 00:28:41,360
 So from here, we can see,

413
00:28:41,360 --> 00:28:44,160
 for much layer of the neural network,

414
00:28:44,240 --> 00:28:47,720
 the activation function must be nonlinear.

415
00:28:47,720 --> 00:28:51,800
 If it's a linear, much layer is equivalent

416
00:28:51,800 --> 00:28:54,400
 to just a single layer.

417
00:28:54,400 --> 00:28:55,240
 Okay.

418
00:28:56,800 --> 00:29:00,400
 So at least this function,

419
00:29:00,400 --> 00:29:03,360
 so-called the activation function of the hidden layer

420
00:29:03,360 --> 00:29:05,680
 must be a nonlinear.

421
00:29:05,680 --> 00:29:10,680
 So that we can separate the w and v

422
00:29:11,160 --> 00:29:14,880
 to make them cannot merge together

423
00:29:14,880 --> 00:29:17,880
 into just one written matrix.

424
00:29:17,880 --> 00:29:18,720
 Okay.

425
00:29:18,720 --> 00:29:23,160
 So the hidden layer, this f must be a nonlinear.

426
00:29:23,160 --> 00:29:27,160
 Otherwise, the multi-layer does not make any sense.

427
00:29:27,160 --> 00:29:30,600
 It's just same as a single layer, right?

428
00:29:33,720 --> 00:29:37,000
 Now, next one, we will first study a single layer

429
00:29:37,920 --> 00:29:39,200
 neural network.

430
00:29:39,200 --> 00:29:42,240
 As I mentioned that for single layer neural network,

431
00:29:42,240 --> 00:29:44,760
 the activation function can be linear.

432
00:29:44,760 --> 00:29:46,800
 If it's a nonlinear function,

433
00:29:46,800 --> 00:29:49,040
 it does not make any role.

434
00:29:49,040 --> 00:29:50,320
 It does not make,

435
00:29:50,320 --> 00:29:55,320
 it does not contribute any sense to the neural network.

436
00:29:55,680 --> 00:29:57,920
 So for single layer network,

437
00:29:57,920 --> 00:29:59,760
 because we have only,

438
00:29:59,760 --> 00:30:02,680
 we can have only one layer of the activation function.

439
00:30:02,680 --> 00:30:04,000
 So this activation function,

440
00:30:04,000 --> 00:30:05,840
 it's a nonlinear or linear,

441
00:30:06,840 --> 00:30:09,280
 does not make any difference.

442
00:30:09,280 --> 00:30:11,639
 So if we first study a single layer,

443
00:30:11,639 --> 00:30:14,520
 then we can express the output by,

444
00:30:14,520 --> 00:30:19,159
 in terms of input just by a matrix multiplication, right?

445
00:30:19,159 --> 00:30:21,879
 Okay, now, from this single layer,

446
00:30:24,000 --> 00:30:25,840
 expression of y and x,

447
00:30:25,840 --> 00:30:28,360
 if we have such kind of the model,

448
00:30:28,360 --> 00:30:33,360
 then how to change this single layer neural network?

449
00:30:33,479 --> 00:30:36,120
 Okay, so we can have a north function.

450
00:30:36,120 --> 00:30:40,240
 We can build up a north function or cost function, right?

451
00:30:40,240 --> 00:30:44,199
 Although now we are usually use the cross entropy and so on,

452
00:30:44,199 --> 00:30:48,800
 but it does not make much difference.

453
00:30:48,800 --> 00:30:51,040
 Here, I just gave an example

454
00:30:51,040 --> 00:30:55,120
 to use the mean square error as the north function.

455
00:30:55,120 --> 00:30:58,280
 Okay, so because it's easier to understand.

456
00:30:58,280 --> 00:31:02,159
 Okay, so anyway, given input goes a network,

457
00:31:02,640 --> 00:31:04,480
 we get the output.

458
00:31:04,480 --> 00:31:07,760
 This output could be different from the ground truth

459
00:31:07,760 --> 00:31:09,160
 for training data.

460
00:31:09,160 --> 00:31:10,600
 Where for the training data,

461
00:31:10,600 --> 00:31:13,320
 we have the ground truth t, right?

462
00:31:13,320 --> 00:31:18,320
 So then the difference or the error will be t minus y,

463
00:31:19,440 --> 00:31:21,400
 or t minus this one.

464
00:31:21,400 --> 00:31:25,120
 This is equals to y output, right?

465
00:31:25,120 --> 00:31:28,680
 Then the mean square error will be this square,

466
00:31:28,680 --> 00:31:33,600
 then takes the average overall training sample.

467
00:31:33,600 --> 00:31:37,400
 Mathematically, it is some kind of expectation, right?

468
00:31:37,400 --> 00:31:40,120
 Expectation is just average, okay?

469
00:31:40,120 --> 00:31:44,560
 So we have the mean square error as the north function.

470
00:31:44,560 --> 00:31:48,200
 So the machine learning to train such a network

471
00:31:48,200 --> 00:31:53,200
 is just to try to find the w to make this y close to this y,

472
00:31:54,200 --> 00:31:59,200
 close to this t so that that means we will minimize

473
00:31:59,720 --> 00:32:02,000
 the mean square error, jw.

474
00:32:02,960 --> 00:32:03,800
 Okay.

475
00:32:05,200 --> 00:32:09,160
 Now, so we utilize the mean square error

476
00:32:09,160 --> 00:32:12,760
 as our north function of this network.

477
00:32:12,760 --> 00:32:17,760
 So to optimize the network is to determine w.

478
00:32:17,800 --> 00:32:22,280
 W contains all parameter of the neural network, right?

479
00:32:22,280 --> 00:32:25,000
 To minimize this north.

480
00:32:26,760 --> 00:32:31,760
 Now, this north is mean square error, right?

481
00:32:33,320 --> 00:32:37,080
 So if we expand this perfect square,

482
00:32:37,080 --> 00:32:39,879
 then we get this expression.

483
00:32:39,879 --> 00:32:43,000
 Now, obviously, we can see the north function

484
00:32:43,000 --> 00:32:47,000
 is a quadratic function of the w.

485
00:32:47,000 --> 00:32:50,040
 W will have the second order of the w, right?

486
00:32:50,080 --> 00:32:53,440
 This is a quadratic function of the w.

487
00:32:53,440 --> 00:32:57,080
 We know what is the shape of the quadratic function.

488
00:32:57,080 --> 00:33:01,639
 A quadratic function will have only a single minimum value

489
00:33:01,639 --> 00:33:06,080
 because this is a shape of the quadratic function, right?

490
00:33:06,080 --> 00:33:07,840
 Of course, to visualize it,

491
00:33:07,840 --> 00:33:12,840
 we just plot the north function against one parameter

492
00:33:13,000 --> 00:33:14,879
 in one dimensional case.

493
00:33:14,879 --> 00:33:16,360
 Otherwise, it is not so

494
00:33:17,360 --> 00:33:22,360
 obvious to visualize it, okay?

495
00:33:24,199 --> 00:33:26,840
 But anyway, all quadratic function

496
00:33:26,840 --> 00:33:29,479
 will be such kind of the shape.

497
00:33:29,479 --> 00:33:32,800
 In such kind of the shape of the north,

498
00:33:32,800 --> 00:33:34,840
 north against the parameter,

499
00:33:34,840 --> 00:33:38,199
 we have only one single minimum, okay?

500
00:33:38,199 --> 00:33:43,199
 So to optimize the parameter w is to get the value

501
00:33:46,520 --> 00:33:51,000
 of the w, this north function is a minimum, okay?

502
00:33:51,000 --> 00:33:56,000
 So for this linear, single, linear work,

503
00:33:56,600 --> 00:34:01,600
 we have no problem to get closed form of what is this w.

504
00:34:02,679 --> 00:34:04,760
 So how to optimize it?

505
00:34:04,760 --> 00:34:09,040
 We just take the gradient of the j against the w

506
00:34:09,040 --> 00:34:12,040
 and then net gradient equals to zero.

507
00:34:12,040 --> 00:34:15,880
 This is a common way to solve the optimization problem,

508
00:34:15,880 --> 00:34:18,600
 or minimization problem, right?

509
00:34:18,600 --> 00:34:22,000
 Now, if we take the mean square error as north function,

510
00:34:22,000 --> 00:34:27,000
 we take its gradient against our unknown parameter w

511
00:34:27,000 --> 00:34:29,800
 and make this gradient equals to zero.

512
00:34:29,800 --> 00:34:32,600
 That means we make this expression,

513
00:34:32,600 --> 00:34:36,400
 differentiation of this expression against w equals to zero.

514
00:34:36,400 --> 00:34:40,480
 We can very easily get this result.

515
00:34:40,480 --> 00:34:44,720
 T, differentiation against w is zero

516
00:34:44,719 --> 00:34:47,799
 because this is a constant against w, right?

517
00:34:47,799 --> 00:34:51,159
 This one against w is a two e x, e to x.

518
00:34:51,159 --> 00:34:55,600
 This against the w, differentiation is also two w e x x.

519
00:34:55,600 --> 00:34:59,399
 So we have get this equation.

520
00:34:59,399 --> 00:35:03,680
 Now, from this equation, we know what is the solution, right?

521
00:35:03,680 --> 00:35:05,600
 We can have closer solution.

522
00:35:05,600 --> 00:35:09,879
 This w, this all parameter of this network

523
00:35:09,879 --> 00:35:13,200
 is just the inverse of this x,

524
00:35:13,200 --> 00:35:16,680
 the expectation of the x times its transpose

525
00:35:16,680 --> 00:35:20,799
 times this expectation of x and t.

526
00:35:22,879 --> 00:35:25,680
 Now, this is in terms of the mathematics.

527
00:35:25,680 --> 00:35:30,680
 So all this mean square error is in terms of the expectation,

528
00:35:31,160 --> 00:35:33,720
 right, of a random variable.

529
00:35:33,720 --> 00:35:38,560
 Now, in the training phase, we can use our all training data

530
00:35:38,560 --> 00:35:40,879
 to estimate this expectation.

531
00:35:40,879 --> 00:35:42,359
 We use the average.

532
00:35:42,360 --> 00:35:47,240
 So we average this x times x transpose

533
00:35:47,240 --> 00:35:49,320
 over all training samples.

534
00:35:49,320 --> 00:35:51,920
 For example, we have to queue samples

535
00:35:51,920 --> 00:35:53,800
 in our training data set.

536
00:35:53,800 --> 00:35:55,840
 Then we average all of them.

537
00:35:55,840 --> 00:36:00,840
 Then this is the estimate of the expectation, right?

538
00:36:01,360 --> 00:36:04,760
 Similarly, for here, we can also use the training data

539
00:36:04,760 --> 00:36:07,440
 and the ground tools of the training data.

540
00:36:07,440 --> 00:36:10,240
 Or for the classification, it's class label

541
00:36:10,240 --> 00:36:14,000
 of the training data to compute this one.

542
00:36:14,919 --> 00:36:19,919
 And then after that, we can get what is our w, right?

543
00:36:21,879 --> 00:36:24,839
 So given a training data, all training samples

544
00:36:24,839 --> 00:36:26,600
 are the column vector.

545
00:36:26,600 --> 00:36:31,600
 If we arrange all this training sample into a matrix,

546
00:36:31,680 --> 00:36:33,560
 one sample is a vector.

547
00:36:33,560 --> 00:36:37,000
 All samples then will be a matrix, right?

548
00:36:37,000 --> 00:36:41,520
 So we can very easily see that this is just

549
00:36:41,520 --> 00:36:44,120
 a matrix multiplication.

550
00:36:44,120 --> 00:36:46,480
 Okay, similarly here, we can also,

551
00:36:46,480 --> 00:36:50,360
 it is also a matrix multiply by a vector.

552
00:36:50,360 --> 00:36:53,920
 One training sample has one class label.

553
00:36:53,920 --> 00:36:57,720
 Okay, so if we put a class label of all training samples

554
00:36:57,720 --> 00:37:00,680
 together, then it will be a vector.

555
00:37:00,680 --> 00:37:05,600
 So we can get the estimation of this term

556
00:37:05,600 --> 00:37:08,440
 in terms of the matrix multiply by a vector.

557
00:37:09,360 --> 00:37:14,360
 Okay, so we, to solve this equation

558
00:37:14,759 --> 00:37:19,759
 to get the optimized parameter, network parameter w,

559
00:37:20,080 --> 00:37:22,560
 is just solve this equation.

560
00:37:22,560 --> 00:37:26,200
 We utilize the training data to estimate

561
00:37:26,200 --> 00:37:28,400
 the expectation value.

562
00:37:28,400 --> 00:37:30,839
 Then we get this solution.

563
00:37:30,880 --> 00:37:35,880
 This is a solution of the optimum network parameter.

564
00:37:36,240 --> 00:37:39,960
 This parameter will minimize the mean square error

565
00:37:39,960 --> 00:37:44,960
 of the output, of course, on the training data, right?

566
00:37:54,680 --> 00:37:58,360
 Now, after we have the w parameter,

567
00:37:58,360 --> 00:37:59,880
 now what is output?

568
00:37:59,880 --> 00:38:03,960
 Output, of course, is a w transpose times x, right?

569
00:38:03,960 --> 00:38:07,880
 So, its output is this one transpose times x.

570
00:38:07,880 --> 00:38:12,600
 Then this one transpose, if the transpose go to inside,

571
00:38:12,600 --> 00:38:15,200
 this term will come to later,

572
00:38:15,200 --> 00:38:17,360
 these two items will come to,

573
00:38:17,360 --> 00:38:20,400
 become to before this item, right?

574
00:38:20,400 --> 00:38:25,400
 So roughly we have the t times x times this inverse

575
00:38:25,400 --> 00:38:26,880
 then times x.

576
00:38:27,880 --> 00:38:31,760
 Now, then from this output, we can see

577
00:38:31,760 --> 00:38:35,240
 the optimum output of the network

578
00:38:35,240 --> 00:38:39,680
 is always meet an inverse of a matrix.

579
00:38:41,320 --> 00:38:44,800
 Now, we call what is the Mahanubi distance?

580
00:38:44,800 --> 00:38:47,840
 Mahanubi distance is also an inverse

581
00:38:47,840 --> 00:38:49,960
 of the covariance matrix.

582
00:38:49,960 --> 00:38:54,960
 While this one is almost the same as the covariance matrix,

583
00:38:55,880 --> 00:38:57,420
 right?

584
00:38:57,420 --> 00:39:02,420
 So, of course, this is not minus its center, right?

585
00:39:02,680 --> 00:39:07,080
 Here we utilize the teacher or utilize the ground truth,

586
00:39:07,080 --> 00:39:08,560
 but in the Mahanubi distance,

587
00:39:08,560 --> 00:39:10,600
 we don't utilize the ground truth,

588
00:39:10,600 --> 00:39:13,920
 but we utilize the center of the two graphs.

589
00:39:13,920 --> 00:39:18,440
 But I believe these two solutions are equivalent.

590
00:39:19,920 --> 00:39:22,380
 Okay, roughly from this expression,

591
00:39:22,380 --> 00:39:26,140
 we can see they are almost equivalent.

592
00:39:26,140 --> 00:39:27,740
 If you are interested,

593
00:39:27,740 --> 00:39:31,220
 you can try to mathematically prove it.

594
00:39:32,420 --> 00:39:36,540
 Okay, Mahanubi distance will be optimal

595
00:39:36,540 --> 00:39:41,540
 for the classification if assume the data is a Gaussian, right?

596
00:39:42,380 --> 00:39:44,820
 Here for single layer of the network,

597
00:39:44,820 --> 00:39:48,100
 the solution is also very similar

598
00:39:48,100 --> 00:39:49,780
 to the Mahanubi distance here.

599
00:39:49,780 --> 00:39:52,460
 We also need an inverse of the matrix.

600
00:39:52,460 --> 00:39:56,020
 And this matrix is very similar to covariance matrix.

601
00:39:56,940 --> 00:39:57,780
 Okay.

602
00:39:58,660 --> 00:40:00,660
 And there must be equivalent,

603
00:40:00,660 --> 00:40:03,300
 otherwise it cannot be optimal

604
00:40:03,300 --> 00:40:07,240
 because anyway, this is a neural network.

605
00:40:07,240 --> 00:40:12,240
 We optimize it, it's just minimize the mean square error.

606
00:40:13,120 --> 00:40:14,820
 Minimize the mean square error,

607
00:40:14,820 --> 00:40:19,820
 we only utilize the second order of the statistic.

608
00:40:21,340 --> 00:40:22,180
 Okay.

609
00:40:22,180 --> 00:40:25,380
 If we only utilize the second order of the statistic,

610
00:40:25,380 --> 00:40:27,620
 not use a higher order,

611
00:40:27,620 --> 00:40:32,620
 then this is only optimal if the PDF is Gaussian.

612
00:40:35,700 --> 00:40:39,180
 Because the Gaussian PDF only specified

613
00:40:39,180 --> 00:40:43,060
 only by second order of the statistic.

614
00:40:43,060 --> 00:40:45,080
 Covarrance matrix is a second order,

615
00:40:45,080 --> 00:40:47,740
 mean is a first order, right?

616
00:40:47,740 --> 00:40:50,820
 So there must be some somehow equivalent.

617
00:40:51,779 --> 00:40:52,620
 Okay.

618
00:40:54,820 --> 00:40:59,820
 Okay, now we know that the North function,

619
00:41:00,340 --> 00:41:02,700
 okay, for single layer of the network,

620
00:41:02,700 --> 00:41:05,900
 you use the linear activity function,

621
00:41:05,900 --> 00:41:08,540
 the North function is a classic function.

622
00:41:09,540 --> 00:41:11,980
 Then from the given a training data,

623
00:41:11,980 --> 00:41:14,940
 we can get a closed solution.

624
00:41:14,940 --> 00:41:19,940
 What is the optimum parameters of the network

625
00:41:20,140 --> 00:41:23,180
 to get this minimum point?

626
00:41:23,180 --> 00:41:27,580
 The North function, the value of the North function

627
00:41:27,580 --> 00:41:31,420
 is minimum at this W position.

628
00:41:31,420 --> 00:41:34,300
 Okay, we can utilize the training data,

629
00:41:34,300 --> 00:41:37,700
 we can directly use this matrix multiplication

630
00:41:37,700 --> 00:41:40,900
 to compute what is this optimum W?

631
00:41:41,900 --> 00:41:43,140
 Okay.

632
00:41:43,140 --> 00:41:45,940
 Now, we can also use a larger way

633
00:41:45,940 --> 00:41:49,340
 to try to find this W,

634
00:41:49,340 --> 00:41:53,080
 optimize the W without this matrix multiplication.

635
00:41:54,060 --> 00:41:59,060
 Here, we introduce a so-called gradient descent method.

636
00:41:59,060 --> 00:42:02,820
 Now, for example, given a current parameter value

637
00:42:02,820 --> 00:42:05,340
 of the network at this value,

638
00:42:05,340 --> 00:42:08,300
 then the North function is not minimized,

639
00:42:08,300 --> 00:42:11,700
 it's much higher than the minimum one, right?

640
00:42:11,700 --> 00:42:15,220
 Now, we can compute the gradient of the North function

641
00:42:15,220 --> 00:42:17,460
 against the W.

642
00:42:17,460 --> 00:42:22,180
 If we know the gradient, the gradient has direction, right?

643
00:42:22,180 --> 00:42:27,180
 So what is the meaning of the direction of the gradient?

644
00:42:27,700 --> 00:42:31,020
 The direction of the gradient is the direction

645
00:42:31,020 --> 00:42:35,180
 the function value will increase rapidly.

646
00:42:35,180 --> 00:42:36,020
 Okay?

647
00:42:36,020 --> 00:42:38,819
 Then at opposite direction of the gradient,

648
00:42:38,819 --> 00:42:43,580
 where the value of the G will decrease most.

649
00:42:43,580 --> 00:42:44,419
 Okay?

650
00:42:44,419 --> 00:42:49,419
 So given any value, parameter value of the network,

651
00:42:50,919 --> 00:42:54,580
 we can modify its value to produce a new value

652
00:42:54,580 --> 00:42:59,580
 or update its parameter value by plus a change

653
00:43:00,540 --> 00:43:02,100
 of the parameter value,

654
00:43:02,100 --> 00:43:05,940
 where this change will be in the opposite direction

655
00:43:05,940 --> 00:43:08,140
 of the gradient.

656
00:43:08,140 --> 00:43:13,140
 So if this step size eta is small,

657
00:43:14,740 --> 00:43:19,740
 then this change will reduce the North,

658
00:43:20,339 --> 00:43:24,100
 reduce this value of the North function, right?

659
00:43:24,100 --> 00:43:26,339
 Graphically, it will see that,

660
00:43:26,339 --> 00:43:31,339
 for example, we have such kind of the North function.

661
00:43:31,660 --> 00:43:36,260
 At this point, the gradient must be at this direction.

662
00:43:36,260 --> 00:43:38,300
 Okay, because it increase.

663
00:43:38,300 --> 00:43:39,140
 Okay?

664
00:43:39,140 --> 00:43:42,060
 Then at the opposite direction of the gradient,

665
00:43:42,060 --> 00:43:43,380
 then it's this direction.

666
00:43:43,380 --> 00:43:46,420
 So if we change the W in this direction,

667
00:43:46,420 --> 00:43:51,420
 then it will reduce the G value, the North function, right?

668
00:43:51,580 --> 00:43:55,260
 Then if we can do that iteratively at the end,

669
00:43:55,260 --> 00:44:00,020
 it will, the W, the update the W,

670
00:44:00,020 --> 00:44:05,020
 after much iterations, then this W will close to this value.

671
00:44:06,100 --> 00:44:08,280
 This G will be minimized.

672
00:44:09,259 --> 00:44:13,780
 Well, at this value, the gradient of the G will be zero.

673
00:44:13,780 --> 00:44:14,900
 Right?

674
00:44:14,900 --> 00:44:16,180
 Okay.

675
00:44:16,180 --> 00:44:21,180
 So we can update the W given any initialized W

676
00:44:22,100 --> 00:44:23,500
 or current W.

677
00:44:23,500 --> 00:44:27,860
 We update it to change the value of the W

678
00:44:27,860 --> 00:44:31,420
 in the opposite direction of the gradient

679
00:44:31,420 --> 00:44:36,060
 of the North function or mean square arrow here.

680
00:44:36,060 --> 00:44:39,700
 But anyway, we need to compute the gradient, right?

681
00:44:39,700 --> 00:44:44,700
 So we, to get this gradient for the single layer,

682
00:44:45,180 --> 00:44:46,820
 it's quite simple.

683
00:44:46,820 --> 00:44:50,300
 For example, the gradient of the G against the W is,

684
00:44:50,300 --> 00:44:51,900
 the G is a mean square arrow.

685
00:44:51,900 --> 00:44:54,700
 So G minus is one square mean, right?

686
00:44:54,700 --> 00:44:56,220
 If we take the gradient,

687
00:44:56,859 --> 00:44:59,220
 it is a two times E times this one,

688
00:44:59,220 --> 00:45:02,459
 and this minus this one is the arrow.

689
00:45:02,459 --> 00:45:03,299
 It's the arrow.

690
00:45:03,299 --> 00:45:05,060
 So it is just E times X.

691
00:45:05,060 --> 00:45:06,339
 E is a scalar.

692
00:45:06,339 --> 00:45:07,660
 X is a vector.

693
00:45:07,660 --> 00:45:10,140
 This vector will determine the direction.

694
00:45:11,779 --> 00:45:13,939
 So we can get this as a gradient.

695
00:45:15,060 --> 00:45:19,939
 So we utilize this gradient to update the parameter.

696
00:45:19,939 --> 00:45:23,220
 So if we do so iteratively at the end,

697
00:45:23,220 --> 00:45:28,220
 this value of the W will come close to this point.

698
00:45:30,339 --> 00:45:34,419
 So this is a iterative process, right?

699
00:45:34,419 --> 00:45:37,740
 So many people only understand this is a machine learning,

700
00:45:37,740 --> 00:45:41,060
 okay, machine learning from the data gradually.

701
00:45:41,060 --> 00:45:43,779
 But in fact, this machine learning,

702
00:45:43,779 --> 00:45:48,459
 it can only close to this solution.

703
00:45:49,340 --> 00:45:50,700
 Okay, this solution,

704
00:45:50,700 --> 00:45:54,300
 you don't need the iterative updates.

705
00:45:54,300 --> 00:45:57,060
 We just use the all training data

706
00:45:57,060 --> 00:45:59,480
 to compute this matrix modification.

707
00:46:05,660 --> 00:46:07,740
 Now, of course here,

708
00:46:07,740 --> 00:46:10,860
 the gradient is a gradient of the north function.

709
00:46:10,860 --> 00:46:13,700
 North function is the expectation of the arrow

710
00:46:13,700 --> 00:46:16,060
 because it's a mean square arrow, right?

711
00:46:16,060 --> 00:46:20,860
 So the real gradient is still here,

712
00:46:20,860 --> 00:46:24,220
 but the expectation is just average

713
00:46:24,220 --> 00:46:26,640
 over all training sample.

714
00:46:26,640 --> 00:46:31,640
 But we can also update the W for each training sample.

715
00:46:31,640 --> 00:46:33,660
 Okay, for each single training sample,

716
00:46:35,180 --> 00:46:37,820
 we just use each training sample to update.

717
00:46:37,820 --> 00:46:40,500
 For next training sample, also update it.

718
00:46:40,500 --> 00:46:42,980
 Or we can use all training sample

719
00:46:42,980 --> 00:46:45,060
 after we average them together,

720
00:46:45,060 --> 00:46:47,340
 then to do the update.

721
00:46:47,340 --> 00:46:52,060
 Depends on which way is better.

722
00:46:52,060 --> 00:46:55,420
 So we can use each individual training sample to update it,

723
00:46:55,420 --> 00:46:59,140
 or we compute this one,

724
00:46:59,140 --> 00:47:01,340
 then average all training sample,

725
00:47:01,340 --> 00:47:03,060
 then to do one update.

726
00:47:04,140 --> 00:47:07,980
 Okay, so this is why in the network training,

727
00:47:07,980 --> 00:47:10,820
 we can choose some batch size.

728
00:47:10,820 --> 00:47:15,020
 How many sample we use to compute this one,

729
00:47:15,140 --> 00:47:17,860
 average them together, then to do the update.

730
00:47:18,900 --> 00:47:21,460
 There are some advantage or disadvantage

731
00:47:21,460 --> 00:47:24,780
 in different way to do the network training.

732
00:47:31,020 --> 00:47:35,740
 Of course, for a single-layer network, okay,

733
00:47:35,740 --> 00:47:38,620
 you may say, why we need this update,

734
00:47:38,620 --> 00:47:43,420
 this iterative algorithm to change it gradually,

735
00:47:43,420 --> 00:47:45,900
 we can directly compute it, right?

736
00:47:45,900 --> 00:47:48,380
 What is the optimal way, W?

737
00:47:48,380 --> 00:47:51,380
 Where the gradient to approach it

738
00:47:51,380 --> 00:47:54,100
 may not be exactly same as this,

739
00:47:54,100 --> 00:47:56,420
 depends on your step size.

740
00:47:56,420 --> 00:48:00,420
 If this step size eta is larger,

741
00:48:00,420 --> 00:48:05,420
 then it could also be alternate around this minimum point

742
00:48:05,460 --> 00:48:08,060
 where they will reach it, right?

743
00:48:08,060 --> 00:48:10,100
 If this eta is small,

744
00:48:10,100 --> 00:48:13,540
 then it can go to the minimum,

745
00:48:13,540 --> 00:48:17,460
 but we need huge number of the iteration,

746
00:48:17,460 --> 00:48:20,339
 it can very, very slowly come to here.

747
00:48:20,339 --> 00:48:24,580
 So the learning process will be very long, right?

748
00:48:25,620 --> 00:48:30,339
 Okay, this is also advantage of how to choose this step size.

749
00:48:30,339 --> 00:48:35,339
 Okay, depends on the amount to update the W.

750
00:48:35,500 --> 00:48:39,500
 Now, you may ask why if we can utilize this one

751
00:48:42,860 --> 00:48:45,340
 to get the exactly minimum value,

752
00:48:45,340 --> 00:48:47,540
 why we need this update,

753
00:48:47,540 --> 00:48:51,460
 use the gradient descent method.

754
00:48:52,380 --> 00:48:56,740
 Now, in 30, 40 years ago, okay,

755
00:48:56,740 --> 00:49:01,740
 our computer computation is not sufficient powerful,

756
00:49:02,379 --> 00:49:04,979
 so the computer cannot compute

757
00:49:04,979 --> 00:49:08,299
 a large matrix inverse and the modification.

758
00:49:09,259 --> 00:49:11,859
 Okay, if we have a small device,

759
00:49:11,859 --> 00:49:13,299
 we want to get this W,

760
00:49:13,299 --> 00:49:16,299
 then we can use this gradient descent method

761
00:49:16,299 --> 00:49:21,299
 to avoid to compute the inverse of large matrix.

762
00:49:24,660 --> 00:49:28,220
 So this is one way is possible,

763
00:49:28,220 --> 00:49:32,339
 we utilize this update in terms of this closed solution.

764
00:49:33,580 --> 00:49:34,419
 Okay.

765
00:49:37,259 --> 00:49:39,779
 So this why, this learning rule,

766
00:49:39,779 --> 00:49:42,580
 in fact, it is just a simple gradient descent

767
00:49:42,580 --> 00:49:46,180
 in the mathematics, it is a very simple one, right?

768
00:49:46,180 --> 00:49:48,620
 But this method is called

769
00:49:48,620 --> 00:49:53,459
 visual learning, visual half learning rule,

770
00:49:53,459 --> 00:49:56,459
 applied in the adaptive syndrome processing.

771
00:49:57,260 --> 00:50:01,500
 Okay, this algorithm is proposed by a professor

772
00:50:01,500 --> 00:50:06,500
 in Stanford University, Weiju and Hof in 1950s.

773
00:50:09,020 --> 00:50:11,420
 Oh, 1950s, yeah, 1950s,

774
00:50:11,420 --> 00:50:14,420
 then apply it to adaptive syndrome processing.

775
00:50:14,420 --> 00:50:17,380
 Adaptive syndrome processing is to process a signal,

776
00:50:17,380 --> 00:50:18,940
 we need a filter, right?

777
00:50:18,940 --> 00:50:23,420
 But the filter parameter is adaptive to the signal

778
00:50:23,420 --> 00:50:27,940
 to minimize the error of the output of the filter.

779
00:50:27,940 --> 00:50:31,860
 Okay, then how to update the filter parameters,

780
00:50:31,860 --> 00:50:34,140
 then utilize this one.

781
00:50:34,140 --> 00:50:37,020
 Okay, so this is a very famous,

782
00:50:37,020 --> 00:50:39,820
 air M is learning algorithm.

783
00:50:40,960 --> 00:50:44,500
 Very famous in the so-called adaptive syndrome processing.

784
00:50:53,420 --> 00:50:55,700
 Now, this is for single layer,

785
00:50:55,700 --> 00:51:00,420
 we can do, we can in such a way to train the network, right?

786
00:51:00,420 --> 00:51:04,060
 Now, so because from the gradient descent,

787
00:51:04,060 --> 00:51:09,060
 we know that the update is a process, it's iterative, right?

788
00:51:10,540 --> 00:51:13,620
 So we can have a so-called learning curve.

789
00:51:13,620 --> 00:51:15,300
 That means at the beginning,

790
00:51:15,300 --> 00:51:18,500
 given any arbitrary parameter of the network,

791
00:51:18,500 --> 00:51:22,540
 the error could be larger, right?

792
00:51:22,540 --> 00:51:26,259
 But during the iteration, we update,

793
00:51:26,259 --> 00:51:30,340
 we change this W based on the gradient descent method,

794
00:51:30,340 --> 00:51:33,740
 then the W were close to the minimum value,

795
00:51:33,740 --> 00:51:37,300
 so the mean square error will decrease

796
00:51:37,300 --> 00:51:41,860
 against the number of the iteration, right?

797
00:51:41,860 --> 00:51:46,420
 So usually it is always decrease, decrease, decrease

798
00:51:46,420 --> 00:51:51,340
 on the mean square error on the training data,

799
00:51:51,340 --> 00:51:54,260
 because in the training of the network,

800
00:51:54,260 --> 00:51:56,700
 we can only use the training data,

801
00:51:56,700 --> 00:52:00,340
 we only have the training data, okay?

802
00:52:00,340 --> 00:52:05,340
 And the training data, it is used to compute the gradient

803
00:52:05,960 --> 00:52:10,500
 to make the gradient decrease, okay?

804
00:52:10,500 --> 00:52:14,580
 So not to make the north function decrease.

805
00:52:14,580 --> 00:52:16,700
 So north function is a mean square error,

806
00:52:16,700 --> 00:52:18,980
 so the north function or mean square error

807
00:52:18,980 --> 00:52:23,300
 is always decrease along with the increase of the iteration.

808
00:52:24,740 --> 00:52:27,980
 Of course, at the sufficient number of the iteration,

809
00:52:27,980 --> 00:52:32,260
 it can be a very flat or a constant value, okay?

810
00:52:32,260 --> 00:52:37,260
 So this curve is very famous called learning curve

811
00:52:38,140 --> 00:52:39,340
 in the machine learning.

812
00:52:41,340 --> 00:52:44,620
 Now, this decrease of the mean square error,

813
00:52:44,620 --> 00:52:47,980
 we can only guarantee each way decrease

814
00:52:48,220 --> 00:52:52,020
 of the mean square error on your training data,

815
00:52:52,020 --> 00:52:56,380
 because the machine can only learn from the training data,

816
00:52:56,380 --> 00:52:57,740
 right?

817
00:52:57,740 --> 00:53:02,740
 Now, if we use the data not exactly same

818
00:53:03,460 --> 00:53:06,220
 as the training data is the test data,

819
00:53:06,220 --> 00:53:09,860
 then the mean square error on a larger set of the data,

820
00:53:09,860 --> 00:53:14,860
 for example, test data, it may not be always decrease,

821
00:53:14,860 --> 00:53:19,860
 it can increase, because this data is not utilized

822
00:53:20,140 --> 00:53:21,500
 in the optimization.

823
00:53:22,780 --> 00:53:26,180
 Of course, this is not difficult to understand, right?

824
00:53:26,180 --> 00:53:31,180
 So if this happened, then this range is called overfitting,

825
00:53:34,020 --> 00:53:39,020
 because your network parameter W is overly fit

826
00:53:39,940 --> 00:53:44,460
 your training data, so that it will perform poorly

827
00:53:44,460 --> 00:53:46,660
 on the test data.

828
00:53:46,660 --> 00:53:50,420
 So this is overfitting problem, right?

829
00:53:52,700 --> 00:53:57,020
 Now, overfitting problem is the most difficult problem

830
00:53:57,020 --> 00:54:01,420
 in all artificial intelligence, in all machine learning.

831
00:54:03,540 --> 00:54:05,340
 It is a very difficult problem,

832
00:54:05,340 --> 00:54:08,580
 because in the machine learning,

833
00:54:08,580 --> 00:54:10,860
 we only have the training data,

834
00:54:10,860 --> 00:54:13,820
 we don't have the test data, right?

835
00:54:13,820 --> 00:54:16,380
 So we don't know the trend result

836
00:54:16,380 --> 00:54:19,380
 is overfitting or training data or not.

837
00:54:19,380 --> 00:54:22,060
 We never know that, okay?

838
00:54:23,160 --> 00:54:28,020
 So if it is overfitting, it can cause a very severe problem,

839
00:54:28,020 --> 00:54:32,540
 that means the network can perform very, very poorly

840
00:54:32,540 --> 00:54:34,020
 on your test data.

841
00:54:35,940 --> 00:54:39,700
 But the purpose of the machine learning

842
00:54:39,700 --> 00:54:44,700
 is to make the machine trained for the test data,

843
00:54:44,939 --> 00:54:46,819
 not for the training data.

844
00:54:48,259 --> 00:54:52,859
 Okay, after the training of your system,

845
00:54:52,859 --> 00:54:56,220
 it must be utilized for the future data,

846
00:54:56,220 --> 00:54:57,740
 for the test data.

847
00:54:59,419 --> 00:55:00,620
 Okay, the training data,

848
00:55:00,620 --> 00:55:04,220
 we already know what is output, right?

849
00:55:04,220 --> 00:55:07,120
 So that we can utilize it as the training data.

850
00:55:07,120 --> 00:55:11,160
 So this overfitting is the most difficult problem

851
00:55:11,160 --> 00:55:12,680
 in all machine learning.

852
00:55:15,839 --> 00:55:20,359
 Now, again, here we introduce this learning rule, right?

853
00:55:20,359 --> 00:55:23,279
 The Wigelhoff learning rule or MS learning rule

854
00:55:23,279 --> 00:55:25,880
 or the gradient descent method.

855
00:55:25,880 --> 00:55:29,600
 This is just for the single layer of the network.

856
00:55:29,600 --> 00:55:30,960
 Single layer of the network,

857
00:55:30,960 --> 00:55:34,040
 we know it's a linear function of the eggs.

858
00:55:34,040 --> 00:55:36,640
 So if we utilize in the classification,

859
00:55:36,640 --> 00:55:41,640
 it can only have a straight line of the class boundary.

860
00:55:42,319 --> 00:55:45,960
 So the capacity of the single layer network

861
00:55:45,960 --> 00:55:48,400
 is greatly limited.

862
00:55:48,400 --> 00:55:52,460
 It can only implement a linear class fear.

863
00:55:52,460 --> 00:55:56,440
 The class boundary can only be a straight line

864
00:55:56,440 --> 00:55:58,040
 in two dimensional case,

865
00:55:58,040 --> 00:56:01,240
 or a hyperplane in three dimensional space,

866
00:56:01,240 --> 00:56:04,680
 or more than three dimensional space.

867
00:56:05,560 --> 00:56:08,359
 So the capacity of this single layer network

868
00:56:08,359 --> 00:56:10,160
 is very limited.

869
00:56:10,160 --> 00:56:12,600
 So this is why we need to study

870
00:56:12,600 --> 00:56:17,600
 matinee network or called matinee perceptome, M-A-P.

871
00:56:20,799 --> 00:56:23,879
 Now, here we will study the matinee perceptome,

872
00:56:23,879 --> 00:56:25,839
 matinee neural network.

873
00:56:25,839 --> 00:56:27,600
 Matinee neural network here,

874
00:56:27,600 --> 00:56:31,720
 we only study two layers of the network,

875
00:56:31,720 --> 00:56:35,560
 because we have the theoretical proof

876
00:56:35,560 --> 00:56:40,120
 that two layers of the network

877
00:56:40,120 --> 00:56:45,120
 is sufficient to implement any nonlinear function.

878
00:56:47,120 --> 00:56:51,120
 If we have sufficient number of the hidden neurons,

879
00:56:52,359 --> 00:56:56,879
 and if we can find the optimized parameter.

880
00:56:56,879 --> 00:57:00,520
 So two layers is already sufficient

881
00:57:00,520 --> 00:57:03,920
 to implement any nonlinear function.

882
00:57:03,920 --> 00:57:07,000
 So here we only study two layers network.

883
00:57:07,920 --> 00:57:10,160
 Now, for two layers network, of course,

884
00:57:10,160 --> 00:57:13,640
 we have to get a study,

885
00:57:13,640 --> 00:57:17,920
 we will study how to train two layer network.

886
00:57:17,920 --> 00:57:20,880
 For two layer network, as I mentioned that before,

887
00:57:20,880 --> 00:57:25,000
 that the actuation function, at least in the hidden layer,

888
00:57:25,000 --> 00:57:27,960
 must be nonlinear, cannot be a linear.

889
00:57:27,960 --> 00:57:32,960
 If it's a linear, there's no use, use matinee, right?

890
00:57:33,280 --> 00:57:37,280
 Okay, so if this hidden neural network,

891
00:57:37,280 --> 00:57:39,200
 hidden neurons of this network

892
00:57:39,200 --> 00:57:41,840
 use the nonlinear activation function,

893
00:57:41,840 --> 00:57:46,840
 then how to get the solution of the optimum parameter?

894
00:57:47,880 --> 00:57:51,120
 Here, we will introduce so-called back propagation,

895
00:57:52,120 --> 00:57:57,120
 algorithm tool training matinee neural network,

896
00:57:57,400 --> 00:57:58,240
 or MLP.

897
00:58:00,799 --> 00:58:03,400
 Now, for matinee here,

898
00:58:03,400 --> 00:58:05,960
 example is just two layers, right?

899
00:58:05,960 --> 00:58:08,960
 This F must be a nonlinear function,

900
00:58:08,960 --> 00:58:11,279
 cannot be a linear.

901
00:58:11,279 --> 00:58:15,000
 So if we just use the mean square error

902
00:58:15,000 --> 00:58:18,440
 as the north function, we have this expression.

903
00:58:18,440 --> 00:58:23,440
 Now, to get the optimum parameter V and W,

904
00:58:24,440 --> 00:58:27,560
 matinee to minimize this north function,

905
00:58:27,560 --> 00:58:29,520
 we need to take the gradient

906
00:58:29,520 --> 00:58:32,200
 and the net gradient equals to zero,

907
00:58:32,200 --> 00:58:34,160
 and then solve this equation,

908
00:58:34,160 --> 00:58:36,480
 then the solution of this equation,

909
00:58:36,480 --> 00:58:38,560
 this W and the W,

910
00:58:38,560 --> 00:58:41,400
 we are minimize the north function, right?

911
00:58:42,600 --> 00:58:47,600
 But unfortunately, because this F is a nonlinear function,

912
00:58:47,640 --> 00:58:49,640
 any nonlinear function here,

913
00:58:49,640 --> 00:58:54,640
 if then we cannot get an analytical result

914
00:58:54,799 --> 00:58:58,720
 of this equation, we cannot solve this equation,

915
00:58:58,720 --> 00:59:01,640
 we don't have the closed solution.

916
00:59:04,040 --> 00:59:07,960
 This is caused by nonlinear function F here.

917
00:59:11,000 --> 00:59:13,160
 Now, because of that,

918
00:59:13,160 --> 00:59:18,160
 the neural network research starts from the 1940s,

919
00:59:18,759 --> 00:59:23,759
 and then up to 1980s, I believe 14 years,

920
00:59:24,399 --> 00:59:28,120
 the people can only research on a single layer

921
00:59:28,120 --> 00:59:33,120
 of the network, cannot extend to matinee network,

922
00:59:33,220 --> 00:59:37,319
 because people find the optimization has no solution.

923
00:59:37,319 --> 00:59:41,000
 We cannot find what is the optimal parameter value W

924
00:59:41,000 --> 00:59:45,520
 and the way to minimize this north function

925
00:59:45,520 --> 00:59:47,440
 by solve this equation.

926
00:59:48,720 --> 00:59:53,720
 This 14 years, so many researchers work on that,

927
00:59:56,920 --> 00:59:58,399
 cannot find the solution.

928
00:59:58,399 --> 01:00:03,399
 I also feel very strange, I don't understand why.

929
01:00:04,359 --> 01:00:08,600
 So up to in 1980s,

930
01:00:08,600 --> 01:00:13,600
 Hinton invoked an algorithm called back propagation,

931
01:00:14,520 --> 01:00:19,200
 so that we can train a matinee network

932
01:00:19,200 --> 01:00:24,200
 to get to learn this parameter V and W.

933
01:00:24,600 --> 01:00:29,600
 Then we can somehow extend the neural network

934
01:00:29,799 --> 01:00:34,799
 from single layer to matinee in 1980s, after 40 years.

935
01:00:38,680 --> 01:00:42,319
 And this back propagation is invented

936
01:00:42,320 --> 01:00:44,640
 by Hinton, okay?

937
01:00:44,640 --> 01:00:49,400
 And then later, the CNN is also invented by Hinton.

938
01:00:50,520 --> 01:00:53,080
 Today, I just read the newspaper.

939
01:00:53,080 --> 01:00:57,400
 Hinton got the Nobel Prize of this year,

940
01:00:58,260 --> 01:01:00,120
 because of this two contribution.

941
01:01:02,080 --> 01:01:05,720
 Okay, just today I learned from the newspaper.

942
01:01:07,340 --> 01:01:10,200
 Now, what is a back propagation?

943
01:01:11,120 --> 01:01:13,200
 Now, the deep learning is also used

944
01:01:13,200 --> 01:01:15,319
 as a so-called back propagation.

945
01:01:15,319 --> 01:01:18,319
 So here, we will learn what is a back propagation.

946
01:01:19,520 --> 01:01:22,960
 Now, just now I mentioned that I also feel very strange

947
01:01:22,960 --> 01:01:25,520
 why in the 14 years of the research,

948
01:01:25,520 --> 01:01:29,319
 from 1940s up to 1980s,

949
01:01:29,319 --> 01:01:33,080
 people don't use the matinee neural network,

950
01:01:33,080 --> 01:01:38,080
 because from here, this we cannot find a closed solution.

951
01:01:41,200 --> 01:01:45,080
 Then we will see what is the back propagation.

952
01:01:45,080 --> 01:01:48,120
 The idea, in fact, is very simple also.

953
01:01:48,120 --> 01:01:53,120
 Also, we will see the method is very straightforward.

954
01:01:54,720 --> 01:01:59,720
 Now, to solve this equation, it is impossible.

955
01:02:00,080 --> 01:02:03,319
 Take the gradient of this north function equals to zero.

956
01:02:03,319 --> 01:02:07,319
 We cannot get the analytic solution.

957
01:02:08,320 --> 01:02:11,360
 But it does not mean we cannot utilize

958
01:02:11,360 --> 01:02:14,320
 the gradient descent method.

959
01:02:14,320 --> 01:02:16,760
 Well, in the gradient descent method,

960
01:02:16,760 --> 01:02:21,280
 we don't need to solve this equation.

961
01:02:21,280 --> 01:02:25,120
 Solve this equation means we should get what is the w,

962
01:02:25,120 --> 01:02:27,040
 what is the w, right?

963
01:02:27,040 --> 01:02:28,400
 To solve this equation.

964
01:02:28,400 --> 01:02:32,840
 But if we only utilize this gradient descent method,

965
01:02:32,840 --> 01:02:34,840
 we don't need solve this equation,

966
01:02:34,840 --> 01:02:38,360
 we only need compute the gradient,

967
01:02:38,360 --> 01:02:41,440
 the numeric value of the gradient,

968
01:02:41,440 --> 01:02:44,280
 given a current value of the parameter w

969
01:02:44,280 --> 01:02:48,320
 and the way to compute the numeric value of the gradient,

970
01:02:48,320 --> 01:02:53,320
 then we can utilize this gradient descent method, right?

971
01:02:54,360 --> 01:02:58,400
 This is already applied in adaptive single processing

972
01:02:58,400 --> 01:03:01,560
 in 1940s or 1950s.

973
01:03:02,560 --> 01:03:07,560
 As we study the single layer of the network, right?

974
01:03:08,880 --> 01:03:13,880
 But we can also apply to multi-layer neural network.

975
01:03:14,480 --> 01:03:17,920
 There's no problem because to compute this gradient

976
01:03:17,920 --> 01:03:21,600
 mathematically, it is not difficult.

977
01:03:21,600 --> 01:03:25,080
 We just utilize the so-called chain rule

978
01:03:25,080 --> 01:03:26,680
 of the differentiation.

979
01:03:28,040 --> 01:03:31,080
 We can get the expression of this gradient,

980
01:03:31,080 --> 01:03:33,120
 although we cannot solve this equation,

981
01:03:33,120 --> 01:03:35,160
 we can get the mathematical expression

982
01:03:35,160 --> 01:03:37,080
 of this two gradient.

983
01:03:37,080 --> 01:03:40,279
 And then we can compute the value of this two gradient.

984
01:03:40,279 --> 01:03:44,600
 Then we can utilize this gradient descent method.

985
01:03:44,600 --> 01:03:49,600
 So here, show how to get the expression

986
01:03:49,680 --> 01:03:51,200
 of this two gradient.

987
01:03:52,120 --> 01:03:55,720
 Okay, it is not difficult, it's very straightforward.

988
01:03:55,720 --> 01:03:58,640
 Just utilize the chain rule, for example.

989
01:03:58,640 --> 01:04:01,319
 Now we define our core, north function

990
01:04:01,319 --> 01:04:02,799
 is a mean square a, right?

991
01:04:02,799 --> 01:04:04,240
 We have this expression.

992
01:04:04,240 --> 01:04:05,920
 We can very easily to come.

993
01:04:05,920 --> 01:04:10,040
 This y is this one.

994
01:04:10,040 --> 01:04:15,040
 If we define some intermediate variable qk equals to this one,

995
01:04:16,259 --> 01:04:19,560
 we can very easily to get the expression of the gradient.

996
01:04:20,799 --> 01:04:24,440
 The differentiation of the j against the v, right?

997
01:04:24,440 --> 01:04:28,200
 Is equals to differentiation of the j against the q.

998
01:04:28,200 --> 01:04:32,000
 And then times differentiation of the q against this v.

999
01:04:32,000 --> 01:04:33,279
 Right?

1000
01:04:33,279 --> 01:04:37,640
 Well, this differentiation of the j against the q from here,

1001
01:04:37,640 --> 01:04:41,200
 it is two times, of course, we define our north function

1002
01:04:41,200 --> 01:04:43,319
 that multiplied by one over two,

1003
01:04:43,319 --> 01:04:46,120
 so that the gradient, we don't have the two.

1004
01:04:46,120 --> 01:04:48,399
 This is just for convenience.

1005
01:04:48,399 --> 01:04:53,399
 So anyway, the gradient of this expression against this q

1006
01:04:53,399 --> 01:04:56,779
 is just this one, right?

1007
01:04:56,780 --> 01:04:59,420
 This square, the differentiation is this one,

1008
01:04:59,420 --> 01:05:01,660
 this one, multiply the differentiation

1009
01:05:01,660 --> 01:05:03,980
 of this activation function.

1010
01:05:03,980 --> 01:05:07,380
 And then multiply this term, the differentiation of the q

1011
01:05:07,380 --> 01:05:10,220
 against the v.

1012
01:05:10,220 --> 01:05:13,140
 Then the result is just that.

1013
01:05:13,140 --> 01:05:18,140
 So this blue color one is the gradient of j

1014
01:05:18,820 --> 01:05:23,820
 against the v, the second layer parameter, right?

1015
01:05:24,820 --> 01:05:28,660
 Now the gradient of the j against the w,

1016
01:05:28,660 --> 01:05:29,940
 I will not go through the details,

1017
01:05:29,940 --> 01:05:31,860
 it is just a normal differentiation.

1018
01:05:31,860 --> 01:05:33,900
 You can very easily to get,

1019
01:05:33,900 --> 01:05:36,260
 it's just a basic calculus.

1020
01:05:36,260 --> 01:05:40,380
 Now, if we want to get the gradient of the j against the w,

1021
01:05:40,380 --> 01:05:42,580
 it is also not difficult,

1022
01:05:42,580 --> 01:05:45,100
 although it appears to be complicated

1023
01:05:45,100 --> 01:05:48,180
 because we have many such kind of a summation,

1024
01:05:48,180 --> 01:05:51,460
 but the gradient is also,

1025
01:05:51,460 --> 01:05:54,860
 we just apply the chain rule of the differentiation.

1026
01:05:54,860 --> 01:05:59,860
 Okay, the differentiation of the j against the v is

1027
01:06:00,700 --> 01:06:05,300
 j against the z, j, delta j over delta z,

1028
01:06:05,300 --> 01:06:07,700
 and then times delta z over delta q,

1029
01:06:07,700 --> 01:06:10,620
 then times delta q over delta w,

1030
01:06:10,620 --> 01:06:14,460
 then we get delta j over delta w.

1031
01:06:14,460 --> 01:06:19,460
 Then for the first items, where is that?

1032
01:06:19,540 --> 01:06:22,060
 That is here, is this one.

1033
01:06:22,060 --> 01:06:23,660
 The second items is here,

1034
01:06:23,660 --> 01:06:26,780
 it's just the differentiation of the activation function.

1035
01:06:26,780 --> 01:06:28,900
 The third term is just the xi.

1036
01:06:30,620 --> 01:06:34,780
 Okay, you can check this one, it's very straightforward.

1037
01:06:34,780 --> 01:06:38,180
 Just apply the chain rule of the differentiation.

1038
01:06:38,180 --> 01:06:42,100
 We can get this expression of the gradient.

1039
01:06:43,940 --> 01:06:46,460
 Now, after we have this expression,

1040
01:06:46,460 --> 01:06:48,820
 then we can compute what is the gradient,

1041
01:06:48,820 --> 01:06:53,820
 then utilize this gradient to update the network parameter.

1042
01:06:55,260 --> 01:06:58,180
 Right, then this is expression of the gradient.

1043
01:06:59,380 --> 01:07:01,700
 Now, from this expression of the gradient,

1044
01:07:01,700 --> 01:07:05,620
 of course, we need to know what is the derivative

1045
01:07:05,620 --> 01:07:08,420
 of this activation function.

1046
01:07:08,420 --> 01:07:10,980
 Here, give an example, if the activation function

1047
01:07:10,980 --> 01:07:12,900
 is this thing more than function,

1048
01:07:12,900 --> 01:07:16,580
 we can very easily see what is its derivative

1049
01:07:16,580 --> 01:07:18,140
 against the q, right?

1050
01:07:18,140 --> 01:07:19,540
 It is this expression.

1051
01:07:19,540 --> 01:07:22,100
 This is just very easily, we can get this one.

1052
01:07:22,100 --> 01:07:23,980
 And it happened to be this one,

1053
01:07:23,980 --> 01:07:27,580
 it's just a times fq one minus fq.

1054
01:07:27,580 --> 01:07:31,339
 So we just utilize the value of fq

1055
01:07:31,339 --> 01:07:34,620
 to multiply them together to produce the value

1056
01:07:34,620 --> 01:07:37,339
 of its derivative.

1057
01:07:37,540 --> 01:07:42,540
 Now, the Hinton core is a back propagation,

1058
01:07:46,420 --> 01:07:49,540
 but in fact, it's just the gradient descent method.

1059
01:07:50,700 --> 01:07:52,980
 To compute the gradient descent method,

1060
01:07:52,980 --> 01:07:56,980
 we just need the mathematical expression of the gradient.

1061
01:07:56,980 --> 01:07:59,100
 The gradient is the differentiation.

1062
01:07:59,100 --> 01:08:02,860
 If we utilize the chain rule of the differentiation,

1063
01:08:02,860 --> 01:08:05,460
 we can very easily get the definition

1064
01:08:05,740 --> 01:08:06,740
 of the gradient.

1065
01:08:06,740 --> 01:08:10,940
 But basically, conceptually,

1066
01:08:10,940 --> 01:08:13,540
 it is some kind of the back propagation,

1067
01:08:13,540 --> 01:08:18,540
 because from here, we can see,

1068
01:08:19,740 --> 01:08:21,740
 from where we can see it.

1069
01:08:22,740 --> 01:08:23,580
 Okay.

1070
01:08:27,420 --> 01:08:32,420
 We need to update the parameter of the first name

1071
01:08:36,420 --> 01:08:38,460
 So the update of the first name,

1072
01:08:38,460 --> 01:08:43,460
 basically it is, we need the gradient of the mean square arrow.

1073
01:08:43,939 --> 01:08:45,899
 Whereas the mean square arrow,

1074
01:08:47,779 --> 01:08:51,620
 this is the mean square arrow of the last name, right?

1075
01:08:51,620 --> 01:08:54,859
 Then we can see this is the mean square arrow

1076
01:08:54,859 --> 01:08:56,300
 of the last name.

1077
01:08:56,300 --> 01:08:59,140
 Now, if we multiply all this together,

1078
01:08:59,140 --> 01:09:03,939
 this part is the mean square arrow of the first name.

1079
01:09:03,939 --> 01:09:08,939
 So we transfer the arrow in the second name

1080
01:09:09,059 --> 01:09:13,379
 to the, this is the arrow of the second name,

1081
01:09:13,379 --> 01:09:16,379
 to the arrow of the first name.

1082
01:09:16,379 --> 01:09:17,979
 Okay.

1083
01:09:17,979 --> 01:09:21,660
 So we transfer the arrow from second name to first name.

1084
01:09:21,660 --> 01:09:26,379
 So we back propagate the arrow from last name

1085
01:09:26,379 --> 01:09:29,740
 to the previous name, and then to the previous name.

1086
01:09:29,740 --> 01:09:31,859
 This is just a physical meaning,

1087
01:09:31,859 --> 01:09:35,420
 but mathematically it is just a chance rule

1088
01:09:35,420 --> 01:09:36,979
 to get its differentiation.

1089
01:09:38,059 --> 01:09:43,059
 So here we call this learning is called back propagation

1090
01:09:44,099 --> 01:09:48,819
 because we have two process in training the network.

1091
01:09:48,819 --> 01:09:51,620
 The first process feed forward,

1092
01:09:51,620 --> 01:09:54,500
 given the current parameter of the network,

1093
01:09:54,500 --> 01:09:55,900
 give a training sample,

1094
01:09:55,900 --> 01:10:00,179
 we need to compute what is output nail by nail.

1095
01:10:00,180 --> 01:10:04,220
 This is the information from the input feed forward

1096
01:10:04,220 --> 01:10:05,300
 to the output.

1097
01:10:06,380 --> 01:10:09,020
 And then after that we need to learning.

1098
01:10:09,020 --> 01:10:14,020
 Then the learning is from the last nail we get the arrow

1099
01:10:16,140 --> 01:10:20,980
 because we compile the output with the ground tools.

1100
01:10:20,980 --> 01:10:23,420
 Then we get the arrow.

1101
01:10:23,420 --> 01:10:26,780
 Then to update the networks parameter,

1102
01:10:26,780 --> 01:10:30,740
 we need to transfer the arrow from last nail

1103
01:10:30,740 --> 01:10:33,139
 to the previous nail.

1104
01:10:33,139 --> 01:10:37,820
 So this is the arrow signal we are back propagate from

1105
01:10:38,700 --> 01:10:41,740
 and of the network to the previous nails.

1106
01:10:42,980 --> 01:10:47,980
 So because of that, this is called back propagation method.

1107
01:10:50,099 --> 01:10:51,500
 Now because of that,

1108
01:10:51,500 --> 01:10:56,500
 the Hinton utilized this to train the network in 1980s.

1109
01:10:57,019 --> 01:11:00,700
 40 years after the research of the neural network,

1110
01:11:00,700 --> 01:11:05,139
 then the neural network research becomes very hard topic.

1111
01:11:05,139 --> 01:11:07,980
 In 1980s, it's very hard topic,

1112
01:11:07,980 --> 01:11:11,059
 meaning general new general appears

1113
01:11:11,059 --> 01:11:12,940
 in name of the neural network.

1114
01:11:12,940 --> 01:11:16,019
 We have a HRP transaction on neural networks.

1115
01:11:16,019 --> 01:11:18,540
 And another general is called the neural networks

1116
01:11:18,540 --> 01:11:20,940
 and neural computing and so on.

1117
01:11:20,940 --> 01:11:23,099
 All of this appear in 1980s

1118
01:11:24,100 --> 01:11:27,260
 because of this back propagation algorithm.

1119
01:11:29,420 --> 01:11:34,420
 Enable us to training a multinational neural network.

1120
01:11:35,980 --> 01:11:40,180
 Because multinational neural network is very powerful.

1121
01:11:40,180 --> 01:11:44,180
 It can implement any nonlinear function,

1122
01:11:45,420 --> 01:11:47,260
 any arbitrary nonlinear function.

1123
01:11:47,260 --> 01:11:51,740
 So the neural network, the capacity is huge.

1124
01:11:51,740 --> 01:11:53,540
 It is very powerful.

1125
01:11:56,820 --> 01:11:58,300
 Okay.

1126
01:11:58,300 --> 01:12:01,860
 Now anyway here introduce the learning process.

1127
01:12:01,860 --> 01:12:04,380
 So to really train such a network,

1128
01:12:04,380 --> 01:12:07,179
 we first need to initialize the parameter value.

1129
01:12:07,179 --> 01:12:09,780
 Then after we initialize parameter value

1130
01:12:09,780 --> 01:12:14,780
 given the training data, x input and its ground tools

1131
01:12:15,059 --> 01:12:18,340
 or its desired output for each training sample,

1132
01:12:18,340 --> 01:12:21,860
 then we can present of this training sample

1133
01:12:21,860 --> 01:12:23,980
 to the inputs of the network,

1134
01:12:23,980 --> 01:12:25,820
 then compute what is output.

1135
01:12:25,820 --> 01:12:28,780
 Utilize the initialized parameter.

1136
01:12:28,780 --> 01:12:29,700
 Right?

1137
01:12:29,700 --> 01:12:33,780
 Then we can use that output to compute the error,

1138
01:12:33,780 --> 01:12:37,340
 then to use this gridding descent method

1139
01:12:37,340 --> 01:12:41,580
 to update the network parameter.

1140
01:12:41,580 --> 01:12:43,140
 So we have two process,

1141
01:12:43,140 --> 01:12:45,540
 phase forward from the input,

1142
01:12:45,540 --> 01:12:48,940
 compute the output, then from the output,

1143
01:12:48,940 --> 01:12:53,140
 utilize the output to compute the gradient

1144
01:12:53,140 --> 01:12:56,300
 to update the parameter of the last layer,

1145
01:12:56,300 --> 01:12:59,420
 second last layer, the third last layer.

1146
01:12:59,420 --> 01:13:02,540
 So we have a backward computation.

1147
01:13:03,460 --> 01:13:05,220
 Of course, this process must be

1148
01:13:05,220 --> 01:13:09,180
 iterating to many, many times.

1149
01:13:09,180 --> 01:13:10,019
 Okay.

1150
01:13:10,540 --> 01:13:11,380
 Okay.

1151
01:13:11,380 --> 01:13:16,180
 So this is a back propagation.

1152
01:13:16,180 --> 01:13:17,020
 Okay.

1153
01:13:17,020 --> 01:13:18,780
 Even now the deep learning,

1154
01:13:18,780 --> 01:13:21,340
 the CNN, the deep learning network

1155
01:13:21,340 --> 01:13:26,340
 still utilize this back propagation basically.

1156
01:13:26,460 --> 01:13:30,460
 Of course, we may have some improvement

1157
01:13:30,460 --> 01:13:32,660
 to solve some problem.

1158
01:13:32,660 --> 01:13:35,980
 For example, one problem is the local minimum.

1159
01:13:36,980 --> 01:13:41,980
 Now, because the machine neural network is very powerful,

1160
01:13:42,259 --> 01:13:44,660
 it is a very highly nonlinear.

1161
01:13:44,660 --> 01:13:48,299
 So the north function or mean square error

1162
01:13:48,299 --> 01:13:50,580
 as a function of the parameter,

1163
01:13:50,580 --> 01:13:55,580
 it is no more just a square, a quadratic function.

1164
01:13:56,459 --> 01:14:00,459
 It can be very complicated nonlinear curve.

1165
01:14:01,700 --> 01:14:02,940
 Okay.

1166
01:14:03,339 --> 01:14:07,259
 Here, just show the north function or mean square error

1167
01:14:07,259 --> 01:14:10,219
 against one parameter.

1168
01:14:10,219 --> 01:14:15,219
 If we can plot several times or hundreds of the minimum.

1169
01:14:15,740 --> 01:14:18,500
 Well, in all this minimum,

1170
01:14:18,500 --> 01:14:22,139
 some minimum has very high error, right?

1171
01:14:23,099 --> 01:14:24,259
 Okay.

1172
01:14:24,259 --> 01:14:27,580
 Now, if the back propagation is basically

1173
01:14:27,580 --> 01:14:30,820
 just a gradient descent method, okay.

1174
01:14:30,820 --> 01:14:34,219
 It will update your network parameter

1175
01:14:34,219 --> 01:14:37,340
 following the direction of the gradient.

1176
01:14:37,340 --> 01:14:41,620
 Of course, it's opposite direction of the gradient, right?

1177
01:14:41,620 --> 01:14:44,860
 But in the local minimum, what is gradient?

1178
01:14:45,980 --> 01:14:48,740
 The gradient is zero.

1179
01:14:49,860 --> 01:14:50,700
 Okay.

1180
01:14:50,700 --> 01:14:51,820
 Because in here, we know,

1181
01:14:51,820 --> 01:14:54,299
 my semester, we know in the minimum point,

1182
01:14:54,299 --> 01:14:56,340
 the gradient is zero.

1183
01:14:56,340 --> 01:14:59,059
 So during the learning process,

1184
01:14:59,060 --> 01:15:02,980
 if the network reach to some parameter value

1185
01:15:02,980 --> 01:15:06,140
 is a local minimum, then the gradient is zero.

1186
01:15:06,140 --> 01:15:11,140
 So we don't know how to update further the network.

1187
01:15:12,100 --> 01:15:14,840
 So this is the big problem.

1188
01:15:14,840 --> 01:15:18,420
 How to stop the nonlinear network,

1189
01:15:18,420 --> 01:15:23,420
 much linear network because of this non-low-convenience.

1190
01:15:24,300 --> 01:15:27,100
 This problem can be also very severe

1191
01:15:27,100 --> 01:15:30,100
 because we can have many local minimum.

1192
01:15:30,100 --> 01:15:32,900
 Some local minimum, the error rate,

1193
01:15:32,900 --> 01:15:37,180
 the error could be much higher than the global minimum.

1194
01:15:37,180 --> 01:15:39,620
 This is just for one parameter.

1195
01:15:39,620 --> 01:15:43,920
 You imagine the parameter is not just one.

1196
01:15:43,920 --> 01:15:48,420
 We have two, three, four, even millions of the parameter.

1197
01:15:49,980 --> 01:15:54,740
 You can see how complicated is this north function

1198
01:15:54,740 --> 01:15:56,640
 against its parameter.

1199
01:15:58,100 --> 01:16:02,100
 Back propagation is gradient descent method.

1200
01:16:02,100 --> 01:16:07,100
 Gradient method is always a local method.

1201
01:16:07,340 --> 01:16:12,020
 It updates the parameter based on only a local information.

1202
01:16:13,260 --> 01:16:14,300
 Okay.

1203
01:16:14,300 --> 01:16:16,860
 Now, if we just base a local information,

1204
01:16:16,860 --> 01:16:20,180
 if this curve is very complicated,

1205
01:16:20,180 --> 01:16:22,860
 somehow it is a problem, right?

1206
01:16:23,860 --> 01:16:27,660
 Now, because of that, thousands of researchers

1207
01:16:27,660 --> 01:16:30,139
 published hundreds of the research papers

1208
01:16:30,139 --> 01:16:32,059
 try to solve this problem.

1209
01:16:33,339 --> 01:16:38,339
 Or at least to improve the algorithm for this problem.

1210
01:16:42,980 --> 01:16:46,299
 So the researcher has a job to do, right?

1211
01:16:46,299 --> 01:16:48,019
 Try to use different methods.

1212
01:16:48,019 --> 01:16:51,500
 Of course, we have some method is called a momentum

1213
01:16:51,500 --> 01:16:53,340
 or better initialization.

1214
01:16:53,340 --> 01:16:58,220
 So because my PhD is also in this year,

1215
01:16:58,220 --> 01:17:03,060
 so somehow my research is also try to solve this problem.

1216
01:17:03,060 --> 01:17:07,100
 I also published one paper to construct the neural network

1217
01:17:07,100 --> 01:17:09,380
 so that the construct neural network

1218
01:17:09,380 --> 01:17:12,620
 has only very close to the global minimum.

1219
01:17:13,780 --> 01:17:16,420
 Okay, to try to solve this problem.

1220
01:17:22,500 --> 01:17:23,340
 Okay.

1221
01:17:26,860 --> 01:17:30,860
 But this is not the only problem of the neural network.

1222
01:17:32,060 --> 01:17:32,900
 Okay.

1223
01:17:32,900 --> 01:17:37,900
 The local minimum, if the network stopped the local minimum,

1224
01:17:38,460 --> 01:17:41,140
 that means your network is not a real trend

1225
01:17:41,140 --> 01:17:46,140
 because you cannot further let the error go down, right?

1226
01:17:46,940 --> 01:17:50,380
 Because it will stop at some local minimum.

1227
01:17:50,980 --> 01:17:55,580
 This is just under fate your training data.

1228
01:17:55,580 --> 01:17:56,860
 Okay.

1229
01:17:56,860 --> 01:18:01,860
 Even you solve this problem to approach to the local minimum

1230
01:18:02,780 --> 01:18:05,940
 to get a very small north function,

1231
01:18:05,940 --> 01:18:09,420
 reduce the north function even to almost zero.

1232
01:18:10,300 --> 01:18:12,900
 But never forget this north function

1233
01:18:12,900 --> 01:18:16,340
 is measured on the training data.

1234
01:18:17,340 --> 01:18:20,180
 Does not mean it will perform good

1235
01:18:20,180 --> 01:18:22,420
 for the unknown test data.

1236
01:18:23,900 --> 01:18:26,980
 To solve all this local minimum problem

1237
01:18:26,980 --> 01:18:29,580
 cannot solve the overfitting problem.

1238
01:18:31,700 --> 01:18:33,780
 Okay, so because of that,

1239
01:18:33,780 --> 01:18:38,860
 so somehow how to make the training not overfit

1240
01:18:38,860 --> 01:18:42,100
 your training data one way,

1241
01:18:42,100 --> 01:18:44,860
 of course there are many different way, right?

1242
01:18:44,860 --> 01:18:46,740
 It could have many different way.

1243
01:18:48,500 --> 01:18:51,540
 Every this different way cannot really solve

1244
01:18:51,540 --> 01:18:56,179
 solve this problem, but somehow it can prove it.

1245
01:18:56,179 --> 01:19:01,179
 One way is utilize the so-called validation data set.

1246
01:19:01,420 --> 01:19:03,660
 So the validation data set is,

1247
01:19:03,660 --> 01:19:08,259
 suppose we use the training data to train the network,

1248
01:19:08,259 --> 01:19:12,380
 so the north function north value will decrease,

1249
01:19:13,340 --> 01:19:17,660
 with the increase of the number of the training iteration.

1250
01:19:17,660 --> 01:19:18,860
 Okay.

1251
01:19:18,860 --> 01:19:23,620
 But if we train this network very long,

1252
01:19:23,620 --> 01:19:25,980
 it could overfit your training data.

1253
01:19:25,980 --> 01:19:30,860
 That means the test data could be somehow north

1254
01:19:30,860 --> 01:19:34,320
 and the test data could be much higher than your training data.

1255
01:19:34,320 --> 01:19:37,620
 So that your training network is useless

1256
01:19:37,620 --> 01:19:39,980
 because the purpose of the training network

1257
01:19:39,980 --> 01:19:42,740
 is never for your training data.

1258
01:19:42,740 --> 01:19:45,419
 It is work for your test data.

1259
01:19:45,419 --> 01:19:48,620
 Okay, so to overcome this problem,

1260
01:19:48,620 --> 01:19:53,620
 we can try to somehow stop the training at some position,

1261
01:19:54,019 --> 01:19:58,700
 not the small value of the north value of the training data,

1262
01:19:58,700 --> 01:20:03,700
 but the small value of some other data,

1263
01:20:04,019 --> 01:20:06,820
 not used in the training.

1264
01:20:06,820 --> 01:20:10,219
 So we call this is a validation data.

1265
01:20:11,219 --> 01:20:14,700
 The validation data we are not used in the training,

1266
01:20:14,700 --> 01:20:19,059
 but we use it to compute what is the north,

1267
01:20:19,059 --> 01:20:21,980
 what is for example, mean square aero.

1268
01:20:21,980 --> 01:20:26,540
 Then if we compute it or find it at this number

1269
01:20:26,540 --> 01:20:30,460
 of the iteration, that validation data go to the minimum,

1270
01:20:30,460 --> 01:20:32,219
 if we further training it,

1271
01:20:32,219 --> 01:20:35,219
 the validation aero will increase.

1272
01:20:35,220 --> 01:20:38,540
 Then we can stop the training at here.

1273
01:20:38,540 --> 01:20:43,140
 Hope this can cause the north

1274
01:20:43,140 --> 01:20:45,740
 on the test data is also small.

1275
01:20:46,900 --> 01:20:50,740
 Okay, so this will partially solve

1276
01:20:50,740 --> 01:20:52,740
 the overfitting problem

1277
01:20:52,740 --> 01:20:56,620
 because the validation data is not used in the training.

1278
01:20:58,660 --> 01:21:01,100
 But still your validation data

1279
01:21:01,100 --> 01:21:04,460
 and the test data is also different.

1280
01:21:04,460 --> 01:21:08,440
 We can still cannot guarantee

1281
01:21:08,440 --> 01:21:11,700
 it will perform good for your test data.

1282
01:21:12,940 --> 01:21:14,860
 By the way, don't forget,

1283
01:21:14,860 --> 01:21:19,860
 the validation data is also part of the training data

1284
01:21:19,860 --> 01:21:23,740
 because the validation data to say how is the performance

1285
01:21:23,740 --> 01:21:24,980
 of the validation data,

1286
01:21:24,980 --> 01:21:28,820
 we must know the ground truth of the validation data.

1287
01:21:29,820 --> 01:21:34,820
 Okay, so that means how to get the validation data.

1288
01:21:35,420 --> 01:21:38,259
 After we have a training data set,

1289
01:21:38,259 --> 01:21:42,860
 we purposely to put part of this training data,

1290
01:21:42,860 --> 01:21:44,780
 not use in the training.

1291
01:21:44,780 --> 01:21:47,460
 We use a subset of the training data

1292
01:21:47,460 --> 01:21:50,460
 to we call it as a validation data,

1293
01:21:50,460 --> 01:21:51,700
 not used in the training,

1294
01:21:51,700 --> 01:21:56,700
 but used as to see compute what is the north value

1295
01:21:58,820 --> 01:22:02,019
 and this data not used in the training.

1296
01:22:02,019 --> 01:22:06,700
 So the validation data is part of your training data.

1297
01:22:07,540 --> 01:22:12,540
 Okay, it may also be different from your test data.

1298
01:22:17,620 --> 01:22:20,620
 Now one good sense is anyway,

1299
01:22:20,620 --> 01:22:25,620
 we can solve the training at some small value of the north

1300
01:22:26,460 --> 01:22:30,460
 and some data not used in the training.

1301
01:22:30,460 --> 01:22:32,740
 We hope this can partially solve

1302
01:22:32,740 --> 01:22:35,220
 this overfitting problem, right?

1303
01:22:38,019 --> 01:22:41,860
 Okay, now here I will show what is the real problem

1304
01:22:41,860 --> 01:22:43,420
 of the neural network.

1305
01:22:43,420 --> 01:22:47,180
 Neural network is very powerful because of that,

1306
01:22:47,180 --> 01:22:52,180
 the people is very exciting in 1980s and the 1990s.

1307
01:22:52,420 --> 01:22:53,940
 Because it's so powerful,

1308
01:22:53,940 --> 01:22:58,500
 it seems a neural network can solve every problem.

1309
01:22:58,500 --> 01:22:59,860
 Every machine learning problem,

1310
01:22:59,860 --> 01:23:01,580
 we can always use the neural network

1311
01:23:01,580 --> 01:23:03,780
 because neural network is very powerful,

1312
01:23:03,780 --> 01:23:07,700
 it can implement any nonlinear function.

1313
01:23:07,700 --> 01:23:10,700
 So it is a so-called universal approximator,

1314
01:23:13,140 --> 01:23:16,700
 it can approximate any nonlinear function.

1315
01:23:16,700 --> 01:23:18,340
 So people are very exciting,

1316
01:23:18,340 --> 01:23:22,660
 but somehow people forget what is the problem

1317
01:23:22,660 --> 01:23:23,860
 of the overfitting.

1318
01:23:23,860 --> 01:23:25,900
 Overfitting can be very serious.

1319
01:23:25,900 --> 01:23:29,740
 Here give an example to show what is overfitting.

1320
01:23:29,740 --> 01:23:34,020
 Now suppose in the function regression

1321
01:23:34,020 --> 01:23:35,300
 or the classification,

1322
01:23:35,300 --> 01:23:39,620
 the real optimal class boundary is this curve.

1323
01:23:40,500 --> 01:23:42,420
 Okay, but it is unknown, right?

1324
01:23:42,420 --> 01:23:44,180
 We need to use the training data

1325
01:23:44,180 --> 01:23:47,860
 to learn this nonlinear curve.

1326
01:23:47,860 --> 01:23:51,780
 But the training data is only discrete point.

1327
01:23:51,780 --> 01:23:54,460
 Whatever huge number of the training data,

1328
01:23:54,460 --> 01:23:56,940
 it must be discrete.

1329
01:23:56,940 --> 01:24:00,740
 But we need to learn a continuous curve

1330
01:24:00,740 --> 01:24:05,259
 because the learned curve should utilize for the test data.

1331
01:24:05,259 --> 01:24:08,820
 Test data can be any value, we don't know, right?

1332
01:24:08,820 --> 01:24:10,700
 So to handle the test data,

1333
01:24:10,700 --> 01:24:12,940
 we must have a continuous curve.

1334
01:24:13,980 --> 01:24:17,099
 Now to learn this continuous curve,

1335
01:24:17,100 --> 01:24:21,100
 we have only the training data is a discrete point.

1336
01:24:21,100 --> 01:24:24,980
 Suppose we have such point is a training data.

1337
01:24:24,980 --> 01:24:26,940
 It's same as the ground truth.

1338
01:24:26,940 --> 01:24:30,100
 Ground truth is this continuous curve, right?

1339
01:24:30,100 --> 01:24:33,380
 How to use this discrete point of the data

1340
01:24:33,380 --> 01:24:36,900
 to learn this continuous curve?

1341
01:24:37,860 --> 01:24:42,620
 Now if we just use a single layer of the network,

1342
01:24:42,620 --> 01:24:45,900
 it can only be a linear class field.

1343
01:24:45,900 --> 01:24:49,299
 The class boundary can only be a line, right?

1344
01:24:49,299 --> 01:24:52,700
 So if we use just a single layer network,

1345
01:24:52,700 --> 01:24:55,259
 then the network can just learn

1346
01:24:55,259 --> 01:24:58,660
 what is the best line to feed your training data.

1347
01:24:59,580 --> 01:25:03,179
 Then your mental result will be this straight line.

1348
01:25:04,299 --> 01:25:08,259
 Then this learning result to get this straight line

1349
01:25:08,259 --> 01:25:11,019
 is under feed your training data

1350
01:25:11,019 --> 01:25:13,820
 because we have large arrow, right?

1351
01:25:15,900 --> 01:25:19,059
 Okay, the mental results compared to the ground truth

1352
01:25:19,059 --> 01:25:21,900
 training data, the arrow could be very large.

1353
01:25:21,900 --> 01:25:23,660
 So this is under feeding.

1354
01:25:25,099 --> 01:25:27,219
 Now if we use multi-layer,

1355
01:25:27,219 --> 01:25:32,219
 so increase the capability of the network, okay?

1356
01:25:33,019 --> 01:25:37,259
 So that the network can implement the nonlinear function,

1357
01:25:37,259 --> 01:25:41,820
 then it could be we will learn somehow a curve,

1358
01:25:41,820 --> 01:25:45,820
 a nonlinear curve in state of just a straight line.

1359
01:25:45,820 --> 01:25:47,860
 Learn from this training data.

1360
01:25:47,860 --> 01:25:51,019
 We can see the arrow will be smaller, right?

1361
01:25:51,019 --> 01:25:56,019
 It will be more close to the training data,

1362
01:25:56,219 --> 01:25:58,700
 compared to this straight line.

1363
01:25:58,700 --> 01:26:02,179
 So the north value or the mean square arrow

1364
01:26:02,179 --> 01:26:05,019
 of this green curve will be much smaller

1365
01:26:05,019 --> 01:26:07,460
 than this straight line.

1366
01:26:07,460 --> 01:26:09,940
 So this is good, right?

1367
01:26:09,940 --> 01:26:12,740
 But we still have some arrow, right?

1368
01:26:12,740 --> 01:26:15,420
 We still have some arrow, even use this curve.

1369
01:26:16,420 --> 01:26:20,460
 Now, if the network is very, very powerful,

1370
01:26:20,460 --> 01:26:24,300
 it can implement a very highly nonlinear function.

1371
01:26:24,300 --> 01:26:27,900
 It can make your north function decrease

1372
01:26:27,900 --> 01:26:30,219
 to the minimum zero.

1373
01:26:30,219 --> 01:26:34,420
 Of course north function usually is non-negative, right?

1374
01:26:34,420 --> 01:26:39,420
 So that means all training data is perfect in your learning.

1375
01:26:40,180 --> 01:26:43,440
 Then it will form this red curve.

1376
01:26:44,500 --> 01:26:48,140
 Now this red curve goes through all training data.

1377
01:26:48,140 --> 01:26:52,740
 That means the arrow is zero.

1378
01:26:52,740 --> 01:26:54,740
 The mean square arrow is zero.

1379
01:26:55,700 --> 01:27:00,700
 But is this red curve better than green curve or not

1380
01:27:01,580 --> 01:27:05,060
 in terms of the whole continuous space?

1381
01:27:06,060 --> 01:27:11,060
 It can, if we utilize this green curve,

1382
01:27:11,340 --> 01:27:16,340
 or this red curve, the arrow on your training data is zero.

1383
01:27:17,020 --> 01:27:21,660
 It can perform worse on your training data,

1384
01:27:21,660 --> 01:27:24,100
 even worse than this straight line.

1385
01:27:27,380 --> 01:27:30,580
 We can imagine it is pretty possible, right?

1386
01:27:31,580 --> 01:27:34,540
 Such a powerful sense can be worse

1387
01:27:34,540 --> 01:27:38,900
 than a simple single nail just to form a straight line.

1388
01:27:40,220 --> 01:27:43,620
 This is a great problem of the neural network,

1389
01:27:43,620 --> 01:27:46,740
 although it is very powerful.

1390
01:27:47,740 --> 01:27:51,700
 So this is why we can get the conclusion

1391
01:27:51,700 --> 01:27:54,340
 of this traditional neural network.

1392
01:27:54,340 --> 01:27:56,540
 It is very powerful.

1393
01:27:56,540 --> 01:27:59,420
 The two nails, just use two nails,

1394
01:27:59,420 --> 01:28:03,180
 can implement anything, can solve any problem.

1395
01:28:04,500 --> 01:28:06,820
 But this is just the capability.

1396
01:28:06,820 --> 01:28:11,820
 To solve any problem just means we can solve any problem

1397
01:28:11,940 --> 01:28:14,060
 on your training data,

1398
01:28:14,060 --> 01:28:16,660
 because we can only measure the training data.

1399
01:28:16,660 --> 01:28:19,420
 Test data is unknown, it's not on,

1400
01:28:19,420 --> 01:28:24,420
 it's unavailable in the training.

1401
01:28:24,460 --> 01:28:27,740
 So this will cause overfitting problem.

1402
01:28:27,740 --> 01:28:30,380
 And this overfitting problem, in my opinion,

1403
01:28:30,380 --> 01:28:33,660
 is much, much severe problem.

1404
01:28:33,660 --> 01:28:36,019
 Difficult problem than underfitting.

1405
01:28:37,500 --> 01:28:39,460
 Because if it is underfitting,

1406
01:28:39,460 --> 01:28:43,860
 we know the arrow is high.

1407
01:28:43,860 --> 01:28:48,860
 Then we can always try to reduce the arrow

1408
01:28:49,900 --> 01:28:52,099
 on the training data, right?

1409
01:28:52,099 --> 01:28:54,260
 To improve the underfitting.

1410
01:28:54,260 --> 01:28:56,660
 But overfitting problem we don't know.

1411
01:28:56,660 --> 01:28:58,580
 We only have the training data.

1412
01:28:58,580 --> 01:29:03,300
 We never know if it somehow overfits your training data.

1413
01:29:03,300 --> 01:29:06,780
 Can perform very poorly on your unknown tested data.

1414
01:29:08,260 --> 01:29:10,540
 So this is why the neural network,

1415
01:29:10,540 --> 01:29:12,940
 although it is powerful in 1980s,

1416
01:29:12,940 --> 01:29:14,660
 people are very exciting.

1417
01:29:14,660 --> 01:29:19,660
 So it caused a huge size of the research community

1418
01:29:20,700 --> 01:29:22,900
 working in neural network.

1419
01:29:22,900 --> 01:29:26,160
 But after 10 years or 20 years research,

1420
01:29:26,160 --> 01:29:29,440
 people found neural network is a usefulness.

1421
01:29:30,760 --> 01:29:32,480
 Although it is very powerful,

1422
01:29:32,480 --> 01:29:37,120
 it can perform very highly complicated function,

1423
01:29:37,120 --> 01:29:42,120
 but the neural network cannot solve the real problem.

1424
01:29:43,120 --> 01:29:46,720
 If we utilize the neural network to the real problem,

1425
01:29:46,720 --> 01:29:48,840
 it cannot, always cannot work.

1426
01:29:48,840 --> 01:29:52,360
 For tested data, some tested data in the practice

1427
01:29:52,360 --> 01:29:55,360
 just slightly different from the training data,

1428
01:29:55,360 --> 01:29:57,719
 then the result is wrong.

1429
01:30:00,000 --> 01:30:03,480
 Okay, so this is why we have the problem.

1430
01:30:03,480 --> 01:30:07,679
 The neural network research start from 1950s,

1431
01:30:07,679 --> 01:30:12,120
 but more than 40 years, it is just starting single layer.

1432
01:30:12,120 --> 01:30:17,040
 Because people find that multi-layer is there's no solution.

1433
01:30:17,040 --> 01:30:18,920
 Okay, then in the 1980s,

1434
01:30:18,920 --> 01:30:22,000
 Hinton invented the back propagation

1435
01:30:22,000 --> 01:30:23,920
 to make multi-layer perceptron,

1436
01:30:23,920 --> 01:30:26,200
 multi-layer network is trainable.

1437
01:30:26,200 --> 01:30:29,120
 We can train the multi-layer network.

1438
01:30:29,120 --> 01:30:31,760
 And the multi-layer network is so powerful,

1439
01:30:31,760 --> 01:30:36,760
 just two layers can be used to solve any problem.

1440
01:30:36,880 --> 01:30:38,880
 Then the neural network research activity

1441
01:30:38,880 --> 01:30:43,880
 goes very, very hard in the 19th and the start of the 2000s.

1442
01:30:46,920 --> 01:30:50,360
 But gradually the neural network research activity

1443
01:30:50,360 --> 01:30:55,360
 go down, go down, go down, go to the middle of the 2000s.

1444
01:30:55,519 --> 01:30:58,440
 Almost nobody care about the neural network.

1445
01:30:59,360 --> 01:31:04,160
 In this years, okay, the top conference, CVTR

1446
01:31:04,160 --> 01:31:06,799
 and the top general, where reject all paper,

1447
01:31:06,799 --> 01:31:10,240
 if your paper submission is research on the neural network.

1448
01:31:12,200 --> 01:31:15,320
 People don't believe the neural network

1449
01:31:15,320 --> 01:31:18,880
 because it cannot solve the real problem.

1450
01:31:18,880 --> 01:31:23,880
 But then Hinton invented the convolutional neural network.

1451
01:31:26,640 --> 01:31:28,960
 And then people find the use the thing

1452
01:31:28,960 --> 01:31:33,960
 and solve the real problem, use the real image.

1453
01:31:34,120 --> 01:31:37,120
 For example, image net, a huge data set.

1454
01:31:37,120 --> 01:31:40,000
 If we use this convolutional neural network,

1455
01:31:40,000 --> 01:31:42,840
 train the network, it can work very well

1456
01:31:42,840 --> 01:31:45,400
 on the unknown test image.

1457
01:31:46,400 --> 01:31:49,120
 The accuracy on the test image

1458
01:31:49,120 --> 01:31:51,120
 were increased from for example,

1459
01:31:51,120 --> 01:31:54,519
 just 70% to 95%.

1460
01:31:55,519 --> 01:31:58,920
 Such a big change is really revolution

1461
01:31:58,920 --> 01:32:00,639
 in the machine learning.

1462
01:32:00,639 --> 01:32:04,839
 So CNN is a real revolution of the machine learning

1463
01:32:04,839 --> 01:32:08,599
 so that the deep learning activity go to high.

1464
01:32:08,599 --> 01:32:11,879
 Now it must be in this position.

1465
01:32:11,880 --> 01:32:16,080
 This curve is a curve I copy from some media

1466
01:32:17,840 --> 01:32:19,720
 somehow eight years ago.

1467
01:32:20,600 --> 01:32:23,960
 Okay, in that year people think that the deep learning

1468
01:32:23,960 --> 01:32:28,960
 will go to some, will also somehow stay at some platform.

1469
01:32:29,640 --> 01:32:33,120
 But in fact, now the deep learning go to very high.

1470
01:32:34,440 --> 01:32:37,400
 Because it really solves the real problem.

1471
01:32:37,400 --> 01:32:39,720
 Okay, so because of the CNN,

1472
01:32:39,720 --> 01:32:41,120
 because of the deep learning,

1473
01:32:41,120 --> 01:32:46,120
 now we have the current artificial intelligence AI.

1474
01:32:47,760 --> 01:32:51,440
 So after break we were studying how the CNN

1475
01:32:51,440 --> 01:32:55,600
 solves the problem of the Martina NEO network, MAOQ.

1476
01:32:56,600 --> 01:33:01,600
 Okay, so we now have 15 minutes or 20 minutes of the break.

1477
01:33:02,280 --> 01:33:05,760
 I believe you can collect your quiz paper from your TA.

1478
01:33:05,760 --> 01:33:10,120
 Okay, we have 20 minutes of the break now.

1479
01:33:35,760 --> 01:33:37,760
 Thank you.

1480
01:34:05,760 --> 01:34:07,760
 Thank you.

1481
01:34:35,760 --> 01:34:37,760
 Thank you.

1482
01:35:05,760 --> 01:35:07,760
 Thank you.

1483
01:35:35,760 --> 01:35:37,760
 Thank you.

1484
01:36:05,760 --> 01:36:07,760
 Thank you.

1485
01:36:35,760 --> 01:36:37,760
 Thank you.

1486
01:37:05,760 --> 01:37:07,760
 Thank you.

1487
01:37:35,760 --> 01:37:37,760
 Thank you.

1488
01:38:05,760 --> 01:38:07,760
 Thank you.

1489
01:38:35,760 --> 01:38:37,760
 Thank you.

1490
01:39:05,760 --> 01:39:07,760
 Thank you.

1491
01:39:35,760 --> 01:39:37,760
 Thank you.

1492
01:40:05,760 --> 01:40:07,760
 Thank you.

1493
01:40:35,760 --> 01:40:37,760
 Thank you.

1494
01:41:05,760 --> 01:41:07,760
 Thank you.

1495
01:41:35,760 --> 01:41:37,760
 Thank you.

1496
01:42:05,760 --> 01:42:07,760
 Thank you.

1497
01:42:35,760 --> 01:42:37,760
 Thank you.

1498
01:43:05,760 --> 01:43:07,760
 Thank you.

1499
01:43:35,760 --> 01:43:37,760
 Thank you.

1500
01:44:05,760 --> 01:44:07,760
 Thank you.

1501
01:44:35,760 --> 01:44:37,760
 Thank you.

1502
01:45:05,760 --> 01:45:07,760
 Thank you.

1503
01:45:35,760 --> 01:45:36,760
 Thank you.

1504
01:46:05,760 --> 01:46:07,760
 Thank you.

1505
01:46:35,760 --> 01:46:37,760
 Thank you.

1506
01:47:05,760 --> 01:47:07,760
 Thank you.

1507
01:47:35,760 --> 01:47:36,760
 Thank you.

1508
01:48:05,760 --> 01:48:07,760
 Thank you.

1509
01:48:35,760 --> 01:48:36,760
 Thank you.

1510
01:49:05,760 --> 01:49:07,760
 Thank you.

1511
01:49:35,760 --> 01:49:37,760
 Thank you.

1512
01:50:05,760 --> 01:50:06,760
 Thank you.

1513
01:50:35,760 --> 01:50:36,760
 Thank you.

1514
01:51:05,760 --> 01:51:07,760
 Thank you.

1515
01:51:35,760 --> 01:51:42,760
 Thank you.

1516
01:52:05,760 --> 01:52:16,760
 Thank you.

1517
01:52:16,760 --> 01:52:22,760
 Thank you.

1518
01:52:22,760 --> 01:52:23,760
 Thank you.

1519
01:52:23,760 --> 01:52:26,760
 Okay, so I think we can resume our lecture.

1520
01:52:26,760 --> 01:52:30,760
 Now, deep learning becomes a so hot topic.

1521
01:52:30,760 --> 01:52:35,760
 or AI system where utilize the deep learning,

1522
01:52:35,760 --> 01:52:38,840
 use some saying all the transformer

1523
01:52:38,840 --> 01:52:42,720
 as the machine learning part in the AI system.

1524
01:52:42,720 --> 01:52:47,720
 Okay, so if we study our recondition system

1525
01:52:47,760 --> 01:52:49,240
 in the traditional way,

1526
01:52:49,240 --> 01:52:51,880
 we have several different steps

1527
01:52:51,880 --> 01:52:56,880
 to complete the recondition task, right?

1528
01:52:57,080 --> 01:52:58,840
 Because in the traditional way,

1529
01:52:58,840 --> 01:53:01,560
 we need pre-processing feature extraction,

1530
01:53:01,560 --> 01:53:04,960
 dimension reduction, and classification, and so on.

1531
01:53:04,960 --> 01:53:07,280
 Then in all these different steps,

1532
01:53:07,280 --> 01:53:12,280
 most of the steps are determined by the human knowledge,

1533
01:53:12,400 --> 01:53:14,360
 are designed by the human,

1534
01:53:14,360 --> 01:53:18,160
 such as some kind of the handcrafted feature

1535
01:53:18,160 --> 01:53:20,280
 is designed by our human.

1536
01:53:20,280 --> 01:53:22,960
 Only in the last stage classification

1537
01:53:22,960 --> 01:53:27,200
 use the training data to estimate some parameter

1538
01:53:27,200 --> 01:53:28,760
 in the class field.

1539
01:53:28,760 --> 01:53:31,360
 Okay, but now because of this deep learning

1540
01:53:31,360 --> 01:53:32,880
 perform very well,

1541
01:53:32,880 --> 01:53:36,240
 now it is the machine learning is not just

1542
01:53:36,240 --> 01:53:40,200
 for the last stage of the recondition system.

1543
01:53:40,200 --> 01:53:43,920
 Now the machine learning will come to from last stage

1544
01:53:43,920 --> 01:53:47,280
 up to the previous stage

1545
01:53:47,280 --> 01:53:50,880
 and at the end up to the beginnings.

1546
01:53:50,880 --> 01:53:53,280
 So in the deep learning area,

1547
01:53:53,280 --> 01:53:56,640
 we have the so-called end-to-end system.

1548
01:53:56,720 --> 01:54:00,640
 That means all the system use the machine learning.

1549
01:54:00,640 --> 01:54:03,760
 Give any raw data input into the system.

1550
01:54:03,760 --> 01:54:07,480
 We just input it into a deep learning network.

1551
01:54:07,480 --> 01:54:11,080
 So the deep learning network can perform all this

1552
01:54:11,080 --> 01:54:14,240
 pre-processing feature extraction, dimension reduction,

1553
01:54:14,240 --> 01:54:17,520
 and classification to get the final result,

1554
01:54:17,520 --> 01:54:20,800
 to produce a so-called end-to-end system.

1555
01:54:21,600 --> 01:54:24,840
 Now how this machine learning is,

1556
01:54:24,840 --> 01:54:27,080
 the deep learning is so powerful.

1557
01:54:27,080 --> 01:54:30,440
 Let's see, it seems we don't need our human knowledge

1558
01:54:30,440 --> 01:54:33,200
 to design the handcrafted feature.

1559
01:54:33,200 --> 01:54:38,200
 And so every task is performed by the network,

1560
01:54:38,800 --> 01:54:41,560
 neural network by the machine learning.

1561
01:54:41,560 --> 01:54:43,480
 Now if in such a way,

1562
01:54:43,480 --> 01:54:46,680
 then that means everything can be done by machine,

1563
01:54:46,680 --> 01:54:48,720
 learn from the training data.

1564
01:54:48,720 --> 01:54:50,960
 Where is our human knowledge?

1565
01:54:50,960 --> 01:54:54,480
 Our human is useless.

1566
01:54:54,480 --> 01:54:58,440
 Everything can be done by the machine learning from the data.

1567
01:54:58,440 --> 01:54:59,800
 Right?

1568
01:54:59,800 --> 01:55:03,960
 But in fact, it is not true.

1569
01:55:03,960 --> 01:55:08,960
 The human knowledge still play a very important role

1570
01:55:09,200 --> 01:55:13,880
 in the system, even in the machine learning system.

1571
01:55:13,880 --> 01:55:15,320
 Now to understand that,

1572
01:55:15,320 --> 01:55:18,080
 we need to understand what is a machine learning.

1573
01:55:18,080 --> 01:55:21,480
 And then what is the problem of the traditional neural network

1574
01:55:21,480 --> 01:55:23,320
 or traditional machine learning?

1575
01:55:23,320 --> 01:55:26,200
 Then we can understand how this deep learning

1576
01:55:26,200 --> 01:55:28,400
 will solve the problem.

1577
01:55:29,400 --> 01:55:32,440
 Okay, so as I mentioned that the machine learning

1578
01:55:32,440 --> 01:55:37,440
 is used some discrete finite number of the training data

1579
01:55:37,719 --> 01:55:42,719
 to estimate a function that map the input to the output.

1580
01:55:43,720 --> 01:55:46,720
 We need a prediction, right?

1581
01:55:46,720 --> 01:55:50,920
 Okay, but XI and YI in the training data,

1582
01:55:50,920 --> 01:55:53,240
 even we have large number of the training data,

1583
01:55:53,240 --> 01:55:55,720
 it is still discrete or finite.

1584
01:55:55,720 --> 01:55:58,400
 We need a continuous function.

1585
01:55:58,400 --> 01:56:01,920
 Okay, so use the training data,

1586
01:56:01,920 --> 01:56:04,640
 we can minimize some kind of the north function,

1587
01:56:04,640 --> 01:56:06,480
 such as the mean square error.

1588
01:56:06,480 --> 01:56:10,360
 But this north function can only measure it

1589
01:56:10,360 --> 01:56:12,200
 on your training data.

1590
01:56:12,760 --> 01:56:17,760
 Okay, so what we get is the estimate of this F, F hat.

1591
01:56:21,640 --> 01:56:23,720
 Then the neural network is some kind

1592
01:56:23,720 --> 01:56:25,520
 of the parametric approach.

1593
01:56:25,520 --> 01:56:28,280
 We have a network, we have a network,

1594
01:56:28,280 --> 01:56:30,960
 that means we have the mathematic model,

1595
01:56:30,960 --> 01:56:33,640
 then we use the training data to estimate

1596
01:56:33,640 --> 01:56:35,680
 all these parameters.

1597
01:56:35,680 --> 01:56:38,560
 Okay, so all this network, in fact,

1598
01:56:38,560 --> 01:56:42,280
 in the mathematic form is just this formula.

1599
01:56:42,280 --> 01:56:44,880
 Okay, they are equivalent, right?

1600
01:56:44,880 --> 01:56:47,960
 Input, multiply by written matrix goes

1601
01:56:47,960 --> 01:56:51,200
 through a activation function, then repeat

1602
01:56:51,200 --> 01:56:54,519
 in the next layer, repeat in the next layer and again.

1603
01:56:54,519 --> 01:56:59,519
 So this mathematic model will include all network structure

1604
01:57:00,800 --> 01:57:05,300
 of the fully connected, matinee neural networks.

1605
01:57:06,679 --> 01:57:10,080
 Well, this H is activation function.

1606
01:57:10,960 --> 01:57:15,960
 Now, to study what is, how this CNN,

1607
01:57:15,960 --> 01:57:17,920
 as a deep learning software problem

1608
01:57:17,920 --> 01:57:19,960
 of the traditional neural network,

1609
01:57:19,960 --> 01:57:23,120
 we need first to see what is the main problem

1610
01:57:23,120 --> 01:57:25,160
 of the neural network.

1611
01:57:25,160 --> 01:57:28,360
 Although the neural network, the problem

1612
01:57:28,360 --> 01:57:30,320
 of the neural network is not,

1613
01:57:30,320 --> 01:57:32,880
 neural network is not powerful.

1614
01:57:32,880 --> 01:57:35,640
 No, neural network is very powerful.

1615
01:57:35,640 --> 01:57:40,040
 So the problem is not the underfitting your training data.

1616
01:57:40,040 --> 01:57:43,200
 You use the training data, we can always try,

1617
01:57:43,200 --> 01:57:48,200
 a neural network decrease the north function

1618
01:57:48,200 --> 01:57:52,040
 to make the error almost zero, okay?

1619
01:57:52,960 --> 01:57:55,400
 Well, we can always try to do that,

1620
01:57:55,400 --> 01:57:59,880
 but this is only on the training data.

1621
01:57:59,880 --> 01:58:04,600
 And we have a theory study that the two layers

1622
01:58:04,600 --> 01:58:07,440
 is already sufficient to implement

1623
01:58:07,440 --> 01:58:09,559
 any highly nonlinear function.

1624
01:58:11,719 --> 01:58:14,360
 Then we have the deep learning.

1625
01:58:14,360 --> 01:58:16,440
 What means the deep learning?

1626
01:58:16,440 --> 01:58:20,080
 Deep learning in fact is just means this aim,

1627
01:58:20,080 --> 01:58:24,200
 the number of the layers is much more than two.

1628
01:58:24,200 --> 01:58:27,040
 It can be several times, several hundreds.

1629
01:58:27,040 --> 01:58:32,040
 Now the large model could have thousand layers.

1630
01:58:33,480 --> 01:58:36,320
 So why we need so many layers

1631
01:58:36,320 --> 01:58:40,440
 if two layers is already sufficient,

1632
01:58:40,440 --> 01:58:44,400
 has powerful capability to implement

1633
01:58:44,400 --> 01:58:46,759
 any nonlinear decision function?

1634
01:58:46,759 --> 01:58:49,400
 Why we need many layers?

1635
01:58:50,440 --> 01:58:54,120
 This is because two layers is difficult

1636
01:58:54,120 --> 01:58:55,400
 to find the solution.

1637
01:58:55,400 --> 01:58:59,940
 We need much, much layer to find the solution

1638
01:58:59,940 --> 01:59:01,759
 to minimize the error.

1639
01:59:03,080 --> 01:59:05,679
 The answer is no, okay?

1640
01:59:06,560 --> 01:59:09,840
 The fully connected network is already very powerful.

1641
01:59:09,840 --> 01:59:12,240
 We can always reduce the error,

1642
01:59:12,240 --> 01:59:14,840
 but the error is on the training data.

1643
01:59:14,840 --> 01:59:17,760
 So the problem of the neural network

1644
01:59:17,760 --> 01:59:22,760
 is not its limited capability.

1645
01:59:23,280 --> 01:59:27,760
 The problem of the neural network is overfitting.

1646
01:59:27,760 --> 01:59:32,000
 Here I will show an example, what is overfitting?

1647
01:59:32,000 --> 01:59:35,240
 And how powerful is the neural network?

1648
01:59:35,240 --> 01:59:39,480
 For example, in the 1990s,

1649
01:59:39,480 --> 01:59:42,639
 somehow the people always study this problem,

1650
01:59:42,639 --> 01:59:44,559
 the classification problem.

1651
01:59:44,559 --> 01:59:47,880
 So the data is just a two-dimensional.

1652
01:59:47,880 --> 01:59:50,400
 Okay, two-dimensional data is just one point

1653
01:59:50,400 --> 01:59:53,719
 in the two-dimensional space, right?

1654
01:59:53,719 --> 01:59:58,719
 So here we have, each point is one sample.

1655
01:59:59,360 --> 02:00:02,760
 So we have, this sample has two class.

1656
02:00:02,760 --> 02:00:06,760
 The white point is the sample of one class,

1657
02:00:06,760 --> 02:00:10,280
 black point is sample of another class.

1658
02:00:10,280 --> 02:00:13,040
 So we can see the white sample,

1659
02:00:13,040 --> 02:00:14,880
 white class and black class,

1660
02:00:14,880 --> 02:00:19,880
 they are spiral, almost fully overlapped.

1661
02:00:22,760 --> 02:00:24,560
 Now, to do the classification,

1662
02:00:24,560 --> 02:00:28,160
 to classify the black dot from the white dot,

1663
02:00:28,160 --> 02:00:31,760
 then the class boundary must be very highly nonlinear.

1664
02:00:31,760 --> 02:00:35,120
 It must be also a spiral, right?

1665
02:00:35,120 --> 02:00:37,880
 Between these two class of the data.

1666
02:00:39,120 --> 02:00:44,120
 Now, if we use this data to train a neural network,

1667
02:00:44,320 --> 02:00:47,320
 even just two layers of the neural network,

1668
02:00:47,320 --> 02:00:51,280
 we have no problem to train the neural network

1669
02:00:51,280 --> 02:00:56,280
 that classify all this training data perfect, correct,

1670
02:00:56,680 --> 02:01:01,680
 arrow free, although we need a very large data.

1671
02:01:01,760 --> 02:01:05,880
 Very highly nonlinear decision boundary, right?

1672
02:01:05,880 --> 02:01:08,560
 But we have no problem to train

1673
02:01:08,560 --> 02:01:12,360
 just two layers network to perfectly classify

1674
02:01:12,360 --> 02:01:15,800
 all this data into two class.

1675
02:01:15,800 --> 02:01:19,320
 But if we look at what is the decision boundary

1676
02:01:19,320 --> 02:01:23,520
 or decision region, we can see this three different solution

1677
02:01:23,520 --> 02:01:27,480
 of the three neural network to get the decision boundary

1678
02:01:27,480 --> 02:01:29,400
 or decision region.

1679
02:01:29,400 --> 02:01:32,679
 We can see the decision region is very funny.

1680
02:01:32,679 --> 02:01:34,200
 From this decision region,

1681
02:01:34,200 --> 02:01:37,400
 we can see that some decision boundary

1682
02:01:37,400 --> 02:01:40,000
 is very close to the training data.

1683
02:01:40,000 --> 02:01:43,759
 That means if the test data is just slightly different

1684
02:01:43,759 --> 02:01:45,120
 from the training data,

1685
02:01:45,120 --> 02:01:48,040
 then immediately the classification result

1686
02:01:48,040 --> 02:01:50,440
 and the test data will be wrong.

1687
02:01:51,400 --> 02:01:55,839
 Although it classify all training data correct,

1688
02:01:55,840 --> 02:02:00,840
 does not mean it can perform well for the test data, right?

1689
02:02:01,400 --> 02:02:04,920
 So this is a problem of the overfitting.

1690
02:02:05,840 --> 02:02:08,520
 So if we understand this,

1691
02:02:08,520 --> 02:02:12,680
 the biggest problem of the traditional neural network

1692
02:02:12,680 --> 02:02:14,240
 is this overfitting,

1693
02:02:14,240 --> 02:02:18,640
 then we can understand how the Cn can solve this problem.

1694
02:02:20,120 --> 02:02:22,760
 To study how the Cn to solve this problem,

1695
02:02:22,760 --> 02:02:27,760
 we need to see that for this overfitting problem,

1696
02:02:27,960 --> 02:02:31,200
 what is the general rule idea

1697
02:02:31,200 --> 02:02:33,240
 to solve this overfitting problem?

1698
02:02:34,200 --> 02:02:36,200
 How to solve this overfitting problem?

1699
02:02:37,400 --> 02:02:40,320
 The overfitting problem cannot solve

1700
02:02:40,320 --> 02:02:43,480
 based on the information of your training data.

1701
02:02:44,460 --> 02:02:46,520
 Because in the training data,

1702
02:02:46,520 --> 02:02:48,800
 we already have the north function.

1703
02:02:48,800 --> 02:02:53,800
 The training process is reduced the north function, right?

1704
02:02:53,880 --> 02:02:57,120
 So the training data does not get the information

1705
02:02:57,120 --> 02:02:58,360
 of the test data.

1706
02:02:58,360 --> 02:03:00,760
 So in the training process,

1707
02:03:00,760 --> 02:03:03,400
 we cannot solve the overfitting problem.

1708
02:03:04,240 --> 02:03:07,360
 Overfitting problem cannot be based on the information

1709
02:03:07,360 --> 02:03:09,960
 on the training data to solve it.

1710
02:03:10,920 --> 02:03:13,560
 Then to solve the overfitting,

1711
02:03:13,560 --> 02:03:14,680
 in my opinion,

1712
02:03:14,680 --> 02:03:17,800
 we must utilize some of our human knowledge

1713
02:03:17,800 --> 02:03:21,760
 to do the regularization in the training process.

1714
02:03:22,680 --> 02:03:24,960
 So what means regularization?

1715
02:03:24,960 --> 02:03:27,760
 Regularization is use our human knowledge

1716
02:03:27,760 --> 02:03:30,600
 to constrain your machine learning,

1717
02:03:30,600 --> 02:03:35,600
 not net the machine freely learn from the training data,

1718
02:03:37,920 --> 02:03:41,320
 to constrain the learning or restrict the learning.

1719
02:03:41,320 --> 02:03:45,720
 This is some kind of the so-called regularization.

1720
02:03:45,720 --> 02:03:49,200
 This regularization cannot be based on the training data,

1721
02:03:49,200 --> 02:03:54,200
 must be based on our human to design the model

1722
02:03:54,200 --> 02:03:57,880
 to constrain the machine learn from the training data.

1723
02:03:59,400 --> 02:04:01,160
 Now, before we go to the sitting end,

1724
02:04:01,160 --> 02:04:03,800
 we will see what is the other possible way

1725
02:04:03,800 --> 02:04:06,400
 to do the regularization.

1726
02:04:06,400 --> 02:04:08,560
 Here, I just show some examples.

1727
02:04:08,560 --> 02:04:11,040
 For example, the machine learning or training

1728
02:04:11,040 --> 02:04:14,440
 is to minimize your north function, right?

1729
02:04:14,440 --> 02:04:17,519
 Okay, or here is a mean square arrow.

1730
02:04:17,519 --> 02:04:19,599
 Now, this is a machine learning,

1731
02:04:19,599 --> 02:04:23,559
 the purpose of the machine is always to make this decrease

1732
02:04:23,559 --> 02:04:27,040
 to decrease the value of the north function.

1733
02:04:27,040 --> 02:04:29,719
 Now, during the learning process,

1734
02:04:29,719 --> 02:04:33,639
 we can constrain the net function f hat

1735
02:04:33,639 --> 02:04:36,440
 to certain boundary.

1736
02:04:36,440 --> 02:04:40,599
 For example, we constrain the f must be a linear function.

1737
02:04:41,440 --> 02:04:46,280
 Okay, so this is one way to do the regularization

1738
02:04:46,280 --> 02:04:47,560
 to constrain the machine,

1739
02:04:47,560 --> 02:04:50,200
 net function must be a linear function

1740
02:04:50,200 --> 02:04:53,720
 or must be just a classic function.

1741
02:04:55,320 --> 02:04:59,200
 It is possible we can constrain the linear function

1742
02:04:59,200 --> 02:05:01,280
 or none a classic function.

1743
02:05:01,280 --> 02:05:04,920
 This is some kind of the regularization.

1744
02:05:04,920 --> 02:05:07,760
 Now, another example of the regularization is

1745
02:05:07,760 --> 02:05:10,400
 we can use the dimension learning reduction

1746
02:05:10,400 --> 02:05:13,160
 to first reduce the high dimensional data

1747
02:05:13,160 --> 02:05:15,240
 into a no dimension.

1748
02:05:15,240 --> 02:05:19,320
 And then after that, to net machine to learn,

1749
02:05:19,320 --> 02:05:22,480
 this to reduce this north function only

1750
02:05:22,480 --> 02:05:25,400
 in the no dimensional space.

1751
02:05:26,440 --> 02:05:29,440
 So the dimension learning reduction, in fact,

1752
02:05:29,440 --> 02:05:32,360
 can take a role of the regularization

1753
02:05:32,360 --> 02:05:35,200
 to recognize the machine learning.

1754
02:05:35,200 --> 02:05:39,400
 Okay, not to net the machine freely,

1755
02:05:39,400 --> 02:05:41,920
 but to learn everything the machine wants

1756
02:05:41,920 --> 02:05:43,879
 from the training data.

1757
02:05:45,960 --> 02:05:50,559
 Now, the last way is somehow during the optimization

1758
02:05:50,559 --> 02:05:53,679
 or machine learning process to minimize the north function,

1759
02:05:53,679 --> 02:05:57,040
 we don't just minimize the north.

1760
02:05:57,040 --> 02:06:01,400
 We can utilize some form of the learned,

1761
02:06:01,400 --> 02:06:05,360
 some regularization of the learned function f hat

1762
02:06:06,320 --> 02:06:09,880
 to minimize them at the same time.

1763
02:06:09,880 --> 02:06:14,880
 For example, we can make the coefficient of the function

1764
02:06:15,240 --> 02:06:18,280
 or the parameter of the function is a sparse,

1765
02:06:19,320 --> 02:06:23,360
 such as sparse coding or sparse representation.

1766
02:06:23,360 --> 02:06:26,519
 This is also a very hot research topic

1767
02:06:26,519 --> 02:06:28,759
 before the deep learning.

1768
02:06:28,759 --> 02:06:31,759
 Okay, before the deep learning in the machine learning area,

1769
02:06:31,759 --> 02:06:33,400
 we have a hot research topic.

1770
02:06:33,519 --> 02:06:35,160
 One is sparse coding,

1771
02:06:35,160 --> 02:06:37,599
 another is dimension learning reduction.

1772
02:06:39,040 --> 02:06:42,200
 Although many research working in this area

1773
02:06:42,200 --> 02:06:44,160
 don't aware of what is the purpose

1774
02:06:44,160 --> 02:06:47,360
 of the dimension learning reduction or sparse coding,

1775
02:06:47,360 --> 02:06:52,360
 in my opinion, they will take some role of the regularization

1776
02:06:52,400 --> 02:06:55,000
 to make the machine learn the result

1777
02:06:55,000 --> 02:06:58,440
 not to overfeed your training data.

1778
02:06:59,440 --> 02:07:03,440
 Then let's see what is the solution of the CNN.

1779
02:07:05,440 --> 02:07:09,160
 Now to find how the CNN solves the problem

1780
02:07:09,160 --> 02:07:14,160
 of this multi-nuclear neural network or MARP,

1781
02:07:14,360 --> 02:07:17,040
 we need to study what is the difference

1782
02:07:17,040 --> 02:07:21,440
 between the CNN and the MARP, right?

1783
02:07:21,440 --> 02:07:25,200
 Then to study the difference between the CNN and MARP,

1784
02:07:25,280 --> 02:07:29,880
 we will simplify the MARP to just one relation,

1785
02:07:29,880 --> 02:07:32,679
 O equals to W transpose X.

1786
02:07:33,519 --> 02:07:38,519
 Although the MARP has this multi-nuclear structure,

1787
02:07:40,280 --> 02:07:44,120
 but as I mentioned that all this network mathematically

1788
02:07:44,120 --> 02:07:47,200
 can be expressed by this one, right?

1789
02:07:47,200 --> 02:07:49,679
 Now from this one, we can say each layer,

1790
02:07:49,679 --> 02:07:53,840
 every layer is always the same model,

1791
02:07:53,840 --> 02:07:56,920
 input multiplied by a weak matrix

1792
02:07:56,920 --> 02:08:00,080
 goes through a activation function, gets the output.

1793
02:08:01,160 --> 02:08:06,160
 For input K, and output K is K plus one.

1794
02:08:06,600 --> 02:08:09,680
 So this is applied to every layer,

1795
02:08:09,680 --> 02:08:12,280
 every layer of the network.

1796
02:08:12,280 --> 02:08:14,960
 So we only study one layer.

1797
02:08:14,960 --> 02:08:16,480
 Okay, we can study just one,

1798
02:08:16,480 --> 02:08:19,600
 because all layer are identical,

1799
02:08:19,600 --> 02:08:21,960
 the mathematical model are identical.

1800
02:08:22,800 --> 02:08:27,160
 Well, in one layer of the network,

1801
02:08:27,160 --> 02:08:30,120
 it comes output and input, right?

1802
02:08:30,120 --> 02:08:34,720
 So the most important sense is this multiplication,

1803
02:08:34,720 --> 02:08:36,240
 matrix multiplication,

1804
02:08:36,240 --> 02:08:41,240
 because this activation function, it is not learnable.

1805
02:08:41,360 --> 02:08:44,480
 The activation function is fixed.

1806
02:08:44,480 --> 02:08:48,800
 The activation function is not learned from the training data.

1807
02:08:48,800 --> 02:08:53,760
 The learned parameter is just this W, okay?

1808
02:08:53,760 --> 02:08:58,760
 So the key role of this neural network

1809
02:08:59,160 --> 02:09:03,040
 is just the input vector multiplied by a matrix,

1810
02:09:03,880 --> 02:09:06,040
 because after that it's just go through

1811
02:09:06,040 --> 02:09:08,480
 a fixed non-linear function.

1812
02:09:08,480 --> 02:09:10,680
 This activation function is fixed,

1813
02:09:10,680 --> 02:09:15,160
 not learnable, not learn from the training data.

1814
02:09:15,160 --> 02:09:18,520
 It's nothing to do with what is your training data.

1815
02:09:18,560 --> 02:09:22,360
 Okay, the learnable part is just this W.

1816
02:09:22,360 --> 02:09:24,400
 This W is a matrix.

1817
02:09:24,400 --> 02:09:28,600
 It contains all parameter of one layers, right?

1818
02:09:29,880 --> 02:09:31,560
 Okay, then we can use,

1819
02:09:31,560 --> 02:09:34,800
 utilize this relation between input and output

1820
02:09:34,800 --> 02:09:39,800
 to represent the basic function of a multi-layer neural network.

1821
02:09:42,560 --> 02:09:45,160
 Then what is the thing?

1822
02:09:45,160 --> 02:09:50,160
 Now, of course, we know the structure of the CNN,

1823
02:09:50,680 --> 02:09:54,160
 because CNN is first very successfully utilized

1824
02:09:54,160 --> 02:09:57,680
 in the image processing, image recognition.

1825
02:09:57,680 --> 02:10:02,680
 So we represent the CNN in terms of recognize the image.

1826
02:10:04,920 --> 02:10:09,480
 Now, I just very quickly go through the principle

1827
02:10:09,480 --> 02:10:11,200
 of the CNN.

1828
02:10:11,200 --> 02:10:13,760
 Okay, so basically from here we can say

1829
02:10:13,760 --> 02:10:16,360
 given an input image,

1830
02:10:16,360 --> 02:10:20,760
 if we have a filter just a small three by three filter,

1831
02:10:20,760 --> 02:10:24,280
 if we need apply this filter to the image,

1832
02:10:24,280 --> 02:10:29,280
 then of course we can produce the output image, right?

1833
02:10:29,840 --> 02:10:33,680
 Given a filter, then the filter running through all position

1834
02:10:33,680 --> 02:10:37,440
 apply to the input image or get the output image.

1835
02:10:37,440 --> 02:10:40,880
 Of course, the relation of the output image

1836
02:10:40,880 --> 02:10:43,080
 and the input image is a convolution.

1837
02:10:43,080 --> 02:10:46,480
 The input image convolve with the filter mask,

1838
02:10:46,480 --> 02:10:49,960
 then produce the output image, right?

1839
02:10:49,960 --> 02:10:54,080
 Now, given an input image and a filter,

1840
02:10:54,080 --> 02:10:56,280
 we can produce one output image.

1841
02:10:56,280 --> 02:11:00,160
 If we use many different filter apply

1842
02:11:00,160 --> 02:11:01,880
 to the same input image,

1843
02:11:01,880 --> 02:11:04,120
 then we can produce many output,

1844
02:11:04,120 --> 02:11:06,600
 different output image, right?

1845
02:11:07,519 --> 02:11:10,080
 Okay, so this is a convolution.

1846
02:11:10,080 --> 02:11:12,920
 In the CNN, the convolutional layer

1847
02:11:12,920 --> 02:11:16,360
 is just an input image goes through a filter

1848
02:11:16,360 --> 02:11:18,920
 to produce the output image.

1849
02:11:18,920 --> 02:11:21,720
 Of course, we can produce many output image,

1850
02:11:21,720 --> 02:11:24,040
 use different filter.

1851
02:11:24,040 --> 02:11:27,720
 And all this filter parameter, this filter mask

1852
02:11:27,720 --> 02:11:30,520
 is not designed by our human,

1853
02:11:30,520 --> 02:11:33,840
 is learned from the training data.

1854
02:11:33,840 --> 02:11:36,360
 Okay, then this is a convolutional layer.

1855
02:11:38,040 --> 02:11:41,320
 Now, from here we can say for the convolutional layer,

1856
02:11:41,440 --> 02:11:45,440
 one input map can produce multiple output map.

1857
02:11:45,440 --> 02:11:47,240
 A map is just an image, right?

1858
02:11:47,240 --> 02:11:49,719
 Here we call it the map, okay?

1859
02:11:49,719 --> 02:11:52,880
 Then we increase the number of the image,

1860
02:11:52,880 --> 02:11:56,880
 we can increase to a very large number of the image, right?

1861
02:11:57,920 --> 02:12:01,200
 So the information in the output image

1862
02:12:01,200 --> 02:12:04,080
 could be redundant, okay?

1863
02:12:04,080 --> 02:12:07,599
 So because we increase the number of the image

1864
02:12:07,599 --> 02:12:09,719
 in this convolution process,

1865
02:12:09,720 --> 02:12:14,120
 then we can reduce the size of the image.

1866
02:12:14,120 --> 02:12:17,640
 So we can have a so-called pooling layer.

1867
02:12:17,640 --> 02:12:20,320
 Here I show a pooling layer is a two by two,

1868
02:12:20,320 --> 02:12:22,880
 it's just for every two by two window,

1869
02:12:22,880 --> 02:12:27,000
 we take one pixel, we take the maximum value,

1870
02:12:27,000 --> 02:12:29,720
 we then this two by two neighborhood,

1871
02:12:29,720 --> 02:12:32,200
 so that we're sampling the image

1872
02:12:32,200 --> 02:12:35,060
 into the half of the size.

1873
02:12:35,060 --> 02:12:37,400
 This is a pooling, okay?

1874
02:12:37,400 --> 02:12:40,440
 One way to do the pooling is the maximum pooling,

1875
02:12:40,440 --> 02:12:42,000
 we take the maximum value,

1876
02:12:43,040 --> 02:12:48,040
 we then a small window to just take one pixel value.

1877
02:12:48,960 --> 02:12:50,320
 So this is the pooling.

1878
02:12:50,320 --> 02:12:54,480
 The pooling will reduce the size of the image

1879
02:12:54,480 --> 02:12:57,320
 or reduce the size of the map, right?

1880
02:12:58,759 --> 02:13:02,320
 Now, if we use the convolution that is a field output

1881
02:13:02,320 --> 02:13:06,759
 and then to do the pooling to reduce the map size,

1882
02:13:06,760 --> 02:13:09,200
 then after several layers,

1883
02:13:09,200 --> 02:13:13,680
 we can produce many output image or output map.

1884
02:13:13,680 --> 02:13:16,320
 For example, here is a 500,

1885
02:13:16,320 --> 02:13:18,600
 more than 500 output image,

1886
02:13:18,600 --> 02:13:22,760
 but each image or map, the size is reduced

1887
02:13:22,760 --> 02:13:26,560
 by could be 15 by 15 or even smaller.

1888
02:13:27,960 --> 02:13:31,720
 Okay, so this is the process of the convolutional neural network.

1889
02:13:32,640 --> 02:13:34,160
 This is a convolution part.

1890
02:13:34,160 --> 02:13:35,560
 After that, of course,

1891
02:13:35,560 --> 02:13:38,520
 at the end we need to do a classification.

1892
02:13:38,520 --> 02:13:43,680
 At the beginning, people also use the two layers

1893
02:13:43,680 --> 02:13:47,040
 or multi-layer neural network or fully connected network

1894
02:13:47,040 --> 02:13:49,080
 to do the classification.

1895
02:13:49,080 --> 02:13:54,080
 But now, because the convolution layer is very powerful,

1896
02:13:54,640 --> 02:13:58,720
 after that, the class, final layer is just a linear

1897
02:13:59,600 --> 02:14:00,760
 combination.

1898
02:14:00,760 --> 02:14:02,520
 We can do the classification.

1899
02:14:02,520 --> 02:14:05,160
 We don't need the neural network.

1900
02:14:05,160 --> 02:14:07,400
 We just use a linear classifier.

1901
02:14:09,080 --> 02:14:12,080
 It can classify the output feature

1902
02:14:12,080 --> 02:14:15,240
 of the convolution layer very well.

1903
02:14:15,240 --> 02:14:19,360
 So you can ignore this part of the convolution

1904
02:14:19,360 --> 02:14:21,680
 of the neural network.

1905
02:14:21,680 --> 02:14:25,440
 We can just use a very simple linear classifier, no problem.

1906
02:14:26,480 --> 02:14:30,200
 Now, the very powerful one of the convolution neural network

1907
02:14:30,200 --> 02:14:34,480
 is this convolution and the convolution and so on, right?

1908
02:14:34,480 --> 02:14:38,759
 To get the output from the input go to a theater.

1909
02:14:40,559 --> 02:14:44,080
 Now, of course, we have the pooling layer reduce the size.

1910
02:14:44,080 --> 02:14:46,240
 Now, what is the role of this one?

1911
02:14:46,240 --> 02:14:48,919
 We first need to understand the convolution

1912
02:14:48,919 --> 02:14:52,160
 is not exactly same as the convolution

1913
02:14:52,160 --> 02:14:55,000
 in the image processing or single processing,

1914
02:14:55,000 --> 02:14:56,839
 but very similar.

1915
02:14:56,839 --> 02:14:59,559
 One difference is in the convolution,

1916
02:15:00,519 --> 02:15:02,200
 it is not very strictly convolution

1917
02:15:02,200 --> 02:15:06,920
 because as I mentioned that for all neural network,

1918
02:15:06,920 --> 02:15:10,240
 the output must have a bias.

1919
02:15:11,599 --> 02:15:15,400
 Any field, besides we have for example,

1920
02:15:15,400 --> 02:15:18,240
 three by three field, we have the three by three field

1921
02:15:18,240 --> 02:15:22,000
 mask convolve with the input,

1922
02:15:22,000 --> 02:15:25,200
 we must also use a bias.

1923
02:15:25,200 --> 02:15:27,519
 Okay, so don't forget the bias.

1924
02:15:27,519 --> 02:15:30,480
 This is very important, okay?

1925
02:15:30,480 --> 02:15:35,480
 Another difference is that usually we apply the convolution,

1926
02:15:35,719 --> 02:15:39,480
 apply the field only to one image, right?

1927
02:15:39,480 --> 02:15:43,200
 But here the input could have multiple image.

1928
02:15:43,200 --> 02:15:48,200
 We need all this multiple image together go to a theater.

1929
02:15:48,919 --> 02:15:51,679
 Okay, well for all different image,

1930
02:15:51,679 --> 02:15:55,919
 in terms of the theater, it is fully connected.

1931
02:15:55,919 --> 02:15:59,120
 So this convolution process in fact mathematically

1932
02:15:59,160 --> 02:16:00,960
 can write in this one.

1933
02:16:00,960 --> 02:16:05,960
 Okay, given the input is image xij is one image,

1934
02:16:06,160 --> 02:16:10,160
 but we have many image or many map as an input, right?

1935
02:16:10,160 --> 02:16:15,160
 So we have xijk, i and j is a different spatial position,

1936
02:16:16,240 --> 02:16:20,740
 k is a different channel or different image, different map.

1937
02:16:21,640 --> 02:16:25,519
 This one goes through the convolution process,

1938
02:16:25,520 --> 02:16:30,520
 the produce yijk, where ij same is the position,

1939
02:16:31,680 --> 02:16:33,880
 spatial position, different picture,

1940
02:16:33,880 --> 02:16:37,440
 k is a different image or different channel.

1941
02:16:37,440 --> 02:16:42,440
 Okay, now then to produce the output image or output map,

1942
02:16:44,240 --> 02:16:48,360
 the i and the j is a spatial position, right?

1943
02:16:48,360 --> 02:16:51,840
 This i and the j is utilized in the convolution.

1944
02:16:51,840 --> 02:16:56,600
 So w a m multiplied i minus a j minus a m.

1945
02:16:56,600 --> 02:17:00,680
 This is a formula of the convolution, right?

1946
02:17:00,680 --> 02:17:05,680
 And then the k and n here is the sum overall

1947
02:17:05,700 --> 02:17:10,700
 so that along the channel it is fully connected.

1948
02:17:10,760 --> 02:17:15,200
 Okay, fully connected, but spatially it's just

1949
02:17:15,200 --> 02:17:19,799
 connect a local area of the, within the theater window.

1950
02:17:20,639 --> 02:17:23,400
 Okay, so anyway, this is a mathematical expression

1951
02:17:23,400 --> 02:17:26,039
 of the input output and the input

1952
02:17:26,039 --> 02:17:28,799
 of the convolutional layer, right?

1953
02:17:31,199 --> 02:17:33,920
 Now this convolution just slightly different

1954
02:17:33,920 --> 02:17:36,719
 from the normal convolution for the image processing

1955
02:17:36,719 --> 02:17:40,439
 or singular processing, but anyway each way,

1956
02:17:40,439 --> 02:17:43,679
 in terms of the i and the j, the spatial position,

1957
02:17:43,679 --> 02:17:48,279
 it is a really a convolution process or fielding process.

1958
02:17:49,200 --> 02:17:54,200
 Now, because we have multiple inputs into the theater,

1959
02:17:55,280 --> 02:17:58,200
 so in this kind of the convolution,

1960
02:17:58,200 --> 02:18:03,200
 it is, we could select or we could choose a theater,

1961
02:18:05,720 --> 02:18:10,720
 the spatial size is just one by one, one by one.

1962
02:18:10,880 --> 02:18:15,880
 So in the convolutional neural network,

1963
02:18:16,879 --> 02:18:21,879
 it will make sense we have so-called one by one convolution.

1964
02:18:22,879 --> 02:18:25,639
 One by one convolution is just spatially,

1965
02:18:25,639 --> 02:18:30,439
 it is just one point from input come to the output,

1966
02:18:30,439 --> 02:18:33,959
 but one point is just spatially one point, okay?

1967
02:18:33,959 --> 02:18:38,959
 But we can combine, we combine different channel,

1968
02:18:39,000 --> 02:18:44,000
 use a different filter parameter along different channel.

1969
02:18:44,000 --> 02:18:46,559
 It's still a weighted sound, right?

1970
02:18:46,559 --> 02:18:51,559
 So here we understand we can use one by one convolution.

1971
02:18:52,520 --> 02:18:57,520
 Okay, now in this network structure of the convolution,

1972
02:18:59,760 --> 02:19:01,639
 basically the network structure,

1973
02:19:01,639 --> 02:19:06,160
 it seems architecture different from the fully connection

1974
02:19:06,160 --> 02:19:08,879
 network and then the output and the input,

1975
02:19:08,879 --> 02:19:11,920
 basically it is some kind of the convolution, right?

1976
02:19:11,920 --> 02:19:14,280
 We also use a simple nonlinear function

1977
02:19:14,280 --> 02:19:18,480
 and we also somehow reduce the size.

1978
02:19:18,480 --> 02:19:21,760
 We also utilize many layers,

1979
02:19:21,760 --> 02:19:25,520
 but all this is not very critical.

1980
02:19:25,520 --> 02:19:29,320
 The critical one is the convolution.

1981
02:19:29,320 --> 02:19:34,320
 So to see the difference between the Cn and MaRp

1982
02:19:35,719 --> 02:19:39,559
 is to study the difference between the convolution

1983
02:19:39,559 --> 02:19:42,519
 and fully connect the weighted summation.

1984
02:19:43,439 --> 02:19:48,160
 Okay, so here we can study the one dimensional case.

1985
02:19:48,160 --> 02:19:51,519
 Suppose we can give even the input

1986
02:19:51,519 --> 02:19:54,160
 is a two dimensional image, right?

1987
02:19:54,160 --> 02:19:58,439
 We can always arrange all this picture into one vector.

1988
02:19:58,439 --> 02:20:03,240
 Okay, then we can, or mathematically it is possible

1989
02:20:03,240 --> 02:20:05,600
 to even for the two dimensional image,

1990
02:20:05,600 --> 02:20:09,480
 we can use a one vector to represent it.

1991
02:20:09,480 --> 02:20:11,840
 For example, we can arrange all these two dimensional

1992
02:20:11,840 --> 02:20:15,680
 picture into just one nonvectors.

1993
02:20:15,680 --> 02:20:19,279
 Okay, so because we want to compile the difference

1994
02:20:19,279 --> 02:20:22,439
 between the fully connected network

1995
02:20:22,439 --> 02:20:24,600
 and the convolution network.

1996
02:20:25,720 --> 02:20:30,720
 So that means if we study just one dimensional convolution,

1997
02:20:31,760 --> 02:20:34,320
 then what is the difference between the convolution

1998
02:20:34,320 --> 02:20:37,160
 and the MaRp?

1999
02:20:37,160 --> 02:20:41,119
 Now we will study the definition of the convolution.

2000
02:20:41,119 --> 02:20:43,280
 For all convolutional layer,

2001
02:20:43,280 --> 02:20:47,840
 the output picture is the input convolve

2002
02:20:47,840 --> 02:20:50,000
 with the filter parameter.

2003
02:20:50,000 --> 02:20:51,920
 This is the convolution.

2004
02:20:51,920 --> 02:20:55,920
 Well, from the definition, this is a convolution.

2005
02:20:55,920 --> 02:20:58,360
 Okay, from this definition of the convolution,

2006
02:20:58,360 --> 02:21:03,360
 we can see if we choose a small convolution kernel,

2007
02:21:03,840 --> 02:21:08,640
 that means this G is only a few nonzero element.

2008
02:21:08,640 --> 02:21:12,520
 If we, the filter window is small,

2009
02:21:12,520 --> 02:21:15,120
 but we can always increase the filter window

2010
02:21:15,120 --> 02:21:18,280
 to a same size as the X,

2011
02:21:18,280 --> 02:21:23,280
 but we take the zero value as the filter parameter.

2012
02:21:23,360 --> 02:21:24,960
 They are equivalent.

2013
02:21:24,960 --> 02:21:27,760
 So it is not difficult to say

2014
02:21:27,760 --> 02:21:32,760
 that the convolution process in fact is also a weighted sum.

2015
02:21:33,440 --> 02:21:37,600
 Only this weighted sum is within the filter window.

2016
02:21:37,600 --> 02:21:40,200
 Within the filter window is weighted sum.

2017
02:21:40,200 --> 02:21:42,640
 If we extend the filter window

2018
02:21:42,640 --> 02:21:45,280
 into the same size of the input,

2019
02:21:45,280 --> 02:21:48,840
 then this weighted sum can be right as

2020
02:21:50,000 --> 02:21:52,720
 two vector multiplied together.

2021
02:21:52,720 --> 02:21:54,080
 It's a convolution.

2022
02:21:54,080 --> 02:21:59,080
 But this vector of the filter, we have meaning zero.

2023
02:21:59,720 --> 02:22:01,480
 Only within the filter window,

2024
02:22:01,480 --> 02:22:04,480
 it is nonzero filter parameter.

2025
02:22:04,480 --> 02:22:07,760
 Outside of the window is zero.

2026
02:22:07,760 --> 02:22:12,400
 But we can still express the convolution result

2027
02:22:12,400 --> 02:22:15,600
 by the multiplication of the two vector.

2028
02:22:18,520 --> 02:22:20,840
 Now, if we're doing so, we can see

2029
02:22:20,840 --> 02:22:23,400
 this multiplication of the two vector,

2030
02:22:23,400 --> 02:22:25,880
 the weighted vector or the filter parameter

2031
02:22:25,880 --> 02:22:29,000
 and all input multiplied together

2032
02:22:29,000 --> 02:22:32,680
 just produce a single pixel output.

2033
02:22:33,520 --> 02:22:35,320
 A single output, right?

2034
02:22:35,320 --> 02:22:40,120
 Now for different J, then this WJ will be different.

2035
02:22:40,120 --> 02:22:42,000
 Then from the convolution process,

2036
02:22:42,000 --> 02:22:46,680
 we know that for different J is we move

2037
02:22:46,680 --> 02:22:49,600
 this filter window into different position.

2038
02:22:50,640 --> 02:22:54,360
 So what I want to say is the convolution

2039
02:22:54,360 --> 02:22:57,040
 can be represented by the multiplication

2040
02:22:57,040 --> 02:22:58,840
 of the two vectors.

2041
02:22:58,840 --> 02:23:03,840
 One vector is input, include all input value component.

2042
02:23:04,240 --> 02:23:09,040
 A lot of vector is a vector of the filter parameter.

2043
02:23:09,040 --> 02:23:11,400
 We extend the filter parameter

2044
02:23:11,400 --> 02:23:16,040
 outside of the filter window by zero, right?

2045
02:23:16,040 --> 02:23:19,840
 So this vector multiplied, this input vector

2046
02:23:19,840 --> 02:23:22,160
 will produce a single output.

2047
02:23:22,160 --> 02:23:24,480
 This is the same as a convolution,

2048
02:23:24,480 --> 02:23:26,480
 but produce a single output.

2049
02:23:26,480 --> 02:23:31,000
 For other output is just move this filter window

2050
02:23:31,000 --> 02:23:32,720
 into different position.

2051
02:23:34,080 --> 02:23:38,400
 So if we shift or translate this filter window

2052
02:23:38,400 --> 02:23:40,080
 into different position,

2053
02:23:40,080 --> 02:23:43,439
 then we'll produce different output J.

2054
02:23:45,279 --> 02:23:48,560
 Now, if we want to put all this output J

2055
02:23:48,560 --> 02:23:50,920
 for different J into one vector,

2056
02:23:51,320 --> 02:23:55,920
 okay, then we need have mainly this WJ

2057
02:23:58,200 --> 02:24:02,040
 for this filter at a different position.

2058
02:24:02,040 --> 02:24:04,760
 So we have a matrix.

2059
02:24:04,760 --> 02:24:09,760
 This matrix, each column is the filter at specific position.

2060
02:24:11,920 --> 02:24:16,920
 Different column is same filter at a different position.

2061
02:24:17,600 --> 02:24:21,040
 Then this matrix multiplied by input

2062
02:24:21,040 --> 02:24:23,680
 will produce the output vector.

2063
02:24:23,680 --> 02:24:28,680
 This output vector indicate all output of one image

2064
02:24:29,080 --> 02:24:33,400
 because one image produced by one filter.

2065
02:24:33,400 --> 02:24:38,400
 Okay, so we can see that for one output map

2066
02:24:38,400 --> 02:24:43,160
 can be also represent by input multiplied by

2067
02:24:44,160 --> 02:24:46,560
 weighting matrix.

2068
02:24:47,400 --> 02:24:51,080
 Now for other output image use different filter.

2069
02:24:51,080 --> 02:24:56,080
 We can just put all this different matrix,

2070
02:24:56,160 --> 02:24:57,400
 this kind of the matrix,

2071
02:24:57,400 --> 02:25:02,400
 but this G are different to increase size of this W.

2072
02:25:03,520 --> 02:25:08,039
 Then all output, even different map

2073
02:25:08,039 --> 02:25:13,039
 can be expressed by the input multiplied by matrix.

2074
02:25:13,560 --> 02:25:18,560
 But this matrix is come from mainly this sub matrix.

2075
02:25:22,760 --> 02:25:26,000
 Where each matrix, sub matrix is one filter.

2076
02:25:26,000 --> 02:25:30,600
 Where one filter indicate within the filter window,

2077
02:25:30,600 --> 02:25:32,720
 small window, it is non-zero,

2078
02:25:32,720 --> 02:25:35,760
 or other outside it is zero

2079
02:25:35,760 --> 02:25:39,800
 because this size is same as the size of the input image.

2080
02:25:39,800 --> 02:25:41,840
 The filter window is very small.

2081
02:25:41,840 --> 02:25:45,680
 Now the different output is just moving this filter window

2082
02:25:45,680 --> 02:25:47,280
 into different position.

2083
02:25:48,400 --> 02:25:52,800
 Then we can express the convolution network.

2084
02:25:52,800 --> 02:25:57,800
 The output is also the input multiplied by weighting matrix.

2085
02:25:58,800 --> 02:26:03,800
 So in such a way we can see that both the convolutional

2086
02:26:03,960 --> 02:26:08,400
 layer and the fully connected layer, the MARP,

2087
02:26:08,400 --> 02:26:13,400
 both can be represent by a same mathematical expression.

2088
02:26:17,080 --> 02:26:19,800
 But what is the difference?

2089
02:26:19,800 --> 02:26:23,240
 If we express the output from the input

2090
02:26:23,240 --> 02:26:25,880
 by same mathematical expression,

2091
02:26:25,880 --> 02:26:28,800
 then what is this W for?

2092
02:26:29,720 --> 02:26:32,600
 What is this W for the MARP?

2093
02:26:32,600 --> 02:26:35,240
 What is this W for convolutional layer?

2094
02:26:36,240 --> 02:26:41,080
 For MARP, all element in this W

2095
02:26:41,080 --> 02:26:44,080
 is learned from the training data.

2096
02:26:45,520 --> 02:26:49,199
 Well, for the convolution network,

2097
02:26:49,199 --> 02:26:51,880
 this W is in this shape.

2098
02:26:53,560 --> 02:26:57,440
 Okay, so that means for the convolutional layer,

2099
02:26:57,440 --> 02:27:02,280
 this transform matrix W, we have many zeros.

2100
02:27:02,280 --> 02:27:06,080
 We force the network parameter zero,

2101
02:27:06,080 --> 02:27:08,560
 not learned from the training data.

2102
02:27:09,560 --> 02:27:13,440
 And even the learned from the training data filter

2103
02:27:13,440 --> 02:27:16,840
 filter parameter, we make this same filter,

2104
02:27:16,840 --> 02:27:21,680
 same parameter repeat in many different column.

2105
02:27:22,680 --> 02:27:25,400
 Okay, all of these are exactly same,

2106
02:27:25,400 --> 02:27:27,840
 learned from the training data.

2107
02:27:27,840 --> 02:27:32,440
 This is the difference between the convolution

2108
02:27:32,440 --> 02:27:34,720
 and the fully connected layer.

2109
02:27:37,160 --> 02:27:41,280
 Now from here, we can see what is the difference.

2110
02:27:41,280 --> 02:27:46,280
 First of all, we see the same is just a simplified MARP.

2111
02:27:46,760 --> 02:27:49,800
 We simplify the learned parameter

2112
02:27:49,800 --> 02:27:54,800
 into very sparse network, sparse matrix.

2113
02:27:55,279 --> 02:27:59,160
 We force many element not learned from the training data,

2114
02:27:59,160 --> 02:28:01,679
 force them into a zero.

2115
02:28:01,679 --> 02:28:06,039
 We only need a few parameter learned from the training data.

2116
02:28:06,039 --> 02:28:10,400
 And this few parameter will further constrain them,

2117
02:28:10,400 --> 02:28:12,840
 must be identical same.

2118
02:28:14,359 --> 02:28:18,840
 This is a very strong regularization.

2119
02:28:18,840 --> 02:28:21,160
 We recognize this machine learning,

2120
02:28:21,160 --> 02:28:23,160
 the way of the machine learning.

2121
02:28:23,960 --> 02:28:27,640
 Okay, force the machine learning following this rule

2122
02:28:27,640 --> 02:28:29,680
 to learn from the training data.

2123
02:28:30,800 --> 02:28:35,800
 So this thing is to simplify this freely learned network

2124
02:28:37,400 --> 02:28:42,240
 into a constrained network to learn from the training data.

2125
02:28:44,560 --> 02:28:47,920
 The simplification is some kind of the regularization.

2126
02:28:47,920 --> 02:28:51,600
 This regularization will not have the parameter

2127
02:28:51,600 --> 02:28:53,640
 not learned from the training data,

2128
02:28:53,640 --> 02:28:56,040
 is some kind of the regularization.

2129
02:28:56,040 --> 02:29:01,040
 This effect is somehow reduce the overfitting problem.

2130
02:29:04,480 --> 02:29:09,480
 Okay, now how this thing is different

2131
02:29:10,560 --> 02:29:15,560
 from the traditional neural network before we go to,

2132
02:29:15,560 --> 02:29:18,280
 further go to this regularization,

2133
02:29:18,280 --> 02:29:21,320
 it will reduce the overfitting.

2134
02:29:21,320 --> 02:29:25,640
 We first introduce some nice property of the CNN.

2135
02:29:25,640 --> 02:29:29,880
 So first the CNN is some kind of a filter, right?

2136
02:29:29,880 --> 02:29:33,040
 Output is input goes through a filter,

2137
02:29:33,040 --> 02:29:35,440
 where the filter is a small window.

2138
02:29:35,440 --> 02:29:40,120
 So this small filter size will force all other parameter

2139
02:29:40,120 --> 02:29:44,560
 outside of the filter window zero to produce the output, right?

2140
02:29:44,560 --> 02:29:48,080
 So because of that, the output of CNN

2141
02:29:48,080 --> 02:29:53,080
 always captures the local structure of the input image.

2142
02:29:54,000 --> 02:29:56,160
 Because given an input image,

2143
02:29:56,160 --> 02:29:58,279
 we have many local structure,

2144
02:29:58,279 --> 02:30:01,920
 such as corner, edge, and the different shape,

2145
02:30:01,920 --> 02:30:05,080
 the shape of the small blob, right?

2146
02:30:05,080 --> 02:30:10,080
 So a filter where output will capture this local structure.

2147
02:30:10,840 --> 02:30:15,880
 How it captures a local structure?

2148
02:30:15,880 --> 02:30:19,960
 Because the filter is learned from the training data, right?

2149
02:30:19,960 --> 02:30:22,400
 The convolution in fact mathematically

2150
02:30:22,400 --> 02:30:25,080
 is the same as the correlation.

2151
02:30:27,080 --> 02:30:29,320
 So because it is a correlation,

2152
02:30:29,320 --> 02:30:32,480
 then somehow the filter parameter,

2153
02:30:32,480 --> 02:30:35,160
 filter is also a small image.

2154
02:30:35,160 --> 02:30:37,520
 It will learn from the training data

2155
02:30:37,520 --> 02:30:42,520
 so that the filter mask is similar to some local structure.

2156
02:30:46,400 --> 02:30:51,400
 So that the output of the filter in the output image

2157
02:30:52,240 --> 02:30:57,240
 will be much value if the filter move to some place,

2158
02:30:58,280 --> 02:31:01,440
 the structure is same as the filter structure.

2159
02:31:02,480 --> 02:31:07,480
 Okay, so it will capture the local structure.

2160
02:31:07,480 --> 02:31:11,199
 If the filter is trained from a particular structure,

2161
02:31:11,199 --> 02:31:14,160
 then it will move the filter into different place.

2162
02:31:14,160 --> 02:31:18,000
 If the place is in the place, the input image,

2163
02:31:18,000 --> 02:31:20,920
 the local structure is similar to the filter,

2164
02:31:20,920 --> 02:31:22,760
 then the output is much.

2165
02:31:22,760 --> 02:31:26,600
 If for some place, the input image,

2166
02:31:26,600 --> 02:31:31,240
 local structure is totally different from the filter structure,

2167
02:31:31,240 --> 02:31:34,119
 then the output will be very small.

2168
02:31:34,120 --> 02:31:38,960
 Okay, so it will produce such kind of the effect.

2169
02:31:40,840 --> 02:31:44,280
 So this effect is because the convolution process

2170
02:31:44,280 --> 02:31:46,560
 is some kind of the correlation.

2171
02:31:51,600 --> 02:31:55,960
 Now, the filter is learned from the training data,

2172
02:31:55,960 --> 02:31:59,440
 try to extract the local structure,

2173
02:31:59,440 --> 02:32:02,960
 such as the corner nine-h curve and so on,

2174
02:32:02,960 --> 02:32:05,519
 as much as possible.

2175
02:32:05,519 --> 02:32:09,080
 Here, we can show an example.

2176
02:32:09,080 --> 02:32:13,279
 Now, as I mentioned that the filter is also an image.

2177
02:32:13,279 --> 02:32:15,960
 We can display the filter mask,

2178
02:32:15,960 --> 02:32:18,640
 it's a similar to an image, right?

2179
02:32:18,640 --> 02:32:21,359
 Now, we have a CNN to

2180
02:32:22,400 --> 02:32:25,359
 make the CNN learn from the training data.

2181
02:32:25,359 --> 02:32:28,880
 Now, if the training data is the so-called,

2182
02:32:29,880 --> 02:32:32,439
 this example shows this training data

2183
02:32:32,439 --> 02:32:36,400
 is a so-called CT database.

2184
02:32:36,400 --> 02:32:39,880
 That means all training image is a picture

2185
02:32:39,880 --> 02:32:44,359
 of taken in the street of a city.

2186
02:32:44,359 --> 02:32:48,320
 So all the picture consists of many buildings, right?

2187
02:32:48,320 --> 02:32:51,920
 The buildings has a window, has door,

2188
02:32:51,920 --> 02:32:55,000
 but the structure of this window and door

2189
02:32:55,000 --> 02:32:59,320
 are almost corner nine-h and vertical-h,

2190
02:32:59,320 --> 02:33:01,840
 many such kind of the structure, right?

2191
02:33:01,840 --> 02:33:04,480
 Now, if we use such kind of the dataset

2192
02:33:04,480 --> 02:33:09,360
 to train a CNN, we will see the filter mask

2193
02:33:09,360 --> 02:33:13,080
 in different layer, or we'll show this pattern.

2194
02:33:13,080 --> 02:33:16,480
 This pattern are similar to the local pattern

2195
02:33:16,480 --> 02:33:17,960
 of your training data,

2196
02:33:19,200 --> 02:33:22,720
 the corner-h or the vertical-h.

2197
02:33:23,720 --> 02:33:26,439
 But if we use the same CNN,

2198
02:33:26,439 --> 02:33:31,119
 but train this CNN, use another dataset of the image.

2199
02:33:31,119 --> 02:33:34,000
 This dataset of the image is come from

2200
02:33:35,240 --> 02:33:39,560
 some kind of the food or fruit,

2201
02:33:39,560 --> 02:33:43,000
 because the food has many circle, round one.

2202
02:33:43,000 --> 02:33:45,560
 Then the trained CNN, the filter mask,

2203
02:33:45,560 --> 02:33:48,039
 the shape are totally different.

2204
02:33:48,039 --> 02:33:52,359
 We have many circle or some kind of the round structure.

2205
02:33:53,560 --> 02:33:58,560
 So here we verify that the CNN learning process

2206
02:33:59,599 --> 02:34:03,560
 is try to learn the filter mask similar

2207
02:34:03,560 --> 02:34:07,400
 to the most local structure of the training data.

2208
02:34:08,760 --> 02:34:13,760
 So that after the learning, if we give input to the CNN,

2209
02:34:16,039 --> 02:34:19,519
 then the CNN will capture the local structure.

2210
02:34:19,520 --> 02:34:24,160
 If the input image has some local structure similar

2211
02:34:24,160 --> 02:34:28,000
 to your trained filter, then the output is large.

2212
02:34:28,000 --> 02:34:31,800
 If the input has no such local structure,

2213
02:34:31,800 --> 02:34:35,880
 the output of the convolution will produce a small value.

2214
02:34:36,880 --> 02:34:39,440
 Okay, so this is why we call,

2215
02:34:39,440 --> 02:34:43,800
 we say that the CNN will capture this local structure.

2216
02:34:43,800 --> 02:34:45,960
 Well, local structure is very important

2217
02:34:46,720 --> 02:34:49,919
 to identify the different content of the image.

2218
02:34:53,000 --> 02:34:58,000
 Now, also the CNN, one filter is one local structure.

2219
02:35:01,480 --> 02:35:04,320
 Because one local structure in the input image

2220
02:35:04,320 --> 02:35:07,320
 may appear in many different positions.

2221
02:35:07,320 --> 02:35:11,080
 Well, the output image is a convolution of the input

2222
02:35:11,080 --> 02:35:12,720
 with the filter mask.

2223
02:35:12,720 --> 02:35:15,320
 So the filter mask in the convolution process,

2224
02:35:15,320 --> 02:35:19,480
 it will move to all possible position of the input.

2225
02:35:19,480 --> 02:35:24,080
 So this convolution process will extract

2226
02:35:24,080 --> 02:35:28,360
 all local structure at a different position, right?

2227
02:35:28,360 --> 02:35:30,600
 Because it is a convolution process.

2228
02:35:31,600 --> 02:35:35,160
 Now, input image has multiple structure,

2229
02:35:35,160 --> 02:35:36,560
 different local structure.

2230
02:35:36,560 --> 02:35:41,560
 So this is why CNN need produce multiple output image

2231
02:35:41,720 --> 02:35:45,240
 to learn multiple different filter.

2232
02:35:46,320 --> 02:35:48,560
 Okay, so this is why the CNN structure

2233
02:35:48,560 --> 02:35:51,039
 always increase the number of the channel,

2234
02:35:51,039 --> 02:35:54,600
 increase the number of the output image, right?

2235
02:35:54,600 --> 02:35:59,600
 So at the end, one output image will have much value

2236
02:36:01,760 --> 02:36:05,720
 only at the position for this local,

2237
02:36:05,720 --> 02:36:08,019
 for particular local structure.

2238
02:36:08,020 --> 02:36:11,940
 So one output map only contains information

2239
02:36:11,940 --> 02:36:13,780
 of one local structure.

2240
02:36:15,200 --> 02:36:20,200
 And one local structure cannot appear at everywhere, right?

2241
02:36:20,380 --> 02:36:25,220
 So that we can to see if the size of the data,

2242
02:36:25,220 --> 02:36:29,340
 we can reduce the spatial size of the image.

2243
02:36:29,340 --> 02:36:32,380
 So the size of the spatial map

2244
02:36:32,380 --> 02:36:34,700
 is always reduce the size.

2245
02:36:35,540 --> 02:36:39,660
 If we use the maximum pooling,

2246
02:36:39,660 --> 02:36:44,580
 the reduced size image always keeps a large output,

2247
02:36:44,580 --> 02:36:47,180
 where we move the small output.

2248
02:36:47,180 --> 02:36:50,060
 So it will keep the local structure

2249
02:36:50,060 --> 02:36:54,020
 and remove the information where there's no,

2250
02:36:54,020 --> 02:36:59,020
 such local structure show in the filter mask, right?

2251
02:36:59,580 --> 02:37:04,580
 So this is some kind of the good property of the,

2252
02:37:05,700 --> 02:37:06,540
 of the CNN.

2253
02:37:06,540 --> 02:37:10,420
 Of course, this is somehow we show that the pooling process

2254
02:37:10,420 --> 02:37:15,300
 will need to some kind of the spatial insensitivity

2255
02:37:15,300 --> 02:37:20,220
 because after we pooling to produce a small image, right?

2256
02:37:20,220 --> 02:37:21,620
 A small map.

2257
02:37:21,620 --> 02:37:25,540
 Then this map where one map will capture

2258
02:37:27,020 --> 02:37:31,300
 the specific local structure at a different position.

2259
02:37:31,300 --> 02:37:34,420
 But because we have a small map,

2260
02:37:34,460 --> 02:37:38,340
 so the exact position of this local structure

2261
02:37:39,500 --> 02:37:42,940
 does not show in this small map.

2262
02:37:42,940 --> 02:37:47,940
 Okay, it will reduce the spatial sensitivity of the image.

2263
02:37:48,140 --> 02:37:51,660
 Well, the spatial sensitivity reduce it is good

2264
02:37:51,660 --> 02:37:54,340
 for the recognition because as I mentioned that,

2265
02:37:54,340 --> 02:37:56,980
 the image has large variation, right?

2266
02:37:56,980 --> 02:37:58,740
 So for different image,

2267
02:37:58,740 --> 02:38:03,740
 the same structure may appear into different position.

2268
02:38:04,020 --> 02:38:08,460
 So we need to reduce the spatial sensitivity

2269
02:38:08,460 --> 02:38:10,860
 of some local structure.

2270
02:38:10,860 --> 02:38:14,660
 This is also implemented by the CNN.

2271
02:38:18,900 --> 02:38:23,740
 Now, why is this deep learning needs a mailing nails?

2272
02:38:23,740 --> 02:38:28,740
 First, the structure, local structure can have small scale,

2273
02:38:28,980 --> 02:38:30,740
 can have large scale.

2274
02:38:30,740 --> 02:38:35,740
 If we use a small window, a small filter mask,

2275
02:38:36,820 --> 02:38:40,940
 we can only extract the small scale of the structure.

2276
02:38:40,940 --> 02:38:42,860
 For a large scale of the structure,

2277
02:38:42,860 --> 02:38:46,260
 we need a large filter, right?

2278
02:38:46,260 --> 02:38:49,100
 Then this large filter can be implemented

2279
02:38:49,100 --> 02:38:52,420
 by multi-mails of the convolution

2280
02:38:52,420 --> 02:38:55,780
 because if one filter has three by three window,

2281
02:38:55,780 --> 02:38:58,780
 then after that, then to do the three by three window,

2282
02:38:58,780 --> 02:39:03,060
 then effectively we have a five by five filter window.

2283
02:39:03,060 --> 02:39:04,860
 Then next three by three,

2284
02:39:04,860 --> 02:39:08,540
 then the effective filter window is seven by seven.

2285
02:39:08,540 --> 02:39:11,140
 So we use multiple layers.

2286
02:39:11,140 --> 02:39:14,260
 We can capture the structure of small scale

2287
02:39:14,260 --> 02:39:18,140
 in the low level of the output

2288
02:39:18,140 --> 02:39:22,180
 and large scale in the high level of the output.

2289
02:39:22,180 --> 02:39:25,260
 So we can capture many different scale

2290
02:39:25,260 --> 02:39:28,260
 of the features and structures of the image.

2291
02:39:28,980 --> 02:39:33,980
 And also, if we want to learn a very complex functions,

2292
02:39:40,980 --> 02:39:44,420
 if we directly learn these complex functions,

2293
02:39:44,420 --> 02:39:47,180
 it will always cause the problem.

2294
02:39:48,660 --> 02:39:51,340
 Overfitting or into the local minimum

2295
02:39:51,340 --> 02:39:55,340
 or somehow the solution cannot really find

2296
02:39:55,340 --> 02:39:57,100
 a reasonable solution.

2297
02:39:57,100 --> 02:40:01,940
 Well, the same is each layer is just a simple convolution.

2298
02:40:01,940 --> 02:40:05,940
 The activation function is almost linear, right?

2299
02:40:05,940 --> 02:40:10,940
 So each layer only learn a very small part of the information

2300
02:40:11,420 --> 02:40:14,540
 so that we need a large number of the layer

2301
02:40:14,540 --> 02:40:18,900
 to learn the information of the training data progressively,

2302
02:40:20,020 --> 02:40:23,760
 slowly one by one to learn the information.

2303
02:40:24,760 --> 02:40:28,400
 Okay, so that it's much more stable.

2304
02:40:30,680 --> 02:40:34,240
 Now at the end, I will address the,

2305
02:40:34,240 --> 02:40:39,240
 so the renew function is a very simple nonlinear function,

2306
02:40:39,600 --> 02:40:40,440
 right?

2307
02:40:40,440 --> 02:40:41,640
 So each layer is very simple.

2308
02:40:41,640 --> 02:40:44,920
 It only learn a small amount of the information

2309
02:40:44,920 --> 02:40:48,840
 so that we have many layers together

2310
02:40:48,840 --> 02:40:52,960
 to get a complex information.

2311
02:40:52,960 --> 02:40:57,119
 Well, this complex information is more realistic

2312
02:40:57,119 --> 02:41:00,839
 information of the image because each layer,

2313
02:41:00,839 --> 02:41:02,439
 it is just a convolution,

2314
02:41:02,439 --> 02:41:05,880
 it's just capture some kind of the image features.

2315
02:41:07,519 --> 02:41:10,679
 Well, this renew activation function is very simple.

2316
02:41:10,679 --> 02:41:15,439
 It's enable, it's possible we can train this network

2317
02:41:15,439 --> 02:41:18,880
 to have a very large number of the layers

2318
02:41:18,880 --> 02:41:21,800
 because to separate different layer,

2319
02:41:21,800 --> 02:41:23,880
 we just use a nonlinear function

2320
02:41:23,880 --> 02:41:27,519
 almost the same as a linear function, right?

2321
02:41:29,160 --> 02:41:31,039
 But we must use a nonlinear function.

2322
02:41:31,039 --> 02:41:34,880
 Otherwise the two layers is just affecting one layer.

2323
02:41:36,880 --> 02:41:41,880
 Now, the very critical point is how this thing

2324
02:41:42,000 --> 02:41:45,240
 and really solve the overfitting problem.

2325
02:41:47,160 --> 02:41:51,560
 Now, as I mentioned that in the fully connection network,

2326
02:41:51,560 --> 02:41:56,560
 the waiting metrics, every element is learned from the data.

2327
02:41:57,320 --> 02:42:01,560
 That means in this, in the MLP,

2328
02:42:01,560 --> 02:42:06,560
 one parameter is fixed to link one input pixel

2329
02:42:07,359 --> 02:42:09,880
 with one output pixel.

2330
02:42:09,880 --> 02:42:13,560
 This link is fixed, okay?

2331
02:42:13,560 --> 02:42:16,400
 So that in the training process,

2332
02:42:16,400 --> 02:42:20,760
 one training image, only one pixel

2333
02:42:20,760 --> 02:42:24,080
 of the training image are used

2334
02:42:24,080 --> 02:42:28,760
 to determine one single parameter, okay?

2335
02:42:28,760 --> 02:42:33,760
 So one parameter only determined by some specific pixel

2336
02:42:34,000 --> 02:42:37,760
 of the input image.

2337
02:42:37,760 --> 02:42:41,160
 Well, in the convolutional network,

2338
02:42:41,160 --> 02:42:46,120
 it is a filter, it is a convolution, okay?

2339
02:42:46,120 --> 02:42:49,280
 This convolution in terms of the prediction

2340
02:42:49,280 --> 02:42:53,200
 or filter the input image to produce the output image,

2341
02:42:53,200 --> 02:42:56,200
 it appears to be nothing new, right?

2342
02:42:56,200 --> 02:42:59,480
 But this concept in the machine learning,

2343
02:42:59,480 --> 02:43:03,120
 it will bring big difference.

2344
02:43:03,120 --> 02:43:05,480
 Because in the learning process,

2345
02:43:05,480 --> 02:43:09,880
 the parameter will move into all positions.

2346
02:43:09,880 --> 02:43:14,880
 That means one parameter will move into all pixel.

2347
02:43:15,580 --> 02:43:17,680
 Then in the learning process,

2348
02:43:17,680 --> 02:43:21,840
 then all pixels of an image is used

2349
02:43:21,840 --> 02:43:24,520
 to determine a single parameter.

2350
02:43:25,840 --> 02:43:30,840
 Now, this is the critical difference

2351
02:43:31,400 --> 02:43:36,400
 of CNN learning and the MLP learning.

2352
02:43:39,080 --> 02:43:40,560
 I hope you understand, right?

2353
02:43:40,560 --> 02:43:44,200
 Because a single parameter in the CNN,

2354
02:43:44,200 --> 02:43:47,360
 we are moving to all positions,

2355
02:43:47,360 --> 02:43:50,200
 moving to go through all pixels.

2356
02:43:50,200 --> 02:43:52,200
 Then in the training process,

2357
02:43:52,200 --> 02:43:55,800
 also it will move into all pixels.

2358
02:43:55,800 --> 02:43:59,920
 So each parameter moving to all pixels,

2359
02:43:59,920 --> 02:44:03,160
 then in the training, all pixels is participate

2360
02:44:03,160 --> 02:44:06,080
 to training a single parameter.

2361
02:44:07,640 --> 02:44:12,640
 This greatly overcome that the learned parameter

2362
02:44:13,640 --> 02:44:17,199
 will over-fit to a specific position.

2363
02:44:18,199 --> 02:44:19,039
 Okay?

2364
02:44:19,039 --> 02:44:21,960
 Because in the learning process,

2365
02:44:21,960 --> 02:44:25,880
 each parameter will move into all positions.

2366
02:44:25,880 --> 02:44:28,039
 So all positions in the training

2367
02:44:28,039 --> 02:44:31,439
 were used to determine a single parameter.

2368
02:44:32,400 --> 02:44:34,400
 This is the big difference.

2369
02:44:34,400 --> 02:44:39,199
 So that the convolution moving the parameter,

2370
02:44:39,199 --> 02:44:42,039
 the field parameter goes through all positions

2371
02:44:42,040 --> 02:44:45,240
 in the training, that means all position information

2372
02:44:45,240 --> 02:44:48,240
 is together utilized to determine

2373
02:44:48,240 --> 02:44:50,480
 what is the good parameter.

2374
02:44:53,120 --> 02:44:56,760
 Now, because of that, in the learning process,

2375
02:44:56,760 --> 02:45:01,120
 the learning of the CNN and the learning of the MLP

2376
02:45:01,120 --> 02:45:04,120
 are totally different, right?

2377
02:45:05,160 --> 02:45:09,520
 This makes a big difference between the performance

2378
02:45:09,520 --> 02:45:13,200
 of the CNN and the MLP.

2379
02:45:13,200 --> 02:45:17,800
 So this is why the MLP is over-fitting the training data,

2380
02:45:17,800 --> 02:45:21,840
 but CNN will not over-fit your training data.

2381
02:45:25,120 --> 02:45:29,800
 Okay, now, because next week will be,

2382
02:45:29,800 --> 02:45:31,440
 I completed this topic,

2383
02:45:31,440 --> 02:45:35,720
 next week I will study some limitation of the CNN

2384
02:45:35,720 --> 02:45:38,240
 and introduce the transformer.

2385
02:45:38,240 --> 02:45:43,240
 Okay, this transformer is currently the best model

2386
02:45:43,920 --> 02:45:45,240
 in the deep learning model.

2387
02:45:45,240 --> 02:45:47,840
 So we will study what is a transformer.

2388
02:45:47,840 --> 02:45:52,840
 Now, because next week is the last week for my lecture, right?

2389
02:45:52,840 --> 02:45:57,840
 So next week I want you to solve any doubt

2390
02:45:58,400 --> 02:46:02,039
 or questions of you for the final exam.

2391
02:46:02,039 --> 02:46:06,160
 So before the next week comes, you can raise your questions.

2392
02:46:06,160 --> 02:46:09,600
 So this week I will talk about a little about

2393
02:46:09,600 --> 02:46:13,119
 the final exam so that if you have the problem

2394
02:46:13,119 --> 02:46:18,119
 or questions we can discuss in next week,

2395
02:46:21,360 --> 02:46:23,480
 the final week of my lecture.

2396
02:46:30,440 --> 02:46:33,480
 Okay, now here I will,

2397
02:46:33,480 --> 02:46:38,480
 what I want to do here is try to reduce the scope

2398
02:46:40,760 --> 02:46:45,760
 of the final exam so that you don't need to spend

2399
02:46:46,880 --> 02:46:49,480
 much time to prepare the final exam

2400
02:46:49,480 --> 02:46:54,439
 because I know all of you take three or four courses.

2401
02:46:54,439 --> 02:46:58,880
 So it is a very heavy work note for you, right?

2402
02:46:58,880 --> 02:47:03,119
 Okay, so here I try to somehow reduce the scope

2403
02:47:03,120 --> 02:47:08,120
 of the final exam so that you can just focus

2404
02:47:08,360 --> 02:47:12,280
 on the topic included in the final exam.

2405
02:47:12,280 --> 02:47:15,880
 Okay, to save your time, right?

2406
02:47:15,880 --> 02:47:18,320
 So basically in the final exam,

2407
02:47:18,320 --> 02:47:23,320
 the question is only related to very fundamental questions.

2408
02:47:23,600 --> 02:47:27,280
 Okay, based on your understanding of the mainstream

2409
02:47:27,280 --> 02:47:32,280
 of the theory, the mainstream of the concept

2410
02:47:33,600 --> 02:47:38,600
 so don't come to some very complicated sense, okay?

2411
02:47:39,760 --> 02:47:44,400
 If you really cannot understand it, you just forget it.

2412
02:47:44,400 --> 02:47:47,280
 Try to understand what you can

2413
02:47:47,280 --> 02:47:49,920
 and try to understand the mainstream,

2414
02:47:49,920 --> 02:47:54,920
 the big theory or the theory we needed

2415
02:47:58,760 --> 02:48:02,960
 to different parts of this topic, okay?

2416
02:48:02,960 --> 02:48:07,679
 So basically the final exam only focus on the basic concept.

2417
02:48:07,679 --> 02:48:11,240
 Okay, don't be troubled by some specific technique

2418
02:48:11,240 --> 02:48:14,839
 or some specific tricks, okay?

2419
02:48:14,839 --> 02:48:19,839
 We only test your understanding of the big pictures

2420
02:48:19,919 --> 02:48:21,160
 in this course.

2421
02:48:22,039 --> 02:48:24,880
 Okay, now in terms of the topics,

2422
02:48:26,640 --> 02:48:30,560
 in the beginning I missed out the 15 topics of this course,

2423
02:48:30,560 --> 02:48:31,400
 right?

2424
02:48:31,400 --> 02:48:34,000
 Of course the final four topics were taught,

2425
02:48:34,000 --> 02:48:37,519
 were teached by some other lectures.

2426
02:48:37,519 --> 02:48:41,279
 I will stop at the next week at the topic in even.

2427
02:48:41,279 --> 02:48:43,760
 So I will talk about in these topics

2428
02:48:43,760 --> 02:48:47,760
 what were exclude from the final exam

2429
02:48:47,760 --> 02:48:52,760
 so that you just focus on three or four topics.

2430
02:48:53,720 --> 02:48:58,199
 Okay, now the first topic is just fundamentals.

2431
02:48:58,200 --> 02:49:01,520
 So this is not including the final exam.

2432
02:49:02,560 --> 02:49:07,560
 Now the second topic, AI-SIS system and transforms.

2433
02:49:07,760 --> 02:49:11,520
 Now we know the filter, the convolution

2434
02:49:11,520 --> 02:49:15,120
 is extremely important even for deep learning.

2435
02:49:15,120 --> 02:49:20,120
 So I want have one question also in this topic,

2436
02:49:20,520 --> 02:49:23,080
 AI-SIS system and transform.

2437
02:49:23,080 --> 02:49:26,120
 Of course in this topic, the most important sense

2438
02:49:26,120 --> 02:49:29,360
 is convolution and the filter, right?

2439
02:49:29,360 --> 02:49:33,960
 So you must really understand what is the filter process?

2440
02:49:35,040 --> 02:49:37,080
 Okay, how to do the convolution

2441
02:49:38,200 --> 02:49:41,640
 from the input to produce the output?

2442
02:49:41,640 --> 02:49:45,600
 Okay, so the second topic will be include

2443
02:49:45,600 --> 02:49:46,920
 in the final exam.

2444
02:49:48,640 --> 02:49:53,120
 Now the topic three, image denoising and enhancement.

2445
02:49:53,120 --> 02:49:56,400
 Now in our quiz, we already have one question,

2446
02:49:56,400 --> 02:49:58,920
 the first one equalization, right?

2447
02:49:58,920 --> 02:50:03,920
 So the third topic we are not include in the final exam

2448
02:50:04,160 --> 02:50:09,160
 because in the quiz we already test part of this one, right?

2449
02:50:10,880 --> 02:50:15,040
 Now next one, the morphological image processing.

2450
02:50:15,040 --> 02:50:19,800
 This one in this year were exclude from the final exam.

2451
02:50:19,800 --> 02:50:23,519
 You don't need to prepare this morphological image processing

2452
02:50:23,519 --> 02:50:24,759
 for your final exam.

2453
02:50:26,840 --> 02:50:31,840
 Okay, my philosophy is not to learn a lot of the sense

2454
02:50:31,840 --> 02:50:35,880
 is you focus on some topic you really understand,

2455
02:50:35,880 --> 02:50:37,920
 go too deep inside.

2456
02:50:37,920 --> 02:50:42,920
 So I have no problem to just reduce the scope

2457
02:50:43,199 --> 02:50:45,920
 of the final exam to a very small part.

2458
02:50:45,920 --> 02:50:48,080
 No problem, you just focus on that,

2459
02:50:48,080 --> 02:50:51,560
 try to really understand this topic.

2460
02:50:51,560 --> 02:50:54,760
 Okay, so morphological image processing

2461
02:50:54,760 --> 02:50:58,039
 were exclude from the final exam.

2462
02:50:58,039 --> 02:51:02,160
 Now next one, topic five is basically help you

2463
02:51:02,160 --> 02:51:07,160
 to intuitively understand the recondition process, right?

2464
02:51:08,039 --> 02:51:10,280
 This is some kind of introduction.

2465
02:51:10,280 --> 02:51:13,280
 So this is not include in the final exam.

2466
02:51:14,280 --> 02:51:19,280
 Next one, MAP decision and class fear.

2467
02:51:20,040 --> 02:51:25,040
 This is already, I already test you in the quiz, right?

2468
02:51:25,280 --> 02:51:28,360
 So in the final exam I will not use this topic.

2469
02:51:30,360 --> 02:51:35,360
 Okay, now next one start estimation and machine learning

2470
02:51:36,440 --> 02:51:40,360
 for this year is also not included in the final exam.

2471
02:51:40,360 --> 02:51:45,360
 The handcraft feature generation and feature selection

2472
02:51:45,560 --> 02:51:50,000
 such as this, this Wila-Jong's approach

2473
02:51:50,000 --> 02:51:55,000
 and this Ada Boost is quite somehow tedious.

2474
02:51:57,880 --> 02:52:02,880
 So you don't care about this one to prepare your final exam.

2475
02:52:05,120 --> 02:52:08,600
 Now we show data dimension reduction

2476
02:52:08,600 --> 02:52:12,600
 and the feature extraction, although it is the topic

2477
02:52:12,600 --> 02:52:17,600
 of the assignment, okay, but it's still very important.

2478
02:52:18,600 --> 02:52:21,800
 Then this topic is in the final exam.

2479
02:52:22,800 --> 02:52:27,800
 Then neural network and deep learning CNN and transformer.

2480
02:52:27,920 --> 02:52:31,560
 As I mentioned that the final exam is only focused

2481
02:52:31,560 --> 02:52:33,440
 on the basic concept.

2482
02:52:34,360 --> 02:52:37,680
 Let you really understand the basic concept.

2483
02:52:37,680 --> 02:52:41,880
 So here the transformer one we are not including

2484
02:52:41,880 --> 02:52:43,000
 the final exam.

2485
02:52:44,120 --> 02:52:48,520
 So that means the next week, the lecture content

2486
02:52:48,520 --> 02:52:51,600
 of the next week we are not including the final exam.

2487
02:52:53,440 --> 02:52:58,440
 So my part, the remaining is only three topic,

2488
02:52:58,800 --> 02:53:01,880
 very small, very narrow topic, right?

2489
02:53:02,800 --> 02:53:05,240
 So in the final exam you have four questions

2490
02:53:05,240 --> 02:53:09,680
 to complete answer the four questions within three hours.

2491
02:53:09,680 --> 02:53:14,680
 Okay, each question is 25 marks together, 100 marks.

2492
02:53:14,680 --> 02:53:18,880
 Of course your final exam is closed booked test.

2493
02:53:18,880 --> 02:53:21,560
 So for the four questions, the question one

2494
02:53:21,560 --> 02:53:25,680
 come from the AOSI system and transform.

2495
02:53:25,680 --> 02:53:29,920
 Question two come from the dimensionality reduction

2496
02:53:29,920 --> 02:53:31,280
 and the feature extraction.

2497
02:53:32,280 --> 02:53:36,360
 Okay, then question three come from the general

2498
02:53:36,360 --> 02:53:39,000
 neural network and CNN.

2499
02:53:39,000 --> 02:53:41,840
 This tonight's lecture.

2500
02:53:43,120 --> 02:53:46,440
 So this is three questions from my part.

2501
02:53:47,880 --> 02:53:52,880
 Now from the last four topics because it is not a lecture

2502
02:53:56,720 --> 02:53:58,240
 by me but another one.

2503
02:53:58,320 --> 02:54:03,039
 So this question I cannot give out what is,

2504
02:54:03,039 --> 02:54:05,119
 we come from which topic.

2505
02:54:05,119 --> 02:54:08,840
 But I just tell you the question four is also

2506
02:54:08,840 --> 02:54:11,560
 very, very basic one.

2507
02:54:11,560 --> 02:54:15,840
 It could be related to both video and 3D

2508
02:54:15,840 --> 02:54:19,880
 but a very simple basic questions.

2509
02:54:19,880 --> 02:54:24,360
 You just understand the very fundamental one, okay?

2510
02:54:28,360 --> 02:54:33,360
 Now although we also use this dimensionality reduction

2511
02:54:36,000 --> 02:54:39,600
 in both assignment and the final exam

2512
02:54:39,600 --> 02:54:43,720
 but for the assignment it's most likely you do the programming

2513
02:54:43,720 --> 02:54:47,600
 to get the result to compare the result, right?

2514
02:54:47,600 --> 02:54:50,760
 The program you may utilize others program

2515
02:54:50,760 --> 02:54:54,160
 or just use the library use the function.

2516
02:54:54,160 --> 02:54:59,039
 So because of that in the final exam,

2517
02:54:59,039 --> 02:55:03,720
 sorry for the final exam for this topic

2518
02:55:03,720 --> 02:55:08,720
 is really very basic ones of the dimensionality reduction.

2519
02:55:10,160 --> 02:55:13,360
 What is the very fundamental basic ones?

2520
02:55:13,360 --> 02:55:15,520
 Minial algebra, right?

2521
02:55:15,520 --> 02:55:17,760
 Matrix multiplication.

2522
02:55:17,760 --> 02:55:20,560
 What is the meaning of the matrix multiplication?

2523
02:55:20,560 --> 02:55:22,760
 How to do the reduced dimension?

2524
02:55:22,760 --> 02:55:26,040
 What is the projection of the data to some

2525
02:55:26,040 --> 02:55:28,400
 no dimensional space?

2526
02:55:28,400 --> 02:55:33,040
 How to express them in a linear algebra, right?

2527
02:55:33,040 --> 02:55:34,800
 Then in the linear algebra,

2528
02:55:36,800 --> 02:55:41,800
 what is the concept of the covariance and the variance?

2529
02:55:43,280 --> 02:55:46,120
 Okay, all of this very simple, basic one

2530
02:55:46,120 --> 02:55:50,200
 is very important where I be tested in question two.

2531
02:55:51,200 --> 02:55:52,040
 Okay.

2532
02:55:56,040 --> 02:56:00,520
 I think that's about all for the final exam, right?

2533
02:56:00,520 --> 02:56:04,400
 So if you have further concern about the final exam

2534
02:56:04,400 --> 02:56:09,280
 you can raise your question or discuss with me next week.

2535
02:56:09,280 --> 02:56:12,480
 Okay, my final lecture in the next week.

2536
02:56:13,640 --> 02:56:16,440
 Okay, so that's all for tonight, thank you.

2537
02:56:16,520 --> 02:56:17,860
 Thank you.

2538
02:56:17,860 --> 02:56:19,280
 armored

2539
02:56:46,440 --> 02:56:48,500
 you

2540
02:57:16,440 --> 02:57:18,500
 you

2541
02:57:46,440 --> 02:57:48,500
 you

2542
02:58:16,440 --> 02:58:18,500
 you

