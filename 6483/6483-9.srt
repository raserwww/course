1
00:00:00,000 --> 00:00:02,000
 you

2
00:00:30,000 --> 00:00:32,000
 you

3
00:01:00,000 --> 00:01:02,000
 you

4
00:01:30,000 --> 00:01:32,000
 you

5
00:02:00,000 --> 00:02:02,000
 you

6
00:02:30,000 --> 00:02:32,000
 you

7
00:03:00,000 --> 00:03:07,000
 you

8
00:03:07,000 --> 00:03:10,000
 you

9
00:03:10,000 --> 00:03:11,000
 sorry

10
00:03:11,000 --> 00:03:14,000
 my level is very high

11
00:03:14,000 --> 00:03:16,000
 okay

12
00:03:16,000 --> 00:03:18,000
 just to

13
00:03:18,000 --> 00:03:20,000
 just a reminder

14
00:03:20,000 --> 00:03:23,000
 today is the due date for

15
00:03:23,000 --> 00:03:26,000
 this homework one

16
00:03:26,000 --> 00:03:28,000
 I have received a couple of

17
00:03:28,000 --> 00:03:30,000
 emails asking about this

18
00:03:30,000 --> 00:03:34,000
 just in case you missed

19
00:03:34,000 --> 00:03:36,000
 what we discussed last week

20
00:03:36,000 --> 00:03:38,000
 so

21
00:03:38,000 --> 00:03:40,000
 and just in case you have not

22
00:03:40,000 --> 00:03:41,000
 submitted your homework

23
00:03:41,000 --> 00:03:43,000
 but you can do multiple attempts

24
00:03:43,000 --> 00:03:45,000
 right we will use the most recent

25
00:03:45,000 --> 00:03:47,000
 submission as your final

26
00:03:47,000 --> 00:03:49,000
 submission for marking

27
00:03:49,000 --> 00:03:51,000
 so for this question again as you

28
00:03:51,000 --> 00:03:52,000
 can see that

29
00:03:52,000 --> 00:03:54,000
 sample number one

30
00:03:54,000 --> 00:03:56,000
 and sample number four have

31
00:03:56,000 --> 00:03:58,000
 exactly the same feature

32
00:03:58,000 --> 00:04:01,000
 but different classification

33
00:04:01,000 --> 00:04:05,000
 right so as I mentioned last week

34
00:04:05,000 --> 00:04:08,000
 this is not an error but it's

35
00:04:08,000 --> 00:04:11,000
 kind of a noisy data although we only

36
00:04:11,000 --> 00:04:14,000
 had eight samples here but you can

37
00:04:14,000 --> 00:04:17,000
 imagine that for a big database of

38
00:04:17,000 --> 00:04:20,000
 millions of data and three you

39
00:04:20,000 --> 00:04:23,000
 bound to have some inconsistency

40
00:04:24,000 --> 00:04:26,000
 with the same feature but the class

41
00:04:26,000 --> 00:04:29,000
 labors are different which could be

42
00:04:29,000 --> 00:04:32,000
 due to laboring error or noisy or

43
00:04:32,000 --> 00:04:35,000
 anomaly so when you build

44
00:04:35,000 --> 00:04:38,000
 decision three you have to decide

45
00:04:38,000 --> 00:04:40,000
 how to deal with this situation

46
00:04:40,000 --> 00:04:44,000
 okay for the part a so you can

47
00:04:44,000 --> 00:04:47,000
 make a decision because in here you

48
00:04:47,000 --> 00:04:49,000
 are using information gain to

49
00:04:49,000 --> 00:04:53,000
 construct the decision three

50
00:04:53,000 --> 00:04:55,000
 therefore your objective is to find

51
00:04:55,000 --> 00:04:59,000
 the features and then the classes

52
00:04:59,000 --> 00:05:02,000
 which allow you to

53
00:05:02,000 --> 00:05:05,000
 maximize your information gain right

54
00:05:05,000 --> 00:05:10,000
 and then for part c is the one

55
00:05:10,000 --> 00:05:12,000
 that you have to now think together

56
00:05:12,000 --> 00:05:16,000
 with part a what can you do

57
00:05:16,000 --> 00:05:19,000
 with that situation so that you

58
00:05:19,000 --> 00:05:23,000
 can prone the trees into

59
00:05:23,000 --> 00:05:26,000
 perhaps something more meaningful

60
00:05:26,000 --> 00:05:29,000
 and can still achieve the same or

61
00:05:29,000 --> 00:05:31,000
 similar performance right so that is

62
00:05:31,000 --> 00:05:34,000
 what the question is asking so think

63
00:05:34,000 --> 00:05:36,000
 about it just write your justification

64
00:05:36,000 --> 00:05:39,000
 it is meant for you to

65
00:05:39,000 --> 00:05:43,000
 to exercise your judgment

66
00:05:43,000 --> 00:05:46,000
 then the second question is

67
00:05:46,000 --> 00:05:49,000
 I think I show one example in

68
00:05:49,000 --> 00:05:52,000
 last lecture you do not need to show

69
00:05:52,000 --> 00:05:55,000
 me all the detail calculation which

70
00:05:55,000 --> 00:05:58,000
 of course you can do that right when

71
00:05:58,000 --> 00:06:01,000
 you try to figure out if you really

72
00:06:01,000 --> 00:06:04,000
 have no clue what I'm asking is within

73
00:06:04,000 --> 00:06:07,000
 this box right if there's any point

74
00:06:07,000 --> 00:06:11,000
 located within this box what class

75
00:06:11,000 --> 00:06:15,000
 will it belong to if you use the

76
00:06:15,000 --> 00:06:17,000
 first sorry one nearest neighbor

77
00:06:17,000 --> 00:06:20,000
 three nearest neighbor or five nearest

78
00:06:20,000 --> 00:06:23,000
 neighbor classifier right so if you

79
00:06:23,000 --> 00:06:25,000
 really do know you can okay start with

80
00:06:25,000 --> 00:06:27,000
 some random point just put some point

81
00:06:27,000 --> 00:06:30,000
 here then you make your judgment then

82
00:06:30,000 --> 00:06:32,000
 after a while you can figure out some

83
00:06:32,000 --> 00:06:36,000
 patterns and then then you know that

84
00:06:36,000 --> 00:06:39,000
 within certain area it will all

85
00:06:39,000 --> 00:06:43,000
 classify as this class solid circle

86
00:06:43,000 --> 00:06:47,000
 right and just shape the areas that

87
00:06:47,000 --> 00:06:51,000
 you think if there's any point located

88
00:06:51,000 --> 00:06:55,000
 in those areas will be classified as

89
00:06:55,000 --> 00:07:00,000
 class solid circle okay so I only need

90
00:07:00,000 --> 00:07:03,000
 you to shape the region you do not need

91
00:07:03,000 --> 00:07:05,000
 to show the detail calculation or this

92
00:07:05,000 --> 00:07:10,000
 thing okay and yeah that's what the

93
00:07:10,000 --> 00:07:12,000
 question is asking you and of course in

94
00:07:12,000 --> 00:07:16,000
 here I assume you use the Euclidean

95
00:07:16,000 --> 00:07:20,000
 distant measure right which is a norm L2

96
00:07:20,000 --> 00:07:22,000
 of course if you want to challenge yourself

97
00:07:22,000 --> 00:07:24,000
 you think about will the answer be

98
00:07:24,000 --> 00:07:28,000
 different if you use L1 right the

99
00:07:28,000 --> 00:07:32,000
 Manhattan distant or if you use other

100
00:07:32,000 --> 00:07:35,000
 so-called distant measure will the

101
00:07:35,000 --> 00:07:38,000
 areas be different so that will allow

102
00:07:38,000 --> 00:07:46,000
 you to think carefully okay and yeah I

103
00:07:46,000 --> 00:07:49,000
 could even say that if you use an SVM

104
00:07:49,000 --> 00:07:56,000
 right to classify the data right so

105
00:07:56,000 --> 00:07:59,000
 what class will they belong to so that

106
00:07:59,000 --> 00:08:02,000
 of course you should think about it

107
00:08:02,000 --> 00:08:05,000
 okay that is the homework again due by

108
00:08:05,000 --> 00:08:13,000
 end of today 11.59 p.m. but please do not

109
00:08:13,000 --> 00:08:16,000
 rush for the last minute because whether

110
00:08:16,000 --> 00:08:19,000
 the CITS NTU learn will be down or not

111
00:08:19,000 --> 00:08:22,000
 I have no idea so give yourself some

112
00:08:22,000 --> 00:08:27,000
 buffer okay so let's come back to

113
00:08:27,000 --> 00:08:33,000
 neural networks and we have started this

114
00:08:33,000 --> 00:08:37,000
 in the last lecture talk about what is a

115
00:08:37,000 --> 00:08:40,000
 neural networks and what is the so-called

116
00:08:40,000 --> 00:08:45,000
 the construction basic unit which is a

117
00:08:45,000 --> 00:08:51,000
 perceptron a perceptron as we cover last

118
00:08:51,000 --> 00:08:55,000
 lecture right is basically try to mimic a

119
00:08:55,000 --> 00:08:59,000
 neuron like this which you receive input

120
00:08:59,000 --> 00:09:02,000
 which is in this case the features x1 to

121
00:09:02,000 --> 00:09:06,000
 xn then you multiply with respective

122
00:09:06,000 --> 00:09:09,000
 weights keep them more emphasis or less

123
00:09:09,000 --> 00:09:13,000
 emphasis then you sum them up this one

124
00:09:13,000 --> 00:09:18,000
 the summation wj xj after you sum them up

125
00:09:18,000 --> 00:09:21,000
 plus b is a so-called the bias right

126
00:09:21,000 --> 00:09:26,000
 this we have give this a name called net j

127
00:09:26,000 --> 00:09:30,000
 that mean the net input to the neuron j

128
00:09:30,000 --> 00:09:35,000
 okay net j then before you go out you have

129
00:09:35,000 --> 00:09:38,000
 this so-called activation function

130
00:09:38,000 --> 00:09:42,000
 sigma with the net j here this is the net

131
00:09:42,000 --> 00:09:46,000
 j input net j output will be y equal to

132
00:09:46,000 --> 00:09:51,000
 sigma net j right and this sigma is

133
00:09:51,000 --> 00:09:54,000
 known as activation function in this case

134
00:09:54,000 --> 00:09:57,000
 you will just charge whether this value is

135
00:09:57,000 --> 00:10:00,000
 less than or equal to zero no output

136
00:10:00,000 --> 00:10:05,000
 otherwise it will send a signal to the

137
00:10:05,000 --> 00:10:11,000
 downstream okay so this x is the accumulation

138
00:10:11,000 --> 00:10:15,000
 of all the input plus a bias and for

139
00:10:15,000 --> 00:10:17,000
 convenient and many people would like to

140
00:10:17,000 --> 00:10:22,000
 absorb this b as a spatial input unit

141
00:10:22,000 --> 00:10:25,000
 with x output equal to one then the b

142
00:10:25,000 --> 00:10:28,000
 become the weights so you think about it

143
00:10:28,000 --> 00:10:31,000
 one-minder type multiply b will be

144
00:10:31,000 --> 00:10:35,000
 equivalent to the b bias input okay so

145
00:10:35,000 --> 00:10:39,000
 that allow you to write this so-called

146
00:10:39,000 --> 00:10:42,000
 neuron as a very so-called elegant and

147
00:10:42,000 --> 00:10:46,000
 compressed dog product vector weights

148
00:10:46,000 --> 00:10:49,000
 dog product with a feature vector x

149
00:10:49,000 --> 00:10:53,000
 okay again it is basically just trying to

150
00:10:53,000 --> 00:10:57,000
 do the hyperplane classification to

151
00:10:57,000 --> 00:11:00,000
 separate the space into two different

152
00:11:00,000 --> 00:11:05,000
 so-called classes okay then we went

153
00:11:05,000 --> 00:11:08,000
 through a couple of very simple example

154
00:11:08,000 --> 00:11:10,000
 you can consider this as the training data

155
00:11:10,000 --> 00:11:15,000
 then if you want them to fulfill all

156
00:11:15,000 --> 00:11:17,000
 this classification result which is a

157
00:11:17,000 --> 00:11:21,000
 target output with a given sample

158
00:11:21,000 --> 00:11:24,000
 training then you must fulfill all this

159
00:11:24,000 --> 00:11:27,000
 condition and you can find multiple

160
00:11:27,000 --> 00:11:30,000
 possible solution for this case okay

161
00:11:30,000 --> 00:11:32,000
 and we also talk about how is it different

162
00:11:32,000 --> 00:11:37,000
 from SVM and unfortunately when you have

163
00:11:37,000 --> 00:11:42,000
 some more so-called complex so-called

164
00:11:42,000 --> 00:11:45,000
 classification boundary then a single

165
00:11:45,000 --> 00:11:49,000
 neuron cannot serve this purpose but you

166
00:11:49,000 --> 00:11:53,000
 could combine a few neurons combine

167
00:11:53,000 --> 00:11:57,000
 link them together so this one I suggest

168
00:11:57,000 --> 00:12:00,000
 you to try out find out what is a W and

169
00:12:00,000 --> 00:12:04,000
 in here how many parameter we have here

170
00:12:04,000 --> 00:12:10,000
 1 2 3 4 5 6 weights plus 3 bias okay so

171
00:12:10,000 --> 00:12:13,000
 which is totally 9 variables you need to

172
00:12:13,000 --> 00:12:17,000
 find solution and that you can classify

173
00:12:17,000 --> 00:12:23,000
 these x or classes then by connecting

174
00:12:23,000 --> 00:12:27,000
 all these so-called neuron together and

175
00:12:27,000 --> 00:12:30,000
 you can form a multi-layer perceptron

176
00:12:30,000 --> 00:12:33,000
 which is a very generic

177
00:12:33,000 --> 00:12:36,000
 neural network and basic neural network

178
00:12:36,000 --> 00:12:39,000
 okay so such topology normally we call

179
00:12:39,000 --> 00:12:46,000
 it N M M C meaning you have N unique

180
00:12:46,000 --> 00:12:50,000
 input right and two hidden layer with M

181
00:12:50,000 --> 00:12:53,000
 unit you could the M could be different

182
00:12:53,000 --> 00:12:57,000
 you could have M1 M2 right M3 M4 etc

183
00:12:57,000 --> 00:13:01,000
 then you have output unit with C neuron

184
00:13:01,000 --> 00:13:04,000
 output okay so typically normally if you

185
00:13:04,000 --> 00:13:07,000
 talk about classification this one the

186
00:13:07,000 --> 00:13:10,000
 output Y1 to YC will be either one or zero

187
00:13:10,000 --> 00:13:13,000
 right or you want the target output will be

188
00:13:13,000 --> 00:13:16,000
 either one or zero while indicate that

189
00:13:16,000 --> 00:13:20,000
 the object input belong to that particular class

190
00:13:20,000 --> 00:13:24,000
 then we say that such a configuration is

191
00:13:24,000 --> 00:13:28,000
 so-called universal approximator right

192
00:13:28,000 --> 00:13:34,000
 MLP is a universal approximator so that

193
00:13:34,000 --> 00:13:36,000
 mean regardless how complex the

194
00:13:36,000 --> 00:13:41,000
 classification regions are given enough

195
00:13:41,000 --> 00:13:45,000
 neurons enough layer you could always

196
00:13:45,000 --> 00:13:50,000
 train the network to differentiate or

197
00:13:50,000 --> 00:13:53,000
 separate the two the all these classes in

198
00:13:53,000 --> 00:13:55,000
 here we only have two classes of black and white

199
00:13:55,000 --> 00:13:58,000
 right so this show you the different layer

200
00:13:58,000 --> 00:14:01,000
 the first layer basically you do a hyperplane

201
00:14:01,000 --> 00:14:05,000
 right linear classification then the

202
00:14:05,000 --> 00:14:07,000
 second layer you can start to combine all

203
00:14:07,000 --> 00:14:12,000
 this linear hyperplane into convex region

204
00:14:12,000 --> 00:14:16,000
 and then by further combining all this

205
00:14:16,000 --> 00:14:19,000
 convex region you can create arbitrary

206
00:14:19,000 --> 00:14:22,000
 so-called areas which may no longer be

207
00:14:22,000 --> 00:14:26,000
 convex okay so that training play a very

208
00:14:26,000 --> 00:14:30,000
 important role and the good news is if you

209
00:14:30,000 --> 00:14:34,000
 use a back propagation and then you can

210
00:14:34,000 --> 00:14:37,000
 train we show this demo as long as you

211
00:14:37,000 --> 00:14:41,000
 have sufficient input data you have the

212
00:14:41,000 --> 00:14:43,000
 target output you can always train the

213
00:14:43,000 --> 00:14:47,000
 network to minimize the error that you

214
00:14:47,000 --> 00:14:50,000
 can define what is your target and

215
00:14:50,000 --> 00:14:54,000
 different between target and output as

216
00:14:54,000 --> 00:14:57,000
 error function one of the important

217
00:14:57,000 --> 00:15:00,000
 thing that we mentioned is the whole

218
00:15:00,000 --> 00:15:04,000
 neural network would not really work if

219
00:15:04,000 --> 00:15:07,000
 you do not have this activation function

220
00:15:07,000 --> 00:15:10,000
 nonlinear activation function right and

221
00:15:10,000 --> 00:15:14,000
 which is very key to make it work like

222
00:15:14,000 --> 00:15:17,000
 our brain it is everyone is a linear

223
00:15:17,000 --> 00:15:21,000
 like this sigma x equal to x so regardless

224
00:15:21,000 --> 00:15:23,000
 how many units and the year you have you

225
00:15:23,000 --> 00:15:27,000
 can call us call collapse them into just

226
00:15:27,000 --> 00:15:30,000
 one single unit which can only then do a

227
00:15:30,000 --> 00:15:33,000
 linear classifier it is not going to solve

228
00:15:33,000 --> 00:15:36,000
 very complex problem then we talk about

229
00:15:36,000 --> 00:15:40,000
 this a sigmoid function usually this

230
00:15:40,000 --> 00:15:43,000
 alpha of course you can select it as a

231
00:15:43,000 --> 00:15:45,000
 one that become one of the sigmoid

232
00:15:45,000 --> 00:15:48,000
 function we saw in the last lecture in

233
00:15:48,000 --> 00:15:51,000
 the later part of the lecture the back

234
00:15:51,000 --> 00:15:57,000
 propagation algorithm and and then this

235
00:15:57,000 --> 00:16:00,000
 actually allow the output to to stay

236
00:16:00,000 --> 00:16:05,000
 between 0 to 1 then 10 x this is the one

237
00:16:05,000 --> 00:16:07,000
 these two actually were used in the young

238
00:16:07,000 --> 00:16:10,000
 the Kuhn learn at activation function

239
00:16:10,000 --> 00:16:13,000
 and there's a sign function if you

240
00:16:13,000 --> 00:16:15,000
 recall last week we mentioned that

241
00:16:15,000 --> 00:16:18,000
 during the training of back propagation

242
00:16:18,000 --> 00:16:21,000
 you always have to use this the so-called

243
00:16:21,000 --> 00:16:25,000
 sigma prime x the derivative of your

244
00:16:25,000 --> 00:16:28,000
 activation function when you do the

245
00:16:28,000 --> 00:16:31,000
 error back propagation from the output

246
00:16:31,000 --> 00:16:34,000
 and that work to the input and right and

247
00:16:34,000 --> 00:16:37,000
 you every step when you compute the

248
00:16:37,000 --> 00:16:39,000
 error term you have to multiply with

249
00:16:39,000 --> 00:16:42,000
 this derivative of the activation

250
00:16:42,000 --> 00:16:45,000
 function so with because of this that

251
00:16:45,000 --> 00:16:48,000
 make this sign function very much useless

252
00:16:48,000 --> 00:16:51,000
 for learning purpose because it's going

253
00:16:51,000 --> 00:16:54,000
 to be zero almost everywhere except when

254
00:16:54,000 --> 00:16:58,000
 x equal to zero which is undefined right

255
00:16:58,000 --> 00:17:00,000
 so they mean if you use that basically

256
00:17:00,000 --> 00:17:02,000
 your network cannot learn right because

257
00:17:02,000 --> 00:17:05,000
 after even you have an error but it's

258
00:17:05,000 --> 00:17:08,000
 going to multiply with zero right no

259
00:17:08,000 --> 00:17:10,000
 error can be propagated back to the

260
00:17:10,000 --> 00:17:12,000
 network so therefore normally we will

261
00:17:12,000 --> 00:17:15,000
 select those function that you have

262
00:17:15,000 --> 00:17:20,000
 so-called non zero derivative of the

263
00:17:20,000 --> 00:17:23,000
 activation function in particular the

264
00:17:23,000 --> 00:17:27,000
 red rule rectify linear unit which is

265
00:17:27,000 --> 00:17:31,000
 very easy to compute because when x is

266
00:17:31,000 --> 00:17:35,000
 larger than zero the derivative sigma x

267
00:17:35,000 --> 00:17:39,000
 is equal to one here right and then zero

268
00:17:39,000 --> 00:17:42,000
 when x is less than zero right of course

269
00:17:42,000 --> 00:17:44,000
 you can have a slight variation but

270
00:17:44,000 --> 00:17:48,000
 normally this a leaky red rule is not

271
00:17:48,000 --> 00:17:50,000
 really as popular or very much different

272
00:17:50,000 --> 00:17:54,000
 for a red rule then we talk about back

273
00:17:54,000 --> 00:17:57,000
 propagation last week the whole idea is

274
00:17:57,000 --> 00:18:00,000
 if you have a network like this you can

275
00:18:00,000 --> 00:18:04,000
 have many many hidden you need output

276
00:18:04,000 --> 00:18:07,000
 you need ultimately what you want is to

277
00:18:07,000 --> 00:18:11,000
 have the actual output as close to the

278
00:18:11,000 --> 00:18:15,000
 target output as possible right so

279
00:18:15,000 --> 00:18:18,000
 therefore you can learn those parameter

280
00:18:18,000 --> 00:18:22,000
 the weights and the buyers by minimizing

281
00:18:22,000 --> 00:18:25,000
 some error function right as long as you

282
00:18:25,000 --> 00:18:29,000
 can express the z the cost function

283
00:18:29,000 --> 00:18:32,000
 error function as a function of those

284
00:18:32,000 --> 00:18:35,000
 unknown parameters right you think about

285
00:18:35,000 --> 00:18:39,000
 the this so-called neural network you

286
00:18:39,000 --> 00:18:43,000
 can always express this right every input

287
00:18:43,000 --> 00:18:46,000
 is a summation right all this summation

288
00:18:46,000 --> 00:18:51,000
 and then activation function and then

289
00:18:51,000 --> 00:18:53,000
 the output now become the input of the

290
00:18:53,000 --> 00:18:56,000
 next neuron so you can always express

291
00:18:56,000 --> 00:18:59,000
 the y in term of all the weights and

292
00:18:59,000 --> 00:19:03,000
 buyers in between right and then you

293
00:19:03,000 --> 00:19:07,000
 can now train the network by using this

294
00:19:07,000 --> 00:19:12,000
 gradient descent which is basically you

295
00:19:12,000 --> 00:19:15,000
 take the partial derivative of this error

296
00:19:15,000 --> 00:19:17,000
 function with respect to any of the

297
00:19:17,000 --> 00:19:20,000
 unknown you want to find right then you

298
00:19:20,000 --> 00:19:22,000
 move into the negative direction because

299
00:19:22,000 --> 00:19:25,000
 if the gradient is positive when you move

300
00:19:25,000 --> 00:19:27,000
 in the positive direction the error will

301
00:19:27,000 --> 00:19:29,000
 increase so you should move in the

302
00:19:29,000 --> 00:19:31,000
 opposite direction therefore you have the

303
00:19:31,000 --> 00:19:34,000
 minus and then the adjustment you try to

304
00:19:34,000 --> 00:19:37,000
 moderate because you do not know the

305
00:19:37,000 --> 00:19:40,000
 magnitude of the change whether it's too

306
00:19:40,000 --> 00:19:44,000
 big or small so you always can adjust it

307
00:19:44,000 --> 00:19:48,000
 by using a learning rate and then you

308
00:19:48,000 --> 00:19:51,000
 update your parameter with the new one

309
00:19:51,000 --> 00:19:54,000
 by moving in the negative direction of

310
00:19:54,000 --> 00:19:57,000
 this gradient right this is called

311
00:19:57,000 --> 00:20:01,000
 gradient descent right and then we talk

312
00:20:01,000 --> 00:20:05,000
 about the impact or effects of different

313
00:20:05,000 --> 00:20:09,000
 type of learning rate which would have on

314
00:20:09,000 --> 00:20:12,000
 the outcome of your learning and this one

315
00:20:12,000 --> 00:20:14,000
 showed that not necessarily you can always

316
00:20:14,000 --> 00:20:16,000
 reach the let's say global minimum

317
00:20:16,000 --> 00:20:19,000
 assuming this is somehow error function

318
00:20:19,000 --> 00:20:22,000
 surface and this is just 2D with X1 X2

319
00:20:22,000 --> 00:20:25,000
 input you can imagine that if you have

320
00:20:25,000 --> 00:20:28,000
 million or billions of parameter and that

321
00:20:28,000 --> 00:20:30,000
 kind of cost function survey is something

322
00:20:30,000 --> 00:20:33,000
 that you really cannot imagine and

323
00:20:33,000 --> 00:20:36,000
 therefore when you start in any random

324
00:20:36,000 --> 00:20:40,000
 so-called location in this surface where

325
00:20:40,000 --> 00:20:44,000
 you will end up with is really uncertain

326
00:20:44,000 --> 00:20:46,000
 right you keep training like this case

327
00:20:46,000 --> 00:20:49,000
 trapped in somehow local minimum and

328
00:20:49,000 --> 00:20:52,000
 these two and all end up with this

329
00:20:52,000 --> 00:20:57,000
 global minimum then at the end we

330
00:20:57,000 --> 00:20:59,000
 talk about this back propagation

331
00:20:59,000 --> 00:21:02,000
 algorithm we quickly walk through these

332
00:21:02,000 --> 00:21:05,000
 two slides but today we are going to go

333
00:21:05,000 --> 00:21:08,000
 through the detail which of course are

334
00:21:08,000 --> 00:21:11,000
 optional but I believe it has some value

335
00:21:11,000 --> 00:21:15,000
 if you really can understand how this

336
00:21:15,000 --> 00:21:18,000
 back propagation algorithm works because

337
00:21:18,000 --> 00:21:22,000
 this is really the core of enabling all

338
00:21:22,000 --> 00:21:25,000
 these new network in fact part of the

339
00:21:25,000 --> 00:21:28,000
 Hinton contribution and one who just

340
00:21:28,000 --> 00:21:31,000
 received the Nobel Prize in physics

341
00:21:31,000 --> 00:21:33,000
 right I think there was an interview

342
00:21:33,000 --> 00:21:35,000
 online he said he felt his physics how

343
00:21:35,000 --> 00:21:37,000
 could someone feel the physics that

344
00:21:37,000 --> 00:21:39,000
 eventually received the Nobel Prize in

345
00:21:39,000 --> 00:21:44,000
 physics thanks to this so-called some of

346
00:21:44,000 --> 00:21:47,000
 the new network work that he has done

347
00:21:47,000 --> 00:21:53,000
 so okay you we can focus on this

348
00:21:53,000 --> 00:21:56,000
 multi-layer perceptron network but you

349
00:21:56,000 --> 00:22:00,000
 can expect or imagine that the network

350
00:22:00,000 --> 00:22:03,000
 can be very complex or very different

351
00:22:03,000 --> 00:22:06,000
 from such configuration you're going to

352
00:22:06,000 --> 00:22:09,000
 see some of these variation later part

353
00:22:09,000 --> 00:22:13,000
 but ultimately as long as you can write

354
00:22:13,000 --> 00:22:18,000
 the okay let me move this a TK target

355
00:22:18,000 --> 00:22:21,000
 output to here this is a target output

356
00:22:21,000 --> 00:22:27,000
 as long as you can write the error

357
00:22:27,000 --> 00:22:32,000
 function here J or you just use E as a

358
00:22:32,000 --> 00:22:36,000
 function of all the parameters that you

359
00:22:36,000 --> 00:22:40,000
 need to find the optimum value you can

360
00:22:40,000 --> 00:22:43,000
 use the back propagation plus the gradient

361
00:22:43,000 --> 00:22:47,000
 descent to train the network right by

362
00:22:47,000 --> 00:22:49,000
 first define what is your error function

363
00:22:49,000 --> 00:22:53,000
 here is a TK minus OK OK is the actual

364
00:22:53,000 --> 00:22:57,000
 output TK is the target output when you

365
00:22:57,000 --> 00:23:00,000
 first start your training like these two

366
00:23:00,000 --> 00:23:02,000
 will not be the same right if they are

367
00:23:02,000 --> 00:23:04,000
 the same not no training will happen

368
00:23:04,000 --> 00:23:06,000
 right you already have the network which

369
00:23:06,000 --> 00:23:09,000
 can do the job so you want it to be

370
00:23:09,000 --> 00:23:11,000
 different then of course in this case we

371
00:23:11,000 --> 00:23:14,000
 define the sum of square errors right

372
00:23:14,000 --> 00:23:21,000
 which is a L2 norm right and these two

373
00:23:21,000 --> 00:23:23,000
 factor is arbitrary just that when we

374
00:23:23,000 --> 00:23:25,000
 derive the formula later you'll find

375
00:23:25,000 --> 00:23:28,000
 that you will not have this so-called

376
00:23:28,000 --> 00:23:31,000
 constant there which will make the

377
00:23:31,000 --> 00:23:37,000
 so-called the adjustment simpler

378
00:23:37,000 --> 00:23:42,000
 okay and then your weights adjustment

379
00:23:42,000 --> 00:23:44,000
 will be the gradient descent right

380
00:23:44,000 --> 00:23:46,000
 partial derivative with this particular

381
00:23:46,000 --> 00:23:48,000
 weight multiplied with the learning rate

382
00:23:48,000 --> 00:23:51,000
 adjust in the negative direction

383
00:23:51,000 --> 00:23:54,000
 similarly for your bias again these

384
00:23:54,000 --> 00:23:58,000
 bias could be a spatial term in this

385
00:23:58,000 --> 00:24:00,000
 W but you can write it separately

386
00:24:00,000 --> 00:24:03,000
 because the formula is slightly different

387
00:24:03,000 --> 00:24:06,000
 but it's also partial derivative is

388
00:24:06,000 --> 00:24:08,000
 still gradient descent with this unknown

389
00:24:08,000 --> 00:24:11,000
 variable right so the way you do the

390
00:24:11,000 --> 00:24:14,000
 learning is the first step is try to

391
00:24:14,000 --> 00:24:16,000
 initialize before the training you don't

392
00:24:16,000 --> 00:24:19,000
 know what will be the appropriate

393
00:24:19,000 --> 00:24:23,000
 weights and bias so one way is to

394
00:24:23,000 --> 00:24:26,000
 initialize all the weights and bias with

395
00:24:26,000 --> 00:24:31,000
 random so-called number and certainly

396
00:24:31,000 --> 00:24:34,000
 you don't want them to be all zero right

397
00:24:34,000 --> 00:24:36,000
 if they're all zero you think about all

398
00:24:36,000 --> 00:24:39,000
 the multiplication whatever input you

399
00:24:39,000 --> 00:24:41,000
 have it will really become zero right

400
00:24:41,000 --> 00:24:45,000
 so nothing will change right so you want

401
00:24:45,000 --> 00:24:49,000
 to you want this weights and bias to

402
00:24:49,000 --> 00:24:52,000
 start with the random values which are

403
00:24:52,000 --> 00:24:55,000
 as different as possible so you can use

404
00:24:55,000 --> 00:24:59,000
 a Gaussian random generator to create the

405
00:24:59,000 --> 00:25:02,000
 weights and bias zero mean and then

406
00:25:02,000 --> 00:25:07,000
 given any training input data X you

407
00:25:07,000 --> 00:25:11,000
 fit the data into the network you

408
00:25:11,000 --> 00:25:14,000
 calculate when you are X1 to Xn right

409
00:25:14,000 --> 00:25:17,000
 then you can now okay this is the input

410
00:25:17,000 --> 00:25:19,000
 you need the output here is equal to X so

411
00:25:19,000 --> 00:25:22,000
 something very unique for the input is

412
00:25:22,000 --> 00:25:25,000
 we do not apply any normally normally we

413
00:25:25,000 --> 00:25:27,000
 do not apply any activation function

414
00:25:27,000 --> 00:25:31,000
 whatever it come in output okay then

415
00:25:31,000 --> 00:25:34,000
 that will become the input to the

416
00:25:34,000 --> 00:25:38,000
 following layer units right and then

417
00:25:38,000 --> 00:25:43,000
 you can compute for any node J your

418
00:25:43,000 --> 00:25:47,000
 first computer net J net input is the

419
00:25:47,000 --> 00:25:50,000
 same perceptron that you have seen

420
00:25:50,000 --> 00:25:52,000
 before then you apply the activation

421
00:25:52,000 --> 00:25:54,000
 function to get the output of that unit

422
00:25:54,000 --> 00:25:58,000
 J okay you do that for every unit

423
00:25:58,000 --> 00:26:00,000
 including the output unit then you get

424
00:26:00,000 --> 00:26:03,000
 okay then you check whether TK and

425
00:26:03,000 --> 00:26:06,000
 okay's are the same if they are not the

426
00:26:06,000 --> 00:26:08,000
 same then you have error term and that

427
00:26:08,000 --> 00:26:13,000
 error term is can be compute by this

428
00:26:13,000 --> 00:26:15,000
 formula we are going to derive this

429
00:26:15,000 --> 00:26:18,000
 formula later okay but for the time being

430
00:26:18,000 --> 00:26:22,000
 you just kind of take this for granted

431
00:26:22,000 --> 00:26:25,000
 that because TK and okay are different at

432
00:26:25,000 --> 00:26:28,000
 the output unit TK minus okay will be

433
00:26:28,000 --> 00:26:32,000
 the error right that error the difference

434
00:26:32,000 --> 00:26:34,000
 between the actual output and the target

435
00:26:34,000 --> 00:26:37,000
 output then when you move this error

436
00:26:37,000 --> 00:26:41,000
 across this unit back propagation okay

437
00:26:41,000 --> 00:26:43,000
 because you only have the error at the

438
00:26:43,000 --> 00:26:46,000
 output if you want to move this forward

439
00:26:46,000 --> 00:26:49,000
 you multiply with the derivative of

440
00:26:49,000 --> 00:26:53,000
 activation function because when you

441
00:26:53,000 --> 00:26:57,000
 compute forward the net J will go to the

442
00:26:57,000 --> 00:26:59,000
 activation function you get the output

443
00:26:59,000 --> 00:27:04,000
 but what this delta K right this error

444
00:27:04,000 --> 00:27:10,000
 term delta K here it means is if you want

445
00:27:10,000 --> 00:27:12,000
 this error basically when you compute

446
00:27:12,000 --> 00:27:14,000
 this right your goal is to adjust the

447
00:27:14,000 --> 00:27:17,000
 weights to make this error smaller right

448
00:27:17,000 --> 00:27:20,000
 so therefore the adjustment that you need

449
00:27:20,000 --> 00:27:24,000
 to make will depend on the gradient of

450
00:27:24,000 --> 00:27:26,000
 the activation function because you want

451
00:27:26,000 --> 00:27:28,000
 to see where it is so that you know

452
00:27:28,000 --> 00:27:31,000
 whether you are going to increase or

453
00:27:31,000 --> 00:27:33,000
 reduce the error therefore this is

454
00:27:33,000 --> 00:27:36,000
 the derivative of the activation function

455
00:27:36,000 --> 00:27:38,000
 will tell you whether you something like

456
00:27:38,000 --> 00:27:40,000
 gradient descent move to the left to the

457
00:27:40,000 --> 00:27:44,000
 right right so that is the local error

458
00:27:44,000 --> 00:27:47,000
 that you want to compute for all the

459
00:27:47,000 --> 00:27:50,000
 output layer and then for the hidden layer

460
00:27:50,000 --> 00:27:55,000
 the delta Z right the same thing delta

461
00:27:55,000 --> 00:27:58,000
 Z is you want to see how much adjustment

462
00:27:58,000 --> 00:28:01,000
 you want to make at the input here so

463
00:28:01,000 --> 00:28:03,000
 that at the end you are in front of

464
00:28:03,000 --> 00:28:05,000
 the error at the output right you have

465
00:28:05,000 --> 00:28:07,000
 the error at the output is further away

466
00:28:07,000 --> 00:28:09,000
 from here this error at the output

467
00:28:09,000 --> 00:28:11,000
 actually will come from when you adjust

468
00:28:11,000 --> 00:28:14,000
 this will also affect the error of this

469
00:28:14,000 --> 00:28:17,000
 too therefore you have to sum the error

470
00:28:17,000 --> 00:28:19,000
 of output you need multiply with the

471
00:28:19,000 --> 00:28:22,000
 assisting weight WKJ something up back

472
00:28:22,000 --> 00:28:25,000
 to here that become the error before

473
00:28:25,000 --> 00:28:28,000
 you move to the front right once you

474
00:28:28,000 --> 00:28:30,000
 move to the front you have to multiply

475
00:28:30,000 --> 00:28:33,000
 with the gradient of the activation function

476
00:28:33,000 --> 00:28:36,000
 this is how you try to remember the formula

477
00:28:36,000 --> 00:28:39,000
 right look at the picture try to see

478
00:28:39,000 --> 00:28:41,000
 okay this is the error I need to compute

479
00:28:41,000 --> 00:28:45,000
 at first is the TK minus OK right when

480
00:28:45,000 --> 00:28:47,000
 I move it to the forward I multiply with

481
00:28:47,000 --> 00:28:50,000
 the gradient of the activation function

482
00:28:50,000 --> 00:28:52,000
 right then you have all the error at the

483
00:28:52,000 --> 00:28:55,000
 output layer then for the hidden layer

484
00:28:55,000 --> 00:28:56,000
 because when I change this you affect

485
00:28:56,000 --> 00:28:59,000
 all this error you have to sum them up

486
00:28:59,000 --> 00:29:01,000
 weight them back here then move to the

487
00:29:01,000 --> 00:29:04,000
 front multiply with the gradient of the

488
00:29:04,000 --> 00:29:07,000
 activation again so it's not difficult

489
00:29:07,000 --> 00:29:09,000
 to remember once you have a charge right

490
00:29:09,000 --> 00:29:12,000
 even this is very complex it just trace

491
00:29:12,000 --> 00:29:15,000
 the error term right and sum them up

492
00:29:15,000 --> 00:29:18,000
 move here right as long as there's a link

493
00:29:18,000 --> 00:29:20,000
 you have to make sure you move the error back

494
00:29:20,000 --> 00:29:24,000
 right it may not be so so-called structure

495
00:29:24,000 --> 00:29:28,000
 every every unit connect to other unit

496
00:29:28,000 --> 00:29:32,000
 so these are the two key formula you

497
00:29:32,000 --> 00:29:36,000
 compute the error this one is already

498
00:29:36,000 --> 00:29:39,000
 no forward propagation and this is a

499
00:29:39,000 --> 00:29:41,000
 backward propagation after you have the error

500
00:29:41,000 --> 00:29:43,000
 then you know how to adjust the weight

501
00:29:43,000 --> 00:29:47,000
 for example WKJ will be the error at this

502
00:29:47,000 --> 00:29:51,000
 output following node delta K

503
00:29:51,000 --> 00:29:54,000
 multiply with the output OZ

504
00:29:54,000 --> 00:29:57,000
 multiply with the eta W right when you

505
00:29:57,000 --> 00:29:59,000
 look at the picture it's very straightforward

506
00:29:59,000 --> 00:30:01,000
 right when you have the computer

507
00:30:01,000 --> 00:30:04,000
 adjustment do this weight you multiply the

508
00:30:04,000 --> 00:30:06,000
 error here multiply the output here

509
00:30:06,000 --> 00:30:09,000
 multiply that learning rate eta and

510
00:30:09,000 --> 00:30:11,000
 that's it and similarly for the bias

511
00:30:11,000 --> 00:30:14,000
 because the bias you can consider it is

512
00:30:14,000 --> 00:30:17,000
 a spatial unit of output one with the

513
00:30:17,000 --> 00:30:20,000
 weight BZ so the delta BZ will be

514
00:30:20,000 --> 00:30:23,000
 equal to again the error term delta Z

515
00:30:23,000 --> 00:30:26,000
 multiply with output here is equal to one

516
00:30:26,000 --> 00:30:29,000
 therefore you don't have the OI here

517
00:30:29,000 --> 00:30:31,000
 because it's equal to one multiply with the

518
00:30:31,000 --> 00:30:35,000
 eta B right this is all the adjustment

519
00:30:35,000 --> 00:30:38,000
 you will make to all the parameter here

520
00:30:38,000 --> 00:30:40,000
 and that's it that will be your back

521
00:30:40,000 --> 00:30:44,000
 propagation formula it is once you put

522
00:30:44,000 --> 00:30:48,000
 them into this charge it seems to be not

523
00:30:48,000 --> 00:30:52,000
 so complicated just trace the error trace

524
00:30:52,000 --> 00:30:55,000
 the output right and then you can imagine

525
00:30:55,000 --> 00:30:58,000
 that all this intermediate value right

526
00:30:58,000 --> 00:31:02,000
 the net Z because you need this the net K

527
00:31:02,000 --> 00:31:06,000
 the output OK the output right the OZ

528
00:31:06,000 --> 00:31:10,000
 OI the error term delta Z delta K

529
00:31:10,000 --> 00:31:15,000
 you have to store them in your memory

530
00:31:15,000 --> 00:31:18,000
 right when you have an input that all

531
00:31:18,000 --> 00:31:20,000
 depend on your input train data every

532
00:31:20,000 --> 00:31:23,000
 sample here you recompute all this OZ

533
00:31:23,000 --> 00:31:28,000
 net Z and delta ZK right then all has to

534
00:31:28,000 --> 00:31:31,000
 be stored in your memory then you can

535
00:31:31,000 --> 00:31:34,000
 make the adjustment W right so that is

536
00:31:34,000 --> 00:31:37,000
 a part that they need a lot of RAM and

537
00:31:37,000 --> 00:31:40,000
 your GPU memory right if you want to

538
00:31:40,000 --> 00:31:43,000
 train a big network and likely you need

539
00:31:43,000 --> 00:31:46,000
 to have many many gigs

540
00:31:46,000 --> 00:31:49,000
 typically the one that you are using

541
00:31:49,000 --> 00:31:54,000
 for your personal PC maybe 4G

542
00:31:54,000 --> 00:32:01,000
 6 gigabyte maybe 12 16 maybe 32 is

543
00:32:01,000 --> 00:32:04,000
 already a lot but those are server you

544
00:32:04,000 --> 00:32:09,000
 could expect by 100 200 or minimum 40 80

545
00:32:09,000 --> 00:32:12,000
 they can have a gigabyte memory for the

546
00:32:12,000 --> 00:32:16,000
 training purpose so if your activation

547
00:32:16,000 --> 00:32:20,000
 function is this a sigmoid as I show

548
00:32:20,000 --> 00:32:22,000
 in the last lecture you can show that

549
00:32:22,000 --> 00:32:25,000
 your gradient of the activation function

550
00:32:25,000 --> 00:32:29,000
 sigma prime it will become sigma

551
00:32:29,000 --> 00:32:33,000
 that net K times one sigma net K

552
00:32:33,000 --> 00:32:36,000
 sigma net K will be equal to OK

553
00:32:36,000 --> 00:32:38,000
 basically what this output like net K

554
00:32:38,000 --> 00:32:40,000
 is input after you apply the activation

555
00:32:40,000 --> 00:32:43,000
 function become the OK so it become

556
00:32:43,000 --> 00:32:46,000
 OK 1 minus OK and I show this in the last

557
00:32:46,000 --> 00:32:49,000
 lecture the derivative of this right

558
00:32:49,000 --> 00:32:54,000
 you can get this and similarly for net

559
00:32:54,000 --> 00:32:58,000
 Z sigma prime is equal to OZ 1 minus OZ

560
00:32:58,000 --> 00:33:00,000
 right and the remaining is still the same

561
00:33:00,000 --> 00:33:04,000
 right for red rule right and you can

562
00:33:04,000 --> 00:33:07,000
 see this is even simpler for red rule

563
00:33:07,000 --> 00:33:09,000
 you just check whether the net K is

564
00:33:09,000 --> 00:33:12,000
 positive or negative it is a positive

565
00:33:12,000 --> 00:33:15,000
 value then this equal to one net

566
00:33:15,000 --> 00:33:18,000
 sigma prime net K equal to one if it's

567
00:33:18,000 --> 00:33:21,000
 a negative value the net input then the

568
00:33:21,000 --> 00:33:23,000
 similar prime net equal to zero that

569
00:33:23,000 --> 00:33:26,000
 means no error will be propagated if the

570
00:33:26,000 --> 00:33:32,000
 input is zero yeah this way we cover

571
00:33:32,000 --> 00:33:36,000
 in the last lecture and of course after

572
00:33:36,000 --> 00:33:39,000
 that for every sample training sample

573
00:33:39,000 --> 00:33:42,000
 you do all the adjustment then you

574
00:33:42,000 --> 00:33:44,000
 continue to repeat this process for all

575
00:33:44,000 --> 00:33:47,000
 the rest of the training samples and

576
00:33:47,000 --> 00:33:49,000
 again and again if one time that is not

577
00:33:49,000 --> 00:33:51,000
 enough because you can see the net

578
00:33:51,000 --> 00:33:53,000
 work is changing right the W keep

579
00:33:53,000 --> 00:33:55,000
 changing even you have used this

580
00:33:55,000 --> 00:33:57,000
 sample before but by the time you

581
00:33:57,000 --> 00:34:00,000
 finish all the training sample that

582
00:34:00,000 --> 00:34:02,000
 net work is already different so you

583
00:34:02,000 --> 00:34:04,000
 can reuse the point again to continue

584
00:34:04,000 --> 00:34:07,000
 to move this so-called W towards the

585
00:34:07,000 --> 00:34:12,000
 right direction and and you repeat this

586
00:34:12,000 --> 00:34:15,000
 until a termination condition is met

587
00:34:15,000 --> 00:34:18,000
 which could be the error is small

588
00:34:18,000 --> 00:34:21,000
 enough the E or J W is very little I

589
00:34:21,000 --> 00:34:23,000
 think in some of the example we saw

590
00:34:23,000 --> 00:34:27,000
 earlier you can see drop then right

591
00:34:27,000 --> 00:34:31,000
 tapper off okay the error and then all

592
00:34:31,000 --> 00:34:35,000
 until all the delta W delta B the

593
00:34:35,000 --> 00:34:38,000
 adjustment become very small right and

594
00:34:38,000 --> 00:34:41,000
 that means not much change to the net

595
00:34:41,000 --> 00:34:46,000
 work or until you run out of time right

596
00:34:46,000 --> 00:34:48,000
 it's have been too long right and then

597
00:34:48,000 --> 00:34:53,000
 you just want to just terminate and

598
00:34:53,000 --> 00:34:56,000
 the weights and the bias can be updated

599
00:34:56,000 --> 00:34:59,000
 in the in some different way we thought

600
00:34:59,000 --> 00:35:02,000
 avoid case updating is every sample you

601
00:35:02,000 --> 00:35:06,000
 change the delta W the W and B right so

602
00:35:06,000 --> 00:35:08,000
 the net work keep changing for every

603
00:35:08,000 --> 00:35:11,000
 sample and that become very chaotic like

604
00:35:11,000 --> 00:35:13,000
 the example we show in the decision 3

605
00:35:13,000 --> 00:35:16,000
 right you could have a two feature

606
00:35:16,000 --> 00:35:18,000
 exactly the same but output different so

607
00:35:18,000 --> 00:35:20,000
 they mean in one sample you move this W

608
00:35:20,000 --> 00:35:22,000
 to this direction in the next example

609
00:35:22,000 --> 00:35:25,000
 you move it back right so it's going to

610
00:35:25,000 --> 00:35:27,000
 be very noisy every sample the network

611
00:35:27,000 --> 00:35:30,000
 keep changing so which has its merit

612
00:35:30,000 --> 00:35:32,000
 but normally we don't do that if you

613
00:35:32,000 --> 00:35:34,000
 have a lot training sample so the other

614
00:35:34,000 --> 00:35:37,000
 way is called airport updating or

615
00:35:37,000 --> 00:35:40,000
 you put up training which when you

616
00:35:40,000 --> 00:35:43,000
 train this train the network you do not

617
00:35:43,000 --> 00:35:45,000
 adjust the parameter right away you just

618
00:35:45,000 --> 00:35:48,000
 accumulate all the delta W delta B but

619
00:35:48,000 --> 00:35:51,000
 you don't change the network the input

620
00:35:51,000 --> 00:35:53,000
 another sample you accumulate the delta

621
00:35:53,000 --> 00:35:56,000
 W delta B so you can't take how averaging

622
00:35:56,000 --> 00:35:59,000
 all the possible changes you're supposed to

623
00:35:59,000 --> 00:36:02,000
 make only at the end after you have seen

624
00:36:02,000 --> 00:36:04,000
 all the training sample you take the

625
00:36:04,000 --> 00:36:06,000
 average of all the delta W delta B

626
00:36:06,000 --> 00:36:09,000
 update the network at the end of the

627
00:36:09,000 --> 00:36:14,000
 airport right so they mean the delta

628
00:36:14,000 --> 00:36:17,000
 delta B can move randomly at the end

629
00:36:17,000 --> 00:36:20,000
 you just take the mean value of all those

630
00:36:20,000 --> 00:36:23,000
 changes it will be more stable but it will

631
00:36:23,000 --> 00:36:25,000
 be slower the network will not update

632
00:36:25,000 --> 00:36:29,000
 immediately and therefore you can

633
00:36:29,000 --> 00:36:32,000
 consider the compromise between the two

634
00:36:32,000 --> 00:36:35,000
 previous approaches by using

635
00:36:35,000 --> 00:36:38,000
 stochastic gradient descent which you

636
00:36:38,000 --> 00:36:41,000
 randomly sample a set of training

637
00:36:41,000 --> 00:36:45,000
 sample and then which you do all the

638
00:36:45,000 --> 00:36:49,000
 compute all the W B W delta W delta B

639
00:36:49,000 --> 00:36:51,000
 then you take the average update the

640
00:36:51,000 --> 00:36:55,000
 network then sample another set of

641
00:36:55,000 --> 00:36:58,000
 training data so you repeat this so

642
00:36:58,000 --> 00:37:00,000
 they mean the network is updated after

643
00:37:00,000 --> 00:37:02,000
 you have seen certain number of

644
00:37:02,000 --> 00:37:05,000
 training sample and then you keep them

645
00:37:05,000 --> 00:37:09,000
 the same until you have exorbit the

646
00:37:09,000 --> 00:37:13,000
 selected number of selected samples

647
00:37:13,000 --> 00:37:15,000
 then you repeat this until all the

648
00:37:15,000 --> 00:37:18,000
 sample have been used for the training

649
00:37:18,000 --> 00:37:23,000
 right they cause stochastic gradient descent

650
00:37:23,000 --> 00:37:26,000
 okay that is the

651
00:37:26,000 --> 00:37:30,000
 that propagation so the few following

652
00:37:30,000 --> 00:37:32,000
 slide I'm going to show you how we

653
00:37:32,000 --> 00:37:38,000
 derive the delta K delta Z and delta W

654
00:37:38,000 --> 00:37:44,000
 delta B I label that as optional so they

655
00:37:44,000 --> 00:37:48,000
 mean if you really do not like to know

656
00:37:48,000 --> 00:37:50,000
 how those formula come from you can

657
00:37:50,000 --> 00:37:55,000
 skip them but I have to mention this

658
00:37:55,000 --> 00:37:58,000
 because the formula you have seen the

659
00:37:58,000 --> 00:38:03,000
 adjustment delta K delta W and or delta

660
00:38:03,000 --> 00:38:06,000
 error term they are all based on this

661
00:38:06,000 --> 00:38:09,000
 sum of square error right if this error

662
00:38:09,000 --> 00:38:12,000
 term is no longer the same which like

663
00:38:12,000 --> 00:38:15,000
 the will be the case for some of the

664
00:38:15,000 --> 00:38:18,000
 network you'll see one later then the

665
00:38:18,000 --> 00:38:20,000
 whole formula will change then you have

666
00:38:20,000 --> 00:38:24,000
 to derive those formula again right so

667
00:38:24,000 --> 00:38:26,000
 it's therefore it's important to know the

668
00:38:26,000 --> 00:38:29,000
 step how you derive those formula as

669
00:38:29,000 --> 00:38:31,000
 and when when you change your error

670
00:38:31,000 --> 00:38:34,000
 function which most likely so then you

671
00:38:34,000 --> 00:38:39,000
 can do the derivative again for those

672
00:38:39,000 --> 00:38:45,000
 formula this of course the gradient

673
00:38:45,000 --> 00:38:47,000
 descent remain unchanged the only thing

674
00:38:47,000 --> 00:38:51,000
 is the error function so we we mentioned

675
00:38:51,000 --> 00:38:54,000
 earlier for this particular error function

676
00:38:54,000 --> 00:38:57,000
 and this is a delta K for output unit

677
00:38:57,000 --> 00:39:01,000
 delta Z for any input unit and this is

678
00:39:01,000 --> 00:39:03,000
 the adjustment you make okay all these

679
00:39:03,000 --> 00:39:08,000
 adjustment formula this and this and

680
00:39:08,000 --> 00:39:12,000
 this only true when your error function

681
00:39:12,000 --> 00:39:15,000
 is this okay if the error function is not

682
00:39:15,000 --> 00:39:19,000
 some of square error then this set of

683
00:39:19,000 --> 00:39:23,000
 formula will change okay so let's look at

684
00:39:23,000 --> 00:39:26,000
 one how even with this error function

685
00:39:26,000 --> 00:39:28,000
 how do we get this set of formula delta

686
00:39:28,000 --> 00:39:33,000
 K delta J and delta W delta B and this

687
00:39:33,000 --> 00:39:39,000
 is of course this activity function you

688
00:39:39,000 --> 00:39:41,000
 could have a sigmoid you could have

689
00:39:41,000 --> 00:39:44,000
 other right so for sigmoid then your

690
00:39:44,000 --> 00:39:47,000
 delta K become this delta J from here

691
00:39:47,000 --> 00:39:49,000
 become this but for other activity

692
00:39:49,000 --> 00:39:51,000
 function it may not have this kind of

693
00:39:51,000 --> 00:39:54,000
 formula okay let's look at how to

694
00:39:54,000 --> 00:39:59,000
 compute the delta K here right this is

695
00:39:59,000 --> 00:40:03,000
 output unit and you have a TK at the

696
00:40:03,000 --> 00:40:08,000
 output target output here TK okay so

697
00:40:08,000 --> 00:40:11,000
 again this is a typical network formula

698
00:40:11,000 --> 00:40:15,000
 you have seen this before so your goal is

699
00:40:15,000 --> 00:40:18,000
 now to see how to adjust this the delta

700
00:40:18,000 --> 00:40:22,000
 WKJ okay and for that you need to

701
00:40:22,000 --> 00:40:26,000
 compute the gradient delta E and delta W

702
00:40:26,000 --> 00:40:31,000
 KJ this is a part that you need to see

703
00:40:31,000 --> 00:40:35,000
 how to link this term to the variable

704
00:40:35,000 --> 00:40:38,000
 that you already know or you want to

705
00:40:38,000 --> 00:40:42,000
 compute so E is here the error between

706
00:40:42,000 --> 00:40:46,000
 TK minus OK square sum of square error

707
00:40:46,000 --> 00:40:52,000
 and WKJ is here right so as a rule of

708
00:40:52,000 --> 00:40:56,000
 thumb you need to express E in term of

709
00:40:56,000 --> 00:41:01,000
 eventually WKJ right otherwise you cannot

710
00:41:01,000 --> 00:41:06,000
 so-called change the fact how much you

711
00:41:06,000 --> 00:41:11,000
 need to change but E is not related to WKJ

712
00:41:11,000 --> 00:41:14,000
 immediately but E you know there is a

713
00:41:14,000 --> 00:41:17,000
 function of okay so you can use this kind

714
00:41:17,000 --> 00:41:21,000
 of change rule right rather than delta E

715
00:41:21,000 --> 00:41:25,000
 delta KJ you can introduce intermediate

716
00:41:25,000 --> 00:41:28,000
 variable in between right E is a function

717
00:41:28,000 --> 00:41:30,000
 of okay you can take this partial

718
00:41:30,000 --> 00:41:34,000
 derivative then okay is a function of

719
00:41:34,000 --> 00:41:39,000
 net K right because okay equal to the

720
00:41:39,000 --> 00:41:43,000
 activation function net K then net K is

721
00:41:43,000 --> 00:41:47,000
 now a function of WKJ right because net K

722
00:41:47,000 --> 00:41:51,000
 equal to summation J WKJ OJ plus BK right

723
00:41:51,000 --> 00:41:54,000
 and that's all you have so this is a

724
00:41:54,000 --> 00:41:57,000
 change rule the rule that you have learned

725
00:41:57,000 --> 00:42:02,000
 in your calculus either in the JC or in

726
00:42:02,000 --> 00:42:07,000
 your uni level one right so then this

727
00:42:07,000 --> 00:42:09,000
 delta E delta okay is very straightforward

728
00:42:09,000 --> 00:42:12,000
 because only okay will change you are

729
00:42:12,000 --> 00:42:15,000
 going to get this minus TK minus okay

730
00:42:15,000 --> 00:42:19,000
 two and two cancel each other out so it's

731
00:42:19,000 --> 00:42:22,000
 from this term only for that particular K

732
00:42:22,000 --> 00:42:26,000
 right and then you have this TK minus okay

733
00:42:26,000 --> 00:42:29,000
 the rest of the TK okay because they are

734
00:42:29,000 --> 00:42:31,000
 at the different output you need they

735
00:42:31,000 --> 00:42:34,000
 will become irrelevant only that's

736
00:42:34,000 --> 00:42:37,000
 specific K okay could be two could be three

737
00:42:37,000 --> 00:42:40,000
 could be 100 you have this minus TK minus

738
00:42:40,000 --> 00:42:44,000
 okay and this is nothing but the

739
00:42:44,000 --> 00:42:46,000
 derivative of the activation function

740
00:42:46,000 --> 00:42:50,000
 right and this one is very simple

741
00:42:50,000 --> 00:42:54,000
 because there's only one term which is

742
00:42:54,000 --> 00:42:58,000
 depending on this specific KJ again KJ

743
00:42:58,000 --> 00:43:00,000
 can be a very specific number like three

744
00:43:00,000 --> 00:43:03,000
 and five okay so therefore you leave with

745
00:43:03,000 --> 00:43:06,000
 this OJ okay and you put them together

746
00:43:06,000 --> 00:43:09,000
 you can write as a minus delta K OJ

747
00:43:09,000 --> 00:43:15,000
 and where delta K is sigma from K TK minus

748
00:43:15,000 --> 00:43:19,000
 okay the error term we introduce error

749
00:43:19,000 --> 00:43:25,000
 term so that the change here will be

750
00:43:25,000 --> 00:43:32,000
 minus A W delta K and OJ here

751
00:43:32,000 --> 00:43:35,000
 okay so these two are the one that you

752
00:43:35,000 --> 00:43:39,000
 saw earlier right so it's nothing just do

753
00:43:39,000 --> 00:43:43,000
 the gradient descent to minimize the

754
00:43:43,000 --> 00:43:47,000
 error by adjusting this parameter WKJ

755
00:43:47,000 --> 00:43:52,000
 then you express this error in term of WKJ

756
00:43:52,000 --> 00:43:54,000
 if not immediately you go to the

757
00:43:54,000 --> 00:43:57,000
 intermediate variable like okay and net

758
00:43:57,000 --> 00:44:00,000
 K then you reach the WKJ applying

759
00:44:00,000 --> 00:44:04,000
 change rule right and then that is the

760
00:44:04,000 --> 00:44:07,000
 formula you use but bear in mind that this

761
00:44:07,000 --> 00:44:11,000
 only apply to when error is sums of

762
00:44:11,000 --> 00:44:16,000
 square if you change this error to other

763
00:44:16,000 --> 00:44:19,000
 term then you go to this process and this

764
00:44:19,000 --> 00:44:22,000
 term will change right and you'll see what

765
00:44:22,000 --> 00:44:24,000
 will happen to this term the other two

766
00:44:24,000 --> 00:44:26,000
 term line will remain because these are

767
00:44:26,000 --> 00:44:28,000
 the network then you can adjust your

768
00:44:28,000 --> 00:44:33,000
 formula accordingly okay then what

769
00:44:33,000 --> 00:44:36,000
 happened to delta Z the one at the

770
00:44:36,000 --> 00:44:39,000
 hidden layer although I only show one

771
00:44:39,000 --> 00:44:41,000
 hidden layer you can generate this to

772
00:44:41,000 --> 00:44:44,000
 multiple hidden layer they all apply

773
00:44:44,000 --> 00:44:47,000
 this same formula okay from the

774
00:44:47,000 --> 00:44:50,000
 foreign unit and come back here so again

775
00:44:50,000 --> 00:44:53,000
 what you need to do is this to find out

776
00:44:53,000 --> 00:44:55,000
 what is the adjustment you need to make

777
00:44:55,000 --> 00:45:00,000
 to this the delta WJI here and again

778
00:45:00,000 --> 00:45:04,000
 E because it is not really immediately a

779
00:45:04,000 --> 00:45:07,000
 function of WJI then you have to go to

780
00:45:07,000 --> 00:45:10,000
 some intermediate parameter which is

781
00:45:10,000 --> 00:45:15,000
 delta E delta OZ output here then delta

782
00:45:15,000 --> 00:45:19,000
 OZ delta net Z which is simple we have

783
00:45:19,000 --> 00:45:23,000
 seen this before then delta net Z can be

784
00:45:23,000 --> 00:45:26,000
 expressed as a function of WJI which is

785
00:45:26,000 --> 00:45:28,000
 this these two terms are the same as

786
00:45:28,000 --> 00:45:30,000
 the earlier one the only difference now is

787
00:45:30,000 --> 00:45:33,000
 this right because E and OJ are still

788
00:45:33,000 --> 00:45:37,000
 quite far away right so you can now

789
00:45:37,000 --> 00:45:43,000
 write this further you can OZ is actually

790
00:45:43,000 --> 00:45:48,000
 not immediately inferencing E but E

791
00:45:48,000 --> 00:45:51,000
 depends on okay and okay actually can be

792
00:45:51,000 --> 00:45:55,000
 a function of OZ right so you further do

793
00:45:55,000 --> 00:45:58,000
 the change rule here right so this is

794
00:45:58,000 --> 00:46:03,000
 delta okay delta OZ E now you have two

795
00:46:03,000 --> 00:46:07,000
 of them delta E delta okay and then delta

796
00:46:07,000 --> 00:46:11,000
 okay delta OZ right then this one now

797
00:46:11,000 --> 00:46:15,000
 net K OZ you can write this formula so at

798
00:46:15,000 --> 00:46:18,000
 the end this is a derivative of this then

799
00:46:18,000 --> 00:46:21,000
 you have this right so therefore it's a

800
00:46:21,000 --> 00:46:24,000
 bit more complex and this is nothing but

801
00:46:24,000 --> 00:46:26,000
 again you trace the error you sum them back

802
00:46:26,000 --> 00:46:31,000
 to link this output to the OZ so

803
00:46:31,000 --> 00:46:33,000
 therefore at the end you have this formula

804
00:46:33,000 --> 00:46:37,000
 with the delta Z slightly more complex

805
00:46:37,000 --> 00:46:40,000
 than the delta K but this is the same

806
00:46:40,000 --> 00:46:45,000
 delta WJ had the same formula okay so

807
00:46:45,000 --> 00:46:50,000
 optional but do take a look and and and

808
00:46:50,000 --> 00:46:53,000
 these two slides to be quite a while to put

809
00:46:53,000 --> 00:46:55,000
 everything together so at least using the

810
00:46:55,000 --> 00:46:59,000
 notation we have been using so that at

811
00:46:59,000 --> 00:47:02,000
 least you can appreciate right actually I

812
00:47:02,000 --> 00:47:04,000
 was tempted by I didn't so far haven't

813
00:47:04,000 --> 00:47:06,000
 done that is I change this error term

814
00:47:06,000 --> 00:47:10,000
 want you to derive all these change of

815
00:47:10,000 --> 00:47:14,000
 the formula right that is very typical

816
00:47:14,000 --> 00:47:19,000
 that for some research you had to do it

817
00:47:22,000 --> 00:47:32,000
 okay question anyone then we have this

818
00:47:32,000 --> 00:47:35,000
 example right and this example once you

819
00:47:35,000 --> 00:47:37,000
 understand how to do that this example

820
00:47:37,000 --> 00:47:40,000
 is nothing by just plug in the number to

821
00:47:40,000 --> 00:47:44,000
 get the output right so in this example

822
00:47:44,000 --> 00:47:49,000
 you have a 3 2 1 network 3 input unit

823
00:47:49,000 --> 00:47:53,000
 2 hidden unit and 1 output unit okay 3 2

824
00:47:53,000 --> 00:47:59,000
 1 right the input is x 1 x 2 x 3 output is 0 6

825
00:47:59,000 --> 00:48:02,000
 you can label this unit as a 1 2 3 4 5 6

826
00:48:02,000 --> 00:48:06,000
 right then the W will be W 4 1 normally

827
00:48:06,000 --> 00:48:13,000
 use the the other end 4 1 W 4 2 5 1 5 2 5 3

828
00:48:13,000 --> 00:48:20,000
 so you have 3 times 2 weight right 3 times

829
00:48:20,000 --> 00:48:29,000
 2 weight plus 2 times 1 okay plus what are

830
00:48:29,000 --> 00:48:34,000
 the other variables that you have

831
00:48:34,000 --> 00:48:48,000
 plus 3 the bias right for 4 5 6 bias right

832
00:48:48,000 --> 00:48:56,000
 so you are going to have 11 yeah

833
00:48:56,000 --> 00:49:07,000
 variables and that's here 1 2 3 4 5 6 7 8 9 10 11

834
00:49:07,000 --> 00:49:10,000
 right and of course the bias of course you

835
00:49:10,000 --> 00:49:12,000
 can although I didn't plot the bias here

836
00:49:12,000 --> 00:49:16,000
 by the indicator is a bias in the parameter

837
00:49:16,000 --> 00:49:20,000
 if you want and which some people do this

838
00:49:20,000 --> 00:49:25,000
 you can draw a bias unit B right which

839
00:49:25,000 --> 00:49:31,000
 you connect to here connect to here connect

840
00:49:31,000 --> 00:49:36,000
 to here okay and this output is always

841
00:49:36,000 --> 00:49:39,000
 equal to 1 it's a bias unit and then you

842
00:49:39,000 --> 00:49:43,000
 have this equal to B4 the weight B4 and

843
00:49:43,000 --> 00:49:53,000
 B5 and B6 right you do see some people

844
00:49:53,000 --> 00:49:56,000
 do that then to explicitly tell you

845
00:49:56,000 --> 00:49:59,000
 there's a bias unit and this bias unit

846
00:49:59,000 --> 00:50:01,000
 again output is always equal to 1 the

847
00:50:01,000 --> 00:50:06,000
 OB is always equal to 1 then you have B4 B5 B6

848
00:50:06,000 --> 00:50:13,000
 3 different one okay and there is a

849
00:50:13,000 --> 00:50:17,000
 network okay similar here is it okay

850
00:50:17,000 --> 00:50:21,000
 that I connect this one to here have

851
00:50:21,000 --> 00:50:31,000
 this a W6 1 anything wrong with that

852
00:50:31,000 --> 00:50:35,000
 can I bypass the hidden layer connect

853
00:50:35,000 --> 00:50:39,000
 the output input directly to the output

854
00:50:39,000 --> 00:50:52,000
 input to the output units can I do that

855
00:50:52,000 --> 00:50:55,000
 can okay and which is equivalent to you

856
00:50:55,000 --> 00:50:58,000
 could introduce a hidden layer doing

857
00:50:58,000 --> 00:51:03,000
 nothing one in one assuming you have a

858
00:51:03,000 --> 00:51:06,000
 virtual hidden unit which is taking

859
00:51:06,000 --> 00:51:09,000
 input and just pass the output with the

860
00:51:09,000 --> 00:51:15,000
 linear activity function to 6 so it can

861
00:51:15,000 --> 00:51:19,000
 still be MLP right just for convenience

862
00:51:19,000 --> 00:51:23,000
 now your W your net 6 will be equal to

863
00:51:23,000 --> 00:51:30,000
 X1 multiplied with W61 plus W04

864
00:51:30,000 --> 00:51:35,000
 multiplied W64 plus O5 multiplied W65

865
00:51:35,000 --> 00:51:39,000
 plus B6 okay there will be your net input

866
00:51:39,000 --> 00:51:42,000
 right so just look at the picture then

867
00:51:42,000 --> 00:51:47,000
 you can figure out how to compute the net

868
00:51:47,000 --> 00:51:51,000
 input and output so these are the given

869
00:51:51,000 --> 00:51:53,000
 number now you have this training

870
00:51:53,000 --> 00:51:58,000
 sample 101 the target output is one so

871
00:51:58,000 --> 00:52:03,000
 they mean the T6 is equal to one the

872
00:52:03,000 --> 00:52:07,000
 target output and then the learning rate

873
00:52:07,000 --> 00:52:13,000
 eta is the same for both the weights and

874
00:52:13,000 --> 00:52:19,000
 the bias equal to 0.9 is such a large

875
00:52:19,000 --> 00:52:23,000
 number then of course in your calculation

876
00:52:23,000 --> 00:52:27,000
 you follow the step that we show earlier

877
00:52:27,000 --> 00:52:31,000
 you first given this X equal to 101 you

878
00:52:31,000 --> 00:52:35,000
 do the forward propagation right this

879
00:52:35,000 --> 00:52:43,000
 input 101 to the unit 4 5 and 6 you have

880
00:52:43,000 --> 00:52:48,000
 101 then this is also X1 X2 X3 then you

881
00:52:48,000 --> 00:52:51,000
 have the 4 which is equal to the

882
00:52:51,000 --> 00:52:56,000
 summation of this W41 times O1 plus W42

883
00:52:56,000 --> 00:53:03,000
 times O2 plus W43 times O3 plus OB4

884
00:53:03,000 --> 00:53:05,000
 okay and they plug in the number here

885
00:53:05,000 --> 00:53:08,000
 you get minus 0.7 which is the net 4

886
00:53:08,000 --> 00:53:12,000
 this is a net 4 then to get the O4 you

887
00:53:12,000 --> 00:53:15,000
 apply the activation function in this

888
00:53:15,000 --> 00:53:18,000
 case is a sigmoid 1 divided by 1 plus E

889
00:53:18,000 --> 00:53:23,000
 minus net 4 which is equal to 0.332

890
00:53:23,000 --> 00:53:27,000
 this is the O4 okay this is O4

891
00:53:27,000 --> 00:53:31,000
 output of the unit 4 then similarly O5

892
00:53:31,000 --> 00:53:34,000
 the same thing you compute the net 5

893
00:53:34,000 --> 00:53:39,000
 if it's 0.1 then compute the O5 which is

894
00:53:39,000 --> 00:53:43,000
 1 divided by 1 plus E minus net 5

895
00:53:43,000 --> 00:53:47,000
 equal to 0.525 so the output here this is

896
00:53:47,000 --> 00:53:52,000
 0.32 0.525 then for 6

897
00:53:52,000 --> 00:53:57,000
 unit the net input will be the W64

898
00:53:57,000 --> 00:54:03,000
 multiply with 0.332 then W65

899
00:54:03,000 --> 00:54:09,000
 multiply with 0.525 plus the bias B6 is 0.1

900
00:54:09,000 --> 00:54:13,000
 B6 0.1 here right then you have the

901
00:54:13,000 --> 00:54:17,000
 output 06 equal to no net 6 equal to

902
00:54:17,000 --> 00:54:21,000
 minus 0.105 the net 06 equal to 0.4

903
00:54:21,000 --> 00:54:25,000
 0.474 which is different from your

904
00:54:25,000 --> 00:54:29,000
 target output T6 equal to 1 right so quite far away

905
00:54:29,000 --> 00:54:33,000
 therefore you have an error right so this is just

906
00:54:33,000 --> 00:54:37,000
 a forward propagation that you get your 0405

907
00:54:37,000 --> 00:54:41,000
 and the actual output 06 okay

908
00:54:41,000 --> 00:54:45,000
 then you have the computer error error is a

909
00:54:45,000 --> 00:54:49,000
 because now you only you now have the 06

910
00:54:49,000 --> 00:54:53,000
 you know it's different from T6 now you

911
00:54:53,000 --> 00:54:57,000
 compute the error at the unit 6 first okay this is a

912
00:54:57,000 --> 00:55:01,000
 step that sometimes students just overlook they will

913
00:55:01,000 --> 00:55:05,000
 try to compute all delta 4 and delta 5 this is a

914
00:55:05,000 --> 00:55:09,000
 delta unit Z here error term

915
00:55:09,000 --> 00:55:13,000
 and before you compute

916
00:55:13,000 --> 00:55:17,000
 delta 4 delta 5 you need to get delta 6 because

917
00:55:17,000 --> 00:55:21,000
 delta 4 and 5 had to come from 6 so you compute this

918
00:55:21,000 --> 00:55:25,000
 this is the formula right sigma prime net K which is

919
00:55:25,000 --> 00:55:29,000
 okay 1 minus okay plus TK is 1

920
00:55:29,000 --> 00:55:33,000
 then the okay is 0.474

921
00:55:33,000 --> 00:55:37,000
 just so coincident that it's same to 1 minus okay

922
00:55:37,000 --> 00:55:41,000
 then you have this old error term

923
00:55:41,000 --> 00:55:45,000
 in the unit 6 is 0.1311

924
00:55:45,000 --> 00:55:49,000
 similarly you apply this formula in this case you only

925
00:55:49,000 --> 00:55:53,000
 have 1 therefore no need to have a summation but just multiply them

926
00:55:53,000 --> 00:55:57,000
 together so you have the delta 4 delta 5

927
00:55:57,000 --> 00:56:01,000
 right so 1 you have the delta 4 delta 5 delta

928
00:56:01,000 --> 00:56:05,000
 6 then you can compute

929
00:56:05,000 --> 00:56:09,000
 changes right which is

930
00:56:09,000 --> 00:56:13,000
 this is original value this is the delta

931
00:56:13,000 --> 00:56:17,000
 K Z delta Z i that you need to compute

932
00:56:17,000 --> 00:56:21,000
 right based on the error you have compute for

933
00:56:21,000 --> 00:56:25,000
 delta 4 5 6 then this will be the new

934
00:56:25,000 --> 00:56:29,000
 weights and bias 11 of them you can

935
00:56:29,000 --> 00:56:33,000
 compute right so once

936
00:56:33,000 --> 00:56:37,000
 you have done that you can check your new error

937
00:56:37,000 --> 00:56:41,000
 output will be if you apply the

938
00:56:41,000 --> 00:56:45,000
 101 again you will find that the actual output

939
00:56:45,000 --> 00:56:49,000
 now will reduce will be will be increased towards

940
00:56:49,000 --> 00:56:53,000
 the 1 larger than 0.474

941
00:56:53,000 --> 00:56:57,000
 the original output and closer to the

942
00:56:57,000 --> 00:57:01,000
 target output 1 okay

943
00:57:01,000 --> 00:57:05,000
 so that is the back

944
00:57:05,000 --> 00:57:09,000
 propagation example

945
00:57:11,000 --> 00:57:15,000
 question anyone have any question before we move on

946
00:57:41,000 --> 00:57:49,000
 okay once you have the basic

947
00:57:49,000 --> 00:57:57,000
 MLP now you would like to use it to process different types of data

948
00:57:57,000 --> 00:58:01,000
 and in the early day of course the image

949
00:58:01,000 --> 00:58:05,000
 will become one of the popular data

950
00:58:05,000 --> 00:58:09,000
 that you want to recognize object inside images

951
00:58:09,000 --> 00:58:13,000
 and then if you apply just

952
00:58:13,000 --> 00:58:17,000
 MLP for image input you will find that it is going to be

953
00:58:17,000 --> 00:58:21,000
 challenging because the image you have

954
00:58:21,000 --> 00:58:25,000
 a lot of so called pixel value

955
00:58:25,000 --> 00:58:29,000
 it will try to connect them let's say image if you have an image input

956
00:58:29,000 --> 00:58:33,000
 here like image

957
00:58:33,000 --> 00:58:37,000
 it could be a lot of pixel value if every pixel

958
00:58:37,000 --> 00:58:41,000
 becomes one of your input every pixel

959
00:58:41,000 --> 00:58:45,000
 so you could have a very very high dimensional input

960
00:58:45,000 --> 00:58:49,000
 assuming you have a thousand by thousand pixel which is already

961
00:58:49,000 --> 00:58:53,000
 one million direction input and then the number

962
00:58:53,000 --> 00:58:57,000
 will be huge in this case for example just consider

963
00:58:57,000 --> 00:59:01,000
 200 by 200 pixels

964
00:59:01,000 --> 00:59:05,000
 3 here because of color image you have RGB for those

965
00:59:05,000 --> 00:59:09,000
 who are familiar with RGB

966
00:59:09,000 --> 00:59:13,000
 image you know that the color you need 3

967
00:59:13,000 --> 00:59:17,000
 value to represent the color so that means your input is already

968
00:59:17,000 --> 00:59:21,000
 200 by 200 by 3 in terms of the

969
00:59:21,000 --> 00:59:25,000
 feature value you just assume that you only

970
00:59:25,000 --> 00:59:29,000
 have 10 hidden units 10 of them

971
00:59:29,000 --> 00:59:33,000
 and that original every pixel

972
00:59:33,000 --> 00:59:37,000
 can connect to a unit with the weights

973
00:59:37,000 --> 00:59:41,000
 then you are going to have 1.2

974
00:59:41,000 --> 00:59:45,000
 million parameters the weights

975
00:59:45,000 --> 00:59:49,000
 so just one layer with 10

976
00:59:49,000 --> 00:59:53,000
 which is normally not really a big network to do anything

977
00:59:53,000 --> 00:59:57,000
 so therefore for

978
00:59:57,000 --> 01:00:01,000
 image if you want to input as image and this is the so called

979
01:00:01,000 --> 01:00:05,000
 data structure for an image you have RGB

980
01:00:05,000 --> 01:00:09,000
 you have to do something so called different because

981
01:00:09,000 --> 01:00:13,000
 if you are going to have many many this input then

982
01:00:13,000 --> 01:00:17,000
 you need a large number of parameters even just for 10

983
01:00:17,000 --> 01:00:21,000
 hidden units and you can end up with these

984
01:00:21,000 --> 01:00:25,000
 too many parameters and which

985
01:00:25,000 --> 01:00:29,000
 become so called a problematic in your

986
01:00:29,000 --> 01:00:33,000
 so called training because it will create these overfitting problems

987
01:00:33,000 --> 01:00:37,000
 here just one illustration for example

988
01:00:37,000 --> 01:00:41,000
 if you want to fit all these training data the

989
01:00:41,000 --> 01:00:45,000
 docs input and output by using a

990
01:00:45,000 --> 01:00:49,000
 line y equal to a plus bx right this could be

991
01:00:49,000 --> 01:00:53,000
 a so called a regression problem it's not perfect

992
01:00:53,000 --> 01:00:57,000
 but it allows you to estimate what will be the output

993
01:00:57,000 --> 01:01:01,000
 of any unseen input x as long as

994
01:01:01,000 --> 01:01:05,000
 it's within this range right and if you want to do better

995
01:01:05,000 --> 01:01:09,000
 you can introduce a slightly higher order

996
01:01:09,000 --> 01:01:13,000
 second order so called parametric curve

997
01:01:13,000 --> 01:01:17,000
 now you can see that it can fit the data better

998
01:01:17,000 --> 01:01:21,000
 but there are still certainly some error for some other

999
01:01:21,000 --> 01:01:25,000
 sample about okay of course if you are still not satisfied

1000
01:01:25,000 --> 01:01:29,000
 with the error you say okay you just need to be perfect

1001
01:01:29,000 --> 01:01:33,000
 then you can continue to increase your so called

1002
01:01:33,000 --> 01:01:37,000
 order of your curve right up to certain point

1003
01:01:37,000 --> 01:01:41,000
 it can fit to all your training data

1004
01:01:41,000 --> 01:01:45,000
 perfectly right in fact there is a formula like how many

1005
01:01:45,000 --> 01:01:49,000
 order you need given how many sample data but that doesn't mean

1006
01:01:49,000 --> 01:01:53,000
 this is better in some sense for unseen data for example if you have

1007
01:01:53,000 --> 01:01:57,000
 a one new data which you have not used for training you come here

1008
01:01:57,000 --> 01:02:01,000
 it may give you a very high output

1009
01:02:01,000 --> 01:02:05,000
 which is very different from what you would expect right

1010
01:02:05,000 --> 01:02:09,000
 let's say it's a housing rental price right this is the

1011
01:02:09,000 --> 01:02:13,000
 the side of the house and this is the price of the

1012
01:02:13,000 --> 01:02:17,000
 rental price so this one seem to be anomaly right

1013
01:02:17,000 --> 01:02:21,000
 smaller than this unit and this unit charge cheaper than this

1014
01:02:21,000 --> 01:02:25,000
 unseen tax so there is called overfitting problem

1015
01:02:25,000 --> 01:02:29,000
 because to some level you start to fit the

1016
01:02:29,000 --> 01:02:33,000
 noise because the train sample may have noise right

1017
01:02:33,000 --> 01:02:37,000
 rather than the fundamental structure or

1018
01:02:37,000 --> 01:02:41,000
 train of the so called data

1019
01:02:41,000 --> 01:02:45,000
 so here is one illustration

1020
01:02:45,000 --> 01:02:49,000
 this green curve show the error there here you

1021
01:02:49,000 --> 01:02:53,000
 can continue to minimize your error of the training sample

1022
01:02:53,000 --> 01:02:57,000
 by increasing the complexity of your

1023
01:02:57,000 --> 01:03:01,000
 model okay but for unseen data

1024
01:03:01,000 --> 01:03:05,000
 test data the error may start to reduce first

1025
01:03:05,000 --> 01:03:09,000
 up to certain point if your network become more and more complex

1026
01:03:09,000 --> 01:03:13,000
 you have more and more unit of the perceptron and the test

1027
01:03:13,000 --> 01:03:17,000
 data error may go up to another direction

1028
01:03:17,000 --> 01:03:21,000
 so that mean your model is no longer

1029
01:03:21,000 --> 01:03:25,000
 suitable for the test data because you

1030
01:03:25,000 --> 01:03:29,000
 try to overfit to your training data so this is the point

1031
01:03:29,000 --> 01:03:33,000
 that you can charge that perhaps

1032
01:03:33,000 --> 01:03:37,000
 this will be the right order above this your test data

1033
01:03:37,000 --> 01:03:41,000
 will not work as well as the training data

1034
01:03:41,000 --> 01:03:45,000
 so therefore it's very important to keep a set

1035
01:03:45,000 --> 01:03:49,000
 of test data to validate the network

1036
01:03:49,000 --> 01:03:53,000
 trained by the training data whether it is

1037
01:03:53,000 --> 01:03:57,000
 good enough so underfitting or overfitting

1038
01:03:57,000 --> 01:04:01,000
 then for images

1039
01:04:01,000 --> 01:04:05,000
 then they come up with this so called

1040
01:04:05,000 --> 01:04:09,000
 a spatialized or simplified form of multi-layer

1041
01:04:09,000 --> 01:04:13,000
 perceptron where the weights

1042
01:04:13,000 --> 01:04:17,000
 are shared through convolutional filter

1043
01:04:17,000 --> 01:04:21,000
 so I walk through this network quickly so that you get some sense

1044
01:04:21,000 --> 01:04:25,000
 although it looks complex but in general this is a

1045
01:04:25,000 --> 01:04:29,000
 simplified version of your multi-layer perceptron

1046
01:04:29,000 --> 01:04:33,000
 the goal is I want to

1047
01:04:33,000 --> 01:04:37,000
 control the number of parameters

1048
01:04:37,000 --> 01:04:41,000
 this is immu data if you connect this data

1049
01:04:41,000 --> 01:04:45,000
 you call into a vector so you actually have a long

1050
01:04:45,000 --> 01:04:49,000
 input so called vector x1 to x

1051
01:04:49,000 --> 01:04:53,000
 perhaps 28 by 28 by 3

1052
01:04:53,000 --> 01:04:57,000
 but this would not

1053
01:04:57,000 --> 01:05:01,000
 so called retain the so called

1054
01:05:01,000 --> 01:05:05,000
 the specific data arrangement inside the image

1055
01:05:05,000 --> 01:05:09,000
 because for image you know that it's a spatial

1056
01:05:09,000 --> 01:05:13,000
 or 3D data and object who located together in

1057
01:05:13,000 --> 01:05:17,000
 the space should be appearing together in the image

1058
01:05:17,000 --> 01:05:21,000
 in some local area so if you

1059
01:05:21,000 --> 01:05:25,000
 break the image into a vector then those objects will

1060
01:05:25,000 --> 01:05:29,000
 somehow spread into different places it will not look like

1061
01:05:29,000 --> 01:05:33,000
 an object at all so they want to keep the original

1062
01:05:33,000 --> 01:05:37,000
 so called space conceptually like this

1063
01:05:37,000 --> 01:05:41,000
 so then if you just look at one layer

1064
01:05:41,000 --> 01:05:45,000
 this one layer forget about the other we call this a convolutional layer

1065
01:05:45,000 --> 01:05:49,000
 this one layer is actually all the perceptron

1066
01:05:49,000 --> 01:05:53,000
 unit the perceptron earlier on you see they are

1067
01:05:53,000 --> 01:05:57,000
 in one column now I arranged them in the spatial

1068
01:05:57,000 --> 01:06:01,000
 like image form and every

1069
01:06:01,000 --> 01:06:05,000
 perceptron can take input from the input image

1070
01:06:05,000 --> 01:06:09,000
 but I restrict the connection such that this particular neuron

1071
01:06:09,000 --> 01:06:13,000
 only look at the small area of the image

1072
01:06:13,000 --> 01:06:17,000
 right in front of it earlier on MOP can connect

1073
01:06:17,000 --> 01:06:21,000
 to all the input value but that will create a lot of

1074
01:06:21,000 --> 01:06:25,000
 weights and bias oh sorry weights

1075
01:06:25,000 --> 01:06:29,000
 but now this neuron can only look at a small region just right in front

1076
01:06:29,000 --> 01:06:33,000
 just like your eyes when your eyes try to focus on certain spot

1077
01:06:33,000 --> 01:06:37,000
 nor all those around this but you focus your eye on this spot

1078
01:06:37,000 --> 01:06:41,000
 okay so what it does is it now will immediately

1079
01:06:41,000 --> 01:06:45,000
 reduce the number of weights possible to connect to this particular neuron

1080
01:06:45,000 --> 01:06:49,000
 and not just that if I move this neuron to

1081
01:06:49,000 --> 01:06:53,000
 the next neuron which also connect to the other

1082
01:06:53,000 --> 01:06:57,000
 small region of the input picture right next to the

1083
01:06:57,000 --> 01:07:01,000
 earlier one I want to force the weights to be the same

1084
01:07:01,000 --> 01:07:05,000
 because when I scan through an image there is no reason

1085
01:07:05,000 --> 01:07:09,000
 I look at the different location using different so called

1086
01:07:09,000 --> 01:07:13,000
 emphasis right when you scan through it should be the same

1087
01:07:13,000 --> 01:07:17,000
 so that allow me to reduce the weights substantially

1088
01:07:17,000 --> 01:07:21,000
 right for the whole set of neuron

1089
01:07:21,000 --> 01:07:25,000
 so that create if I continue to do that to form many many

1090
01:07:25,000 --> 01:07:29,000
 neuron in one layer and each neuron can

1091
01:07:29,000 --> 01:07:33,000
 each perceptron can only have a smaller number

1092
01:07:33,000 --> 01:07:37,000
 of weights connected to the input right in front of this perceptron

1093
01:07:37,000 --> 01:07:41,000
 right and then the weights W, K, Z, W, J, I

1094
01:07:41,000 --> 01:07:45,000
 will be all the same even you come from different

1095
01:07:45,000 --> 01:07:49,000
 neuron right neuron perceptron so that

1096
01:07:49,000 --> 01:07:53,000
 will allow me to create one layer of hidden

1097
01:07:53,000 --> 01:07:57,000
 layer right but then you

1098
01:07:57,000 --> 01:08:01,000
 even argue why are so distinctive right

1099
01:08:01,000 --> 01:08:05,000
 why there is only one set of weights by right every

1100
01:08:05,000 --> 01:08:09,000
 perceptron could have very different weights then you create another

1101
01:08:09,000 --> 01:08:13,000
 layer which allow you to use another set of weights

1102
01:08:13,000 --> 01:08:17,000
 so this layer every layer

1103
01:08:17,000 --> 01:08:21,000
 is one set of the weights which is the same for all the perceptron

1104
01:08:21,000 --> 01:08:25,000
 right then you allow variation

1105
01:08:25,000 --> 01:08:29,000
 so in physical meaning that mean when I look for object inside

1106
01:08:29,000 --> 01:08:33,000
 the image this particular weight may be looking for

1107
01:08:33,000 --> 01:08:37,000
 whether there is an H right

1108
01:08:37,000 --> 01:08:41,000
 I can use another weight to look for whether there is a surger

1109
01:08:41,000 --> 01:08:45,000
 I can use another set of weight to look for whether

1110
01:08:45,000 --> 01:08:49,000
 certain kind of color appear in the image so that mean

1111
01:08:49,000 --> 01:08:53,000
 you could have as many of this feature maps each one

1112
01:08:53,000 --> 01:08:57,000
 of them use a certain set of weights and that

1113
01:08:57,000 --> 01:09:01,000
 weights can be trained you do not have to figure out what

1114
01:09:01,000 --> 01:09:05,000
 number going to that you just provide this kind of structure

1115
01:09:05,000 --> 01:09:09,000
 so that it can learn the weights which

1116
01:09:09,000 --> 01:09:13,000
 minimized the error at the end so that is the idea behind

1117
01:09:13,000 --> 01:09:17,000
 convolutional neural network the reason

1118
01:09:17,000 --> 01:09:21,000
 they do that is to simplify the MLP to reduce

1119
01:09:21,000 --> 01:09:25,000
 or to control the number of weights and there are also

1120
01:09:25,000 --> 01:09:29,000
 reason behind it because for those if you have learned image processing

1121
01:09:29,000 --> 01:09:33,000
 you know that this convolution the weights is actually

1122
01:09:33,000 --> 01:09:37,000
 equivalent to a set of filter in image processing

1123
01:09:37,000 --> 01:09:41,000
 the filter is like what you have learned in signal and system

1124
01:09:41,000 --> 01:09:45,000
 or signal digital signal processing like low pass, high pass,

1125
01:09:45,000 --> 01:09:49,000
 band pass, edge detection, corner detection

1126
01:09:49,000 --> 01:09:53,000
 those filtering process so that is basically the

1127
01:09:53,000 --> 01:09:57,000
 idea but they do not pre-specified what kind of filter you need

1128
01:09:57,000 --> 01:10:01,000
 they learn the weights by using a lot of training

1129
01:10:01,000 --> 01:10:05,000
 sample that create the first

1130
01:10:05,000 --> 01:10:09,000
 hidden layer but because of this unique structure

1131
01:10:09,000 --> 01:10:13,000
 of the weights restriction and also look at the region they call it

1132
01:10:13,000 --> 01:10:17,000
 convolutional layer and that has a physical

1133
01:10:17,000 --> 01:10:21,000
 image processing meaning behind convolutional operation

1134
01:10:21,000 --> 01:10:25,000
 okay and then after

1135
01:10:25,000 --> 01:10:29,000
 that you could apply this called pooling this is nothing but

1136
01:10:29,000 --> 01:10:33,000
 just reduce the number of data combine 4 of them

1137
01:10:33,000 --> 01:10:37,000
 into 1 right the still the same 10 feature map

1138
01:10:37,000 --> 01:10:41,000
 this had no learning process at all just try to

1139
01:10:41,000 --> 01:10:45,000
 control the number of the data site

1140
01:10:45,000 --> 01:10:49,000
 then you could now apply another convolution layer

1141
01:10:49,000 --> 01:10:53,000
 but this one now is you have a filter but it is not just

1142
01:10:53,000 --> 01:10:57,000
 apply to one layer of data you can actually apply to all the

1143
01:10:57,000 --> 01:11:01,000
 layer of the data right just like

1144
01:11:01,000 --> 01:11:05,000
 earlier on this filter apply to the 3 color plane RGB

1145
01:11:05,000 --> 01:11:09,000
 plus the local spatial region now this one could apply to

1146
01:11:09,000 --> 01:11:13,000
 a small let's say 3 by 3 plus all the 10 layer

1147
01:11:13,000 --> 01:11:17,000
 input that is your input data now to one of the

1148
01:11:17,000 --> 01:11:21,000
 perceptron here you still control the weights had to be the same

1149
01:11:21,000 --> 01:11:25,000
 for the same conversion layer in this case you create

1150
01:11:25,000 --> 01:11:29,000
 the total 20 set of filter weights

1151
01:11:29,000 --> 01:11:33,000
 they do the pooling again and until the data site now

1152
01:11:33,000 --> 01:11:37,000
 is smaller but you have 20 layer like this then

1153
01:11:37,000 --> 01:11:41,000
 at the end you can also call fact then basically

1154
01:11:41,000 --> 01:11:45,000
 concatenate all the data into a

1155
01:11:45,000 --> 01:11:49,000
 line vector that we seen at a normal MLP now you have

1156
01:11:49,000 --> 01:11:53,000
 a one column of x input right we call this a

1157
01:11:53,000 --> 01:11:57,000
 fracturing process then this will be the typical MLP connect to everyone

1158
01:11:57,000 --> 01:12:01,000
 right which connect to all the input data down to

1159
01:12:01,000 --> 01:12:05,000
 10 so this is the CFA 10

1160
01:12:05,000 --> 01:12:09,000
 classification at the end you want to see what object

1161
01:12:09,000 --> 01:12:13,000
 the class of the 10 that belong then there is

1162
01:12:13,000 --> 01:12:17,000
 a softmax activation function we will draw a body later

1163
01:12:17,000 --> 01:12:21,000
 which give you the output of 0 to 1 and the

1164
01:12:21,000 --> 01:12:25,000
 one with the largest output value between 0 to 1 will be the

1165
01:12:25,000 --> 01:12:29,000
 classes that you are looking for so that is a convolutional

1166
01:12:29,000 --> 01:12:33,000
 network and it is also known as

1167
01:12:33,000 --> 01:12:37,000
 a CNN or convex

1168
01:12:37,000 --> 01:12:41,000
 and the filtering process the perceptron multiply with the same

1169
01:12:41,000 --> 01:12:45,000
 weight it is nothing but this you have this weight equal

1170
01:12:45,000 --> 01:12:49,000
 101 010 101 basically

1171
01:12:49,000 --> 01:12:53,000
 when you compute the output in the

1172
01:12:53,000 --> 01:12:57,000
 conversion layer each of the value depend on the input

1173
01:12:57,000 --> 01:13:01,000
 then go to become the net Z then go

1174
01:13:01,000 --> 01:13:05,000
 to the activation function you compute this 434

1175
01:13:05,000 --> 01:13:09,000
 for color then your weight will be 3 of them so is this one

1176
01:13:09,000 --> 01:13:13,000
 is the filter of 3 times 3 you have 9 value

1177
01:13:13,000 --> 01:13:17,000
 each time they go to your input data output

1178
01:13:17,000 --> 01:13:21,000
 sorry the output data for the previous layer

1179
01:13:21,000 --> 01:13:25,000
 these are the value in the first layer will be your input image

1180
01:13:25,000 --> 01:13:29,000
 then you do the multiplication with your weights and sum them up

1181
01:13:29,000 --> 01:13:33,000
 that will be the output

1182
01:13:33,000 --> 01:13:37,000
 now you have to compute the output of the

1183
01:13:37,000 --> 01:13:41,000
 perceptron unit at the next layer

1184
01:13:41,000 --> 01:13:45,000
 so you see that if you start to

1185
01:13:45,000 --> 01:13:49,000
 imagine the picture if you have your images

1186
01:13:49,000 --> 01:13:53,000
 which is 100 by 100 pixel and then you have

1187
01:13:53,000 --> 01:13:57,000
 many many of this filter K and you have many layer

1188
01:13:57,000 --> 01:14:01,000
 and many hidden layer and your training data you could have

1189
01:14:01,000 --> 01:14:05,000
 create a lot of computation you need for you to compute

1190
01:14:05,000 --> 01:14:09,000
 like if you try to imagine your

1191
01:14:09,000 --> 01:14:13,000
 black propagation all the OZ, OI, WB

1192
01:14:13,000 --> 01:14:17,000
 sorry the delta K, delta Z

1193
01:14:17,000 --> 01:14:21,000
 delta W, delta B for every

1194
01:14:21,000 --> 01:14:25,000
 input send image so a lot of weights and adjustment you need

1195
01:14:25,000 --> 01:14:29,000
 therefore you need GPU and that can do very very

1196
01:14:29,000 --> 01:14:33,000
 intensive computation for the learning process

1197
01:14:33,000 --> 01:14:37,000
 here is another view to look at

1198
01:14:37,000 --> 01:14:41,000
 the convolutional layer and which again this is input

1199
01:14:41,000 --> 01:14:45,000
 image data this is one of the perceptron unit

1200
01:14:45,000 --> 01:14:49,000
 you have a set of weights here multiply with the input data

1201
01:14:49,000 --> 01:14:53,000
 multiply with the weights then you just get the output of this particular

1202
01:14:53,000 --> 01:14:57,000
 perceptron

1203
01:14:57,000 --> 01:15:01,000
 and then this weight will be the same where you move

1204
01:15:01,000 --> 01:15:05,000
 all the compute the perceptron for all the other

1205
01:15:05,000 --> 01:15:09,000
 location at other location within this so called layer

1206
01:15:09,000 --> 01:15:13,000
 we call this a feature map or convolutional layer this is a

1207
01:15:13,000 --> 01:15:17,000
 one map feature map this feature map depend on

1208
01:15:17,000 --> 01:15:21,000
 what filter you use every filter you create one feature

1209
01:15:21,000 --> 01:15:25,000
 map one layer of feature map

1210
01:15:25,000 --> 01:15:29,000
 this sometimes people call activation map because it's activation

1211
01:15:29,000 --> 01:15:33,000
 function output and the weights that you use is called

1212
01:15:33,000 --> 01:15:37,000
 kernel or the filters so example

1213
01:15:37,000 --> 01:15:41,000
 you have this input image and this is

1214
01:15:41,000 --> 01:15:45,000
 so called the weights the filter and this could be learned by

1215
01:15:45,000 --> 01:15:49,000
 some training so you apply the first filter here

1216
01:15:49,000 --> 01:15:53,000
 which is a particular weight you want to create a feature

1217
01:15:53,000 --> 01:15:57,000
 map you go through this you scan through

1218
01:15:57,000 --> 01:16:01,000
 basically you multiply this so called weights

1219
01:16:01,000 --> 01:16:05,000
 with the corresponding input image the area

1220
01:16:05,000 --> 01:16:09,000
 then you kind of compute them scan through then you

1221
01:16:09,000 --> 01:16:13,000
 get the first feature map which is in the foreign

1222
01:16:13,000 --> 01:16:17,000
 hidden layer so this feature map

1223
01:16:17,000 --> 01:16:21,000
 basically look at whether there are vertical so called lines

1224
01:16:21,000 --> 01:16:25,000
 in the image because of this particular filter

1225
01:16:25,000 --> 01:16:29,000
 try to detect changes along this so called X direction

1226
01:16:29,000 --> 01:16:33,000
 I mean looking for vertical line that create a one

1227
01:16:33,000 --> 01:16:37,000
 feature map here one layer

1228
01:16:37,000 --> 01:16:41,000
 of the feature map which look at whether there are vertical

1229
01:16:41,000 --> 01:16:45,000
 so called line then you can select

1230
01:16:45,000 --> 01:16:49,000
 the second filter which go through again create

1231
01:16:49,000 --> 01:16:53,000
 another feature map in this case you have 3 by

1232
01:16:53,000 --> 01:16:57,000
 6 18 feature map

1233
01:16:57,000 --> 01:17:01,000
 18 layer in the conversion layer

1234
01:17:01,000 --> 01:17:05,000
 ok so the number of the feature maps

1235
01:17:05,000 --> 01:17:09,000
 in each conversion layer depend on the number of the filters

1236
01:17:09,000 --> 01:17:13,000
 how many filter you have decide how many feature maps you have

1237
01:17:13,000 --> 01:17:17,000
 and the feature map also depend on

1238
01:17:17,000 --> 01:17:21,000
 the filter side and other operation like the strike

1239
01:17:21,000 --> 01:17:25,000
 zero padding and pooling we are going to talk about this operation

1240
01:17:25,000 --> 01:17:29,000
 in the following slides

1241
01:17:29,000 --> 01:17:33,000
 so your convolutional

1242
01:17:33,000 --> 01:17:37,000
 neural network is basically efficiently reduce the number of parameter

1243
01:17:37,000 --> 01:17:41,000
 as compared to the general multi-layer

1244
01:17:41,000 --> 01:17:45,000
 perceptron that we mentioned earlier

1245
01:17:45,000 --> 01:17:49,000
 I hope you

1246
01:17:49,000 --> 01:17:53,000
 you don't see this like

1247
01:17:53,000 --> 01:17:57,000
 why there are so called deep theory behind

1248
01:17:57,000 --> 01:18:01,000
 all the operation you are going to see later is basically try to control the number of

1249
01:18:01,000 --> 01:18:05,000
 weights because if you just use the traditional MLP

1250
01:18:05,000 --> 01:18:09,000
 the weights number will be

1251
01:18:09,000 --> 01:18:13,000
 too big for you to handle so all this operation is

1252
01:18:13,000 --> 01:18:17,000
 actually try many many operation

1253
01:18:17,000 --> 01:18:21,000
 the goal is how can I reduce the number of weights but still maintain

1254
01:18:21,000 --> 01:18:25,000
 certain performance right so this strike is

1255
01:18:25,000 --> 01:18:29,000
 one way just now we talk about the filter move around the images

1256
01:18:29,000 --> 01:18:33,000
 you can move one picture by one picture

1257
01:18:33,000 --> 01:18:37,000
 you know that the images within the small neighborhood

1258
01:18:37,000 --> 01:18:41,000
 they look very similar or you can move by every other

1259
01:18:41,000 --> 01:18:45,000
 picture or every five pictures right so by moving

1260
01:18:45,000 --> 01:18:49,000
 skid some of the local then your output feature map will be smaller

1261
01:18:49,000 --> 01:18:53,000
 because only one location you get one output and I will get one output

1262
01:18:53,000 --> 01:18:57,000
 so this is called strike

1263
01:18:57,000 --> 01:19:01,000
 how the filter move around the input to reduce

1264
01:19:01,000 --> 01:19:05,000
 the side of the output for example in this case strike equal to one so that mean

1265
01:19:05,000 --> 01:19:09,000
 every time including assuming these three are the

1266
01:19:09,000 --> 01:19:13,000
 feature value the connection right this one each time you move one

1267
01:19:13,000 --> 01:19:17,000
 this is input data each time you already move one location one picture

1268
01:19:17,000 --> 01:19:21,000
 right this one move one move one move one so you

1269
01:19:21,000 --> 01:19:25,000
 get the output here or you can each time you move two

1270
01:19:25,000 --> 01:19:29,000
 picture here move two picture here

1271
01:19:29,000 --> 01:19:33,000
 stay immediately you reduce the side from

1272
01:19:33,000 --> 01:19:37,000
 five to three and you can do this for

1273
01:19:37,000 --> 01:19:41,000
 2D again this is the blue color is the input data

1274
01:19:41,000 --> 01:19:45,000
 and the gray three by three is the filter

1275
01:19:45,000 --> 01:19:49,000
 the weights and the green one is the output in the

1276
01:19:49,000 --> 01:19:53,000
 convolutional layer the feature map so

1277
01:19:53,000 --> 01:19:57,000
 you can see that strike equal to two every time this filter jump

1278
01:19:57,000 --> 01:20:01,000
 two picture to the right and then you create

1279
01:20:01,000 --> 01:20:05,000
 every time jump two picture you get one output so you change from

1280
01:20:05,000 --> 01:20:09,000
 five by five input image to

1281
01:20:09,000 --> 01:20:13,000
 three by three output convolutional layer

1282
01:20:13,000 --> 01:20:17,000
 feature map and that is called strike right the

1283
01:20:17,000 --> 01:20:21,000
 larger the strike value that mean you skip more picture you create

1284
01:20:21,000 --> 01:20:25,000
 the output which is smaller

1285
01:20:25,000 --> 01:20:29,000
 and you sometimes also need to do zero padding

1286
01:20:29,000 --> 01:20:33,000
 like this case because when you want to compute

1287
01:20:33,000 --> 01:20:37,000
 this particular so called output center here

1288
01:20:37,000 --> 01:20:41,000
 you find that some of this filter value will be outside

1289
01:20:41,000 --> 01:20:45,000
 right and which origin is not have value so you can

1290
01:20:45,000 --> 01:20:49,000
 pad rows or column of zero outside

1291
01:20:49,000 --> 01:20:53,000
 so that you can still compute the output of

1292
01:20:53,000 --> 01:20:57,000
 this particular location right zero padding is how many

1293
01:20:57,000 --> 01:21:01,000
 row or column of zero you pad

1294
01:21:01,000 --> 01:21:05,000
 and you can now the input data

1295
01:21:05,000 --> 01:21:09,000
 so that output can still get some value

1296
01:21:09,000 --> 01:21:13,000
 for those picture at the boundary

1297
01:21:13,000 --> 01:21:17,000
 and pooling is another operation which is basically try to reduce the number

1298
01:21:17,000 --> 01:21:21,000
 of the output like earlier on when we talk about this

1299
01:21:21,000 --> 01:21:25,000
 network right we say oh this is a pooling layer basically reduce

1300
01:21:25,000 --> 01:21:29,000
 four by four picture become one

1301
01:21:29,000 --> 01:21:33,000
 operation it doesn't really have any so called physical

1302
01:21:33,000 --> 01:21:37,000
 reason behind I just want to reduce the number of

1303
01:21:37,000 --> 01:21:41,000
 data as long as the performance doesn't decree much

1304
01:21:41,000 --> 01:21:45,000
 so you can have max pooling basically

1305
01:21:45,000 --> 01:21:49,000
 you take four value right then take keep only the maximum

1306
01:21:49,000 --> 01:21:53,000
 value one two three the maximum value is three

1307
01:21:53,000 --> 01:21:57,000
 then you move one picture strike equal to one to the right

1308
01:21:57,000 --> 01:22:01,000
 so you take this four value two two three five and then

1309
01:22:01,000 --> 01:22:05,000
 take the maximum is five so and then you move one unit

1310
01:22:05,000 --> 01:22:09,000
 two two five seven maximum is seven

1311
01:22:09,000 --> 01:22:13,000
 right then you move one unit down here two three zero two

1312
01:22:13,000 --> 01:22:17,000
 then the maximum is two so this color behind just show you

1313
01:22:17,000 --> 01:22:21,000
 where this so called mask is located

1314
01:22:21,000 --> 01:22:25,000
 so this called maximum pooling with a two by two filter

1315
01:22:25,000 --> 01:22:29,000
 and strike one or you can do the average pooling

1316
01:22:29,000 --> 01:22:33,000
 right the maximum you take the average value of this four picture

1317
01:22:33,000 --> 01:22:37,000
 become two then strike two you move two

1318
01:22:37,000 --> 01:22:41,000
 picture to the right here right the average become four

1319
01:22:41,000 --> 01:22:45,000
 right this how you reduce the data in the

1320
01:22:45,000 --> 01:22:49,000
 pooling layer right or you can use even the

1321
01:22:49,000 --> 01:22:53,000
 L2 norm computer L2 norm of the input

1322
01:22:53,000 --> 01:22:57,000
 data okay all this operation is again

1323
01:22:57,000 --> 01:23:01,000
 the goal is try to reduce the number of weights

1324
01:23:01,000 --> 01:23:05,000
 and in your struct in your network

1325
01:23:05,000 --> 01:23:09,000
 so another operation

1326
01:23:09,000 --> 01:23:13,000
 which is again there are many many such so called little tricks

1327
01:23:13,000 --> 01:23:17,000
 and so called small changes in the

1328
01:23:17,000 --> 01:23:21,000
 network all the idea is see how I can

1329
01:23:21,000 --> 01:23:25,000
 minimize the error output and make

1330
01:23:25,000 --> 01:23:29,000
 the network can do the classification or prediction

1331
01:23:29,000 --> 01:23:33,000
 better so one of them is called regularization

1332
01:23:33,000 --> 01:23:37,000
 okay so in fact if you recall when I

1333
01:23:37,000 --> 01:23:41,000
 thought about one single neuron I say oh when you try to do the n

1334
01:23:41,000 --> 01:23:45,000
 function there are many possible lines can do

1335
01:23:45,000 --> 01:23:49,000
 the job right you can separate the one class from the other

1336
01:23:49,000 --> 01:23:53,000
 so that is without regularization

1337
01:23:53,000 --> 01:23:57,000
 but for SVM you will always find the line give you the maximum

1338
01:23:57,000 --> 01:24:01,000
 margin so if you recall in SVM

1339
01:24:01,000 --> 01:24:05,000
 when you try to find the so called the boundary

1340
01:24:05,000 --> 01:24:09,000
 you try to minimize this weight

1341
01:24:09,000 --> 01:24:13,000
 the vector right divided by two right when we

1342
01:24:13,000 --> 01:24:17,000
 try to find the SVM right not just here to do the classification

1343
01:24:17,000 --> 01:24:21,000
 correctly allow you to classify one and zero

1344
01:24:21,000 --> 01:24:25,000
 you also want to find the boundary which give you the maximum margin

1345
01:24:25,000 --> 01:24:29,000
 and that margin the way you find is minimizing this

1346
01:24:29,000 --> 01:24:33,000
 W L2 norm divided by two

1347
01:24:33,000 --> 01:24:37,000
 right so that is actually the idea for perceptron

1348
01:24:37,000 --> 01:24:41,000
 you have many possible line can I find the line which

1349
01:24:41,000 --> 01:24:45,000
 maximize the margin in the training right although I do not

1350
01:24:45,000 --> 01:24:49,000
 know it as SVM but in my training I have

1351
01:24:49,000 --> 01:24:53,000
 a few possible value can I find the one which give me the

1352
01:24:53,000 --> 01:24:57,000
 maximum margin which is the regularization

1353
01:24:57,000 --> 01:25:01,000
 this is the original JW the error function basically tell the class target

1354
01:25:01,000 --> 01:25:05,000
 and the output whether they are the same or close

1355
01:25:05,000 --> 01:25:09,000
 to each other you can add on another error

1356
01:25:09,000 --> 01:25:13,000
 or cost function right it's not error anymore therefore

1357
01:25:13,000 --> 01:25:17,000
 it's a loss function or cost function which is a function of the

1358
01:25:17,000 --> 01:25:21,000
 weights right so if you look at this L2

1359
01:25:21,000 --> 01:25:25,000
 regularization is a risk you find that what they

1360
01:25:25,000 --> 01:25:29,000
 try to minimize you know object to minimize this error right is this

1361
01:25:29,000 --> 01:25:33,000
 L2 norm which is the same as this

1362
01:25:33,000 --> 01:25:37,000
 you just append the original error by another

1363
01:25:37,000 --> 01:25:41,000
 loss function or cost function which is

1364
01:25:41,000 --> 01:25:45,000
 a parameter of your weights so this L2

1365
01:25:45,000 --> 01:25:49,000
 regularization is equivalent to I find the boundary

1366
01:25:49,000 --> 01:25:53,000
 can minimizing the weight ideally should be zero at the same time

1367
01:25:53,000 --> 01:25:57,000
 I maximize the margin L2 norm which is

1368
01:25:57,000 --> 01:26:01,000
 SVM therefore the neural network

1369
01:26:01,000 --> 01:26:05,000
 by adding this regularization it

1370
01:26:05,000 --> 01:26:09,000
 actually will make the network more robust to noise

1371
01:26:09,000 --> 01:26:13,000
 when your train data has noise you will find the one

1372
01:26:13,000 --> 01:26:17,000
 not just give you the best classification

1373
01:26:17,000 --> 01:26:21,000
 or prediction possible minimizing the error

1374
01:26:21,000 --> 01:26:25,000
 you also find the one give you the maximum margin

1375
01:26:25,000 --> 01:26:29,000
 once you have that you can think of oh I could think of other

1376
01:26:29,000 --> 01:26:33,000
 so-called cost function right L1 regularization

1377
01:26:33,000 --> 01:26:37,000
 or a combination of L1 part L2 norm called elastic

1378
01:26:37,000 --> 01:26:41,000
 net regularization you cannot introduce another parameter

1379
01:26:41,000 --> 01:26:45,000
 beta and all this beta, alpha and this become

1380
01:26:45,000 --> 01:26:49,000
 the weights or parameter that you can optimize

1381
01:26:49,000 --> 01:26:53,000
 right so there is the connection between

1382
01:26:53,000 --> 01:26:57,000
 neural network with SVM

1383
01:26:57,000 --> 01:27:01,000
 if you just introduce this regularization perhaps the network you

1384
01:27:01,000 --> 01:27:05,000
 train will give you the SVM result okay but you don't have to

1385
01:27:05,000 --> 01:27:09,000
 remember when we compute the SVM we have to go through this

1386
01:27:09,000 --> 01:27:13,000
 this is a very complex set of formula

1387
01:27:13,000 --> 01:27:17,000
 right find the solution find the

1388
01:27:17,000 --> 01:27:21,000
 support vector for that you decide now you just learn you don't even need to

1389
01:27:21,000 --> 01:27:25,000
 care about the formula you just train data target data

1390
01:27:25,000 --> 01:27:29,000
 you are converged hopefully to what you want

1391
01:27:29,000 --> 01:27:33,000
 so another form of regularization is this

1392
01:27:33,000 --> 01:27:37,000
 early stopping we saw this earlier right when you train your train

1393
01:27:37,000 --> 01:27:41,000
 data error you set aside a test data set to

1394
01:27:41,000 --> 01:27:45,000
 compute the validation or test error so when you compare this

1395
01:27:45,000 --> 01:27:49,000
 until the test error start to increase again you can

1396
01:27:49,000 --> 01:27:53,000
 do a early stop so this early stopping

1397
01:27:53,000 --> 01:27:57,000
 is another kind of regularization try to regular the

1398
01:27:57,000 --> 01:28:01,000
 network so that or control the network so that it doesn't fit

1399
01:28:01,000 --> 01:28:05,000
 the same as SVM doesn't really

1400
01:28:05,000 --> 01:28:09,000
 so called try to fit those

1401
01:28:09,000 --> 01:28:13,000
 error to the network and there are many other

1402
01:28:13,000 --> 01:28:17,000
 and some of you may heard about this drop out

1403
01:28:17,000 --> 01:28:21,000
 drop out is another form of regularization try to make the network

1404
01:28:21,000 --> 01:28:25,000
 more robust because the network could be very

1405
01:28:25,000 --> 01:28:29,000
 complex you only use some of the perceptron to do the

1406
01:28:29,000 --> 01:28:33,000
 learning in some training process

1407
01:28:33,000 --> 01:28:37,000
 then with that

1408
01:28:37,000 --> 01:28:41,000
 operation then how do you compute the data side from one level to

1409
01:28:41,000 --> 01:28:45,000
 the other let me see so I will just cover this then

1410
01:28:45,000 --> 01:28:49,000
 okay maybe two more slides then we will take a

1411
01:28:49,000 --> 01:28:53,000
 break after you come back I will show some of the

1412
01:28:53,000 --> 01:28:57,000
 program maybe some additional exercise

1413
01:28:57,000 --> 01:29:01,000
 so that you can have better appreciation of this

1414
01:29:01,000 --> 01:29:05,000
 so one of the thing that we want to

1415
01:29:05,000 --> 01:29:09,000
 know right if you have an input data

1416
01:29:09,000 --> 01:29:13,000
 image data side then when you try to

1417
01:29:13,000 --> 01:29:17,000
 design your network you have a convolution layer you have

1418
01:29:17,000 --> 01:29:21,000
 more than one convolution layer and pooling layer what will be the

1419
01:29:21,000 --> 01:29:25,000
 output data side right so that depend on as we

1420
01:29:25,000 --> 01:29:29,000
 look at the output data side number

1421
01:29:29,000 --> 01:29:33,000
 depend on the strike number depend on the filter number

1422
01:29:33,000 --> 01:29:37,000
 depend on the pooling number so if you look at operation carefully

1423
01:29:37,000 --> 01:29:41,000
 you can delay the weight and

1424
01:29:41,000 --> 01:29:45,000
 high of this so called hidden layer

1425
01:29:45,000 --> 01:29:49,000
 from the previous weight and high of the

1426
01:29:49,000 --> 01:29:53,000
 so called previous layer for example if you have this

1427
01:29:53,000 --> 01:29:57,000
 one you have the w2h2 the width and the high and the

1428
01:29:57,000 --> 01:30:01,000
 depth d2 right the d2 you know that how many

1429
01:30:01,000 --> 01:30:05,000
 feature that you have depend on just how many

1430
01:30:05,000 --> 01:30:09,000
 set of filter you have okay right early on we talk about 10

1431
01:30:09,000 --> 01:30:13,000
 20 filter then you have 10 and 20 so called

1432
01:30:13,000 --> 01:30:17,000
 depth okay and then the width and the

1433
01:30:17,000 --> 01:30:21,000
 high will be delay to w2 will be equal to

1434
01:30:21,000 --> 01:30:25,000
 w1 minus f f is the filter side early on we have seen

1435
01:30:25,000 --> 01:30:29,000
 3 by 3 that means this is 3 p is how many

1436
01:30:29,000 --> 01:30:33,000
 row of 0 you insert in on one side

1437
01:30:33,000 --> 01:30:37,000
 you have by s plus 2 okay so that

1438
01:30:37,000 --> 01:30:41,000
 you can figure out what is the width of the foreign layer

1439
01:30:41,000 --> 01:30:45,000
 why is the high foreign layer let's take one example

1440
01:30:45,000 --> 01:30:49,000
 w minus f plus 2 p divided by s right here

1441
01:30:49,000 --> 01:30:53,000
 in this case your input level w1 h1 is 5

1442
01:30:53,000 --> 01:30:57,000
 so h1 w1 is 5 by 5

1443
01:30:57,000 --> 01:31:01,000
 your s is equal to 2 your

1444
01:31:01,000 --> 01:31:05,000
 pooling layer is equal p is equal to 1

1445
01:31:05,000 --> 01:31:09,000
 because you have one row of 0 or one color

1446
01:31:09,000 --> 01:31:13,000
 of 0 on one side okay so 5

1447
01:31:13,000 --> 01:31:17,000
 by 5 the filter is 3 by 3

1448
01:31:17,000 --> 01:31:21,000
 so 5 minus 3

1449
01:31:21,000 --> 01:31:25,000
 you have 2 right plus 2 times 1 you have 4

1450
01:31:25,000 --> 01:31:29,000
 divided by 2 strike equal to 2 become 2 plus

1451
01:31:29,000 --> 01:31:33,000
 1 you have 3 therefore output is 3 by 3

1452
01:31:33,000 --> 01:31:37,000
 okay so if you continue through

1453
01:31:37,000 --> 01:31:41,000
 that you can see that output if you apply another 3 by 3 filter

1454
01:31:41,000 --> 01:31:45,000
 and strike equal to 2

1455
01:31:45,000 --> 01:31:49,000
 p in this case also equal to 1 you will see what is the next level

1456
01:31:49,000 --> 01:31:53,000
 output okay this

1457
01:31:53,000 --> 01:31:57,000
 apply to h as well so your input

1458
01:31:57,000 --> 01:32:01,000
 data this is the image you can consider this is a d0

1459
01:32:01,000 --> 01:32:05,000
 w0 h0 d0 equal to 3 because you have a color

1460
01:32:05,000 --> 01:32:09,000
 image w0 h0 will be equal to 32 by 32

1461
01:32:09,000 --> 01:32:13,000
 so remember this formula later on when we look at some of the

1462
01:32:13,000 --> 01:32:17,000
 so-called neural network like lernet, lxnet

1463
01:32:17,000 --> 01:32:21,000
 you can use it to compute the data site verify yourself

1464
01:32:21,000 --> 01:32:25,000
 why the number look like that right like this lernet

1465
01:32:25,000 --> 01:32:29,000
 so the lernet actually is the one that we

1466
01:32:29,000 --> 01:32:33,000
 show earlier yang le kun use it to compute

1467
01:32:33,000 --> 01:32:37,000
 this

1468
01:32:37,000 --> 01:32:41,000
 digit recognize the mnist digit

1469
01:32:41,000 --> 01:32:45,000
 output 0 to 9 right so input

1470
01:32:45,000 --> 01:32:49,000
 is a 32 by 32 digit number

1471
01:32:49,000 --> 01:32:53,000
 so in the first layer they have 6

1472
01:32:53,000 --> 01:32:57,000
 feature maps let me use a 6 filter to create

1473
01:32:57,000 --> 01:33:01,000
 the first so-called convolutional layer

1474
01:33:01,000 --> 01:33:05,000
 and then it does a sub-sampling this is a pooling

1475
01:33:05,000 --> 01:33:09,000
 a pooling process 28 by 20 become 14 by 14 there is still keep the

1476
01:33:09,000 --> 01:33:13,000
 number of layers 6 then there is a conversion layer

1477
01:33:13,000 --> 01:33:17,000
 now they use a 16 filter right you have this

1478
01:33:17,000 --> 01:33:21,000
 now 10 by 10 you should try to compute how to get from 22

1479
01:33:21,000 --> 01:33:25,000
 38 by 38 to 14 by 14 then

1480
01:33:25,000 --> 01:33:29,000
 10 by 10 but it's using some formula earlier pooling is

1481
01:33:29,000 --> 01:33:33,000
 easy 2 by 2 you basically reduce the dimension

1482
01:33:33,000 --> 01:33:37,000
 of 2 in each height of width then you have

1483
01:33:37,000 --> 01:33:41,000
 another 16 with 5 by 5 then

1484
01:33:41,000 --> 01:33:45,000
 after that you collect them into a convolutional

1485
01:33:45,000 --> 01:33:49,000
 fully connected network for 3 of them so output

1486
01:33:49,000 --> 01:33:53,000
 is 10 so this is the first

1487
01:33:53,000 --> 01:33:57,000
 conversion level neural network used for

1488
01:33:57,000 --> 01:34:01,000
 like the mdist digit recognition it has 2

1489
01:34:01,000 --> 01:34:05,000
 conversion layer 1 and 2

1490
01:34:05,000 --> 01:34:09,000
 with 3 with average pooling so this is a

1491
01:34:09,000 --> 01:34:13,000
 1 average pooling layer 2 average pooling layer then

1492
01:34:13,000 --> 01:34:17,000
 3 fully connected network 1 2 3 this is MLP

1493
01:34:17,000 --> 01:34:21,000
 right all this can connect to each other all the input

1494
01:34:21,000 --> 01:34:25,000
 and then the output is a softmax function

1495
01:34:25,000 --> 01:34:29,000
 this is a very special kind of activation

1496
01:34:29,000 --> 01:34:33,000
 function if you want to think it like that so earlier on remember

1497
01:34:33,000 --> 01:34:37,000
 for activation function if you take the net input you apply the nonlinear function

1498
01:34:37,000 --> 01:34:41,000
 you get the output okay so this one

1499
01:34:41,000 --> 01:34:45,000
 OI is you have the net Z net I the net input

1500
01:34:45,000 --> 01:34:49,000
 to the unit but rather than you apply a nonlinear function

1501
01:34:49,000 --> 01:34:53,000
 you actually raise it to the power of E net I

1502
01:34:53,000 --> 01:34:57,000
 for each of them then you normalize it with a sum

1503
01:34:57,000 --> 01:35:01,000
 for all the net I so what will this do

1504
01:35:01,000 --> 01:35:05,000
 it will make this OI between 0 the value between 0 to 1

1505
01:35:05,000 --> 01:35:09,000
 right and the summation of all the OI here

1506
01:35:09,000 --> 01:35:13,000
 will be equal to 1 so it kind of transform any output

1507
01:35:13,000 --> 01:35:17,000
 value it could be negative could be positive to

1508
01:35:17,000 --> 01:35:21,000
 a value which is between 0 to 1 and the sum

1509
01:35:21,000 --> 01:35:25,000
 is equal to 1 kind of transform the output

1510
01:35:25,000 --> 01:35:29,000
 value to some kind of probability space which the highest

1511
01:35:29,000 --> 01:35:33,000
 value tell you this cast

1512
01:35:33,000 --> 01:35:37,000
 belong to this this image belong to this particular cast

1513
01:35:37,000 --> 01:35:41,000
 so therefore the classification now become finding the value of OI

1514
01:35:41,000 --> 01:35:45,000
 after this a softmax right the one

1515
01:35:45,000 --> 01:35:49,000
 having the largest OI value and there is also the

1516
01:35:49,000 --> 01:35:53,000
 kind of probability of the object belong to this cast Y

1517
01:35:53,000 --> 01:35:57,000
 okay so this is a very special

1518
01:35:57,000 --> 01:36:01,000
 activation function applied to the output unit here

1519
01:36:01,000 --> 01:36:05,000
 only here and for the error function because the output

1520
01:36:05,000 --> 01:36:09,000
 is only 1 and 0 for classification problem so it doesn't

1521
01:36:09,000 --> 01:36:13,000
 use the so called the sum of square error because

1522
01:36:13,000 --> 01:36:17,000
 most of them will be 0 only the one which is the cast Y

1523
01:36:17,000 --> 01:36:21,000
 1 will be have a value will have a value close to 1

1524
01:36:21,000 --> 01:36:25,000
 so it use this a spatial error function TK is

1525
01:36:25,000 --> 01:36:29,000
 the actual output okay

1526
01:36:29,000 --> 01:36:33,000
 target output okay is actual output so most of this

1527
01:36:33,000 --> 01:36:37,000
 TK will be 0 except the one which is the correct cast

1528
01:36:37,000 --> 01:36:41,000
 so that mean log okay multiplied by TK

1529
01:36:41,000 --> 01:36:45,000
 will be 0 for all the rest except

1530
01:36:45,000 --> 01:36:49,000
 that target cast which is 1 log okay

1531
01:36:49,000 --> 01:36:53,000
 and this okay will have a value between 0 to 1 the closer

1532
01:36:53,000 --> 01:36:57,000
 to 1 the smaller this value will be

1533
01:36:57,000 --> 01:37:01,000
 then the smaller negative value the smaller this error will be

1534
01:37:01,000 --> 01:37:05,000
 for the rest will be very large value but since their TK equal to 0

1535
01:37:05,000 --> 01:37:09,000
 it will not contribute to the error term so this is

1536
01:37:09,000 --> 01:37:13,000
 called cross entropy lost

1537
01:37:13,000 --> 01:37:17,000
 only for output equal to 1 and 0 so that mean if you

1538
01:37:17,000 --> 01:37:21,000
 use the error function like this the earlier back

1539
01:37:21,000 --> 01:37:25,000
 the equation algorithm will not apply to this case

1540
01:37:25,000 --> 01:37:29,000
 you have to recompute all the delta K delta Z W

1541
01:37:29,000 --> 01:37:33,000
 right because now the error function change

1542
01:37:33,000 --> 01:37:37,000
 which you can do it easily because you can do the partial derivative of this

1543
01:37:37,000 --> 01:37:41,000
 to okay something that we have seen earlier right it's a log function

1544
01:37:41,000 --> 01:37:45,000
 so that is learn at

1545
01:37:45,000 --> 01:37:49,000
 agree here function okay let's take a break here

1546
01:37:49,000 --> 01:37:53,000
 come back at 1125

1547
01:37:53,000 --> 01:37:57,000
 then we'll continue the rest pass some program

1548
01:37:57,000 --> 01:38:01,000
 and exercise

1549
01:38:19,000 --> 01:38:23,000
 okay

1550
01:38:49,000 --> 01:38:53,000
 okay

1551
01:39:19,000 --> 01:39:23,000
 okay

1552
01:39:49,000 --> 01:39:53,000
 okay

1553
01:40:19,000 --> 01:40:23,000
 okay

1554
01:40:49,000 --> 01:40:53,000
 okay

1555
01:41:19,000 --> 01:41:23,000
 okay

1556
01:41:49,000 --> 01:41:53,000
 okay

1557
01:42:19,000 --> 01:42:23,000
 okay

1558
01:42:49,000 --> 01:42:53,000
 okay

1559
01:43:19,000 --> 01:43:23,000
 okay

1560
01:43:49,000 --> 01:43:53,000
 okay

1561
01:44:19,000 --> 01:44:23,000
 okay

1562
01:44:49,000 --> 01:44:53,000
 okay

1563
01:45:19,000 --> 01:45:23,000
 okay

1564
01:45:49,000 --> 01:45:53,000
 okay

1565
01:46:19,000 --> 01:46:23,000
 okay

1566
01:46:49,000 --> 01:46:53,000
 okay

1567
01:47:19,000 --> 01:47:23,000
 okay

1568
01:47:49,000 --> 01:47:53,000
 okay

1569
01:48:19,000 --> 01:48:23,000
 okay

1570
01:48:49,000 --> 01:48:53,000
 okay

1571
01:49:19,000 --> 01:49:23,000
 okay

1572
01:49:49,000 --> 01:49:53,000
 okay

1573
01:50:19,000 --> 01:50:23,000
 okay

1574
01:50:49,000 --> 01:50:53,000
 okay

1575
01:51:19,000 --> 01:51:23,000
 okay

1576
01:51:49,000 --> 01:51:53,000
 okay

1577
01:52:19,000 --> 01:52:23,000
 okay

1578
01:52:49,000 --> 01:52:53,000
 okay

1579
01:53:19,000 --> 01:53:23,000
 okay

1580
01:53:49,000 --> 01:53:53,000
 okay

1581
01:54:19,000 --> 01:54:23,000
 okay

1582
01:54:49,000 --> 01:54:53,000
 okay

1583
01:55:19,000 --> 01:55:23,000
 okay

1584
01:55:49,000 --> 01:55:53,000
 okay

1585
01:56:19,000 --> 01:56:23,000
 okay

1586
01:56:49,000 --> 01:56:53,000
 okay

1587
01:56:53,000 --> 01:56:57,000
 okay

1588
01:56:57,000 --> 01:57:01,000
 okay

1589
01:57:01,000 --> 01:57:05,000
 okay

1590
01:57:05,000 --> 01:57:09,000
 okay

1591
01:57:09,000 --> 01:57:13,000
 okay

1592
01:57:13,000 --> 01:57:17,000
 okay

1593
01:57:17,000 --> 01:57:21,000
 okay

1594
01:57:21,000 --> 01:57:25,000
 okay

1595
01:57:25,000 --> 01:57:29,000
 okay

1596
01:57:29,000 --> 01:57:33,000
 okay

1597
01:57:33,000 --> 01:57:37,000
 okay

1598
01:57:37,000 --> 01:57:41,000
 okay

1599
01:57:41,000 --> 01:57:45,000
 okay

1600
01:57:45,000 --> 01:57:49,000
 okay

1601
01:57:49,000 --> 01:57:53,000
 okay

1602
01:57:53,000 --> 01:57:57,000
 okay

1603
01:57:57,000 --> 01:58:01,000
 okay

1604
01:58:01,000 --> 01:58:05,000
 okay

1605
01:58:05,000 --> 01:58:09,000
 okay

1606
01:58:09,000 --> 01:58:13,000
 okay

1607
01:58:13,000 --> 01:58:17,000
 okay

1608
01:58:17,000 --> 01:58:21,000
 okay

1609
01:58:21,000 --> 01:58:25,000
 okay

1610
01:58:25,000 --> 01:58:29,000
 okay

1611
01:58:29,000 --> 01:58:33,000
 okay

1612
01:58:33,000 --> 01:58:37,000
 okay

1613
01:58:37,000 --> 01:58:41,000
 okay

1614
01:58:41,000 --> 01:58:45,000
 okay

1615
01:58:45,000 --> 01:58:49,000
 okay

1616
01:58:49,000 --> 01:58:53,000
 okay

1617
01:58:53,000 --> 01:58:57,000
 okay

1618
01:58:57,000 --> 01:59:01,000
 okay

1619
01:59:01,000 --> 01:59:05,000
 okay

1620
01:59:05,000 --> 01:59:09,000
 okay

1621
01:59:09,000 --> 01:59:13,000
 okay

1622
01:59:13,000 --> 01:59:17,000
 okay

1623
01:59:17,000 --> 01:59:21,000
 okay

1624
01:59:21,000 --> 01:59:25,000
 okay

1625
01:59:25,000 --> 01:59:29,000
 okay

1626
01:59:29,000 --> 01:59:33,000
 okay

1627
01:59:33,000 --> 01:59:37,000
 okay

1628
01:59:37,000 --> 01:59:41,000
 okay

1629
01:59:41,000 --> 01:59:45,000
 okay

1630
01:59:45,000 --> 01:59:49,000
 okay

1631
01:59:49,000 --> 01:59:53,000
 okay

1632
01:59:53,000 --> 01:59:57,000
 okay

1633
01:59:57,000 --> 02:00:01,000
 okay

1634
02:00:01,000 --> 02:00:05,000
 okay

1635
02:00:05,000 --> 02:00:09,000
 okay

1636
02:00:09,000 --> 02:00:13,000
 okay

1637
02:00:13,000 --> 02:00:17,000
 okay

1638
02:00:17,000 --> 02:00:21,000
 okay

1639
02:00:21,000 --> 02:00:25,000
 okay

1640
02:00:25,000 --> 02:00:29,000
 okay

1641
02:00:29,000 --> 02:00:33,000
 okay

1642
02:00:33,000 --> 02:00:37,000
 okay

1643
02:00:37,000 --> 02:00:41,000
 okay

1644
02:00:41,000 --> 02:00:45,000
 okay

1645
02:00:45,000 --> 02:00:49,000
 okay

1646
02:00:49,000 --> 02:00:53,000
 okay

1647
02:00:53,000 --> 02:00:57,000
 okay

1648
02:00:57,000 --> 02:01:01,000
 okay

1649
02:01:01,000 --> 02:01:05,000
 okay

1650
02:01:05,000 --> 02:01:09,000
 okay

1651
02:01:09,000 --> 02:01:13,000
 okay

1652
02:01:13,000 --> 02:01:17,000
 okay

1653
02:01:17,000 --> 02:01:21,000
 okay

1654
02:01:21,000 --> 02:01:25,000
 okay

1655
02:01:25,000 --> 02:01:29,000
 okay

1656
02:01:29,000 --> 02:01:37,000
 okay so so there is again just to try it out to see how fast and how easy it is

1657
02:01:37,000 --> 02:01:45,000
 of course this problem can also be solved by using decision three right and in fact you think carefully

1658
02:01:45,000 --> 02:01:55,000
 if I had this problem to you I could ask the question right if I have this two-dimensional data belonging to two classes and like this

1659
02:01:55,000 --> 02:02:01,000
 and what are the other classifier what give me three possible classifier you can do to separate

1660
02:02:01,000 --> 02:02:07,000
 one class from the other right we just talk about SVM right what else

1661
02:02:07,000 --> 02:02:13,000
 three possible four at least you can learn quite a number right to separate

1662
02:02:13,000 --> 02:02:17,000
 the to do the classification job

1663
02:02:17,000 --> 02:02:21,000
 anyone beside SVM what else can you use

1664
02:02:21,000 --> 02:02:25,000
 beside SVM what else can you use

1665
02:02:25,000 --> 02:02:29,000
 huh

1666
02:02:29,000 --> 02:02:33,000
 beside SVM what else can you use

1667
02:02:33,000 --> 02:02:37,000
 huh

1668
02:02:37,000 --> 02:02:41,000
 yes K nearest neighbor right one or two in fact the assignment

1669
02:02:41,000 --> 02:02:45,000
 is asking you to find the region which belong to what class I just so happened I have more

1670
02:02:45,000 --> 02:02:49,000
 point than the sixth I use in the example

1671
02:02:49,000 --> 02:02:53,000
 K nearest neighbor can also do the job what else

1672
02:02:53,000 --> 02:02:57,000
 SVM K nearest neighbor what else

1673
02:02:57,000 --> 02:03:01,000
 huh

1674
02:03:01,000 --> 02:03:05,000
 RNN

1675
02:03:05,000 --> 02:03:09,000
 what is RNN recurrent

1676
02:03:09,000 --> 02:03:13,000
 neural network we will talk about it but there is more suitable for sequential data

1677
02:03:13,000 --> 02:03:19,000
 although you can still program it in do this but it may not be the most efficient

1678
02:03:19,000 --> 02:03:23,000
 how about decision tree

1679
02:03:23,000 --> 02:03:27,000
 decision tree

1680
02:03:27,000 --> 02:03:31,000
 can you do decision

1681
02:03:31,000 --> 02:03:35,000
 three to do this you have two input X1

1682
02:03:35,000 --> 02:03:39,000
 X2 yes you can right the next one is a decision

1683
02:03:39,000 --> 02:03:43,000
 tree right decision tree

1684
02:03:43,000 --> 02:03:47,000
 right train the data okay and then this is a model

1685
02:03:47,000 --> 02:03:51,000
 fitting using decision tree and you can have this

1686
02:03:51,000 --> 02:03:55,000
 right and that allow you to create this decision

1687
02:03:55,000 --> 02:03:59,000
 tree but now they look at X1 less than this using entropy

1688
02:03:59,000 --> 02:04:03,000
 52 and 4 right rather than

1689
02:04:03,000 --> 02:04:07,000
 I ask those so called classes

1690
02:04:07,000 --> 02:04:11,000
 I say okay if the X2 less than this value X1

1691
02:04:11,000 --> 02:04:15,000
 right so at the end I can classify most of them correctly

1692
02:04:15,000 --> 02:04:19,000
 so and this is a boundary I use to classify

1693
02:04:19,000 --> 02:04:23,000
 one class to the other right so it can be very

1694
02:04:23,000 --> 02:04:27,000
 generic okay decision tree of course

1695
02:04:27,000 --> 02:04:31,000
 yeah this is one you can program it then the other

1696
02:04:31,000 --> 02:04:35,000
 is just how a student ask me for the

1697
02:04:35,000 --> 02:04:39,000
 multi-layer perceptron can I have a so called

1698
02:04:39,000 --> 02:04:43,000
 output value which is not classes not binary

1699
02:04:43,000 --> 02:04:47,000
 right here is one okay so I show you the

1700
02:04:47,000 --> 02:04:51,000
 simple the so called curve you want to fit a line or

1701
02:04:51,000 --> 02:04:55,000
 curve to the data right now today I have this very complex function

1702
02:04:55,000 --> 02:04:59,000
 these are train data I give you only X and Y of course

1703
02:04:59,000 --> 02:05:03,000
 when I create this data I have some very simple function

1704
02:05:03,000 --> 02:05:07,000
 right cos sign sign and then using random

1705
02:05:07,000 --> 02:05:11,000
 variable 800 of them then I create another test

1706
02:05:11,000 --> 02:05:15,000
 set data which is another 200 so I use 80% for training

1707
02:05:15,000 --> 02:05:19,000
 to 20% for testing right so it look like this

1708
02:05:19,000 --> 02:05:23,000
 if you do not know the original function

1709
02:05:23,000 --> 02:05:27,000
 then I only give you input and output X output is Y

1710
02:05:27,000 --> 02:05:31,000
 so can you build a MLP

1711
02:05:31,000 --> 02:05:35,000
 multi-layer perceptron to approximate

1712
02:05:35,000 --> 02:05:39,000
 to do the prediction given X predict what is the value of Y

1713
02:05:39,000 --> 02:05:43,000
 then become a regression problem right so and then

1714
02:05:43,000 --> 02:05:47,000
 you see in another way it's very easy I just create a multi-layer

1715
02:05:47,000 --> 02:05:51,000
 perceptron input is one unit look at this

1716
02:05:51,000 --> 02:05:55,000
 this is input layer only one unit X right the active unit function is linear

1717
02:05:55,000 --> 02:05:59,000
 because whatever input I give to the output

1718
02:05:59,000 --> 02:06:03,000
 so let's try to create a

1719
02:06:03,000 --> 02:06:07,000
 a layer

1720
02:06:07,000 --> 02:06:11,000
 which has a 64 unit okay let me run this one

1721
02:06:11,000 --> 02:06:15,000
 okay

1722
02:06:15,000 --> 02:06:19,000
 so I first create a now I'm using the

1723
02:06:19,000 --> 02:06:23,000
 runtime okay a GPO it's a

1724
02:06:23,000 --> 02:06:27,000
 CPU you locate me a CPU rather than

1725
02:06:27,000 --> 02:06:31,000
 you do this you take a while

1726
02:06:31,000 --> 02:06:35,000
 okay so I use the

1727
02:06:35,000 --> 02:06:39,000
 200 okay now it's a once

1728
02:06:39,000 --> 02:06:43,000
 one unit input then 64 hidden layer unit and one unit

1729
02:06:43,000 --> 02:06:47,000
 output so 164.1 so now it's doing the

1730
02:06:47,000 --> 02:06:51,000
 train the training process right using one

1731
02:06:51,000 --> 02:06:55,000
 April or 800 sample it could be using the error

1732
02:06:55,000 --> 02:06:59,000
 right now I train for how many

1733
02:06:59,000 --> 02:07:03,000
 I train for

1734
02:07:03,000 --> 02:07:07,000
 100 okay so now

1735
02:07:07,000 --> 02:07:11,000
 this output right with 64 hidden

1736
02:07:11,000 --> 02:07:15,000
 because the input data so compare I can only come up with this

1737
02:07:15,000 --> 02:07:19,000
 line to approximate this yeah it does something

1738
02:07:19,000 --> 02:07:23,000
 but really error is

1739
02:07:23,000 --> 02:07:27,000
 8 right this is one of the training data this is a test data error

1740
02:07:27,000 --> 02:07:31,000
 always larger than this okay certainly not good enough

1741
02:07:31,000 --> 02:07:35,000
 right so then that's how you think about one 64 unit

1742
02:07:35,000 --> 02:07:39,000
 and one there's only one layer right then you know that okay

1743
02:07:39,000 --> 02:07:43,000
 it with only one hidden layer each one is a one line

1744
02:07:43,000 --> 02:07:47,000
 right one line is one D that so each one is a basically

1745
02:07:47,000 --> 02:07:51,000
 try to predict a value of a line

1746
02:07:51,000 --> 02:07:55,000
 then certainly even combine many line together this is the best

1747
02:07:55,000 --> 02:07:59,000
 it can do so apparently this one you need to have more than

1748
02:07:59,000 --> 02:08:03,000
 one layer let me add another layer of

1749
02:08:03,000 --> 02:08:07,000
 128 okay

1750
02:08:07,000 --> 02:08:11,000
 okay let's

1751
02:08:11,000 --> 02:08:15,000
 try to run everything again

1752
02:08:15,000 --> 02:08:19,000
 so now I have a one 128 and 64

1753
02:08:19,000 --> 02:08:23,000
 so two hidden layer the first layer had one 28 unit

1754
02:08:23,000 --> 02:08:27,000
 the second layer 64 this is number of weights and

1755
02:08:27,000 --> 02:08:31,000
 bias now you see the result okay it

1756
02:08:31,000 --> 02:08:35,000
 seem to be better now like than 60

1757
02:08:35,000 --> 02:08:39,000
 okay so I see you the training is a model 15

1758
02:08:39,000 --> 02:08:43,000
 very simple when you do the training just call this function

1759
02:08:43,000 --> 02:08:47,000
 you specify the network this is the network you need to construct

1760
02:08:47,000 --> 02:08:51,000
 MLP this is the key to create the network

1761
02:08:51,000 --> 02:08:55,000
 it's quite easy just specify how many unit you want

1762
02:08:55,000 --> 02:08:59,000
 what is the error function what is the activation function error is a mean

1763
02:08:59,000 --> 02:09:03,000
 square error right the one that we have and then we

1764
02:09:03,000 --> 02:09:07,000
 use a rectangular linear unit right to all

1765
02:09:07,000 --> 02:09:11,000
 now with 128 and 64

1766
02:09:11,000 --> 02:09:15,000
 it does better job

1767
02:09:15,000 --> 02:09:19,000
 then at least try to fit certain component error now

1768
02:09:19,000 --> 02:09:23,000
 is at least much better earlier on 60 something now drop to 40

1769
02:09:23,000 --> 02:09:27,000
 something right and this case interesting the

1770
02:09:27,000 --> 02:09:31,000
 test error is even smaller than the training error

1771
02:09:31,000 --> 02:09:35,000
 normally it's the other way around for some cases because the network is

1772
02:09:35,000 --> 02:09:39,000
 still not good yet okay let's try to create one more hidden layer

1773
02:09:39,000 --> 02:09:43,000
 another 64

1774
02:09:43,000 --> 02:09:47,000
 I create now 128 64

1775
02:09:47,000 --> 02:09:51,000
 and 64

1776
02:09:51,000 --> 02:09:55,000
 run everything again

1777
02:09:55,000 --> 02:09:59,000
 yeah this is the training

1778
02:09:59,000 --> 02:10:03,000
 now is better less than 40

1779
02:10:03,000 --> 02:10:07,000
 wow

1780
02:10:07,000 --> 02:10:11,000
 even less than

1781
02:10:11,000 --> 02:10:15,000
 3 okay now you see

1782
02:10:15,000 --> 02:10:19,000
 this is one layer

1783
02:10:19,000 --> 02:10:23,000
 64

1784
02:10:23,000 --> 02:10:27,000
 so that is the idea of

1785
02:10:27,000 --> 02:10:31,000
 so called

1786
02:10:31,000 --> 02:10:35,000
 universal approximator

1787
02:10:35,000 --> 02:10:39,000
 as long as I have many many unit

1788
02:10:39,000 --> 02:10:43,000
 I can go to the same

1789
02:10:43,000 --> 02:10:47,000
 back propagation the gradient descent find how many parameter

1790
02:10:47,000 --> 02:10:51,000
 here I have about

1791
02:10:51,000 --> 02:10:55,000
 12,739 trainable parameter

1792
02:10:55,000 --> 02:10:59,000
 right 12,700

1793
02:10:59,000 --> 02:11:03,000
 plus weights and mires

1794
02:11:03,000 --> 02:11:07,000
 just by using 1,800

1795
02:11:07,000 --> 02:11:11,000
 I can solve this 12,000

1796
02:11:11,000 --> 02:11:15,000
 give me this right this is not bad

1797
02:11:15,000 --> 02:11:19,000
 right not bad

1798
02:11:19,000 --> 02:11:23,000
 this is a training error the blue one is the training

1799
02:11:23,000 --> 02:11:27,000
 error the red one is the test error

1800
02:11:27,000 --> 02:11:31,000
 because I create the data using the same variance

1801
02:11:31,000 --> 02:11:35,000
 turn out test and variation train data are performing

1802
02:11:35,000 --> 02:11:39,000
 same right if my test data are slightly different

1803
02:11:39,000 --> 02:11:43,000
 using different way to create them it may have a highest error

1804
02:11:43,000 --> 02:11:47,000
 so that is the MLP

1805
02:11:47,000 --> 02:11:51,000
 of course convolution in your network this is

1806
02:11:51,000 --> 02:11:55,000
 amnist okay I put it here I will not go through the detail again

1807
02:11:55,000 --> 02:11:59,000
 but this is now for image data input right this is

1808
02:11:59,000 --> 02:12:03,000
 digit image training data

1809
02:12:03,000 --> 02:12:07,000
 28 by 28 so in here I use the

1810
02:12:07,000 --> 02:12:11,000
 learn net file right this is how all you need to create the learn net file

1811
02:12:11,000 --> 02:12:15,000
 right look at it the structure of the network

1812
02:12:15,000 --> 02:12:19,000
 right so if you go

1813
02:12:19,000 --> 02:12:23,000
 back to compare to this one you can see it's a convolutional 2D network

1814
02:12:23,000 --> 02:12:27,000
 using the 10H activation function input data is

1815
02:12:27,000 --> 02:12:31,000
 28 by 28 break and y image

1816
02:12:31,000 --> 02:12:35,000
 I use the first filter is a 5 by 5 side the filter

1817
02:12:35,000 --> 02:12:39,000
 side and then I have a 6 layer

1818
02:12:39,000 --> 02:12:43,000
 6 activation function map this is all you need you see therefore a lot

1819
02:12:43,000 --> 02:12:47,000
 of research once you have this template you can change your model

1820
02:12:47,000 --> 02:12:51,000
 quickly just change the number here you do not really need to do a lot

1821
02:12:51,000 --> 02:12:55,000
 programming a lot of machine learning is find the right sample code

1822
02:12:55,000 --> 02:12:59,000
 then whatever you do just add one more line cut and pay

1823
02:12:59,000 --> 02:13:03,000
 change the activation function change the filter side change the feature

1824
02:13:03,000 --> 02:13:07,000
 map change the pooling average pooling right

1825
02:13:07,000 --> 02:13:11,000
 fattening this is a 3 activation function using

1826
02:13:11,000 --> 02:13:15,000
 sigmoid sigmoid the last layer is a softmax to create

1827
02:13:15,000 --> 02:13:19,000
 output 0 to 1 and that's all you need to build a network

1828
02:13:19,000 --> 02:13:23,000
 right if you can learn all this within a day

1829
02:13:23,000 --> 02:13:27,000
 those writing a lot of paper

1830
02:13:27,000 --> 02:13:31,000
 okay so let me go back here

1831
02:13:31,000 --> 02:13:35,000
 so if you just use the network to train and this is a number of

1832
02:13:35,000 --> 02:13:39,000
 tell you the number of parameters you have you can see that

1833
02:13:39,000 --> 02:13:43,000
 in here the early stage the number of parameters

1834
02:13:43,000 --> 02:13:47,000
 is not as large because we try to use

1835
02:13:47,000 --> 02:13:51,000
 the weights to control the weight therefore the number of

1836
02:13:51,000 --> 02:13:55,000
 parameters were fewer for those convolutional layer

1837
02:13:55,000 --> 02:13:59,000
 this pooling layer doesn't have any weight

1838
02:13:59,000 --> 02:14:03,000
 but at the end during the so-called fully connected network

1839
02:14:03,000 --> 02:14:07,000
 which is MLP right at the last three layer you can see the number of

1840
02:14:07,000 --> 02:14:11,000
 parameters increased substantially so normally the earlier part

1841
02:14:11,000 --> 02:14:15,000
 you need a lot of memory because you have big data

1842
02:14:15,000 --> 02:14:19,000
 you have to store all this and a lot of computation as well

1843
02:14:19,000 --> 02:14:23,000
 but the later part you need a lot of sorry you need a lot of parameter

1844
02:14:23,000 --> 02:14:27,000
 you need a lot of weight early you need a lot of computation later you need a lot of memory to store

1845
02:14:27,000 --> 02:14:31,000
 all these weights so total there are 44,000

1846
02:14:31,000 --> 02:14:35,000
 of parameters and this is the training process for the

1847
02:14:35,000 --> 02:14:39,000
 you see all the training you need to do is this is actually the

1848
02:14:39,000 --> 02:14:43,000
 the back propagation learning right I use a

1849
02:14:43,000 --> 02:14:47,000
 cross entropy which is a learn that cross entropy error function rather than

1850
02:14:47,000 --> 02:14:51,000
 mean square error and this optimizer add them is the

1851
02:14:51,000 --> 02:14:55,000
 one decide the learning rate right it's not just fix rate

1852
02:14:55,000 --> 02:14:59,000
 they're based on how the error change where the

1853
02:14:59,000 --> 02:15:03,000
 direction move in term of the weight and adjust the learning rate

1854
02:15:03,000 --> 02:15:07,000
 automatically so and then

1855
02:15:07,000 --> 02:15:11,000
 and then this is a training I use a I think

1856
02:15:11,000 --> 02:15:15,000
 in here how many airport 20 of them

1857
02:15:15,000 --> 02:15:19,000
 batch size is 32 so it's a stochastic training process

1858
02:15:19,000 --> 02:15:23,000
 and then this is a I think it take about

1859
02:15:23,000 --> 02:15:27,000
 about a few seconds for one airport

1860
02:15:27,000 --> 02:15:31,000
 so I'm going to run this you can go back and just use the run time run

1861
02:15:31,000 --> 02:15:35,000
 all you can see the training so and that

1862
02:15:35,000 --> 02:15:39,000
 allow me to get the error

1863
02:15:39,000 --> 02:15:43,000
 here right accuracy is close

1864
02:15:43,000 --> 02:15:47,000
 to one it's quite good close to 100% the loss is 0.050

1865
02:15:47,000 --> 02:15:51,000
 0.560 right so I could

1866
02:15:51,000 --> 02:15:55,000
 build another model which is a simpler

1867
02:15:55,000 --> 02:15:59,000
 right rather than the early one but now I use a rack rule activation function

1868
02:15:59,000 --> 02:16:03,000
 and and then I can

1869
02:16:03,000 --> 02:16:07,000
 achieve almost the same performance

1870
02:16:07,000 --> 02:16:11,000
 and smaller error right then this is

1871
02:16:11,000 --> 02:16:15,000
 this is the training

1872
02:16:15,000 --> 02:16:19,000
 accuracy or error this is the training error

1873
02:16:19,000 --> 02:16:23,000
 and this is a testing I set aside you can see that first they reduce

1874
02:16:23,000 --> 02:16:27,000
 then they start to speed again so some kind of

1875
02:16:27,000 --> 02:16:31,000
 overfitting occur at the end right so this is the

1876
02:16:31,000 --> 02:16:35,000
 blue curve is the training error and this is a testing error

1877
02:16:35,000 --> 02:16:39,000
 variation loss and training loss

1878
02:16:39,000 --> 02:16:43,000
 so this is the

1879
02:16:43,000 --> 02:16:47,000
 version for the cf10 in case you want to try out for real images

1880
02:16:47,000 --> 02:16:51,000
 so this is the one for the cf10

1881
02:16:51,000 --> 02:16:55,000
 classes object image classification

1882
02:16:55,000 --> 02:16:59,000
 actually 110 this is cf10

1883
02:16:59,000 --> 02:17:03,000
 okay so and

1884
02:17:03,000 --> 02:17:07,000
 and this is the learn it the one that we just cover

1885
02:17:07,000 --> 02:17:11,000
 right and this again we have built this learn it before the same thing here

1886
02:17:11,000 --> 02:17:15,000
 so total now parameter 62,000

1887
02:17:15,000 --> 02:17:19,000
 and this is where they are so you can use the formula we cover

1888
02:17:19,000 --> 02:17:23,000
 in the lecture to see how you get all this number

1889
02:17:23,000 --> 02:17:27,000
 the site of the filter the data

1890
02:17:27,000 --> 02:17:31,000
 d2h2w2 right and

1891
02:17:31,000 --> 02:17:35,000
 the number of parameter try to understand how do you how many

1892
02:17:35,000 --> 02:17:39,000
 weights and bias you need right get an idea so this is

1893
02:17:39,000 --> 02:17:43,000
 the for the data you can see this one even the network

1894
02:17:43,000 --> 02:17:47,000
 is not very good and because this is exactly

1895
02:17:47,000 --> 02:17:51,000
 why the mNIST is good for the chip when they try to

1896
02:17:51,000 --> 02:17:55,000
 apply to more like complex data like this it doesn't work as well

1897
02:17:55,000 --> 02:17:59,000
 right the error is actually the accuracy is only of 87%

1898
02:17:59,000 --> 02:18:03,000
 right sorry 60 70%

1899
02:18:03,000 --> 02:18:07,000
 right so error is quite big right so I can

1900
02:18:07,000 --> 02:18:11,000
 another one you can find there just use the same kernel site at

1901
02:18:11,000 --> 02:18:15,000
 RedGru another cnn you can do and do any kind of

1902
02:18:15,000 --> 02:18:19,000
 try and error yourself you can achieve better performance

1903
02:18:19,000 --> 02:18:23,000
 right by reducing this error for training data

1904
02:18:23,000 --> 02:18:27,000
 but the test data is always challenging now I can achieve 80% accuracy

1905
02:18:27,000 --> 02:18:31,000
 for training data which I just

1906
02:18:31,000 --> 02:18:35,000
 randomly or quickly customer another network

1907
02:18:35,000 --> 02:18:39,000
 so this is AlexNet the one we are going to talk about next

1908
02:18:39,000 --> 02:18:43,000
 and this is a network they use for

1909
02:18:43,000 --> 02:18:47,000
 ImageNet the one that Alex

1910
02:18:47,000 --> 02:18:51,000
 and Suskiver another

1911
02:18:51,000 --> 02:18:55,000
 openai co-founder and Jeff Hinton right Jeff Hinton again the

1912
02:18:55,000 --> 02:18:59,000
 Nobel Prize winner so the first network that came out to beat

1913
02:18:59,000 --> 02:19:03,000
 the ImageNet in the cnn you think carefully

1914
02:19:03,000 --> 02:19:07,000
 this is very similar to LearnNet you look at the code this is all you need

1915
02:19:07,000 --> 02:19:11,000
 right for the AlexNet

1916
02:19:11,000 --> 02:19:15,000
 and this is the code that they auctioned

1917
02:19:15,000 --> 02:19:19,000
 it for 44 millions Google purchased the company

1918
02:19:19,000 --> 02:19:23,000
 they start a dnn and auctioned it in

1919
02:19:23,000 --> 02:19:27,000
 Las Vegas and at the end Google payed 44

1920
02:19:27,000 --> 02:19:31,000
 million to buy this few lines of code

1921
02:19:31,000 --> 02:19:35,000
 together with three of them will work for them for a certain

1922
02:19:35,000 --> 02:19:39,000
 period so and this is number of parameter 21

1923
02:19:39,000 --> 02:19:43,000
 thousand parameter and you can train the AlexNet

1924
02:19:43,000 --> 02:19:47,000
 the same thing this is a training process

1925
02:19:47,000 --> 02:19:51,000
 add them the optimizer to figure out the learning rate

1926
02:19:51,000 --> 02:19:55,000
 and the cross entropy loss right and then

1927
02:19:55,000 --> 02:19:59,000
 the training right and you can see the accuracy

1928
02:19:59,000 --> 02:20:03,000
 here so AlexNet and this is the

1929
02:20:03,000 --> 02:20:07,000
 validation error not always smooth but this is a training is smoother

1930
02:20:07,000 --> 02:20:11,000
 but testing depends on how well we have the data is

1931
02:20:11,000 --> 02:20:15,000
 and then if you train you can save the network into a parameter

1932
02:20:15,000 --> 02:20:19,000
 and of course other can just load this one to do the training

1933
02:20:19,000 --> 02:20:23,000
 as well and there are many such a model people have trained

1934
02:20:23,000 --> 02:20:27,000
 for different application and they will share this as a so called

1935
02:20:27,000 --> 02:20:31,000
 network parameter online you can go to this

1936
02:20:31,000 --> 02:20:35,000
 website to download some of these network training

1937
02:20:35,000 --> 02:20:39,000
 because training takes a lot of time if you have a huge network like the

1938
02:20:39,000 --> 02:20:43,000
 large language model unlike you can train them yourself

1939
02:20:43,000 --> 02:20:47,000
 and each training could cost millions of dollars to do that

1940
02:20:47,000 --> 02:20:51,000
 but after they have trained they could share such a network

1941
02:20:51,000 --> 02:20:55,000
 parameter available some of some

1942
02:20:55,000 --> 02:20:59,000
 version or the best version they will not share so go

1943
02:20:59,000 --> 02:21:03,000
 and try it out in case you find this

1944
02:21:03,000 --> 02:21:07,000
 necessary for you to do your project or for

1945
02:21:07,000 --> 02:21:11,000
 your learning so running the program at least you will let you understand

1946
02:21:11,000 --> 02:21:15,000
 actually it's not so difficult because one of the reason why

1947
02:21:15,000 --> 02:21:19,000
 machine learning or computer science becomes so popular because

1948
02:21:19,000 --> 02:21:23,000
 many people share their codes or model

1949
02:21:23,000 --> 02:21:27,000
 available online and which

1950
02:21:27,000 --> 02:21:31,000
 you can access almost immediately and for you to do the tuning

1951
02:21:31,000 --> 02:21:35,000
 changes you just take the template and do some kind of

1952
02:21:35,000 --> 02:21:39,000
 find so called changes of tuning you do not need to

1953
02:21:39,000 --> 02:21:43,000
 develop everything from scratch if you know where to look

1954
02:21:43,000 --> 02:21:47,000
 and where to get those resources that you need

1955
02:21:47,000 --> 02:21:51,000
 therefore many, many students can do this even

1956
02:21:51,000 --> 02:21:55,000
 if they are done in secondary school they are doing this kind of training

1957
02:21:55,000 --> 02:21:59,000
 I didn't teach him he didn't want me to teach so he go and figure out from

1958
02:21:59,000 --> 02:22:03,000
 youtube and he can also do this

1959
02:22:03,000 --> 02:22:07,000
 apply to robotics try to recognize signage robot

1960
02:22:07,000 --> 02:22:11,000
 run away so he doesn't understand

1961
02:22:11,000 --> 02:22:15,000
 all the mathematics behind but he just follow the step

1962
02:22:15,000 --> 02:22:19,000
 quite easy for everyone to pick it up

1963
02:22:19,000 --> 02:22:23,000
 next I would like to cover this

1964
02:22:23,000 --> 02:22:27,000
 because some students have asked whether they could have

1965
02:22:27,000 --> 02:22:31,000
 some more example

1966
02:22:31,000 --> 02:22:35,000
 for

1967
02:22:35,000 --> 02:22:39,000
 practicing so I have

1968
02:22:39,000 --> 02:22:43,000
 uploaded this to NTU Learn website so in case you want

1969
02:22:43,000 --> 02:22:47,000
 to do more practice yourself and this

1970
02:22:47,000 --> 02:22:51,000
 is actually taken from the last past year homework question

1971
02:22:51,000 --> 02:22:55,000
 okay so since I already have the solution

1972
02:22:55,000 --> 02:22:59,000
 I just make it available to you and

1973
02:22:59,000 --> 02:23:03,000
 of course this year of course your question are not the same

1974
02:23:03,000 --> 02:23:07,000
 so therefore you can look at this to get more practice

1975
02:23:07,000 --> 02:23:11,000
 so today in particular let's look at this network

1976
02:23:11,000 --> 02:23:15,000
 and in this question I have solution here

1977
02:23:15,000 --> 02:23:19,000
 you can go back to check but I will suggest you to try out

1978
02:23:19,000 --> 02:23:23,000
 first before you look at the solution okay same password

1979
02:23:23,000 --> 02:23:27,000
 before you look at solution

1980
02:23:27,000 --> 02:23:31,000
 this is how you

1981
02:23:31,000 --> 02:23:35,000
 compute the decision network

1982
02:23:35,000 --> 02:23:39,000
 decision 3 and last year I even asked to calculate this

1983
02:23:39,000 --> 02:23:43,000
 so-called performance indicator measure and this is

1984
02:23:43,000 --> 02:23:47,000
 the solution for the second part is a bit

1985
02:23:47,000 --> 02:23:51,000
 tedious but you can take a look at it

1986
02:23:51,000 --> 02:23:55,000
 so this question let's come back to the

1987
02:23:55,000 --> 02:23:59,000
 question again so this question asks you a

1988
02:23:59,000 --> 02:24:03,000
 simple network a 2-1-2-newner network

1989
02:24:03,000 --> 02:24:07,000
 okay 2 input layer 1 hidden unit and then

1990
02:24:07,000 --> 02:24:11,000
 2 output layer I label them as 1 2 3

1991
02:24:11,000 --> 02:24:15,000
 4 5 unit number okay and the weights

1992
02:24:15,000 --> 02:24:19,000
 are initialized randomly as indicated in figure

1993
02:24:19,000 --> 02:24:23,000
 1 that means W 3 1 is equal to 0.1

1994
02:24:23,000 --> 02:24:27,000
 W 3 2 equal to 0.2

1995
02:24:27,000 --> 02:24:31,000
 W 4 3 equal to 0.2

1996
02:24:31,000 --> 02:24:35,000
 W 5 3 equal to 0.3 right and then

1997
02:24:35,000 --> 02:24:39,000
 the bias at the unit 3

1998
02:24:39,000 --> 02:24:43,000
 4 5 although I didn't plot it I initialize it as

1999
02:24:43,000 --> 02:24:47,000
 0.500 okay so again you can have a bias

2000
02:24:47,000 --> 02:24:51,000
 unit connect to 3 4 5 okay

2001
02:24:51,000 --> 02:24:55,000
 assume that the back

2002
02:24:55,000 --> 02:24:59,000
 propagation algorithm is used to train the network using the sum of

2003
02:24:59,000 --> 02:25:03,000
 square error right that means the formula that we covered in the lecture you

2004
02:25:03,000 --> 02:25:07,000
 can use because that is sum of square error okay

2005
02:25:07,000 --> 02:25:11,000
 and then the

2006
02:25:11,000 --> 02:25:15,000
 rack rule function right is chosen as activation

2007
02:25:15,000 --> 02:25:19,000
 function max 0 adds right there is an activation function

2008
02:25:19,000 --> 02:25:23,000
 and then the learning rate for all the weights and bias is

2009
02:25:23,000 --> 02:25:27,000
 0.3 0.3 okay

2010
02:25:27,000 --> 02:25:31,000
 so compute the actual output at the 2 output

2011
02:25:31,000 --> 02:25:35,000
 unit 0 4 and 0 5 for the training

2012
02:25:35,000 --> 02:25:39,000
 sample 1 and 1 the target output

2013
02:25:39,000 --> 02:25:43,000
 at the unit 4 and 5 are given as a 0 1

2014
02:25:43,000 --> 02:25:47,000
 right so when you have input 1 and 1 you expect to get the target

2015
02:25:47,000 --> 02:25:51,000
 0 1 but of course because your network may not be

2016
02:25:51,000 --> 02:25:55,000
 optimized yet the first output may not be close to 0 1

2017
02:25:55,000 --> 02:25:59,000
 right so there is a question A

2018
02:25:59,000 --> 02:26:03,000
 then after that compute the error function

2019
02:26:03,000 --> 02:26:07,000
 at each unit right delta 4 delta 5 delta

2020
02:26:07,000 --> 02:26:11,000
 3 right you do not have error in the delta 1 delta 2

2021
02:26:11,000 --> 02:26:15,000
 because they do not have weight for you to adjust okay

2022
02:26:15,000 --> 02:26:19,000
 adjust all the weights and bias accordingly after you compute the delta

2023
02:26:19,000 --> 02:26:23,000
 with the training sample 1 and 1 then you can

2024
02:26:23,000 --> 02:26:27,000
 adjust the weights then with the new

2025
02:26:27,000 --> 02:26:31,000
 output unit and bias compute the new outputs

2026
02:26:31,000 --> 02:26:35,000
 at 0 4 0 5 again right and you will see that

2027
02:26:35,000 --> 02:26:39,000
 these 2 outputs will be closer to 0 to 1

2028
02:26:39,000 --> 02:26:43,000
 okay

2029
02:26:43,000 --> 02:26:47,000
 so

2030
02:26:47,000 --> 02:26:51,000
 okay let's look at the you see the

2031
02:26:51,000 --> 02:26:55,000
 of course if you are given this question the first thing

2032
02:26:55,000 --> 02:26:59,000
 this one do come out in some kind of

2033
02:26:59,000 --> 02:27:03,000
 homework you have to identify all these parameter

2034
02:27:03,000 --> 02:27:08,000
 okay give the label properly right B 3 B 4 B 5 W

2035
02:27:08,000 --> 02:27:13,000
 key 2 4 key 5 of function okay then the actual

2036
02:27:13,000 --> 02:27:18,000
 output at the unit 4 and 5 right can be computed as this

2037
02:27:18,000 --> 02:27:24,000
 right actual output you first have to compute

2038
02:27:24,000 --> 02:27:29,000
 the output of O 3 first right this O 3

2039
02:27:29,000 --> 02:27:34,000
 output will be equal to first why is the net input

2040
02:27:34,000 --> 02:27:40,000
 1 times 0 power 1 plus 1 times 0.2 plus

2041
02:27:40,000 --> 02:27:45,000
 the bias B 3 is 0.5 right so this is the one

2042
02:27:45,000 --> 02:27:50,000
 for the net 3 input right then you get the 0.8

2043
02:27:50,000 --> 02:27:55,000
 as the net 3 then because you are using

2044
02:27:55,000 --> 02:28:00,000
 the output since this is a positive will be 0.8

2045
02:28:00,000 --> 02:28:05,000
 as well right so once you have O 3 now you can go

2046
02:28:05,000 --> 02:28:09,000
 back to compute the net 4 net 4 will be O 3

2047
02:28:09,000 --> 02:28:16,000
 times 0.2 right the weight W 4 3 and then net 4

2048
02:28:16,000 --> 02:28:23,000
 will be this 0.16 net 5 will be O 3 times 0.3

2049
02:28:23,000 --> 02:28:27,000
 0.24 right because both are positive then the output

2050
02:28:27,000 --> 02:28:31,000
 O 4 and O 5 will be the same value and positive right

2051
02:28:31,000 --> 02:28:37,000
 that's how you compute then once you have

2052
02:28:37,000 --> 02:28:41,000
 so-called gone through the forward propagation you get

2053
02:28:41,000 --> 02:28:45,000
 the O 4 and O 5 then you can compute with the target

2054
02:28:45,000 --> 02:28:49,000
 T 4 and T 5 which is 0 and 1 right then you

2055
02:28:49,000 --> 02:28:54,000
 compute the error term at delta 4 which is T 4 minus

2056
02:28:54,000 --> 02:29:00,000
 O 4 then this is sigma plum net 4 right because

2057
02:29:00,000 --> 02:29:05,000
 now is a reg rule function sigma plum F 4 so

2058
02:29:05,000 --> 02:29:09,000
 sigma plum net 4 depend on the value of net 4 net 4

2059
02:29:09,000 --> 02:29:11,000
 in this case is a positive value early on we've seen

2060
02:29:11,000 --> 02:29:15,000
 therefore sigma plum become 1 1 times T 4 minus

2061
02:29:15,000 --> 02:29:21,000
 O 4 is minus 0.16 right so there is error term delta 4

2062
02:29:21,000 --> 02:29:25,000
 similarly delta 5 this is also 1 because net 4

2063
02:29:25,000 --> 02:29:28,000
 net 5 are positive value then you have T 5 1 minus

2064
02:29:28,000 --> 02:29:35,000
 actual output O 5 becomes 0.76 then for once you

2065
02:29:35,000 --> 02:29:39,000
 have delta 4 delta 5 you can compute the delta 3 which

2066
02:29:39,000 --> 02:29:45,000
 will be equal to delta 4 times 0.2 plus delta 5 times 0.3

2067
02:29:45,000 --> 02:29:51,000
 times the activation function sigma plum net 3 net 3

2068
02:29:51,000 --> 02:29:56,000
 is also positive so you have 1 so you take this one so

2069
02:29:56,000 --> 02:30:01,000
 compute this you get 0.196 so then you have all the delta 3

2070
02:30:01,000 --> 02:30:06,000
 delta 4 delta 5 so with that now you can compute the

2071
02:30:06,000 --> 02:30:10,000
 changes right which is just a all this formula we have seen

2072
02:30:10,000 --> 02:30:17,000
 before right you can get all the new weights 4 3 5 3 3 1 3 2

2073
02:30:17,000 --> 02:30:23,000
 and b 4 b 5 b 3 right then with that new network parameter

2074
02:30:23,000 --> 02:30:29,000
 right some changes happen you can compute the new 0 4 0 5

2075
02:30:29,000 --> 02:30:36,000
 again you'll find that it's now become 0.11 0.699

2076
02:30:36,000 --> 02:30:40,000
 right the target is 0 and 1 right originally the output is

2077
02:30:40,000 --> 02:30:49,000
 0.16 0.24 now 0.16 reduce to 0.11 then 0.64

2078
02:30:49,000 --> 02:30:55,000
 increase to 0.69 right one closer to 0 the other closer to

2079
02:30:55,000 --> 02:30:59,000
 1 so they mean the error reduce right just one training one

2080
02:30:59,000 --> 02:31:05,000
 step right so if you continue to train at the end you will

2081
02:31:05,000 --> 02:31:10,000
 hopefully get to the 0 and 1 target eventually solution is

2082
02:31:10,000 --> 02:31:14,000
 available online you can go and just download from the NTU

2083
02:31:14,000 --> 02:31:19,000
 learn to but I strongly suggest you to do it by hand first

2084
02:31:19,000 --> 02:31:22,000
 before you look at the solution right because when you really

2085
02:31:22,000 --> 02:31:26,000
 do it you can realize certain step you may not be one other

2086
02:31:26,000 --> 02:31:30,000
 person sure where to find the value where to find the formula

2087
02:31:30,000 --> 02:31:34,000
 or the step right the sequence of the step those are important

2088
02:31:34,000 --> 02:31:39,000
 things that only you work it out yourself you realize what could

2089
02:31:39,000 --> 02:31:45,000
 be missing okay so these are two additional exercise for you

2090
02:31:45,000 --> 02:31:51,000
 to practice in case you really need more or if you really

2091
02:31:51,000 --> 02:31:55,000
 even more go to check the website on the past exam paper

2092
02:31:55,000 --> 02:32:00,000
 I believe there will be one or two on NTU library and try them

2093
02:32:00,000 --> 02:32:07,000
 out if you want to then let's continue a few other networks

2094
02:32:07,000 --> 02:32:13,000
 before we wrap up the section today so after learn it and as

2095
02:32:13,000 --> 02:32:18,000
 we cover in the introduction section and as we also

2096
02:32:18,000 --> 02:32:22,000
 suggest now learn it although it works very well for

2097
02:32:22,000 --> 02:32:27,000
 MNIST data but we apply to those color images like the

2098
02:32:27,000 --> 02:32:32,000
 CFAR 10 it doesn't work as well right other classifier

2099
02:32:32,000 --> 02:32:40,000
 perhaps SVM or decision tree or may work as well or less

2100
02:32:41,000 --> 02:32:47,000
 computational expensive so and then there was a AI winter

2101
02:32:47,000 --> 02:32:54,000
 until around 2012 and the FAFE and a few of them started this

2102
02:32:54,000 --> 02:32:58,000
 image net large-scale visual recognition challenge there was

2103
02:32:58,000 --> 02:33:04,000
 slightly before 2012 2010 2011 so the goal is because people

2104
02:33:04,000 --> 02:33:08,000
 criticize that oh neural network can only work for simple data

2105
02:33:08,000 --> 02:33:14,000
 like those MNIST and if you really want to show the so-called

2106
02:33:14,000 --> 02:33:20,000
 capability of network you have to target at images like this

2107
02:33:20,000 --> 02:33:25,000
 and see who are inside the image right so that is considered

2108
02:33:25,000 --> 02:33:31,000
 so-called a challenge for computer vision and machine learning

2109
02:33:31,000 --> 02:33:35,000
 community so therefore the set up is a common challenge so that

2110
02:33:35,000 --> 02:33:39,000
 everyone can propose idea to see whether they can achieve what

2111
02:33:39,000 --> 02:33:46,000
 they want to achieve so the task has 1.2 million images right a

2112
02:33:46,000 --> 02:33:52,000
 large number and each labor with one of the thousand categories

2113
02:33:52,000 --> 02:33:57,000
 right so rather than 10 or 100 now you have 1000 category of

2114
02:33:57,000 --> 02:34:04,000
 different type of so-called content so among them one

2115
02:34:05,000 --> 02:34:09,000
 point two million actually they have more than this one point

2116
02:34:09,000 --> 02:34:13,000
 two millions are for training purpose and they are another

2117
02:34:13,000 --> 02:34:19,000
 100,000 test images right for you to verify where did they

2118
02:34:19,000 --> 02:34:25,000
 your so-called model right you can assemble training error you

2119
02:34:25,000 --> 02:34:29,000
 can have a test error but when you come to competition they keep

2120
02:34:29,000 --> 02:34:33,000
 aside a set of images they never share with public right then

2121
02:34:33,000 --> 02:34:38,000
 you go and run your program to see how good you are so for this

2122
02:34:38,000 --> 02:34:43,000
 kind of competition normally they will now allow you to keep

2123
02:34:43,000 --> 02:34:48,000
 trying right every day they may allow you to test the set for

2124
02:34:48,000 --> 02:34:53,000
 certain amount of time right because they do want you to keep

2125
02:34:53,000 --> 02:34:58,000
 optimizing your code to the actual so-called data so set up

2126
02:34:58,000 --> 02:35:07,000
 some very clear rule to I mean to kind of avoid such a situation

2127
02:35:07,000 --> 02:35:11,000
 from happening but then there will still be people using different

2128
02:35:11,000 --> 02:35:16,000
 kind of trick try to so-called game the system and some actually

2129
02:35:16,000 --> 02:35:20,000
 were exposed you shouldn't do that because the idea is not like

2130
02:35:20,000 --> 02:35:23,000
 who is winning is whether you have a better network so everyone

2131
02:35:23,000 --> 02:35:28,000
 should follow the rule right then trying to fit the noise or fit

2132
02:35:28,000 --> 02:35:35,000
 the so-called error right so because the problem is too

2133
02:35:35,000 --> 02:35:42,000
 complex right if only consider the correct class as a so-called

2134
02:35:42,000 --> 02:35:47,000
 answer then the performance will be very poor so they allow each

2135
02:35:47,000 --> 02:35:53,000
 image you can predict five output as long as the correct class is

2136
02:35:53,000 --> 02:35:58,000
 within this five then you are considered correct right so because

2137
02:35:58,000 --> 02:36:02,000
 1000 objects some of them could be very similar so you are allowed to

2138
02:36:02,000 --> 02:36:10,000
 predict five top choice okay so that continue for a few years

2139
02:36:10,000 --> 02:36:16,000
 and and Alex net 2012 the one we saw before they come up with this

2140
02:36:16,000 --> 02:36:21,000
 this so-called structure although that it looks complex but in fact it

2141
02:36:21,000 --> 02:36:24,000
 shouldn't be and the reason they have these two parallel path because

2142
02:36:24,000 --> 02:36:29,000
 of limitation by the GPU right they cannot fit all the data into one

2143
02:36:29,000 --> 02:36:34,000
 GPU they had to somehow compromise come up with the two parallel path

2144
02:36:34,000 --> 02:36:41,000
 to train part of the so-called data right and it is very similar to

2145
02:36:41,000 --> 02:36:45,000
 learn it the conceptually right they still have this so-called

2146
02:36:45,000 --> 02:36:51,000
 convolution layer pooling layer right pooling rather than average pooling

2147
02:36:51,000 --> 02:36:55,000
 they use a max pooling convolution layer then at the end they have a fully

2148
02:36:55,000 --> 02:36:59,000
 connected network they have the output now because 1000 classes you have

2149
02:36:59,000 --> 02:37:06,000
 1000 output unit rather than 10 right so fundamentally it's similar right

2150
02:37:06,000 --> 02:37:12,000
 just to make it bigger more this is a so-called feature matte level

2151
02:37:12,000 --> 02:37:16,000
 different kind of filter side early on the u5 and 5 this one they can use

2152
02:37:16,000 --> 02:37:22,000
 up to 11 by 11 right and they have seven healing layer rather than two

2153
02:37:22,000 --> 02:37:28,000
 now they have seven and 60 million parameter right and using red rule

2154
02:37:28,000 --> 02:37:34,000
 as activation function here are some of the intermediate so-called output

2155
02:37:34,000 --> 02:37:38,000
 you can see and the filter it learn that turn out to be like this the filter

2156
02:37:38,000 --> 02:37:42,000
 of the first convolution layer you can see that the first layer

2157
02:37:42,000 --> 02:37:48,000
 basically look for very basic pattern right the ages in different

2158
02:37:48,000 --> 02:37:55,000
 direction and color box occur at different places and this is the

2159
02:37:55,000 --> 02:38:01,000
 activation of feature map output at this you think about they use a red

2160
02:38:01,000 --> 02:38:07,000
 rule then when the value is less than 0 right output is 0 only for the

2161
02:38:07,000 --> 02:38:14,000
 value larger than 0 they keep the value therefore the output is has a lot of

2162
02:38:14,000 --> 02:38:18,000
 0 only the partner with a strong response they have output here the

2163
02:38:18,000 --> 02:38:25,000
 rest of them are a lot of 0 there I got the first one or first layer

2164
02:38:25,000 --> 02:38:29,000
 level you can still see something but then this are the filter or second

2165
02:38:29,000 --> 02:38:35,000
 convolution layer right you have about 128 of them right k equal to 128

2166
02:38:35,000 --> 02:38:41,000
 this one you have 48 of them 128 and now you probably harder for the

2167
02:38:41,000 --> 02:38:45,000
 see certain pattern and we go to the further the activation map because even

2168
02:38:45,000 --> 02:38:49,000
 sparks more sparks and this only has certain sport and this show you a

2169
02:38:49,000 --> 02:38:55,000
 strong evidence something might happen there right and this filter feature

2170
02:38:55,000 --> 02:39:01,000
 map may be looking for a face or looking for a dog so at the end you

2171
02:39:01,000 --> 02:39:07,000
 decide okay so then people after they perform so well and a lot of people

2172
02:39:07,000 --> 02:39:12,000
 try to do further analysis why it works all right the training why it can

2173
02:39:12,000 --> 02:39:17,000
 actually recognize this object right so this are some of the so-called

2174
02:39:17,000 --> 02:39:23,000
 reverse engineer study so for example in the level one this are the

2175
02:39:23,000 --> 02:39:29,000
 feature that will give a lot of so-called activation output and for

2176
02:39:29,000 --> 02:39:35,000
 level two for example if you input this kind of image as some of the level

2177
02:39:35,000 --> 02:39:40,000
 feature map you will see this have a very strong output this these are not

2178
02:39:40,000 --> 02:39:45,000
 the actual output but it's like if you have this input to do the reconstruction

2179
02:39:45,000 --> 02:39:50,000
 to the activation function map and this should be the feature you are going to

2180
02:39:50,000 --> 02:39:55,000
 see you can see that it can successfully detect some of the circle right

2181
02:39:55,000 --> 02:40:01,000
 activation map and you go further higher higher level three right just like

2182
02:40:01,000 --> 02:40:08,000
 when we do the one b2d one layer second layer you can detect the more

2183
02:40:08,000 --> 02:40:13,000
 compressed region similarly when you go layer three now you can start to

2184
02:40:13,000 --> 02:40:19,000
 detect a human shape right somehow car the wheel right some kind of fans

2185
02:40:19,000 --> 02:40:26,000
 barcode right so and then until even the level four level five right lxf

2186
02:40:26,000 --> 02:40:32,000
 a six layer right so you can see some of the object like face right appear here

2187
02:40:32,000 --> 02:40:37,000
 and these are dog face right as long as you were dog you can detect it so

2188
02:40:37,000 --> 02:40:42,000
 that is the layer once you go to more and more hidden the year you can pick up

2189
02:40:43,000 --> 02:40:49,000
 this kind of complex structure right so that was the success of Alex net right

2190
02:40:49,000 --> 02:40:56,000
 so early on before 2012 and this is the image net challenge top five error so

2191
02:40:56,000 --> 02:41:01,000
 these are the one which are the core shadow they do not really use a

2192
02:41:01,000 --> 02:41:05,000
 neural network to do the deep network to do the computation

2193
02:41:05,000 --> 02:41:15,000
 so when Alex net came out in 2012 it dropped the performance I mean improve

2194
02:41:15,000 --> 02:41:23,000
 the performance by reaching error rate from 26% to 16% almost 10% drop right

2195
02:41:23,000 --> 02:41:27,000
 early on you see the drop will be very minimum two or three percent now

2196
02:41:27,000 --> 02:41:35,000
 suddenly 10% so after this the foreign network are all CNN network they just

2197
02:41:35,000 --> 02:41:41,000
 use a different complication and the error continue to reduce and this GF net

2198
02:41:41,000 --> 02:41:46,000
 is very much similar to Alex net they just use out the parameter right just

2199
02:41:46,000 --> 02:41:53,000
 maybe different so-called filter side different so-called layer right layer is

2200
02:41:53,000 --> 02:42:02,000
 the same just maybe the filter they use are different and some of the so-called

2201
02:42:02,000 --> 02:42:07,000
 activation function could be different and then and then after that there is this

2202
02:42:07,000 --> 02:42:13,000
 Alex net you can see a lot of once you understand the structure you can just

2203
02:42:13,000 --> 02:42:18,000
 represent network by using this you learn by you learn filter convolution level

2204
02:42:18,000 --> 02:42:27,000
 and and this is the output so-called how many of these feature map care how many

2205
02:42:27,000 --> 02:42:32,000
 filter you use right and these are the fully connection network what is the

2206
02:42:32,000 --> 02:42:37,000
 number of unit you have at the end yet the softmax activation function then VGG

2207
02:42:37,000 --> 02:42:45,000
 come out in 2014 and the beauty of this VGG is they use the same 3 by 3 filter

2208
02:42:45,000 --> 02:42:51,000
 rather than different side right and still the same almost same structure

2209
02:42:51,000 --> 02:42:54,000
 convolution level pooling pooling convolution level fully connect them

2210
02:42:54,000 --> 02:43:01,000
 like work right and you can call all these are very similar concept as a learn net

2211
02:43:01,000 --> 02:43:07,000
 then Google net come out with 2014 they have very this very interesting structure

2212
02:43:07,000 --> 02:43:12,000
 and this structure is like what I show earlier that you can connect sometimes

2213
02:43:12,000 --> 02:43:19,000
 connect to the following layer and this structure turned out to be it's still

2214
02:43:19,000 --> 02:43:26,000
 like those feature multiply with some kind of a new non network but they have

2215
02:43:26,000 --> 02:43:30,000
 this kind of different structures such that you can take multiple path right

2216
02:43:30,000 --> 02:43:35,000
 and just a single path now you can take multiple path what it actually does is

2217
02:43:35,000 --> 02:43:39,000
 rather than one single network you can create multiple different version of

2218
02:43:39,000 --> 02:43:45,000
 network at the end which are past can contribute the best to optimize the

2219
02:43:45,000 --> 02:43:51,000
 so-called output you will be given more weight the rest will be body minimum

2220
02:43:51,000 --> 02:43:55,000
 weight so and then the other thing you try to do is reduce the number of

2221
02:43:55,000 --> 02:44:02,000
 parameter from 60 million up to 144 million now this start to 23 million

2222
02:44:02,000 --> 02:44:07,000
 even they have more layer but because of this structure they can actually

2223
02:44:07,000 --> 02:44:12,000
 achieve the similar performance with fewer number of parameter because this

2224
02:44:12,000 --> 02:44:16,000
 is actually in fact some kind of network in the network allow you to create

2225
02:44:16,000 --> 02:44:21,000
 different type of network within one network right so there is a lot of

2226
02:44:21,000 --> 02:44:30,000
 so-called crevice I go into this then this the residue net come up by Microsoft

2227
02:44:30,000 --> 02:44:34,000
 they have this a jump right and they go through this right to allow the X to

2228
02:44:34,000 --> 02:44:39,000
 connect to the following network by skipping some of the hidden they were

2229
02:44:39,000 --> 02:44:46,000
 the one I draw earlier so this allow them to handle the problem we call it

2230
02:44:46,000 --> 02:44:52,000
 gradient vanishing problem if you recall the training process right when you

2231
02:44:52,000 --> 02:44:57,000
 adjust the error adjust the weight you always need to multiply the error from

2232
02:44:57,000 --> 02:45:03,000
 the back with the derivative of the activation function sigma prime net

2233
02:45:03,000 --> 02:45:12,000
 right so that normally if you look at some of these software functions at the

2234
02:45:12,000 --> 02:45:18,000
 value could be less than one right if you have multiple level connect together

2235
02:45:18,000 --> 02:45:25,000
 then every time when you propagate the error to the the the foreign layer you

2236
02:45:25,000 --> 02:45:29,000
 always multiply something with activation function derivative which could be

2237
02:45:29,000 --> 02:45:34,000
 less than one so if you go through many layer and that number the error will

2238
02:45:34,000 --> 02:45:38,000
 reduce to very very small number and then you cannot learn anymore this

2239
02:45:38,000 --> 02:45:44,000
 call gradient vanishing problem so this keep connection allow the error to go

2240
02:45:44,000 --> 02:45:49,000
 through this faster so you still keep the error right and make it disappear

2241
02:45:49,000 --> 02:45:54,000
 right this is how they can have many layer early on you only have 22 layer now

2242
02:45:54,000 --> 02:46:01,000
 you can kept up to 152 layer right those healing layer while still will not

2243
02:46:01,000 --> 02:46:07,000
 really let the gradient disappear right okay they can see that performance

2244
02:46:07,000 --> 02:46:15,000
 continue to improve until 3.57 yeah so and and this is considered something

2245
02:46:15,000 --> 02:46:23,000
 significant because it was considered a cow beating the human performance

2246
02:46:23,000 --> 02:46:31,000
 okay because there was a researcher and he himself tried out by humanly check

2247
02:46:31,000 --> 02:46:38,000
 classification and if I they can achieve something maybe close to 4% error

2248
02:46:38,000 --> 02:46:43,000
 but this rest net can actually first time cow consider like achieving better

2249
02:46:43,000 --> 02:46:49,000
 performance at human right based on what they claim right so after that of course

2250
02:46:49,000 --> 02:46:53,000
 there were many many further improvement with a bigger bigger network but people

2251
02:46:53,000 --> 02:46:59,000
 already accept this is probably the right network to tackle such a problem and

2252
02:46:59,000 --> 02:47:08,000
 they no longer continue to purpose this data okay okay let's stop here today and

2253
02:47:08,000 --> 02:47:13,000
 I will talk about this recurrent network and other network and including at the

2254
02:47:13,000 --> 02:47:18,000
 end transformer in the end I'll see you next week will be the last lecture I'm

2255
02:47:18,000 --> 02:47:22,000
 going to cover and then after that another lecture will come into takeover

2256
02:47:22,000 --> 02:47:38,000
 those who have questions can come here just now

2257
02:47:52,000 --> 02:47:54,000
 you

2258
02:48:22,000 --> 02:48:24,000
 you

2259
02:48:52,000 --> 02:48:54,000
 you

2260
02:49:22,000 --> 02:49:24,000
 you

2261
02:49:52,000 --> 02:49:54,000
 you

2262
02:50:22,000 --> 02:50:24,000
 you

2263
02:50:52,000 --> 02:50:54,000
 you

2264
02:51:22,000 --> 02:51:24,000
 you

2265
02:51:52,000 --> 02:51:54,000
 you

2266
02:52:22,000 --> 02:52:24,000
 you

2267
02:52:52,000 --> 02:52:54,000
 you

2268
02:53:22,000 --> 02:53:24,000
 you

2269
02:53:52,000 --> 02:53:54,000
 you

2270
02:54:22,000 --> 02:54:24,000
 you

2271
02:54:52,000 --> 02:54:54,000
 you

2272
02:55:22,000 --> 02:55:24,000
 you

2273
02:55:52,000 --> 02:55:54,000
 you

2274
02:56:22,000 --> 02:56:24,000
 you

2275
02:56:52,000 --> 02:56:54,000
 you

2276
02:57:22,000 --> 02:57:24,000
 you

2277
02:57:52,000 --> 02:57:54,000
 you

2278
02:58:22,000 --> 02:58:24,000
 you

2279
02:58:52,000 --> 02:58:54,000
 you

2280
02:59:22,000 --> 02:59:24,000
 you

2281
02:59:52,000 --> 02:59:54,000
 you

