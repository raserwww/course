1
00:00:00,000 --> 00:00:04,900
Okay, I think now we can start.

2
00:00:04,900 --> 00:00:09,080
So first of all, I mean, thanks for coming and good afternoon to everyone.

3
00:00:09,080 --> 00:00:12,320
I believe I'm the second instructor you have seen here, right?

4
00:00:12,320 --> 00:00:16,920
So before Prof. Chen Yipui was taking care of the first part, I believe all of you finished

5
00:00:16,920 --> 00:00:17,920
that part.

6
00:00:17,920 --> 00:00:23,320
So original plan is that I'm going to be your third instructor, you know, I should teach

7
00:00:23,320 --> 00:00:28,720
you part three, and you should see Prof. Tang Yangping actually this week, who is your

8
00:00:28,720 --> 00:00:33,720
second instructor, but because he's actually very busy, he's our, you know, vice president,

9
00:00:33,720 --> 00:00:34,720
so he's traveling.

10
00:00:34,720 --> 00:00:39,520
So I'm going to give my first lecture in week six first, okay?

11
00:00:39,520 --> 00:00:42,520
And it's totally fine, because I'm going to talk about unsupervised learning.

12
00:00:42,520 --> 00:00:45,360
So you don't have to learn the network, okay?

13
00:00:45,360 --> 00:00:51,000
So it's going to be like quite standalone, and also the good part for me to talk first

14
00:00:51,000 --> 00:00:54,800
is that I wish to brief you about the mini projects, okay?

15
00:00:54,800 --> 00:00:58,400
So I'm going to talk about that very soon, but I want to make sure all of you know that

16
00:00:58,400 --> 00:01:00,840
you need to do a project, okay?

17
00:01:00,840 --> 00:01:07,280
Anyone doesn't know about this, like, so for this class, the mini project was 20%, okay,

18
00:01:07,280 --> 00:01:12,760
which is a decent amount of marks, okay, so you must be well prepared for it, okay?

19
00:01:12,760 --> 00:01:15,640
So later, I'm going to talk about that.

20
00:01:15,640 --> 00:01:21,000
So okay, so first let me introduce myself, my name is Bi Han, so you can, so my research

21
00:01:21,000 --> 00:01:25,080
field is about, you know, machine learning, image processing, computer vision.

22
00:01:25,080 --> 00:01:30,960
So I'm going to cover four lectures, right, mainly on, you know, unsupervised learning,

23
00:01:30,960 --> 00:01:35,640
and a little bit about statistical learning theory, like how can you analyze whether your

24
00:01:35,640 --> 00:01:40,440
network is working well, or something is wrong, how can you fix the problems when you train

25
00:01:40,440 --> 00:01:44,880
the networks, and also I'm going to talk a little bit about, you know, some of the specific

26
00:01:44,880 --> 00:01:48,480
methods relevant to, you know, unsupervised learning, okay?

27
00:01:48,480 --> 00:01:51,660
So that will be basically what I'm going to cover.

28
00:01:51,660 --> 00:01:54,840
So this is a plan, right, as you can see there's an interruption.

29
00:01:54,840 --> 00:01:59,480
So I'm going to do this in this week, week six, about the unsupervised learning.

30
00:01:59,480 --> 00:02:04,240
So mainly we're going to talk about clustering, and also regression, okay?

31
00:02:04,240 --> 00:02:09,400
And after that, starting from next week, you want to see me, you know, next week, so Prof.

32
00:02:09,400 --> 00:02:13,280
Tan-Yap is going to take over, and give you four other lectures, all right?

33
00:02:13,280 --> 00:02:19,880
So I'm going to come back again in week 11, okay, we're going to talk about the method

34
00:02:19,880 --> 00:02:23,080
for regularization and optimization for deep models.

35
00:02:23,080 --> 00:02:27,520
And this is after Prof. Tan already introduced you a bunch of networks, like convolutional

36
00:02:27,520 --> 00:02:32,280
neural networks, recurrent neural networks, so you have some idea how can you design or

37
00:02:32,280 --> 00:02:34,600
how can you use some network, right?

38
00:02:34,600 --> 00:02:40,040
So I'm going to tell you what could be wrong if you train a model, right, and how can you

39
00:02:40,040 --> 00:02:41,040
fix it, okay?

40
00:02:41,040 --> 00:02:43,120
So I think that's like a nice order.

41
00:02:43,120 --> 00:02:47,880
And last but not least, we're going to look at the Bayesian reasoning, which is another

42
00:02:47,880 --> 00:02:52,760
different approach, which you can classify, right, so it's not deterministic, something

43
00:02:52,760 --> 00:02:58,040
like stochastic, all right, and also how can we do dimensionality reduction to avoid

44
00:02:58,040 --> 00:03:00,840
overfitting, to deal with high-dimensional data, right?

45
00:03:00,840 --> 00:03:06,240
So and last but not least, right, so week 13, that's pretty much the last week, right,

46
00:03:06,240 --> 00:03:08,200
last lecture we're going to do.

47
00:03:08,200 --> 00:03:13,360
So I decided to make this week not exact, okay, so in other words, I'm going to talk

48
00:03:13,360 --> 00:03:19,560
about how can we do low dimensionality modeling of the model, but it's just for your interest,

49
00:03:19,560 --> 00:03:20,560
all right?

50
00:03:20,560 --> 00:03:22,760
So I won't give you any questions on that part.

51
00:03:22,760 --> 00:03:27,560
Instead, I'm going to reserve the time for a summary, okay, I'm going to give you a summary

52
00:03:27,560 --> 00:03:32,640
about my part so that you can better prepare for your final exam, and also I can take any

53
00:03:32,640 --> 00:03:35,440
question regarding the mini project, okay?

54
00:03:35,440 --> 00:03:37,360
So this is like the plan for my part.

55
00:03:37,360 --> 00:03:41,960
Okay, another thing I want to say is that I'm not sure about other instructor, but for

56
00:03:41,960 --> 00:03:46,920
my lecture, you can ask me question anytime, okay?

57
00:03:46,920 --> 00:03:52,200
Feel free to interrupt me if you have any question, anything unclear, anything like

58
00:03:52,200 --> 00:03:56,840
maybe I run too fast, I run too slow, you can raise your hand and let me know, okay?

59
00:03:56,840 --> 00:04:01,520
I'm actually welcome that because, you know, sometimes you have questions better to ask

60
00:04:01,520 --> 00:04:06,880
right now rather than waiting until the end, because if you wait, you may forget your question,

61
00:04:06,880 --> 00:04:07,880
okay?

62
00:04:07,880 --> 00:04:09,880
So do ask me question anytime if you want.

63
00:04:09,880 --> 00:04:13,280
That's perfectly fine, and actually that's encouraged, okay?

64
00:04:13,280 --> 00:04:19,680
All right, so before I move on to the actual content, like the unsupervised learning, right,

65
00:04:19,680 --> 00:04:22,320
all of you should already know that we need to do a mini project.

66
00:04:22,320 --> 00:04:27,240
So I do receive some questions because I think for this group of students, you are master

67
00:04:27,240 --> 00:04:28,240
students, right?

68
00:04:28,240 --> 00:04:29,240
How many of you are PhD?

69
00:04:29,240 --> 00:04:30,240
I don't know.

70
00:04:30,240 --> 00:04:31,240
Do we have any PhD students here?

71
00:04:31,240 --> 00:04:33,760
All of you are master students, right?

72
00:04:33,760 --> 00:04:39,200
So probably this is new to you because we do have some subjects in NTU which involve

73
00:04:39,200 --> 00:04:40,200
project, right?

74
00:04:40,200 --> 00:04:45,360
But if this is new to you, you better listen to what I'm going to tell you because you

75
00:04:45,360 --> 00:04:49,200
need to know what you are, what we're expecting from the project, okay?

76
00:04:49,200 --> 00:04:52,860
So the mini project should be made available to you already.

77
00:04:52,860 --> 00:04:59,720
If you go to the folder under my name, there's a, like a folder says, you know, mini project.

78
00:04:59,720 --> 00:05:06,240
So basically the due date is the last Friday of week 13, okay?

79
00:05:06,240 --> 00:05:10,800
So basically it can work all the way until the last week, but I set the deadline to be

80
00:05:10,800 --> 00:05:14,900
the Friday of that week, which is November 15th, okay?

81
00:05:14,900 --> 00:05:17,080
So start, this is actually week six, right?

82
00:05:17,080 --> 00:05:21,840
You have, how many, you have like more than several weeks.

83
00:05:21,840 --> 00:05:26,080
There's also recess week, so you have eight weeks to do it, right, which should be enough.

84
00:05:26,080 --> 00:05:29,220
But most importantly is that this is a group project.

85
00:05:29,220 --> 00:05:34,360
So you are not supposed to do this independent, it's sort of individually, but you can group

86
00:05:34,400 --> 00:05:36,760
with two other members, okay?

87
00:05:36,760 --> 00:05:40,160
That's why earlier I set an announcement.

88
00:05:40,160 --> 00:05:43,440
I don't know how many of you, all of you should receive the announcement, right?

89
00:05:43,440 --> 00:05:46,880
So maybe I can go to that announcement I made earlier.

90
00:05:48,040 --> 00:05:50,240
You go to the announcement, right?

91
00:05:50,240 --> 00:05:54,680
So there was announcement before I made here, right?

92
00:05:54,680 --> 00:06:00,740
So I'm saying that, well, the very first thing I need you to do before even starting a project

93
00:06:00,740 --> 00:06:02,840
is about finding a group, right?

94
00:06:02,840 --> 00:06:09,600
So you need to kind of team up and form a group of three to do the project.

95
00:06:09,600 --> 00:06:11,320
So I offer two options.

96
00:06:11,320 --> 00:06:15,360
You can either propose your group, for example, you want to work with your friends, you want

97
00:06:15,360 --> 00:06:20,520
to work with some of your other members you know very well, you can then, you can actually

98
00:06:20,520 --> 00:06:24,600
click this link and go to this form, right?

99
00:06:24,600 --> 00:06:28,240
Okay, I should, I have to sign in.

100
00:06:28,240 --> 00:06:31,560
Okay, anyway, you can sign in using your account because if I sign in, I'm going to see the

101
00:06:32,080 --> 00:06:37,120
statistics I want to show you, but if you have a team in mind, you want to propose your

102
00:06:37,120 --> 00:06:41,920
team, please use this form to tell me who are the members, okay?

103
00:06:41,920 --> 00:06:46,000
You need to give me the name of the three member in your groups, not only the name but

104
00:06:46,000 --> 00:06:47,000
also the email.

105
00:06:47,000 --> 00:06:51,840
Okay, the reason is that we do have some students with the same name, okay, unfortunately or

106
00:06:51,840 --> 00:06:52,840
fortunately.

107
00:06:52,840 --> 00:06:56,600
So I need to know who is the person in your group.

108
00:06:56,600 --> 00:07:01,120
So give me the name of your three members, the emails, and also the metric number because

109
00:07:01,680 --> 00:07:03,240
these are weak, okay?

110
00:07:03,240 --> 00:07:10,400
So we do, I think I have received like a 30 group, 30 something group already, if you

111
00:07:10,400 --> 00:07:15,480
haven't done so, please do it at least before recess, okay?

112
00:07:15,480 --> 00:07:19,680
I mean you have a week for recess to kind of work out the group, right?

113
00:07:19,680 --> 00:07:25,240
But let's say that if you don't find a group, right, oh, by the way, so the other important

114
00:07:25,240 --> 00:07:30,240
thing is only one of your team members do this on behalf of the whole group.

115
00:07:30,240 --> 00:07:36,320
So in other words, don't submit duplicate forms, all right, so for example, if I or

116
00:07:36,320 --> 00:07:41,400
two of you are working in the group, right, so one of us submit once, that's enough, okay?

117
00:07:41,400 --> 00:07:46,120
So don't everybody, don't just submit like from everybody, okay, because I want to avoid

118
00:07:46,120 --> 00:07:48,640
duplicates, okay?

119
00:07:48,640 --> 00:07:54,560
And okay, use your true name, okay, we do have some student give me some nickname or

120
00:07:54,560 --> 00:07:57,220
your English name, I have no idea who you are, okay?

121
00:07:57,220 --> 00:08:01,420
Give me the name that shows up on your metric card, okay, then I can trace back who is the

122
00:08:01,420 --> 00:08:05,780
guy because I need to give you the marks, okay, correspondingly, if you give me the

123
00:08:05,780 --> 00:08:09,880
funny name, you end up with zero marks, okay, you take the risk, all right?

124
00:08:09,880 --> 00:08:15,020
So but have to say that if you cannot find a group, which is perfectly fine, so I know

125
00:08:15,020 --> 00:08:20,780
nobody, I don't know who I want to work with, and then you can use this form, okay?

126
00:08:20,780 --> 00:08:28,380
So use this form to give me that, okay, who am I, and I want, I don't have a team, I want

127
00:08:28,380 --> 00:08:33,580
your help to give me a team, so what I'm going to do is I will wait until the end of recess

128
00:08:33,580 --> 00:08:38,140
week, which is two weeks from now, end of the recess week, I'm going to look at this

129
00:08:38,140 --> 00:08:43,340
form, so for those of you, for those of you give me the name in this form, I'm going to

130
00:08:43,340 --> 00:08:47,180
randomly assign you together as a group of three, okay?

131
00:08:47,180 --> 00:08:51,940
So basically you can think about it, right, you want to do random teaming up with others

132
00:08:51,940 --> 00:08:55,420
or you want to work with somebody you know, all right, you need to make a decision, so

133
00:08:55,420 --> 00:09:02,220
no, either way, we're going to end up with a grouping after the recess, and then you

134
00:09:02,220 --> 00:09:07,780
can start to do, I mean, then you should already get ready to start your mini project, all

135
00:09:07,780 --> 00:09:08,780
right?

136
00:09:08,780 --> 00:09:15,420
So basically I wish this can be done basically a couple days after recess week, okay, but

137
00:09:15,620 --> 00:09:20,500
any question regarding this teaming, no, grouping up?

138
00:09:20,500 --> 00:09:25,260
Anyone has any question, like maybe you don't understand what you're supposed to do?

139
00:09:25,260 --> 00:09:30,820
So basically everybody must do, sorry, not everybody, like as a team, right, every team

140
00:09:30,820 --> 00:09:34,980
must do either way, right, you either give me a team of members or you tell me if you

141
00:09:34,980 --> 00:09:36,860
don't have a group, all right?

142
00:09:36,860 --> 00:09:40,420
So if I see no response, I'm going to email you or maybe I just assume that you don't

143
00:09:40,420 --> 00:09:43,420
have a team, okay?

144
00:09:43,420 --> 00:09:44,420
Clear?

145
00:09:44,420 --> 00:09:45,420
Any question?

146
00:09:45,420 --> 00:09:48,420
Okay, great, so this is the grouping, right?

147
00:09:48,420 --> 00:09:49,420
Yes?

148
00:09:49,420 --> 00:09:57,420
Yes, yes, okay, very good question, thank you, so okay, very good point, I almost forgot

149
00:09:57,420 --> 00:09:58,420
that.

150
00:09:58,420 --> 00:10:03,420
So in the form I ask you to specify which option, so I'm going to tell you that we have

151
00:10:03,420 --> 00:10:08,420
two options for you to do mini project, you do either one, and in the form I ask you to

152
00:10:08,420 --> 00:10:12,420
indicate which option you want to proceed, right, but you can change it, you know, I'm

153
00:10:12,420 --> 00:10:14,420
totally fine, you want to change it, all right?

154
00:10:14,420 --> 00:10:20,420
So that's just like for me to kind of see what's the preference, like some survey, but

155
00:10:20,420 --> 00:10:23,420
you can change that, no problem, you know.

156
00:10:23,420 --> 00:10:24,420
So, yes?

157
00:10:24,420 --> 00:10:26,420
Do we have a peer assessment?

158
00:10:26,420 --> 00:10:27,420
Sorry?

159
00:10:27,420 --> 00:10:28,420
Peer assessment.

160
00:10:28,420 --> 00:10:30,420
You have, sorry, peer?

161
00:10:30,420 --> 00:10:31,420
Peer assessment.

162
00:10:31,420 --> 00:10:34,420
Oh, peer assessment, oh, peer assessment, okay, good point.

163
00:10:34,420 --> 00:10:37,420
You need, yes, I'm going to get there very soon.

164
00:10:37,420 --> 00:10:44,420
So in fact, in the project introduction, I say that in the end of the, sorry, in the

165
00:10:44,420 --> 00:10:49,420
end when you submit the report, means like the, you know, the form report, you better

166
00:10:49,420 --> 00:10:54,420
tell me who is contributing to what, okay, because I'm expecting you to, let's say, you

167
00:10:54,420 --> 00:11:00,420
know, maybe one can focus on algorithm, the other one can maybe focus on survey or something,

168
00:11:00,420 --> 00:11:05,420
it's perfectly fine, you can tell me who is contributing to what, or if you say both of

169
00:11:05,420 --> 00:11:09,420
you are not happy about the third member, you can also tell me, basically, yeah.

170
00:11:09,420 --> 00:11:14,420
So in some sense, yes, it's kind of like peer assessment, yes, very good point.

171
00:11:14,420 --> 00:11:16,420
All right, any other questions?

172
00:11:16,420 --> 00:11:17,420
I mean, that's great.

173
00:11:17,420 --> 00:11:21,420
You know, you asked me, I share this to everybody, so all of us are synchronized, right?

174
00:11:21,420 --> 00:11:22,420
Great, thank you.

175
00:11:22,420 --> 00:11:23,420
Any other questions?

176
00:11:23,420 --> 00:11:30,420
Okay, so if no more questions, I'm going to go back to my introduction, right?

177
00:11:30,420 --> 00:11:36,420
So like I mentioned, okay, another principle, right, I forgot to mention, you can ask me

178
00:11:36,420 --> 00:11:41,420
a question anytime, okay, I don't mind that, I actually encourage that, but if while I'm

179
00:11:41,420 --> 00:11:46,420
talking, if you are talking, it's going to be very hard to organize, all right, because

180
00:11:46,420 --> 00:11:52,420
we have a huge class here, so I really appreciate if you can like kind of keep quiet, and let's

181
00:11:52,420 --> 00:11:56,420
just discuss, you know, if you ask me, that's fine, or you want to discuss with your friends,

182
00:11:56,420 --> 00:11:58,420
let's do that in the break, okay?

183
00:11:58,420 --> 00:12:01,420
Otherwise, it becomes like very noisy here, okay?

184
00:12:01,420 --> 00:12:02,420
Appreciate that.

185
00:12:02,420 --> 00:12:08,420
So we're going to work on a group of three, and for every group, you just need to prepare

186
00:12:08,420 --> 00:12:10,420
one report, okay?

187
00:12:10,420 --> 00:12:15,420
So later, when we create a submission portal, it's the same that you don't have to submit

188
00:12:15,420 --> 00:12:20,420
a report by everybody, you just need to, one, representing the whole group to submit one

189
00:12:20,420 --> 00:12:24,420
report, and of course, with the peer assessments, right?

190
00:12:24,420 --> 00:12:29,420
So you clearly specify the team members in the group, okay, give me, so who are the members,

191
00:12:29,420 --> 00:12:34,420
give me the name, give me the email address, or maybe even the metric number, and state

192
00:12:34,420 --> 00:12:37,420
the respective contribution of each member to the project.

193
00:12:37,420 --> 00:12:43,420
All right, so this is actually what I'm expecting in the end, right, as the report, okay?

194
00:12:43,420 --> 00:12:48,420
Okay, now, as I mentioned, right, we have two options for you, two project options for

195
00:12:48,420 --> 00:12:52,420
you, you can do either one, all right, so the, I mean, apparently the difference is

196
00:12:52,420 --> 00:12:54,420
what kind of data we're dealing with, right?

197
00:12:54,420 --> 00:12:59,420
So the first option, the name is like a sentiment analysis, right?

198
00:12:59,420 --> 00:13:02,420
Basically, we're talking about like some kind of product review, right?

199
00:13:02,420 --> 00:13:06,420
You think about you are selling something in, let's say, Taobao, or I don't know, in

200
00:13:06,420 --> 00:13:07,420
Amazon, right?

201
00:13:07,420 --> 00:13:11,420
You get some review from the customers, and you need to have a model to determine whether

202
00:13:11,420 --> 00:13:16,420
this guy is happy, or unhappy, or he's angry, or he's positive and active.

203
00:13:16,420 --> 00:13:21,420
So basically, this is definitely a natural language processing problem, but we make it

204
00:13:21,420 --> 00:13:26,420
simple, right, just a binary classification, zero or one, happy or unhappy, negative or

205
00:13:26,420 --> 00:13:27,420
positive, right?

206
00:13:27,420 --> 00:13:33,420
So you need to come up with this binary classification method to process those, you know, kind of

207
00:13:33,420 --> 00:13:37,420
text review comments and give me your prediction.

208
00:13:37,420 --> 00:13:42,420
So you will have two data sets, this is same for both options, right?

209
00:13:42,420 --> 00:13:48,420
You will have a training data set, which involves not only the product review, but also the

210
00:13:48,420 --> 00:13:49,420
sentiment.

211
00:13:49,420 --> 00:13:53,420
In other words, you also have the labels, zero or one, okay?

212
00:13:53,420 --> 00:14:01,420
Can you see my, oops, wait a second, maybe I use a pointer, right, laser pointer is better.

213
00:14:01,420 --> 00:14:05,420
Okay, so you will have the label, you can use them for training, you have the data,

214
00:14:05,420 --> 00:14:07,420
so this is the training data set we're talking about.

215
00:14:07,420 --> 00:14:12,420
So you have those, you can do whatever you like, you can separate them into validation,

216
00:14:12,420 --> 00:14:15,420
training, or you're going to use all of them, it's up to you, all right?

217
00:14:15,420 --> 00:14:18,420
But what I'm caring about is the testing data, right?

218
00:14:18,420 --> 00:14:25,420
So you will have a set, which is for you to predict, but this testing data, we do not

219
00:14:25,420 --> 00:14:27,420
offer you the ground truth.

220
00:14:27,420 --> 00:14:33,420
So it's just the text, there's no label, and you need to use your model to predict the

221
00:14:33,420 --> 00:14:37,420
sentiment of those texts or those reviews.

222
00:14:37,420 --> 00:14:42,420
So you can perform this binary classification to give me your predicted label.

223
00:14:43,420 --> 00:14:49,420
Okay, zero or one, and I have the ground truth, I'm going to evaluate your result, okay?

224
00:14:49,420 --> 00:14:53,420
So of course I wish you have a better result, but let's make it clear, right?

225
00:14:53,420 --> 00:14:56,420
I'm not looking for something like a state of the art.

226
00:14:56,420 --> 00:14:59,420
As long as you give me the reasonable result, I'm fine, okay?

227
00:14:59,420 --> 00:15:01,420
Don't give me, say, hey, random guess or something.

228
00:15:01,420 --> 00:15:04,420
You give me something kind of reasonable, I'm okay with that.

229
00:15:04,420 --> 00:15:08,420
I'm not looking for, say, hell, you have to be the top of the leaderboard, no, okay?

230
00:15:08,420 --> 00:15:11,420
What I don't care about is other aspects, which I'm going to share with you very soon,

231
00:15:11,420 --> 00:15:12,420
all right?

232
00:15:12,420 --> 00:15:18,420
But you need to perform your, run your model over the testing data and submit the prediction,

233
00:15:18,420 --> 00:15:19,420
okay?

234
00:15:19,420 --> 00:15:22,420
So this is kind of true for both options, okay?

235
00:15:22,420 --> 00:15:28,420
But the first option focuses on text, natural language processing, NLP, all right?

236
00:15:28,420 --> 00:15:32,420
We do have a second option, which is more about computer vision, all right?

237
00:15:32,420 --> 00:15:35,420
So this is, we call it DAWP versus CATS.

238
00:15:35,420 --> 00:15:37,420
So it's also a binary classification.

239
00:15:37,420 --> 00:15:39,420
I'll give you a bunch of images.

240
00:15:39,420 --> 00:15:42,420
Some of them are dog images, some of them are cats images.

241
00:15:42,420 --> 00:15:46,420
So again, you will have a training data, you will have a testing data.

242
00:15:46,420 --> 00:15:50,420
For training data, the dogs and cats are stored in two folders separately.

243
00:15:50,420 --> 00:15:55,420
So you know which images are cats, which images are dogs, okay?

244
00:15:55,420 --> 00:16:00,420
You can use this for training, and you do whatever you like for the training data,

245
00:16:00,420 --> 00:16:05,420
and then you need to give me a prediction result of the testing data set, right?

246
00:16:05,420 --> 00:16:10,420
So you will have a bunch of images without knowing whether their cats are dogs, okay?

247
00:16:10,420 --> 00:16:16,420
So you need to run your classifier and give me the predicted labels, 0 or 1, okay?

248
00:16:16,420 --> 00:16:17,420
Very similar.

249
00:16:17,420 --> 00:16:26,420
So basically for both objects, I'm expecting a CSV file where you will have the ID of the testing data,

250
00:16:26,420 --> 00:16:30,420
which is given, and then you need to give me the corresponding prediction, the label.

251
00:16:30,420 --> 00:16:34,420
So you need to change here into either 0 or 1.

252
00:16:34,420 --> 00:16:38,420
All right, so this is the file you need to submit, okay?

253
00:16:38,420 --> 00:16:41,420
So fill out this prediction result here, so don't change the ID.

254
00:16:41,420 --> 00:16:44,420
Just keep the ID, okay, because I need to run this.

255
00:16:44,420 --> 00:16:46,420
So don't just swap the order.

256
00:16:46,420 --> 00:16:53,420
So keep the order of the ID and input or generate the labels your model is predicting, okay?

257
00:16:53,420 --> 00:16:56,420
So submit this as part of your report.

258
00:16:56,420 --> 00:17:02,420
So you need a PDF report plus this kind of CSV file as your prediction, all right?

259
00:17:02,420 --> 00:17:08,420
And, well, I mean, I'm perfectly fine if some students say, I tried many times,

260
00:17:08,420 --> 00:17:11,420
but somehow I couldn't get any meaningful results.

261
00:17:11,420 --> 00:17:14,420
Okay, maybe not for this group of students, but for undergraduates, that's pretty common.

262
00:17:14,420 --> 00:17:18,420
You know, they just try many things, but they have bugs, they have errors everywhere,

263
00:17:18,420 --> 00:17:20,420
so they really cannot give me any results.

264
00:17:20,420 --> 00:17:25,420
If so, still please describe what you have done, okay?

265
00:17:25,420 --> 00:17:31,420
Give me something, give me partial results or showing the efforts you made towards that goal,

266
00:17:31,420 --> 00:17:33,420
and hopefully I can give you some marks, okay?

267
00:17:33,420 --> 00:17:35,420
If you don't give me anything, I have to give you 0.

268
00:17:35,420 --> 00:17:36,420
I mean, it's very simple, right?

269
00:17:36,420 --> 00:17:40,420
So try to give me something, even if you don't have meaningful results,

270
00:17:40,420 --> 00:17:44,420
but you can demonstrate that you have tried something, you tried to understand the problem,

271
00:17:44,420 --> 00:17:49,420
you tried to come up with the solution, and if those make sense, I can still give you some marks, right?

272
00:17:49,420 --> 00:17:52,420
It can be like screenshots or part of the codes.

273
00:17:52,420 --> 00:17:55,420
Yeah, so basically that's my suggestion, right?

274
00:17:55,420 --> 00:17:58,420
But up to you, I mean, you can decide, right?

275
00:17:58,420 --> 00:18:00,420
But it's just one suggestion.

276
00:18:00,420 --> 00:18:09,420
So, again, the deadline will be the last Friday, Friday of the week 13, so keep in mind that's the deadline.

277
00:18:09,420 --> 00:18:13,420
If you got late, I mean, maybe one or two days, I can give you penalty, right?

278
00:18:13,420 --> 00:18:15,420
You will be deducting marks.

279
00:18:15,420 --> 00:18:17,420
If it's too late, you got 0.

280
00:18:17,420 --> 00:18:20,420
All right, so it's very straightforward.

281
00:18:20,420 --> 00:18:25,420
Okay, any question?

282
00:18:25,420 --> 00:18:27,420
Any question?

283
00:18:27,420 --> 00:18:34,420
Okay, so if no question, I wish to share with you, in case you haven't checked the project details, right,

284
00:18:34,420 --> 00:18:39,420
I want to share with you and highlight a few things, I mean, in case you are not aware of that.

285
00:18:39,420 --> 00:18:42,420
So if you go here, okay, let me show you slowly, right?

286
00:18:42,420 --> 00:18:51,420
So go to the content of your, you know, cost side, find my name here, the other name, I mean, I don't know what they have there,

287
00:18:51,420 --> 00:18:55,420
but under my name, there's a folder called mini project, all right?

288
00:18:55,420 --> 00:19:02,420
If you click that, you will have option one, option two, and a briefing slide, which is the same as what I showed you just now, right?

289
00:19:02,420 --> 00:19:07,420
But I suggest you to read this project description, okay?

290
00:19:07,420 --> 00:19:10,420
So you have a bunch of files here.

291
00:19:10,420 --> 00:19:18,420
Those here are those files which, like a script for you to, let's say, get the training data, get the testing data,

292
00:19:18,420 --> 00:19:20,420
something for you to start with, right?

293
00:19:20,420 --> 00:19:25,420
But there's a PDF, so this is something I wish you to pay attention to.

294
00:19:25,420 --> 00:19:28,420
So let me just quickly go through this.

295
00:19:28,420 --> 00:19:31,420
So this PDF explains the details of the project.

296
00:19:31,420 --> 00:19:36,420
I tried to give some of the description of the dataset, description of the problems.

297
00:19:36,420 --> 00:19:40,420
Okay, you just read them carefully before you start to do the project.

298
00:19:40,420 --> 00:19:44,420
But the most important thing I wish you to pay attention is here, is here.

299
00:19:44,420 --> 00:19:54,420
So from this line, the following things stated what you need to put in your report,

300
00:19:54,420 --> 00:20:00,420
because in the end of the day, you need to submit a report representing the whole group, right?

301
00:20:00,420 --> 00:20:06,420
So I tried to give you all of the questions that I'm looking for answers in your reports.

302
00:20:06,420 --> 00:20:09,420
So basically, it's not just you write anything in the report,

303
00:20:09,420 --> 00:20:15,420
but your report is supposed to answer all of the questions I list out here, okay?

304
00:20:15,420 --> 00:20:17,420
So you can find there are many.

305
00:20:17,420 --> 00:20:22,420
So for this group of students, because you are master students, you are at the graduate level,

306
00:20:22,420 --> 00:20:29,420
you need to do a literature survey before even starting to, say, coming up with your solution, right?

307
00:20:29,420 --> 00:20:33,420
Which is quite common practice, suppose if you want to do a PhD or you want to do research,

308
00:20:33,420 --> 00:20:35,420
you always need to do surveying, right?

309
00:20:35,420 --> 00:20:43,420
So basically, I wish you can basically study some of the tutorial, maybe books, or maybe survey paper

310
00:20:43,420 --> 00:20:49,420
that talking about this field, for example, like computer vision, image classification, right?

311
00:20:49,420 --> 00:20:55,420
You want to study, say, what is the problem definition for image classification, right?

312
00:20:56,420 --> 00:21:01,420
And specifically, I wish you to clarify the problem definition

313
00:21:01,420 --> 00:21:06,420
and give me some kind of idea about what challenges we have to solve the problem,

314
00:21:06,420 --> 00:21:14,420
like open problems, challenges, and also what are the common solution type under different settings.

315
00:21:14,420 --> 00:21:17,420
So I haven't said that. Later what we're going to see, right?

316
00:21:17,420 --> 00:21:22,420
Even for classification, the problem can be supervised, can be unsupervised,

317
00:21:22,420 --> 00:21:26,420
or the type of class can be open set or closed set, right?

318
00:21:26,420 --> 00:21:31,420
Or we're testing data with or without domain shift.

319
00:21:31,420 --> 00:21:37,420
So tell me something from the survey you have done, and you don't have to come up with something new,

320
00:21:37,420 --> 00:21:44,420
but I believe all of this will be discussed if you just Google, say, tutorial or survey paper on image classification.

321
00:21:44,420 --> 00:21:50,420
You should find those answers in the paper, but I wish you to study them and give me some of this understanding.

322
00:21:51,420 --> 00:21:55,420
And also, try to figure out some of the relevant papers.

323
00:21:55,420 --> 00:22:00,420
You can actually refer to those reference pages of the tutorial paper.

324
00:22:00,420 --> 00:22:04,420
They will tell you, okay, maybe these are some of the papers which are most popular

325
00:22:04,420 --> 00:22:08,420
or maybe most recent, representing this field and the progress of the field.

326
00:22:08,420 --> 00:22:14,420
So give me some kind of a study on the conference paper, journal paper,

327
00:22:14,420 --> 00:22:17,420
relevant to this field in the most recent years.

328
00:22:17,420 --> 00:22:21,420
You can quickly read them and summarize the paper in the high level,

329
00:22:21,420 --> 00:22:26,420
demonstrate that you understand the field, just like a very quick reading of the papers.

330
00:22:26,420 --> 00:22:31,420
And then, after that, based on your understanding, please identify, let's say,

331
00:22:31,420 --> 00:22:37,420
one or two top performing or state-of-the-art papers for that problem,

332
00:22:37,420 --> 00:22:41,420
and give me a more detailed summary.

333
00:22:41,420 --> 00:22:43,420
You need to write a summary of these selected papers.

334
00:22:43,420 --> 00:22:45,420
You don't have to be many, just one or two.

335
00:22:46,420 --> 00:22:51,420
So for those things, you can read quickly for a general survey,

336
00:22:51,420 --> 00:22:57,420
and then I need you to do a detailed survey for one or two top performing papers.

337
00:22:58,420 --> 00:23:03,420
So this is a pretty standard practice for graduate students to do research.

338
00:23:03,420 --> 00:23:08,420
So write a few things before you start to work out a solution.

339
00:23:08,420 --> 00:23:11,420
And last but not least, after you finish the survey,

340
00:23:11,420 --> 00:23:15,420
now is the time for you to come up with your solution for this problem,

341
00:23:15,420 --> 00:23:18,420
image classification or text classification.

342
00:23:18,420 --> 00:23:22,420
So let's try to explain which approach or which method,

343
00:23:22,420 --> 00:23:24,420
maybe from the one you surveyed,

344
00:23:24,420 --> 00:23:29,420
and you believe they are the most suitable baseline to start with.

345
00:23:29,420 --> 00:23:32,420
Give me some motivation or give me some reason.

346
00:23:32,420 --> 00:23:38,420
And maybe you can choose that as your baseline, and you modify on top of the baseline.

347
00:23:39,420 --> 00:23:45,420
And also demonstrate and explain what improvement you need to make to,

348
00:23:45,420 --> 00:23:48,420
let's say you make on top of the existing baseline,

349
00:23:48,420 --> 00:23:51,420
start to propose your solution for this problem.

350
00:23:51,420 --> 00:23:54,420
Just like some mini paper you are writing,

351
00:23:54,420 --> 00:23:57,420
I'm not looking for a lot of novelty,

352
00:23:57,420 --> 00:24:02,420
but we should understand what should be done when you write a research paper.

353
00:24:02,420 --> 00:24:04,420
So these are the survey papers.

354
00:24:05,420 --> 00:24:08,420
So this is unique because we don't have this for undergraduate.

355
00:24:08,420 --> 00:24:10,420
For you, that's something special.

356
00:24:10,420 --> 00:24:12,420
And this works 20%.

357
00:24:12,420 --> 00:24:15,420
So if you're doing this well, I can give you 20%.

358
00:24:15,420 --> 00:24:21,420
And of course, the following questions are really for your solution.

359
00:24:21,420 --> 00:24:24,420
So the question including data processing,

360
00:24:24,420 --> 00:24:30,420
including model selection, evaluation of the model,

361
00:24:30,420 --> 00:24:35,420
the parameter settings, your model architecture design,

362
00:24:35,420 --> 00:24:37,420
and also your prediction.

363
00:24:37,420 --> 00:24:40,420
So 50% will be given to your prediction results.

364
00:24:40,420 --> 00:24:42,420
Not about 100%, but 50%.

365
00:24:42,420 --> 00:24:44,420
So you need to make sure your results are reasonable.

366
00:24:44,420 --> 00:24:48,420
And also, you need to demonstrate some analysis.

367
00:24:48,420 --> 00:24:51,420
After you get the results, maybe from your validation data,

368
00:24:51,420 --> 00:24:54,420
analyze what was wrong and what was okay.

369
00:24:54,420 --> 00:25:00,420
So why give me some analysis about pro and cons of your methods?

370
00:25:00,420 --> 00:25:05,420
And also, you want to discuss different choices of feature maps you're using

371
00:25:05,420 --> 00:25:08,420
and also why they are something in favor,

372
00:25:08,420 --> 00:25:13,420
how they are going to affect your project in terms of resource consumption,

373
00:25:13,420 --> 00:25:18,420
memory consumption, computational efficiency, and also accuracy.

374
00:25:18,420 --> 00:25:20,420
So these are some performance analysis.

375
00:25:20,420 --> 00:25:24,420
All of these are discussion about the results you have got.

376
00:25:24,420 --> 00:25:26,420
Okay.

377
00:25:26,420 --> 00:25:30,420
And then we actually give H and I, which are a little bit more challenging.

378
00:25:30,420 --> 00:25:37,420
So H is that suppose you are doing another project,

379
00:25:37,420 --> 00:25:38,420
let's say a separate project,

380
00:25:38,420 --> 00:25:42,420
that you also need to do this sentiment classification,

381
00:25:42,420 --> 00:25:44,420
but it's on a different dataset.

382
00:25:44,420 --> 00:25:47,420
So here is like a hotel, let's say hotel reviews.

383
00:25:48,420 --> 00:25:53,420
But those reviews consist of only the reviews in the text,

384
00:25:53,420 --> 00:25:56,420
but doesn't really come with the rating score.

385
00:25:56,420 --> 00:26:01,420
So how can you actually make use of your solution right now

386
00:26:01,420 --> 00:26:06,420
and generalize to another dataset which only has the text,

387
00:26:06,420 --> 00:26:08,420
but doesn't really contain the labels?

388
00:26:08,420 --> 00:26:09,420
All right.

389
00:26:09,420 --> 00:26:10,420
So this is like an open problem.

390
00:26:10,420 --> 00:26:16,420
So you can describe how your classification algorithm will perform in this new task

391
00:26:16,420 --> 00:26:20,420
and what you need to do to perform well in a new dataset.

392
00:26:20,420 --> 00:26:22,420
Okay.

393
00:26:22,420 --> 00:26:26,420
So try to read them and every question for certain marks.

394
00:26:26,420 --> 00:26:31,420
So make sure you have a reasonable response to every kind of sub-question.

395
00:26:31,420 --> 00:26:33,420
And also there's another one.

396
00:26:33,420 --> 00:26:40,420
It's like if we have some noise in the annotation,

397
00:26:40,420 --> 00:26:42,420
which is possible in practice,

398
00:26:42,420 --> 00:26:45,420
and discuss how the method will perform in this new project,

399
00:26:45,420 --> 00:26:48,420
weighs the noise in your training set, right?

400
00:26:48,420 --> 00:26:50,420
And name some approaches.

401
00:26:50,420 --> 00:26:53,420
Let's say there are three approaches that you can improve your algorithm

402
00:26:53,420 --> 00:26:56,420
to perform even better in the new algorithm, right?

403
00:26:56,420 --> 00:26:57,420
So where do we learn some of them?

404
00:26:57,420 --> 00:26:59,420
For example, how can we deal with noise?

405
00:26:59,420 --> 00:27:01,420
How can we prevent overfitting?

406
00:27:01,420 --> 00:27:03,420
How can we deal with domain shift?

407
00:27:03,420 --> 00:27:05,420
So if you cannot answer them right now,

408
00:27:05,420 --> 00:27:08,420
you are more than welcome to search, find a solution,

409
00:27:08,420 --> 00:27:11,420
or you can pay attention to the thing we're going to talk about in week 11.

410
00:27:11,420 --> 00:27:13,420
All right?

411
00:27:13,420 --> 00:27:19,420
So I suggest you to rate the description first before moving on to the solution.

412
00:27:19,420 --> 00:27:23,420
And my grading criteria will be exactly following this distribution.

413
00:27:23,420 --> 00:27:24,420
All right?

414
00:27:24,420 --> 00:27:27,420
So basically I'm going to look at whether you answer this question well.

415
00:27:27,420 --> 00:27:29,420
If yes, I give you 10 marks.

416
00:27:29,420 --> 00:27:31,420
If you do this very well, I give you 10 marks.

417
00:27:31,420 --> 00:27:35,420
So you need to make sure every question is well answered.

418
00:27:35,420 --> 00:27:39,420
And then you should get a pretty good result.

419
00:27:39,420 --> 00:27:41,420
Okay?

420
00:27:41,420 --> 00:27:42,420
So I'm going to use this example.

421
00:27:42,420 --> 00:27:45,420
For example, we have the same thing for the other options.

422
00:27:45,420 --> 00:27:47,420
I won't really go through it, but please read it.

423
00:27:47,420 --> 00:27:49,420
And if you have any questions, you can get back to me.

424
00:27:49,420 --> 00:27:51,420
I'm happy to clarify.

425
00:27:51,420 --> 00:27:52,420
All right?

426
00:27:52,420 --> 00:27:54,420
Does the report have any limit?

427
00:27:54,420 --> 00:27:55,420
Sorry?

428
00:27:55,420 --> 00:27:57,420
Does the report have any limit?

429
00:27:57,420 --> 00:27:58,420
No, we don't have any limit.

430
00:27:58,420 --> 00:28:02,420
But even though we might need to write it down, we're okay.

431
00:28:02,420 --> 00:28:04,420
So the whole idea is to answer all of the questions.

432
00:28:04,420 --> 00:28:05,420
Okay?

433
00:28:05,420 --> 00:28:09,420
So you won't be penalized, but it doesn't really help, actually.

434
00:28:09,420 --> 00:28:10,420
Okay?

435
00:28:10,420 --> 00:28:13,420
So as long as you believe those questions are well answered, that's good enough.

436
00:28:13,420 --> 00:28:14,420
Okay?

437
00:28:14,420 --> 00:28:15,420
Good question.

438
00:28:15,420 --> 00:28:17,420
Any other questions?

439
00:28:20,420 --> 00:28:21,420
Okay?

440
00:28:21,420 --> 00:28:22,420
Last call.

441
00:28:22,420 --> 00:28:23,420
Any questions?

442
00:28:23,420 --> 00:28:24,420
Okay.

443
00:28:24,420 --> 00:28:26,420
Now I'm going to continue my lecture today, right?

444
00:28:26,420 --> 00:28:27,420
Okay.

445
00:28:27,420 --> 00:28:28,420
Let me think.

446
00:28:28,420 --> 00:28:30,420
Did I miss anything?

447
00:28:30,420 --> 00:28:33,420
Oh, by the way, one more thing.

448
00:28:33,420 --> 00:28:35,420
I don't know whether the prof chair announced.

449
00:28:35,420 --> 00:28:37,420
So we're going to set up some office hours.

450
00:28:38,420 --> 00:28:44,420
Not now, but around week 11, week 12, which means we expect some questions from you when

451
00:28:44,420 --> 00:28:46,420
we're approaching to the deadline of the mini project.

452
00:28:46,420 --> 00:28:51,420
So we have some TAs who are going to set up some office hours.

453
00:28:51,420 --> 00:28:52,420
We can go there and talk to them.

454
00:28:52,420 --> 00:28:57,420
Those TAs are our PhD students, so they should know very well about this project.

455
00:28:57,420 --> 00:28:58,420
So we can talk to them.

456
00:28:58,420 --> 00:29:02,420
Any questions, like questions regarding the project, questions regarding the exam, you

457
00:29:02,420 --> 00:29:03,420
can ask them.

458
00:29:03,420 --> 00:29:04,420
Right?

459
00:29:04,420 --> 00:29:09,420
We can ask them later, but they will be around the last few weeks.

460
00:29:09,420 --> 00:29:10,420
Right.

461
00:29:10,420 --> 00:29:12,420
Basically, when I come back to see you again.

462
00:29:12,420 --> 00:29:13,420
Right.

463
00:29:13,420 --> 00:29:14,420
Week 11.

464
00:29:14,420 --> 00:29:15,420
All right?

465
00:29:15,420 --> 00:29:16,420
Okay.

466
00:29:16,420 --> 00:29:17,420
So.

467
00:29:17,420 --> 00:29:18,420
Oh, okay.

468
00:29:18,420 --> 00:29:19,420
Another thing.

469
00:29:19,420 --> 00:29:22,420
So we are going to have a very long lecture.

470
00:29:22,420 --> 00:29:23,420
I don't need to realize.

471
00:29:23,420 --> 00:29:24,420
Three hours.

472
00:29:24,420 --> 00:29:27,420
So even though I don't like it, but that's how it works here.

473
00:29:27,420 --> 00:29:30,420
We're going to have a break in between, by the way.

474
00:29:30,420 --> 00:29:31,420
I don't know how prof chair did it.

475
00:29:31,420 --> 00:29:35,420
But I will say, let's take a break around 1.45.

476
00:29:35,420 --> 00:29:36,420
All right?

477
00:29:36,420 --> 00:29:37,420
Something in between.

478
00:29:37,420 --> 00:29:38,420
So let's take a 15-minute break.

479
00:29:38,420 --> 00:29:39,420
And we'll continue.

480
00:29:39,420 --> 00:29:41,420
And I'll try to finish things a little bit earlier.

481
00:29:41,420 --> 00:29:42,420
Right?

482
00:29:42,420 --> 00:29:43,420
Because ideally, we should have two breaks.

483
00:29:43,420 --> 00:29:46,420
Let's make it one, and I can finish a little bit earlier.

484
00:29:46,420 --> 00:29:47,420
I'll try my best.

485
00:29:47,420 --> 00:29:48,420
All right?

486
00:29:48,420 --> 00:29:49,420
So I hope that's fine for everyone.

487
00:29:49,420 --> 00:29:50,420
All right?

488
00:29:50,420 --> 00:29:53,420
But I have to say that if you have anything you want to go, feel free.

489
00:29:53,420 --> 00:29:54,420
It's an open class.

490
00:29:54,420 --> 00:29:55,420
Anyway.

491
00:29:55,420 --> 00:29:56,420
Okay.

492
00:29:56,420 --> 00:29:57,420
So now, back to business.

493
00:29:57,420 --> 00:29:58,420
Right?

494
00:29:59,420 --> 00:30:02,420
Everything about unsupervised learning for this course.

495
00:30:02,420 --> 00:30:03,420
Okay?

496
00:30:03,420 --> 00:30:05,420
So two main problems we're going to see.

497
00:30:05,420 --> 00:30:06,420
First one is clustering.

498
00:30:06,420 --> 00:30:07,420
All right.

499
00:30:07,420 --> 00:30:09,420
How many of you know what is clustering?

500
00:30:09,420 --> 00:30:10,420
Let's show of hands.

501
00:30:10,420 --> 00:30:12,420
How many of you already know what is clustering?

502
00:30:12,420 --> 00:30:13,420
Okay.

503
00:30:13,420 --> 00:30:14,420
We have a few.

504
00:30:14,420 --> 00:30:15,420
Right?

505
00:30:15,420 --> 00:30:19,420
By the way, do you have background more like from signal processing or computer science?

506
00:30:19,420 --> 00:30:21,420
How many of you have computer science background?

507
00:30:21,420 --> 00:30:22,420
I just want to know.

508
00:30:22,420 --> 00:30:24,420
Because this year, we have a few.

509
00:30:24,420 --> 00:30:25,420
Okay?

510
00:30:25,420 --> 00:30:26,420
Great.

511
00:30:26,420 --> 00:30:27,420
Great.

512
00:30:27,420 --> 00:30:29,420
So this is all of your signal processing background.

513
00:30:29,420 --> 00:30:30,420
Right?

514
00:30:30,420 --> 00:30:32,420
So from this year, we have a mixture.

515
00:30:32,420 --> 00:30:33,420
Right?

516
00:30:33,420 --> 00:30:37,420
For the SPML program, we have a mixture of signal processing and machine learning.

517
00:30:37,420 --> 00:30:40,420
This course actually is more machine learning.

518
00:30:40,420 --> 00:30:45,420
But I think I make it should be okay for those signal processing background.

519
00:30:45,420 --> 00:30:46,420
Right?

520
00:30:46,420 --> 00:30:49,420
Because in fact, this is the first time we offer this course for undergraduate.

521
00:30:49,420 --> 00:30:50,420
Sorry.

522
00:30:50,420 --> 00:30:51,420
For graduate.

523
00:30:51,420 --> 00:30:53,420
It's actually an undergraduate final year course.

524
00:30:53,420 --> 00:30:57,420
But have a say that for those of you who, let's say coming from signal processing background,

525
00:30:57,420 --> 00:31:00,420
if you have anything you find that, oh, I don't know what you're talking about.

526
00:31:00,420 --> 00:31:02,420
Or you want me to elaborate more?

527
00:31:02,420 --> 00:31:03,420
You want me to slow down?

528
00:31:03,420 --> 00:31:05,420
You want me to change my pace?

529
00:31:05,420 --> 00:31:06,420
Let me know.

530
00:31:06,420 --> 00:31:07,420
Okay.

531
00:31:07,420 --> 00:31:08,420
Because I really need your feedback.

532
00:31:08,420 --> 00:31:11,420
This is my first time I teach master student for this subject.

533
00:31:11,420 --> 00:31:16,420
So if anything feel like not okay with you, let me know.

534
00:31:16,420 --> 00:31:19,420
I'm actually welcoming any feedback we have.

535
00:31:19,420 --> 00:31:20,420
All right.

536
00:31:20,420 --> 00:31:21,420
Okay.

537
00:31:21,420 --> 00:31:22,420
So clustering.

538
00:31:22,420 --> 00:31:27,420
So basically I'm going to explain what's the definition of clustering we're talking about here.

539
00:31:27,420 --> 00:31:34,420
And for any clustering method, right, it's always important to have this distance metric.

540
00:31:34,420 --> 00:31:38,420
Because distance metrics give you the idea of similarity.

541
00:31:38,420 --> 00:31:43,420
Later we're going to see clustering is all about grouping data with the similar nature.

542
00:31:43,420 --> 00:31:44,420
All right.

543
00:31:44,420 --> 00:31:45,420
So how to measure the similarity.

544
00:31:45,420 --> 00:31:47,420
We need to know the metric system.

545
00:31:47,420 --> 00:31:48,420
Okay.

546
00:31:48,420 --> 00:31:49,420
Distance metric.

547
00:31:49,420 --> 00:31:53,420
And we're going to learn two specific algorithms.

548
00:31:53,420 --> 00:31:56,420
K-means and HAC, right.

549
00:31:56,420 --> 00:31:58,420
Hierarchical algorithm narrative clustering.

550
00:31:58,420 --> 00:32:03,420
The reason is that each of them representing one type of clustering method.

551
00:32:03,420 --> 00:32:04,420
All right.

552
00:32:04,420 --> 00:32:07,420
K-means is the method for partition-based clustering.

553
00:32:07,420 --> 00:32:10,420
HAC is the method representing hierarchical clustering.

554
00:32:10,420 --> 00:32:11,420
All right.

555
00:32:11,420 --> 00:32:13,420
So there are many, many other algorithms.

556
00:32:13,420 --> 00:32:16,420
In fact, this is still a very active research field.

557
00:32:16,420 --> 00:32:17,420
But let's keep it simple.

558
00:32:17,420 --> 00:32:19,420
Just one method for one type.

559
00:32:19,420 --> 00:32:20,420
Okay.

560
00:32:20,420 --> 00:32:23,420
So in the end, I'm also going to share some of the examples.

561
00:32:23,420 --> 00:32:24,420
Right.

562
00:32:24,420 --> 00:32:30,420
Visual examples, numerical examples so that you know how you can, how you should apply those methods and what could be wrong.

563
00:32:30,420 --> 00:32:31,420
Right.

564
00:32:31,420 --> 00:32:32,420
So this is roughly the outline.

565
00:32:32,420 --> 00:32:39,420
And for my part, for my lectures, I always offer some questions in the beginning such that I wish you to think about it.

566
00:32:39,420 --> 00:32:40,420
Right.

567
00:32:40,420 --> 00:32:44,420
So you should be able to find the answer by listening to this lecture.

568
00:32:44,420 --> 00:32:45,420
Right.

569
00:32:45,420 --> 00:32:47,420
If you don't know the answer, we're going to see them together.

570
00:32:47,420 --> 00:32:48,420
Right.

571
00:32:48,420 --> 00:32:49,420
In the end.

572
00:32:49,420 --> 00:32:50,420
But keep this question in mind.

573
00:32:50,420 --> 00:32:51,420
For example, what is clustering?

574
00:32:51,420 --> 00:32:52,420
Right.

575
00:32:52,420 --> 00:32:53,420
The definition of it.

576
00:32:53,420 --> 00:32:54,420
And what is the cluster?

577
00:32:54,420 --> 00:32:55,420
Right.

578
00:32:55,420 --> 00:32:56,420
What's the definition of cluster?

579
00:32:56,420 --> 00:32:58,420
And there's a difference between clustering and the classification.

580
00:32:58,420 --> 00:32:59,420
What's the difference?

581
00:32:59,420 --> 00:33:02,420
And also, you need to know the limitations.

582
00:33:02,420 --> 00:33:03,420
Okay.

583
00:33:03,420 --> 00:33:07,420
Or in other words, what could be wrong when you do K-means?

584
00:33:07,420 --> 00:33:10,420
What could be wrong when you do HAC?

585
00:33:10,420 --> 00:33:13,420
So the limitation of those methods must be very clear.

586
00:33:13,420 --> 00:33:16,420
Because in practice, there's no perfect solution.

587
00:33:16,420 --> 00:33:17,420
Right.

588
00:33:17,420 --> 00:33:19,420
Every method will always have some drawbacks.

589
00:33:19,420 --> 00:33:25,420
So as a software engineer or as a researcher, you need to know what is the problem.

590
00:33:25,420 --> 00:33:27,420
What's wrong with those methods?

591
00:33:27,420 --> 00:33:30,420
And then that's how you can make a decision when you do a project.

592
00:33:30,420 --> 00:33:31,420
Right.

593
00:33:31,420 --> 00:33:32,420
So this is also very important.

594
00:33:32,420 --> 00:33:33,420
Okay.

595
00:33:33,420 --> 00:33:39,420
So originally, this slide is for those who have already finished proof of task part.

596
00:33:39,420 --> 00:33:42,420
But anyway, we have talked about classification.

597
00:33:42,420 --> 00:33:43,420
Right.

598
00:33:43,420 --> 00:33:45,420
Just now, I mean, those projects are classification problems.

599
00:33:45,420 --> 00:33:50,420
So classification problem is very common in computer vision, where you can, let's say,

600
00:33:50,420 --> 00:33:52,420
I give an image to the algorithm.

601
00:33:52,420 --> 00:33:56,420
Say, hey, tell me whether this image is a cat or not.

602
00:33:56,420 --> 00:33:57,420
Right.

603
00:33:57,420 --> 00:33:58,420
Do we have a cat there?

604
00:33:58,420 --> 00:34:03,420
And if this algorithm give me an answer, but this answer is like a disgrace, yes or no,

605
00:34:03,420 --> 00:34:09,420
with or without cat, this is actually a very standard kind of classification problems.

606
00:34:09,420 --> 00:34:10,420
Right.

607
00:34:10,420 --> 00:34:13,420
So if you give them a dog image, the algorithm is supposed to tell you no.

608
00:34:13,420 --> 00:34:15,420
There's no cat in the image.

609
00:34:15,420 --> 00:34:20,420
So this is like a classification, which is very common in computer vision.

610
00:34:20,420 --> 00:34:27,420
But the problem is that for classification, I mean, for this course, for this subject,

611
00:34:27,420 --> 00:34:29,420
always need training data.

612
00:34:29,420 --> 00:34:30,420
Okay.

613
00:34:30,420 --> 00:34:32,420
In practice, this is not true.

614
00:34:32,420 --> 00:34:34,420
I mean, we do have unsupervised classification.

615
00:34:34,420 --> 00:34:38,420
But for this course, all of the classification, we need the training.

616
00:34:38,420 --> 00:34:44,420
Training means a data set with example images of cat, example image was dog, and the labels.

617
00:34:44,420 --> 00:34:45,420
Right.

618
00:34:45,420 --> 00:34:49,420
You need to tell computer what do you mean by cat, what do you mean by dog.

619
00:34:49,420 --> 00:34:50,420
Right.

620
00:34:50,420 --> 00:34:52,420
So the training is necessary for classification.

621
00:34:52,420 --> 00:34:56,420
And that's why we call classification a supervised problem.

622
00:34:56,420 --> 00:34:57,420
Okay.

623
00:34:57,420 --> 00:35:01,420
But I mean, here means you have to give them images labeled as cats.

624
00:35:01,420 --> 00:35:02,420
Right.

625
00:35:02,420 --> 00:35:03,420
Images plus label.

626
00:35:03,420 --> 00:35:06,420
This is something as a must for classification.

627
00:35:06,420 --> 00:35:11,420
Now, the question is, what happens if we do not have label at all?

628
00:35:11,420 --> 00:35:12,420
Right.

629
00:35:12,420 --> 00:35:13,420
It's very common in practice.

630
00:35:13,420 --> 00:35:16,420
You have a lot of data, but there's no label.

631
00:35:16,420 --> 00:35:17,420
Nobody annotated the data.

632
00:35:17,420 --> 00:35:18,420
Right.

633
00:35:18,420 --> 00:35:21,420
So what can we do about these kind of images?

634
00:35:21,420 --> 00:35:27,420
For example, if this is the image I have, I wish to know which pixel actually belong

635
00:35:27,420 --> 00:35:28,420
to the flower.

636
00:35:28,420 --> 00:35:30,420
But I have no idea what are the flower.

637
00:35:30,420 --> 00:35:31,420
I have no example.

638
00:35:31,420 --> 00:35:35,420
I just want the computer to tell me out of the whole image, right, which pixel are forming

639
00:35:35,420 --> 00:35:36,420
the flowers.

640
00:35:36,420 --> 00:35:43,420
So this kind of problem is not a supervised problem because I don't really prepare any

641
00:35:43,420 --> 00:35:46,420
data set with images plus label.

642
00:35:46,420 --> 00:35:48,420
But can I do it?

643
00:35:48,420 --> 00:35:54,420
Well, in fact, if you want to talk about the pixel that form one object, you are making

644
00:35:54,420 --> 00:36:00,420
the assumption that those pixel must be similar because all of them belong to flower.

645
00:36:00,420 --> 00:36:01,420
Right.

646
00:36:01,420 --> 00:36:07,420
So you are exploring not just labels, but you are exploring the data intrinsic structure,

647
00:36:07,420 --> 00:36:12,420
or in other words, similarity that belong to the same group.

648
00:36:12,420 --> 00:36:14,420
Here, the group is a flower.

649
00:36:14,420 --> 00:36:18,420
You are exploring and say, hey, tell me which pixel are similar to each other such that

650
00:36:18,420 --> 00:36:20,420
it can form the flower.

651
00:36:20,420 --> 00:36:28,420
So this is actually informally called clustering because I'm giving you some of the pixels.

652
00:36:28,420 --> 00:36:33,420
If you treat every pixel as a data sample, I'm trying to figure out which are the pixels

653
00:36:33,420 --> 00:36:36,420
themselves are similar to each other.

654
00:36:36,420 --> 00:36:38,420
And this process doesn't involve any training.

655
00:36:38,420 --> 00:36:42,420
So it's going to be an unsupervised learning process.

656
00:36:42,420 --> 00:36:45,420
You're still learning, but you don't have any labels.

657
00:36:45,420 --> 00:36:46,420
So it's not supervised.

658
00:36:46,420 --> 00:36:49,420
It's a kind of learning without supervision.

659
00:36:49,420 --> 00:36:51,420
Right here, supervision means labels.

660
00:36:51,420 --> 00:36:52,420
Okay.

661
00:36:52,420 --> 00:36:57,420
So here, unsupervised learning is defined as some kind of self-organized learning that

662
00:36:57,420 --> 00:37:00,420
helps you to find some kind of patterns.

663
00:37:00,420 --> 00:37:01,420
Okay.

664
00:37:01,420 --> 00:37:02,420
The patterns are unknown.

665
00:37:02,420 --> 00:37:03,420
Undefined.

666
00:37:03,420 --> 00:37:04,420
Right.

667
00:37:04,420 --> 00:37:08,420
Not defined in training, but they have it in the data set.

668
00:37:08,420 --> 00:37:12,420
You can still figure that out without pre-existing labels.

669
00:37:12,420 --> 00:37:15,420
This is actually called unsupervised learning.

670
00:37:15,420 --> 00:37:20,420
So for our case, we are specifically interested in clustering.

671
00:37:20,420 --> 00:37:24,420
So clustering is a type of unsupervised learning problem.

672
00:37:24,420 --> 00:37:25,420
Okay.

673
00:37:25,420 --> 00:37:28,420
It's a one sub-category of unsupervised learning.

674
00:37:28,420 --> 00:37:33,420
And the task of clustering is to, let's say, I give you a collection of data.

675
00:37:33,420 --> 00:37:40,420
I want to group or organize the data such that those data in the same group will be

676
00:37:40,420 --> 00:37:42,420
more similar to each other.

677
00:37:42,420 --> 00:37:48,420
So the first thing is data within the group, same group, must be similar to each other.

678
00:37:48,420 --> 00:37:54,420
And also the data outside the same group, say in different groups, must be dissimilar

679
00:37:54,420 --> 00:37:55,420
to each other.

680
00:37:55,420 --> 00:37:56,420
Right.

681
00:37:56,420 --> 00:38:01,420
So you want to make sure the same group of data will be more similar than other pairs.

682
00:38:01,420 --> 00:38:02,420
Right.

683
00:38:02,420 --> 00:38:03,420
The data without, not in the group.

684
00:38:03,420 --> 00:38:06,420
So this is the whole idea about clustering.

685
00:38:06,420 --> 00:38:08,420
Similarity versus dissimilarity.

686
00:38:08,420 --> 00:38:09,420
Right.

687
00:38:09,420 --> 00:38:14,420
So, and in this case, the group we're talking about here is called a cluster.

688
00:38:14,420 --> 00:38:19,420
So the cluster is a set of data that you want to make them similar to each other.

689
00:38:19,420 --> 00:38:20,420
Right.

690
00:38:20,420 --> 00:38:22,420
So this is technically called a cluster.

691
00:38:22,420 --> 00:38:23,420
Okay.

692
00:38:23,420 --> 00:38:26,420
So basically this is the definition of clustering.

693
00:38:26,420 --> 00:38:27,420
Okay.

694
00:38:27,420 --> 00:38:28,420
I hope everybody's now clear.

695
00:38:28,420 --> 00:38:30,420
But of course, these are just definitions.

696
00:38:30,420 --> 00:38:32,420
We also want to see some examples.

697
00:38:32,420 --> 00:38:33,420
All right.

698
00:38:33,420 --> 00:38:36,420
So let's take the same example, the flower image.

699
00:38:36,420 --> 00:38:37,420
It's unsupervised learning.

700
00:38:37,420 --> 00:38:40,420
So I won't give you any label or class information.

701
00:38:40,420 --> 00:38:41,420
Nothing.

702
00:38:41,420 --> 00:38:42,420
Just the image.

703
00:38:42,420 --> 00:38:43,420
Right.

704
00:38:43,420 --> 00:38:49,420
So what I'm going to do is I'm going to perform clustering to group them according to the

705
00:38:49,420 --> 00:38:50,420
similarity.

706
00:38:50,420 --> 00:38:54,420
Of course, here we have many, many ways to define similarity.

707
00:38:54,420 --> 00:38:55,420
Right.

708
00:38:55,420 --> 00:38:57,420
Later we're going to see what is the distance metric.

709
00:38:57,420 --> 00:39:02,420
But let's just think about the most simple measure, the intensity of the pixel.

710
00:39:02,420 --> 00:39:03,420
All right.

711
00:39:03,420 --> 00:39:04,420
So this is the black and white image.

712
00:39:04,420 --> 00:39:07,420
So you can measure how white or how black the pixel is.

713
00:39:07,420 --> 00:39:08,420
Right.

714
00:39:09,420 --> 00:39:14,420
So I plot the histogram where the vertical axis is the intensity.

715
00:39:14,420 --> 00:39:19,420
I mean, the higher the wider, the lower the blacker.

716
00:39:19,420 --> 00:39:20,420
Right.

717
00:39:20,420 --> 00:39:22,420
The horizontal axis is the intensity.

718
00:39:22,420 --> 00:39:23,420
Sorry.

719
00:39:23,420 --> 00:39:24,420
This is the intensity.

720
00:39:24,420 --> 00:39:26,420
You know, zero means it's completely black.

721
00:39:26,420 --> 00:39:28,420
255 is completely white.

722
00:39:28,420 --> 00:39:29,420
All right.

723
00:39:29,420 --> 00:39:34,420
So I'm going to see how many pixels are with specific value, specific intensity.

724
00:39:34,420 --> 00:39:40,420
So the bar here measures the amount of pixels that have that specific intensity.

725
00:39:40,420 --> 00:39:42,420
So this is called histogram.

726
00:39:42,420 --> 00:39:43,420
Right.

727
00:39:43,420 --> 00:39:45,420
I believe some of you may have heard of the term histogram.

728
00:39:45,420 --> 00:39:49,420
So this is one way to show the statistics of a group of data.

729
00:39:49,420 --> 00:39:50,420
Right.

730
00:39:50,420 --> 00:39:55,420
So after you have this histogram, you can use that to measure the similarity.

731
00:39:55,420 --> 00:39:59,420
So basically here the similarity is measured in the intensity domain.

732
00:39:59,420 --> 00:40:03,420
So you have two pixels that have very similar intensity.

733
00:40:03,420 --> 00:40:05,420
I group them together.

734
00:40:05,420 --> 00:40:06,420
Right.

735
00:40:06,420 --> 00:40:08,420
So black with black, white with white.

736
00:40:08,420 --> 00:40:09,420
Something like that.

737
00:40:09,420 --> 00:40:12,420
And then the other group, I form them differently.

738
00:40:12,420 --> 00:40:17,420
So I can use this to group them separately according to their intensity.

739
00:40:17,420 --> 00:40:18,420
All right.

740
00:40:18,420 --> 00:40:23,420
So in this case, I can then separate them into two parts.

741
00:40:23,420 --> 00:40:29,420
And here I believe that those pixels forming the flower should be, let's say, brighter than the background.

742
00:40:29,420 --> 00:40:30,420
Right.

743
00:40:30,420 --> 00:40:31,420
So I make this assumption.

744
00:40:31,420 --> 00:40:32,420
I group them together.

745
00:40:32,420 --> 00:40:37,420
I can extract the main body of the flower, all of our image, without any labels I have.

746
00:40:37,420 --> 00:40:38,420
Right.

747
00:40:38,420 --> 00:40:41,420
So this is a very simple and toy example of clustering.

748
00:40:41,420 --> 00:40:45,420
But I hope I kind of give you some intuition what is happening here.

749
00:40:45,420 --> 00:40:46,420
Right.

750
00:40:46,420 --> 00:40:47,420
So this is one example.

751
00:40:47,420 --> 00:40:56,420
But of course, in practice, without predefined labels, you have no idea what does this yellow, sorry, what does this red and green box mean.

752
00:40:56,420 --> 00:41:00,420
You only know they are two clusters, but you have no idea which one is flower, which one is background.

753
00:41:00,420 --> 00:41:02,420
But you know they will be different.

754
00:41:02,420 --> 00:41:04,420
So that's all clustering can do.

755
00:41:04,420 --> 00:41:05,420
Make them separate it.

756
00:41:05,420 --> 00:41:07,420
Make them different.

757
00:41:07,420 --> 00:41:09,420
All right.

758
00:41:09,420 --> 00:41:10,420
So this is toy example.

759
00:41:10,420 --> 00:41:13,420
But you will end up with two clusters.

760
00:41:13,420 --> 00:41:17,420
And those samples in the cluster will be similar to each other.

761
00:41:17,420 --> 00:41:20,420
And those samples in two different clusters will be different to each other.

762
00:41:20,420 --> 00:41:21,420
Right.

763
00:41:21,420 --> 00:41:24,420
This is what you can get by running clustering.

764
00:41:24,420 --> 00:41:25,420
Okay.

765
00:41:25,420 --> 00:41:26,420
Now let's summarize.

766
00:41:27,420 --> 00:41:33,420
The main difference between classification and clustering is whether or not we have supervision.

767
00:41:33,420 --> 00:41:38,420
So for this class, we can assume that all of the classification methods are always supervised.

768
00:41:38,420 --> 00:41:40,420
But in practice, it's not true.

769
00:41:40,420 --> 00:41:41,420
Okay.

770
00:41:41,420 --> 00:41:44,420
For some of you who actually do research, you know that there are a lot of unsupervised classifications.

771
00:41:44,420 --> 00:41:50,420
But for this class, let's assume that classification always needs to have labels in the training.

772
00:41:50,420 --> 00:41:54,420
And you have to predict the label for the testing data.

773
00:41:54,420 --> 00:41:55,420
Right.

774
00:41:55,420 --> 00:41:57,420
Instead, clustering is the opposite.

775
00:41:57,420 --> 00:41:59,420
You don't have any labels.

776
00:41:59,420 --> 00:42:00,420
You only have data.

777
00:42:00,420 --> 00:42:02,420
So it's unsupervised.

778
00:42:02,420 --> 00:42:09,420
And you can still understand the intrinsic structure of your underlying data by grouping according to the similarity.

779
00:42:09,420 --> 00:42:10,420
Right.

780
00:42:10,420 --> 00:42:11,420
So this is the difference.

781
00:42:11,420 --> 00:42:12,420
Okay.

782
00:42:12,420 --> 00:42:13,420
Very straightforward.

783
00:42:13,420 --> 00:42:16,420
Now, let's just summarize.

784
00:42:16,420 --> 00:42:17,420
Right.

785
00:42:17,420 --> 00:42:21,420
Clustering, we all know that it's unsupervised.

786
00:42:21,420 --> 00:42:24,420
And the basic idea is to group them together with similar data points.

787
00:42:24,420 --> 00:42:32,420
So to run the clustering methods, your input is just a group of data, right, without any label.

788
00:42:32,420 --> 00:42:35,420
So this will be the input you need for running clustering.

789
00:42:35,420 --> 00:42:42,420
And the output will be the cluster, or in other words, the membership of your data points.

790
00:42:42,420 --> 00:42:45,420
So you know which sample belongs to cluster one.

791
00:42:45,420 --> 00:42:47,420
You know which sample belongs to cluster two.

792
00:42:47,420 --> 00:42:52,420
So the algorithm is going to give you some membership for every sample you provided as input.

793
00:42:52,420 --> 00:42:57,420
So this is basically the overall structure of the clustering method.

794
00:42:57,420 --> 00:42:59,420
Any clustering method, right.

795
00:42:59,420 --> 00:43:02,420
So the algorithm is different, but the input-output is always the same.

796
00:43:02,420 --> 00:43:04,420
Okay.

797
00:43:04,420 --> 00:43:06,420
Questions?

798
00:43:06,420 --> 00:43:07,420
Okay.

799
00:43:07,420 --> 00:43:08,420
So far, so good.

800
00:43:08,420 --> 00:43:09,420
All right.

801
00:43:09,420 --> 00:43:20,420
So before I introduce the exact algorithms, one thing we haven't really talked about is that we are saying the clustering always groups the data with a similar nature.

802
00:43:20,420 --> 00:43:21,420
Right.

803
00:43:21,420 --> 00:43:24,420
But how can we measure the similarity?

804
00:43:24,420 --> 00:43:30,420
Just like the way we do the flower image, we are saying that we measure the similarity by comparing the intensity.

805
00:43:30,420 --> 00:43:31,420
So that's our measure.

806
00:43:31,420 --> 00:43:32,420
That's our metric.

807
00:43:32,420 --> 00:43:38,420
But in general case, how can we define similarity for a clustering algorithm?

808
00:43:38,420 --> 00:43:39,420
Okay.

809
00:43:39,420 --> 00:43:42,420
So we have to make sure this is clear first.

810
00:43:42,420 --> 00:43:46,420
Because usually we use something called distance to measure the similarity.

811
00:43:46,420 --> 00:43:51,420
So distance actually quantify the dissimilarity, but the opposite of dissimilarity.

812
00:43:51,420 --> 00:43:52,420
Right.

813
00:43:52,420 --> 00:43:56,420
And there are many ways to define distance, but let's just make it simple.

814
00:43:56,420 --> 00:44:00,420
Let's just come up with a convention of the notation first.

815
00:44:00,420 --> 00:44:01,420
Right.

816
00:44:01,420 --> 00:44:05,420
So throughout my part, I'm going to always use a vector.

817
00:44:05,420 --> 00:44:13,420
So here, x1, x2, xn, which I use the bold letter, I represent, the bold letter represent a vector.

818
00:44:13,420 --> 00:44:14,420
Okay.

819
00:44:14,420 --> 00:44:18,420
So every x, x1, x2, every x represent a data sample.

820
00:44:18,420 --> 00:44:23,420
So for clustering, you will be given a set of data, not only one, but a set of data.

821
00:44:23,420 --> 00:44:29,420
So I will represent the n samples as n different vectors from x1 to xn.

822
00:44:29,420 --> 00:44:30,420
Right.

823
00:44:30,420 --> 00:44:32,420
So these are the data we're going to cluster.

824
00:44:32,420 --> 00:44:33,420
All right.

825
00:44:33,420 --> 00:44:36,420
So of course, every sample can be high dimensional.

826
00:44:36,420 --> 00:44:37,420
Right.

827
00:44:37,420 --> 00:44:38,420
It can be d dimensional.

828
00:44:38,420 --> 00:44:41,420
So we're talking about not a single number, but a vector.

829
00:44:41,420 --> 00:44:42,420
All right.

830
00:44:42,420 --> 00:44:46,420
So my convention is that I use bold letter as the vector.

831
00:44:46,420 --> 00:44:47,420
Right.

832
00:44:47,420 --> 00:44:49,420
Normal letter is just a number scalar.

833
00:44:49,420 --> 00:44:50,420
Okay.

834
00:44:50,420 --> 00:44:52,420
So I hope everybody's comfortable with this notation.

835
00:44:52,420 --> 00:44:53,420
Right.

836
00:44:53,420 --> 00:44:54,420
Anyone doesn't know what I'm talking about.

837
00:44:54,420 --> 00:44:55,420
Right.

838
00:44:55,420 --> 00:44:56,420
I assume everybody knows what is a vector.

839
00:44:56,420 --> 00:44:57,420
Right.

840
00:44:57,420 --> 00:44:58,420
I don't have to repeat what is a vector.

841
00:44:58,420 --> 00:44:59,420
Okay.

842
00:44:59,420 --> 00:45:00,420
Cool.

843
00:45:00,420 --> 00:45:02,420
So a vector is nothing but a column.

844
00:45:02,420 --> 00:45:07,420
We usually use a column vector to denote, let's say, a d dimensional vector.

845
00:45:07,420 --> 00:45:08,420
Right.

846
00:45:08,420 --> 00:45:11,420
So let's say a distance metric is actually a function.

847
00:45:11,420 --> 00:45:12,420
Okay.

848
00:45:12,420 --> 00:45:16,420
Distance is a function which have to take two vector.

849
00:45:16,420 --> 00:45:17,420
Okay.

850
00:45:17,420 --> 00:45:22,420
This d function take two vector, two points as the input.

851
00:45:22,420 --> 00:45:23,420
Always.

852
00:45:23,420 --> 00:45:25,420
You cannot measure distance only one point.

853
00:45:25,420 --> 00:45:26,420
It must be two points.

854
00:45:26,420 --> 00:45:27,420
Right.

855
00:45:27,420 --> 00:45:29,420
And the output will be a real scalar.

856
00:45:29,420 --> 00:45:33,420
The output is a single number that measure the distance.

857
00:45:33,420 --> 00:45:37,420
So this is actually the distance function we're talking about.

858
00:45:37,420 --> 00:45:38,420
All right.

859
00:45:38,420 --> 00:45:42,420
So I use d to denote the distance metric for this part.

860
00:45:42,420 --> 00:45:43,420
Okay.

861
00:45:43,420 --> 00:45:49,420
Now, so what are the function of d that can be qualified for distance metric?

862
00:45:49,420 --> 00:45:54,420
I mean, there are many function can actually map two vector into one value.

863
00:45:54,420 --> 00:45:55,420
Right.

864
00:45:55,420 --> 00:45:57,420
But not every function is qualified as a distance.

865
00:45:57,420 --> 00:46:04,420
So to qualify as a distance metric, they must satisfy the following three properties.

866
00:46:04,420 --> 00:46:05,420
Okay.

867
00:46:05,420 --> 00:46:07,420
So what are the three properties we're talking about?

868
00:46:07,420 --> 00:46:17,420
So first property is that if you have a distance, let's say xi and j, larger than zero.

869
00:46:17,420 --> 00:46:18,420
Okay.

870
00:46:18,420 --> 00:46:23,420
So the distance function must output something that is always non-negative.

871
00:46:23,420 --> 00:46:26,420
So we don't have negative distance in this course.

872
00:46:26,420 --> 00:46:28,420
So all the distance must be non-negative.

873
00:46:28,420 --> 00:46:29,420
All right.

874
00:46:30,420 --> 00:46:34,420
And the distance equal to zero only if the two points are the same.

875
00:46:34,420 --> 00:46:35,420
All right.

876
00:46:35,420 --> 00:46:38,420
So if you have two different points, you cannot have zero distance.

877
00:46:38,420 --> 00:46:40,420
This is true for every distance metric.

878
00:46:40,420 --> 00:46:41,420
All right.

879
00:46:41,420 --> 00:46:43,420
That's something we call the non-negative property.

880
00:46:43,420 --> 00:46:45,420
So you can imagine that you have two points in the space.

881
00:46:45,420 --> 00:46:46,420
Right.

882
00:46:46,420 --> 00:46:47,420
You measure distance.

883
00:46:47,420 --> 00:46:48,420
How can you get a negative distance?

884
00:46:48,420 --> 00:46:49,420
No way.

885
00:46:49,420 --> 00:46:50,420
Right.

886
00:46:50,420 --> 00:46:51,420
If they are the same, you got zero.

887
00:46:51,420 --> 00:46:52,420
All right.

888
00:46:52,420 --> 00:46:54,420
So this is number one.

889
00:46:54,420 --> 00:46:56,420
So we call it the non-negative property.

890
00:46:56,420 --> 00:46:57,420
All right.

891
00:46:57,420 --> 00:46:58,420
So number two.

892
00:46:58,420 --> 00:47:00,420
This is called triangular inequality.

893
00:47:00,420 --> 00:47:01,420
Right.

894
00:47:01,420 --> 00:47:02,420
Some of you may know it.

895
00:47:02,420 --> 00:47:04,420
So we're talking about three points right now.

896
00:47:04,420 --> 00:47:05,420
Right.

897
00:47:05,420 --> 00:47:09,420
So you have point xl, point xi, and point xj.

898
00:47:09,420 --> 00:47:16,420
So what the property says is that the distance between ij plus the distance between l and

899
00:47:16,420 --> 00:47:22,420
j, summed together, must be larger or equal to the distance that is directly measured

900
00:47:22,420 --> 00:47:24,420
between i and l.

901
00:47:24,420 --> 00:47:25,420
Right.

902
00:47:25,420 --> 00:47:30,420
So you have to have this kind of triangle kind of inequality to qualify as a distance

903
00:47:30,420 --> 00:47:31,420
metric.

904
00:47:31,420 --> 00:47:34,420
I hope this is also easy to understand.

905
00:47:34,420 --> 00:47:40,420
But this is a condition to say that, OK, this distance metric can be used.

906
00:47:40,420 --> 00:47:41,420
OK.

907
00:47:41,420 --> 00:47:43,420
So this is second one, triangle inequality.

908
00:47:43,420 --> 00:47:46,420
So, I mean, also easy to understand.

909
00:47:46,420 --> 00:47:47,420
Right.

910
00:47:47,420 --> 00:47:52,420
Because, you know, the only equal case is that lji is in the same line, then they will

911
00:47:52,420 --> 00:47:53,420
be equal.

912
00:47:53,420 --> 00:47:54,420
Right.

913
00:47:54,420 --> 00:47:56,420
They will be equal to right-hand side.

914
00:47:56,420 --> 00:47:57,420
OK.

915
00:47:57,420 --> 00:48:01,420
So last but not least, the distance metric must be due-directional.

916
00:48:01,420 --> 00:48:03,420
So in other words, symmetric.

917
00:48:03,420 --> 00:48:04,420
Right.

918
00:48:04,420 --> 00:48:08,420
So distance between x and, sorry, i and j must be equal to the distance between j and

919
00:48:08,420 --> 00:48:09,420
i.

920
00:48:09,420 --> 00:48:10,420
Right.

921
00:48:10,420 --> 00:48:11,420
Which is also very easy to understand.

922
00:48:11,420 --> 00:48:16,420
So these three properties is always a condition to qualify a distance metric.

923
00:48:16,420 --> 00:48:17,420
OK.

924
00:48:17,420 --> 00:48:21,420
Not arbitrary function that mapping from two vector to one scalar can be qualified.

925
00:48:21,420 --> 00:48:22,420
Right.

926
00:48:22,420 --> 00:48:25,420
They can be satisfied of the following three properties.

927
00:48:25,420 --> 00:48:26,420
OK.

928
00:48:26,420 --> 00:48:27,420
Make sense?

929
00:48:27,420 --> 00:48:29,420
So far so good.

930
00:48:29,420 --> 00:48:30,420
OK.

931
00:48:30,420 --> 00:48:31,420
Simple math.

932
00:48:31,420 --> 00:48:32,420
OK.

933
00:48:32,420 --> 00:48:33,420
I hope this is OK.

934
00:48:33,420 --> 00:48:34,420
I mean, not very, very.

935
00:48:34,420 --> 00:48:38,420
I tried to minimize the math for this course, by the way, but still have some math.

936
00:48:38,420 --> 00:48:40,420
I hope this is fine with you.

937
00:48:40,420 --> 00:48:45,420
But, OK, I'll give you some example of the distance that we could use in this course,

938
00:48:45,420 --> 00:48:48,420
which is also the most commonly used distance metric.

939
00:48:48,420 --> 00:48:49,420
Right.

940
00:48:49,420 --> 00:48:51,420
This one is called Euclidean distance.

941
00:48:51,420 --> 00:48:52,420
Right.

942
00:48:52,420 --> 00:48:58,420
It's actually the most commonly used distance, which is nothing but L2 long distance.

943
00:48:58,420 --> 00:48:59,420
Right.

944
00:48:59,420 --> 00:49:03,420
So numerically, it means I'm going to measure the difference.

945
00:49:03,420 --> 00:49:09,420
I take the square root of the, what is that?

946
00:49:09,420 --> 00:49:13,420
I take the square root of the distance squared.

947
00:49:13,420 --> 00:49:14,420
Right.

948
00:49:14,420 --> 00:49:15,420
Something like that.

949
00:49:15,420 --> 00:49:18,420
X minus xi minus xj.

950
00:49:18,420 --> 00:49:21,420
And then you take the square and then sum them up.

951
00:49:21,420 --> 00:49:22,420
I take the square root.

952
00:49:22,420 --> 00:49:23,420
Right.

953
00:49:23,420 --> 00:49:24,420
So it's something like a straight line.

954
00:49:24,420 --> 00:49:25,420
Right.

955
00:49:25,420 --> 00:49:26,420
I have point x and y.

956
00:49:26,420 --> 00:49:29,420
I want to measure the straight line distance between the two.

957
00:49:29,420 --> 00:49:32,420
So this is called Euclidean distance.

958
00:49:32,420 --> 00:49:33,420
Right.

959
00:49:33,420 --> 00:49:34,420
So you can check.

960
00:49:34,420 --> 00:49:39,420
It definitely satisfied the three conditions, the three properties we were talking about.

961
00:49:39,420 --> 00:49:42,420
But this is not the only distance we can use.

962
00:49:42,420 --> 00:49:43,420
Right.

963
00:49:43,420 --> 00:49:45,420
This is the square root distance, which we can also use.

964
00:49:45,420 --> 00:49:46,420
And they are also qualified.

965
00:49:46,420 --> 00:49:49,420
The second one is called Manhattan distance.

966
00:49:49,420 --> 00:49:54,420
The only difference is that they are not measuring the square.

967
00:49:54,420 --> 00:49:58,420
They are taking the absolute value between the two.

968
00:49:58,420 --> 00:50:02,420
And of course, you don't have to take the square root because here there's no square.

969
00:50:02,420 --> 00:50:05,420
And usually we call this L1 norm.

970
00:50:05,420 --> 00:50:10,420
If you want to visualize that, the distance actually is something like you take the horizontal

971
00:50:10,420 --> 00:50:12,420
distance plus the vertical distance.

972
00:50:12,420 --> 00:50:13,420
Right.

973
00:50:13,420 --> 00:50:17,420
You don't take the straight line, but you project the straight line into the horizontal

974
00:50:17,420 --> 00:50:18,420
and the vertical axis.

975
00:50:18,420 --> 00:50:19,420
You sum them up.

976
00:50:19,420 --> 00:50:20,420
Right.

977
00:50:20,420 --> 00:50:22,420
So this is called Manhattan distance.

978
00:50:22,420 --> 00:50:23,420
Right.

979
00:50:23,420 --> 00:50:26,420
It still satisfied the three properties, by the way.

980
00:50:26,420 --> 00:50:28,420
You can verify if you don't believe that.

981
00:50:28,420 --> 00:50:30,420
But this is also sometimes quite useful.

982
00:50:30,420 --> 00:50:31,420
Right.

983
00:50:31,420 --> 00:50:35,420
Especially for those image processing, you know, imposed sparsity.

984
00:50:35,420 --> 00:50:36,420
Right.

985
00:50:36,420 --> 00:50:37,420
That's why people love it.

986
00:50:37,420 --> 00:50:38,420
Okay.

987
00:50:38,420 --> 00:50:39,420
Manhattan distance.

988
00:50:39,420 --> 00:50:44,420
The last but not least, some people also use so-called infinity distance, or maybe people

989
00:50:44,420 --> 00:50:45,420
call it the sub-distance.

990
00:50:45,420 --> 00:50:46,420
Right.

991
00:50:46,420 --> 00:50:51,420
So here, it's only care about the maximum distance in certain dimension.

992
00:50:51,420 --> 00:50:52,420
All right.

993
00:50:52,420 --> 00:50:57,420
So it's like I have distance decomposing to x and y, or horizontal axis, but I only care

994
00:50:57,420 --> 00:51:00,420
about the dimension that give me the maximum distance.

995
00:51:00,420 --> 00:51:02,420
Others, I forget about it.

996
00:51:02,420 --> 00:51:03,420
Right.

997
00:51:03,420 --> 00:51:05,420
So it's something like, you know, for example here.

998
00:51:05,420 --> 00:51:06,420
Right.

999
00:51:06,420 --> 00:51:07,420
So this is the two, this is the three.

1000
00:51:07,420 --> 00:51:10,420
My maximum distance will only measure the three.

1001
00:51:10,420 --> 00:51:12,420
The two, I just forget about it.

1002
00:51:12,420 --> 00:51:13,420
Right.

1003
00:51:13,420 --> 00:51:14,420
So this is where the infinity come from.

1004
00:51:14,420 --> 00:51:15,420
Right.

1005
00:51:15,420 --> 00:51:18,420
I take the max, all of all dimensions.

1006
00:51:18,420 --> 00:51:19,420
Okay.

1007
00:51:19,420 --> 00:51:20,420
So, sunrise.

1008
00:51:20,420 --> 00:51:21,420
Right.

1009
00:51:21,420 --> 00:51:25,420
You can actually use this example to compare the three.

1010
00:51:25,420 --> 00:51:28,420
Euclidean distance just measure the straight line between the two points.

1011
00:51:28,420 --> 00:51:29,420
This is Euclidean.

1012
00:51:29,420 --> 00:51:33,420
Manhattan is the horizontal, sorry, the vertical plus horizontal.

1013
00:51:33,420 --> 00:51:36,420
So it's going to be two plus four.

1014
00:51:36,420 --> 00:51:37,420
Right.

1015
00:51:37,420 --> 00:51:41,420
And if you take the straight line, it's going to be four squared plus two squared, take

1016
00:51:41,420 --> 00:51:42,420
the square root.

1017
00:51:42,420 --> 00:51:47,420
So, sub-distance will be the maximum out of two and four.

1018
00:51:47,420 --> 00:51:52,420
So that's kind of the one single example if you want to memorize to differentiate the

1019
00:51:52,420 --> 00:51:53,420
three.

1020
00:51:53,420 --> 00:51:54,420
Okay.

1021
00:51:54,420 --> 00:51:58,420
I hope it's straightforward for this kind of comparison.

1022
00:51:58,420 --> 00:52:04,420
If I haven't said that, right, for this class, if I don't give you any specification in your

1023
00:52:04,420 --> 00:52:06,420
question, let's use Euclidean distance by default.

1024
00:52:06,420 --> 00:52:07,420
All right.

1025
00:52:07,420 --> 00:52:10,420
So if I don't say anything, you can assume that the distance is Euclidean.

1026
00:52:10,420 --> 00:52:16,420
But if I tell you in the question, say, hey, please do K-means by using Manhattan distance.

1027
00:52:16,420 --> 00:52:18,420
They have to do Manhattan distance.

1028
00:52:18,420 --> 00:52:19,420
All right.

1029
00:52:19,420 --> 00:52:22,420
So if I don't say anything, let's use Euclidean by default.

1030
00:52:22,420 --> 00:52:23,420
Okay.

1031
00:52:23,420 --> 00:52:25,420
Because it's easy.

1032
00:52:25,420 --> 00:52:26,420
Right.

1033
00:52:26,420 --> 00:52:27,420
Okay.

1034
00:52:27,420 --> 00:52:28,420
Okay.

1035
00:52:28,420 --> 00:52:30,420
So this is all about distance.

1036
00:52:30,420 --> 00:52:36,420
So now, after we understand the measure of similarity, we can now introduce the actual

1037
00:52:36,420 --> 00:52:39,420
algorithms to do clustering.

1038
00:52:39,420 --> 00:52:40,420
All right.

1039
00:52:40,420 --> 00:52:42,420
So we're going to look at two kinds of algorithms.

1040
00:52:42,420 --> 00:52:44,420
One is called partition algorithm.

1041
00:52:44,420 --> 00:52:46,420
One is called hierarchical algorithm.

1042
00:52:46,420 --> 00:52:47,420
All right.

1043
00:52:47,420 --> 00:52:50,420
So I use this kind of Simpson example to illustrate.

1044
00:52:50,420 --> 00:52:56,420
So partition is like you just want to exclusively group the sample into, say, two groups or

1045
00:52:56,420 --> 00:52:57,420
three groups.

1046
00:52:57,420 --> 00:52:58,420
Right.

1047
00:52:58,420 --> 00:52:59,420
This is called partition.

1048
00:52:59,420 --> 00:53:05,420
Hierarchical is that you have some nested groups from causal level to the final.

1049
00:53:05,420 --> 00:53:08,420
So it's a different way to separate the samples.

1050
00:53:08,420 --> 00:53:11,420
You do progressive way or you do like partition way.

1051
00:53:11,420 --> 00:53:16,420
So for each of the type, we only learn one method for that.

1052
00:53:16,420 --> 00:53:17,420
Okay.

1053
00:53:17,420 --> 00:53:18,420
So let's just illustrate the difference.

1054
00:53:18,420 --> 00:53:24,420
So partition-based method, the key idea is that we have to group the samples into, okay,

1055
00:53:24,420 --> 00:53:27,420
this is important, non-overlapping clusters.

1056
00:53:27,420 --> 00:53:33,420
In other words, if it's a partition-based method, such as k-means, your cluster cannot

1057
00:53:33,420 --> 00:53:35,420
have overlap.

1058
00:53:35,420 --> 00:53:38,420
So they should be mutually exclusive to each other.

1059
00:53:38,420 --> 00:53:39,420
All right.

1060
00:53:39,420 --> 00:53:42,420
And this is literally what the partition means.

1061
00:53:42,420 --> 00:53:43,420
Right.

1062
00:53:43,420 --> 00:53:50,420
And also, every data sample will be always assigned to exactly one cluster, no duplicate.

1063
00:53:51,420 --> 00:53:56,420
You cannot say like my point, this point belong to both cluster one and cluster two.

1064
00:53:56,420 --> 00:53:58,420
It must be either one or two.

1065
00:53:58,420 --> 00:54:01,420
So every sample will be exactly in one cluster.

1066
00:54:01,420 --> 00:54:04,420
So these are the important thing to qualify partition algorithm.

1067
00:54:04,420 --> 00:54:05,420
All right.

1068
00:54:05,420 --> 00:54:08,420
So you can imagine that in the space, you have so many points.

1069
00:54:08,420 --> 00:54:13,420
I just need to make them somehow grouped such that every sample belong to exactly one cluster.

1070
00:54:13,420 --> 00:54:14,420
Right.

1071
00:54:14,420 --> 00:54:15,420
So this is partition.

1072
00:54:15,420 --> 00:54:16,420
Okay.

1073
00:54:16,420 --> 00:54:18,420
Now hierarchical is different.

1074
00:54:18,420 --> 00:54:24,420
Hierarchical is that, okay, I will progressively group the points.

1075
00:54:24,420 --> 00:54:28,420
Maybe in the first trial, I say, well, two and three are similar.

1076
00:54:28,420 --> 00:54:31,420
I group them first, but I leave one and four.

1077
00:54:31,420 --> 00:54:34,420
But later I find that, well, I don't like three cluster.

1078
00:54:34,420 --> 00:54:35,420
I want to do two clusters.

1079
00:54:35,420 --> 00:54:38,420
I will further group four together with two and three.

1080
00:54:38,420 --> 00:54:39,420
Right.

1081
00:54:39,420 --> 00:54:45,420
So the grouping will be progressively kind of merging together, but not in one shot.

1082
00:54:45,420 --> 00:54:46,420
Right.

1083
00:54:46,420 --> 00:54:53,420
So in the end, you will have a set of nested cluster organized in your hierarchical tree.

1084
00:54:53,420 --> 00:54:54,420
Okay.

1085
00:54:54,420 --> 00:55:01,420
You don't have this for partition, but for hierarchical, this is what you will have as the clustering result, membership.

1086
00:55:01,420 --> 00:55:02,420
All right.

1087
00:55:02,420 --> 00:55:03,420
All right.

1088
00:55:03,420 --> 00:55:04,420
So this is the difference.

1089
00:55:04,420 --> 00:55:09,420
Now, like I mentioned, right, for every type of algorithm, we will introduce one method.

1090
00:55:09,420 --> 00:55:10,420
Okay.

1091
00:55:10,420 --> 00:55:14,420
You only need to know the representative method for each type.

1092
00:55:14,420 --> 00:55:18,420
So for partition-based method, we're going to learn k-means.

1093
00:55:18,420 --> 00:55:23,420
For hierarchical method, we're going to learn the HAC, right, Hierarchical Agglomerative Algorithm.

1094
00:55:23,420 --> 00:55:29,420
So, but for those two, you need to be very clear about the algorithms, how to run them,

1095
00:55:29,420 --> 00:55:33,420
what could be wrong, and especially for k-means, when to stop.

1096
00:55:33,420 --> 00:55:34,420
All right.

1097
00:55:34,420 --> 00:55:35,420
Because it's an iterative method.

1098
00:55:35,420 --> 00:55:36,420
Okay.

1099
00:55:36,420 --> 00:55:39,420
So let's start by first talking about k-means.

1100
00:55:39,420 --> 00:55:40,420
Right.

1101
00:55:40,420 --> 00:55:42,420
I believe some of you may already know k-means.

1102
00:55:42,420 --> 00:55:43,420
Right.

1103
00:55:43,420 --> 00:55:45,420
And how many of you have already know what is k-means?

1104
00:55:45,420 --> 00:55:46,420
Okay.

1105
00:55:46,420 --> 00:55:47,420
You have some.

1106
00:55:47,420 --> 00:55:48,420
We have some already.

1107
00:55:48,420 --> 00:55:50,420
Maybe at least heard of it, right?

1108
00:55:50,420 --> 00:55:51,420
Any of you?

1109
00:55:51,420 --> 00:55:52,420
Okay.

1110
00:55:52,420 --> 00:55:53,420
Any of you have never heard of k-means?

1111
00:55:53,420 --> 00:55:55,420
All of you heard of it, right?

1112
00:55:55,420 --> 00:55:56,420
It's a very common algorithm.

1113
00:55:56,420 --> 00:55:57,420
Like, people love it.

1114
00:55:57,420 --> 00:55:58,420
It's simple.

1115
00:55:58,420 --> 00:56:02,420
But you better know the details, right?

1116
00:56:02,420 --> 00:56:04,420
Because sometimes people can get it wrong for k-means.

1117
00:56:04,420 --> 00:56:07,420
And k-means is a very risky method.

1118
00:56:07,420 --> 00:56:09,420
You can easily get it wrong.

1119
00:56:09,420 --> 00:56:12,420
So we use the same convention, right?

1120
00:56:12,420 --> 00:56:18,420
Data samples themselves are represented as a list of vectors, x1, x2, all the way to

1121
00:56:18,420 --> 00:56:19,420
xn.

1122
00:56:19,420 --> 00:56:22,420
And the question will specify a distance metric.

1123
00:56:22,420 --> 00:56:24,420
So you will be given some d function.

1124
00:56:24,420 --> 00:56:27,420
But by default, it's going to be Euclidean distance.

1125
00:56:27,420 --> 00:56:28,420
Okay.

1126
00:56:28,420 --> 00:56:37,420
So the goal of the k-means will be trying to distribute or split the data sample into

1127
00:56:37,420 --> 00:56:38,420
k clusters.

1128
00:56:38,420 --> 00:56:42,420
Okay, by the way, the k here is the parameters of the k-means.

1129
00:56:42,420 --> 00:56:46,420
So in other words, you have to specify what is the k, right?

1130
00:56:46,420 --> 00:56:47,420
Number of clusters.

1131
00:56:47,420 --> 00:56:49,420
So you cannot learn this k.

1132
00:56:49,420 --> 00:56:51,420
The k is not part of the learning.

1133
00:56:51,420 --> 00:56:52,420
All right?

1134
00:56:52,420 --> 00:56:58,420
So if you do so, each cluster will have a bunch of samples.

1135
00:56:58,420 --> 00:57:05,420
And it will be represented using a d-dimensional vector, mu k, which is called the centroid.

1136
00:57:06,420 --> 00:57:09,420
So it's like every sample will be represented as a vector.

1137
00:57:09,420 --> 00:57:10,420
All right?

1138
00:57:10,420 --> 00:57:14,420
But the cluster also can be represented as a vector.

1139
00:57:14,420 --> 00:57:17,420
But the vector is called the centroid of the cluster.

1140
00:57:17,420 --> 00:57:19,420
The dimension is the same.

1141
00:57:19,420 --> 00:57:20,420
All right?

1142
00:57:20,420 --> 00:57:25,420
So xk are the samples, mu are the centroid of the cluster.

1143
00:57:25,420 --> 00:57:31,420
So you will have mu1, mu2, all the way to mu k, because you have k cluster.

1144
00:57:31,420 --> 00:57:33,420
Every cluster will have a centroid.

1145
00:57:33,420 --> 00:57:34,420
All right?

1146
00:57:34,420 --> 00:57:36,420
So then what is the objective, right?

1147
00:57:36,420 --> 00:57:39,420
So you have to partition the data in any way, in arbitrary way.

1148
00:57:39,420 --> 00:57:42,420
But not every partition is something we like.

1149
00:57:42,420 --> 00:57:49,420
For k-means, the objective of class array is to make sure the summation of the distance,

1150
00:57:49,420 --> 00:57:55,420
all the distance, between the sample xi to the corresponding centroid mu k,

1151
00:57:55,420 --> 00:57:59,420
sum them together, must be minimized.

1152
00:57:59,420 --> 00:58:04,420
In other words, I'm trying to give you a partition such that if I measure the distance

1153
00:58:04,420 --> 00:58:10,420
between the sample to the centroid, I sum them up together, it must be minimized.

1154
00:58:10,420 --> 00:58:14,420
If I minimize the summation of distance, I'm done with my k-means.

1155
00:58:14,420 --> 00:58:15,420
Right?

1156
00:58:15,420 --> 00:58:18,420
So this is what k-means try to achieve.

1157
00:58:18,420 --> 00:58:19,420
Okay?

1158
00:58:19,420 --> 00:58:20,420
So we'll get back to it.

1159
00:58:20,420 --> 00:58:23,420
So this is just some kind of rough description of the objective.

1160
00:58:23,420 --> 00:58:25,420
Right.

1161
00:58:25,420 --> 00:58:27,420
But, you know, have a say there.

1162
00:58:27,420 --> 00:58:29,420
So k-means will need a distance metric.

1163
00:58:29,420 --> 00:58:32,420
It's going to be Euclidean distance by default.

1164
00:58:32,420 --> 00:58:35,420
Every cluster will be represented as a mu k,

1165
00:58:35,420 --> 00:58:40,420
which is actually the average of all the samples belonging to that cluster.

1166
00:58:40,420 --> 00:58:41,420
Right?

1167
00:58:41,420 --> 00:58:44,420
So geometrically, it's going to be the center of the cluster.

1168
00:58:44,420 --> 00:58:49,420
Numerically, you just take the average of all the samples belonging to that cluster.

1169
00:58:49,420 --> 00:58:51,420
That's going to be your mu k.

1170
00:58:51,420 --> 00:58:52,420
All right?

1171
00:58:52,420 --> 00:58:56,420
So you can compute the mu k if you know the samples belonging to it.

1172
00:58:56,420 --> 00:58:57,420
Right?

1173
00:58:57,420 --> 00:59:00,420
And your idea, your job is to split the data points

1174
00:59:00,420 --> 00:59:04,420
such that each data point will belong to exactly one cluster.

1175
00:59:04,420 --> 00:59:05,420
Only one.

1176
00:59:05,420 --> 00:59:07,420
Cannot be two, but only one.

1177
00:59:07,420 --> 00:59:08,420
All right?

1178
00:59:08,420 --> 00:59:11,420
So here is the actual algorithm.

1179
00:59:11,420 --> 00:59:12,420
Right?

1180
00:59:12,420 --> 00:59:14,420
Actually, k-means is very simple.

1181
00:59:14,420 --> 00:59:18,420
As you can see, just a few lines can explain what k-means is.

1182
00:59:18,420 --> 00:59:21,420
But let's go through them very slowly.

1183
00:59:21,420 --> 00:59:26,420
So before even running the k-means, there are two things you have to determine.

1184
00:59:26,420 --> 00:59:28,420
First one is the parameter k.

1185
00:59:28,420 --> 00:59:29,420
All right?

1186
00:59:29,420 --> 00:59:33,420
So you have to pick the k or maybe a question tell you what is k.

1187
00:59:33,420 --> 00:59:37,420
So this k is something you have to assume or you have to know beforehand.

1188
00:59:37,420 --> 00:59:38,420
Right?

1189
00:59:38,420 --> 00:59:39,420
You cannot change.

1190
00:59:39,420 --> 00:59:40,420
You cannot learn.

1191
00:59:40,420 --> 00:59:41,420
That's something fixed.

1192
00:59:41,420 --> 00:59:45,420
So after you know the k as the parameter of the algorithm,

1193
00:59:45,420 --> 00:59:48,420
you have to do something we call initialization.

1194
00:59:48,420 --> 00:59:49,420
Okay?

1195
00:59:49,420 --> 00:59:55,420
So you need to give some initial centroid which will present the clusters.

1196
00:59:55,420 --> 00:59:58,420
But how to pick the centroid is completely random.

1197
00:59:58,420 --> 01:00:01,420
I mean, you can just randomly pick some points as the center.

1198
01:00:01,420 --> 01:00:04,420
When we say initialization, it doesn't matter how good or bad.

1199
01:00:04,420 --> 01:00:07,420
Because anyway, we're going to update it later.

1200
01:00:07,420 --> 01:00:10,420
But you need to have this starting point.

1201
01:00:10,420 --> 01:00:12,420
Otherwise, you cannot proceed with the k-means.

1202
01:00:12,420 --> 01:00:13,420
Okay?

1203
01:00:13,420 --> 01:00:16,420
So in practice, it's either given by you.

1204
01:00:16,420 --> 01:00:21,420
So for example, in the exam, I could give you the centroid which I suggested.

1205
01:00:21,420 --> 01:00:24,420
So you use that as the initialization.

1206
01:00:24,420 --> 01:00:25,420
Right.

1207
01:00:25,420 --> 01:00:30,420
Or if I don't say anything, you can randomly put some points as the initialization.

1208
01:00:30,420 --> 01:00:32,420
But you have to do it.

1209
01:00:32,420 --> 01:00:33,420
All right?

1210
01:00:33,420 --> 01:00:40,420
And after you download the initialization, the main body of k-means just has two steps.

1211
01:00:40,420 --> 01:00:41,420
Okay?

1212
01:00:41,420 --> 01:00:42,420
Very simple.

1213
01:00:42,420 --> 01:00:49,420
You have to keep running the two steps many, many times until the stopping criteria is actually triggered.

1214
01:00:49,420 --> 01:00:51,420
So this is actually looping.

1215
01:00:51,420 --> 01:00:53,420
Consider a while loop.

1216
01:00:53,420 --> 01:01:00,420
You are doing a looping through the two steps until you reach some condition that you can stop.

1217
01:01:00,420 --> 01:01:02,420
So this is called iterative algorithm.

1218
01:01:02,420 --> 01:01:06,420
You are doing multiple iterations until stopping.

1219
01:01:06,420 --> 01:01:07,420
Right.

1220
01:01:07,420 --> 01:01:09,420
And then what are the two steps we have here?

1221
01:01:09,420 --> 01:01:15,420
So the first step is called like a membership assignment.

1222
01:01:15,420 --> 01:01:19,420
So for the step one, you need to fix the centroid.

1223
01:01:19,420 --> 01:01:20,420
Fix the mu k.

1224
01:01:20,420 --> 01:01:24,420
You don't change it because you just initialize the mu k.

1225
01:01:24,420 --> 01:01:31,420
Keep it, but now assign every data point to its closest cluster center.

1226
01:01:32,420 --> 01:01:40,420
So in other words, for every sample, I need to measure the distance to mu 1, the distance to mu 2, the distance to mu 3,

1227
01:01:40,420 --> 01:01:43,420
and I compare which one gives me the minimum distance.

1228
01:01:43,420 --> 01:01:48,420
The minimum distance will become the membership for this sample.

1229
01:01:48,420 --> 01:01:55,420
So for example, if my x is the closest to my mu 2, my x will be assigned to the second cluster.

1230
01:01:55,420 --> 01:01:58,420
So this is actually the membership assignment.

1231
01:01:58,420 --> 01:02:01,420
I always belong depending on the distance metric.

1232
01:02:01,420 --> 01:02:09,420
So it's like finding the mu k such that the distance between my sample and the corresponding mu k is minimized.

1233
01:02:09,420 --> 01:02:10,420
Right.

1234
01:02:10,420 --> 01:02:17,420
So by the end of step one, every sample will have a unique assignment to one of the cluster.

1235
01:02:17,420 --> 01:02:19,420
And the exact one.

1236
01:02:19,420 --> 01:02:20,420
So you cannot have two.

1237
01:02:20,420 --> 01:02:22,420
Exact one assignment.

1238
01:02:22,420 --> 01:02:23,420
All right.

1239
01:02:23,420 --> 01:02:26,420
So this is the membership assignment.

1240
01:02:26,420 --> 01:02:27,420
Any questions?

1241
01:02:27,420 --> 01:02:29,420
Make sense?

1242
01:02:29,420 --> 01:02:31,420
Or you think it's too easy?

1243
01:02:31,420 --> 01:02:34,420
If I think it's too easy, I can run faster.

1244
01:02:34,420 --> 01:02:35,420
All right.

1245
01:02:35,420 --> 01:02:36,420
Okay.

1246
01:02:36,420 --> 01:02:38,420
So then after you finish step one, you should do step two.

1247
01:02:38,420 --> 01:02:40,420
Step two is also three, four.

1248
01:02:40,420 --> 01:02:42,420
Now you have all the membership.

1249
01:02:42,420 --> 01:02:46,420
You know what other points belong to cluster one, what other points belong to cluster two.

1250
01:02:46,420 --> 01:02:51,420
So now is the time to fix that membership, then update the mu.

1251
01:02:51,420 --> 01:02:54,420
Remember the mu are just randomly assigned, so it can be run.

1252
01:02:54,420 --> 01:03:00,420
So now I have to use my membership and recalculate what are the mus.

1253
01:03:00,420 --> 01:03:01,420
And how to do it?

1254
01:03:01,420 --> 01:03:03,420
You just take average.

1255
01:03:03,420 --> 01:03:08,420
You just find all the points belong to cluster one, take average, and update my new one.

1256
01:03:08,420 --> 01:03:16,420
You do the same thing for cluster two, cluster three, and until you update every mu for basically all the clusters.

1257
01:03:16,420 --> 01:03:18,420
Then you are done with step two.

1258
01:03:18,420 --> 01:03:21,420
So step two is called centroid update.

1259
01:03:21,420 --> 01:03:25,420
Update the centroid one more time using your membership.

1260
01:03:25,420 --> 01:03:28,420
And then you go back to step one.

1261
01:03:28,420 --> 01:03:30,420
Now your centroids are changed.

1262
01:03:30,420 --> 01:03:34,420
You have reassigned the sample according to the new distance.

1263
01:03:34,420 --> 01:03:39,420
Remember, your mu key are changed, so your distance will be also different.

1264
01:03:39,420 --> 01:03:44,420
Then maybe some of the points will be changing its membership.

1265
01:03:44,420 --> 01:03:49,420
Again, once the membership are updated, you go to the centroid update.

1266
01:03:49,420 --> 01:03:52,420
You recompute the new centroids.

1267
01:03:52,420 --> 01:04:03,420
You keep doing this one, two, and one, two, all the way until you find that no more points have been changed about their membership.

1268
01:04:03,420 --> 01:04:10,420
So if you do like, say, two consecutive steps, you find that, oh, if I do this one more time, I don't have any change in my membership.

1269
01:04:10,420 --> 01:04:12,420
Then that's the signal for stopping.

1270
01:04:12,420 --> 01:04:14,420
You can stop.

1271
01:04:15,420 --> 01:04:19,420
And just output the current membership as the final update.

1272
01:04:19,420 --> 01:04:22,420
So that's it. That's all about k-means, just a few lines.

1273
01:04:24,420 --> 01:04:26,420
Okay, any questions?

1274
01:04:26,420 --> 01:04:28,420
Actually, this is all about k-means.

1275
01:04:31,420 --> 01:04:32,420
Any questions?

1276
01:04:34,420 --> 01:04:35,420
No?

1277
01:04:36,420 --> 01:04:38,420
Okay, then I will ask you some questions.

1278
01:04:38,420 --> 01:04:40,420
I'll see whether you understand this correctly.

1279
01:04:40,420 --> 01:04:45,420
So here I say that the stopping criteria will be no more points.

1280
01:04:45,420 --> 01:04:47,420
Assignments change.

1281
01:04:47,420 --> 01:04:50,420
So if you see the assignment unchanged, you can stop.

1282
01:04:50,420 --> 01:04:55,420
But what happens if you see that my mu's are not changed?

1283
01:04:55,420 --> 01:04:57,420
Let's say I run the first situation.

1284
01:04:57,420 --> 01:05:00,420
I get my mu, mu 1, mu 2, mu 3.

1285
01:05:00,420 --> 01:05:03,420
I do another one, I find that, okay, my mu are unchanged.

1286
01:05:03,420 --> 01:05:05,420
Can I stop?

1287
01:05:06,420 --> 01:05:09,420
Or how many of you think I can stop?

1288
01:05:09,420 --> 01:05:11,420
I'll give you a few.

1289
01:05:11,420 --> 01:05:13,420
Others assume that we cannot stop.

1290
01:05:16,420 --> 01:05:17,420
Okay.

1291
01:05:18,420 --> 01:05:20,420
Actually, you can stop.

1292
01:05:20,420 --> 01:05:23,420
Basically, if mu are not changing, membership will not change.

1293
01:05:23,420 --> 01:05:25,420
So it's kind of dependent to each other.

1294
01:05:25,420 --> 01:05:26,420
So think about it.

1295
01:05:26,420 --> 01:05:30,420
If your mu are not changed, how can you possibly change your membership?

1296
01:05:30,420 --> 01:05:32,420
Because x are the constant.

1297
01:05:32,420 --> 01:05:33,420
It's given data.

1298
01:05:33,420 --> 01:05:36,420
If your mu's are unchanged, you cannot have different distance.

1299
01:05:36,420 --> 01:05:38,420
So your membership will also be the same.

1300
01:05:38,420 --> 01:05:39,420
So it's actually either way.

1301
01:05:39,420 --> 01:05:43,420
You either observe the membership or you observe the mu's.

1302
01:05:43,420 --> 01:05:45,420
Either one seems unchanged, you can stop.

1303
01:05:45,420 --> 01:05:49,420
Alright, so this is actually one thing you can easily think about.

1304
01:05:49,420 --> 01:05:52,420
So either way, it can give you a signal.

1305
01:05:53,420 --> 01:05:55,420
Alright, the second question is that

1306
01:05:55,420 --> 01:06:01,420
just now I'm saying that we can randomly set the mu's as the starting point.

1307
01:06:01,420 --> 01:06:04,420
So if I give you a different mu in the beginning,

1308
01:06:04,420 --> 01:06:07,420
let's say I try one time, I give you a set of mu's,

1309
01:06:07,420 --> 01:06:09,420
you run the K-means, you get some results.

1310
01:06:09,420 --> 01:06:11,420
But next time I give you another set of mu's,

1311
01:06:11,420 --> 01:06:13,420
different points, randomly again,

1312
01:06:13,420 --> 01:06:15,420
will I get my same results?

1313
01:06:17,420 --> 01:06:19,420
So let me repeat the question.

1314
01:06:19,420 --> 01:06:22,420
The question is like, I will do K-means two times.

1315
01:06:22,420 --> 01:06:25,420
The same data, I do two times K-means.

1316
01:06:25,420 --> 01:06:29,420
First time I give you one set of mu's as the starting point.

1317
01:06:29,420 --> 01:06:32,420
Okay, maybe like a point A, point B, point C,

1318
01:06:32,420 --> 01:06:34,420
as my mu one, mu two, mu three, right?

1319
01:06:34,420 --> 01:06:37,420
I run my K-means until I converge,

1320
01:06:37,420 --> 01:06:40,420
I get my final output, my membership.

1321
01:06:40,420 --> 01:06:44,420
Now I redo the K-means, but I give you three different points.

1322
01:06:44,420 --> 01:06:46,420
Another three points as the mu's.

1323
01:06:46,420 --> 01:06:49,420
Maybe, you know, E, F, G, right, as my points.

1324
01:06:49,420 --> 01:06:50,420
So they'll be different.

1325
01:06:50,420 --> 01:06:54,420
I rerun the K-means in the exact same way,

1326
01:06:54,420 --> 01:06:57,420
will I get the same results as my first run?

1327
01:06:58,420 --> 01:07:00,420
Or how many of you think are no?

1328
01:07:00,420 --> 01:07:02,420
Okay, you and the others.

1329
01:07:02,420 --> 01:07:05,420
Or how many of you think you will get the same results?

1330
01:07:05,420 --> 01:07:06,420
Okay, we have some.

1331
01:07:06,420 --> 01:07:10,420
All right, so the answer is you may not get the same results,

1332
01:07:10,420 --> 01:07:12,420
which is a big problem for K-means.

1333
01:07:12,420 --> 01:07:13,420
I'm going to tell you why later, right?

1334
01:07:13,420 --> 01:07:15,420
But bear in mind that this is something

1335
01:07:15,420 --> 01:07:16,420
to give you different results.

1336
01:07:16,420 --> 01:07:18,420
So don't be surprised.

1337
01:07:18,420 --> 01:07:19,420
If you change your initialization,

1338
01:07:19,420 --> 01:07:20,420
get different results,

1339
01:07:20,420 --> 01:07:22,420
doesn't mean you are doing that wrong,

1340
01:07:22,420 --> 01:07:23,420
you're doing that correctly.

1341
01:07:23,420 --> 01:07:24,420
But that's the problem for K-means,

1342
01:07:24,420 --> 01:07:29,420
because K-means will be very sensitive to initialization,

1343
01:07:29,420 --> 01:07:32,420
and there's no guarantee of convergence even.

1344
01:07:32,420 --> 01:07:35,420
Sometimes you may be in a pretty weird shape,

1345
01:07:35,420 --> 01:07:37,420
but these are something you have to bear in mind.

1346
01:07:37,420 --> 01:07:40,420
Those are limitations of K-means.

1347
01:07:40,420 --> 01:07:43,420
So we will get there, but you have to think deeper, right?

1348
01:07:43,420 --> 01:07:44,420
So not only about the algorithm,

1349
01:07:44,420 --> 01:07:47,420
but what could be wrong with the algorithm.

1350
01:07:47,420 --> 01:07:50,420
Okay, so let's see some examples first.

1351
01:07:50,420 --> 01:07:53,420
So most of the examples I showed in the class

1352
01:07:53,420 --> 01:07:55,420
would be like two-dimensional, right?

1353
01:07:55,420 --> 01:07:58,420
Because two-dimensional are easy to visualize.

1354
01:07:58,420 --> 01:08:00,420
Our human can only match up to three-dimensional.

1355
01:08:00,420 --> 01:08:02,420
Two-dimensional is a plane,

1356
01:08:02,420 --> 01:08:04,420
but all of these methods can be wrong

1357
01:08:04,420 --> 01:08:06,420
for much higher dimensional,

1358
01:08:06,420 --> 01:08:07,420
just like you cannot visualize it.

1359
01:08:07,420 --> 01:08:11,420
So for visualization, we picked the two-dimensional space.

1360
01:08:11,420 --> 01:08:16,420
I give you, let's say, eight points here,

1361
01:08:16,420 --> 01:08:20,420
and I tell you that the parameter K is equal to three, right?

1362
01:08:20,420 --> 01:08:23,420
Now we use Euclidean distance,

1363
01:08:23,420 --> 01:08:30,420
and also say that I initiate my mu's as a1, b1, and c1.

1364
01:08:30,420 --> 01:08:35,420
Now apply K-means to estimate the final three cluster.

1365
01:08:35,420 --> 01:08:38,420
So this can be something quite standard,

1366
01:08:38,420 --> 01:08:41,420
quite possible as an exam question, right?

1367
01:08:41,420 --> 01:08:43,420
So in the exam question, I just give you some other points.

1368
01:08:43,420 --> 01:08:46,420
I tell you, well, let's try K equal to two, right?

1369
01:08:46,420 --> 01:08:48,420
And I give you two different mu's.

1370
01:08:48,420 --> 01:08:49,420
Give me the results.

1371
01:08:49,420 --> 01:08:52,420
So this is one thing very possible to show up in the exam.

1372
01:08:52,420 --> 01:08:56,420
So you need to look at what is the right approach to solve it.

1373
01:08:56,420 --> 01:08:59,420
How can you apply K-means to solve these kind of questions?

1374
01:08:59,420 --> 01:09:02,420
All right, so tell me what the step one should do,

1375
01:09:02,420 --> 01:09:03,420
or you should do.

1376
01:09:03,420 --> 01:09:05,420
I mean, if you run K-means for this question,

1377
01:09:05,420 --> 01:09:08,420
what's the step one you should do?

1378
01:09:08,420 --> 01:09:10,420
Assign the samples.

1379
01:09:10,420 --> 01:09:11,420
Sorry?

1380
01:09:11,420 --> 01:09:12,420
Assign the samples.

1381
01:09:12,420 --> 01:09:13,420
Assign the samples.

1382
01:09:13,420 --> 01:09:14,420
How?

1383
01:09:14,420 --> 01:09:15,420
How to assign the samples?

1384
01:09:15,420 --> 01:09:17,420
Like the minimize distance.

1385
01:09:17,420 --> 01:09:18,420
Distance.

1386
01:09:18,420 --> 01:09:19,420
OK, very good.

1387
01:09:19,420 --> 01:09:22,420
So you do step one, which is assigning the membership

1388
01:09:22,420 --> 01:09:24,420
for every point we have here.

1389
01:09:24,420 --> 01:09:26,420
And the question already tells you the initialization,

1390
01:09:26,420 --> 01:09:28,420
so you don't have to worry about that.

1391
01:09:28,420 --> 01:09:32,420
So a1 will be mu1, b1 will be mu2, and c1 will be mu3.

1392
01:09:32,420 --> 01:09:37,420
You just need to run step one to basically assign them

1393
01:09:37,420 --> 01:09:39,420
to the corresponding cluster, right?

1394
01:09:39,420 --> 01:09:42,420
So basically, this is how you can visualize the eight points

1395
01:09:42,420 --> 01:09:43,420
in the 2D space, right?

1396
01:09:43,420 --> 01:09:46,420
So I'm actually plotting the first dimension,

1397
01:09:46,420 --> 01:09:50,420
so every point will be a sample point in the 2D space, right?

1398
01:09:50,420 --> 01:09:53,420
So in practice, like I mentioned,

1399
01:09:53,420 --> 01:09:57,420
to determine the membership, you need

1400
01:09:57,420 --> 01:10:03,420
to compute the distance between every sample to every centroid.

1401
01:10:03,420 --> 01:10:06,420
In other words, here I'm showing a table,

1402
01:10:06,420 --> 01:10:08,420
like a pairwise distance table.

1403
01:10:08,420 --> 01:10:11,420
So what you need is the column with highlight,

1404
01:10:11,420 --> 01:10:14,420
highlight with yellow light.

1405
01:10:14,420 --> 01:10:19,420
Because here, my a1, b1, c1 are the centroids, right?

1406
01:10:19,420 --> 01:10:22,420
They are the mu1, mu2, mu3.

1407
01:10:22,420 --> 01:10:26,420
I need to compute the distance between every sample we have

1408
01:10:26,420 --> 01:10:28,420
here to those centroids.

1409
01:10:28,420 --> 01:10:31,420
So you need to have this yellow column

1410
01:10:31,420 --> 01:10:35,420
before you can determine what are the membership.

1411
01:10:35,420 --> 01:10:36,420
Make sense?

1412
01:10:36,420 --> 01:10:40,420
So numerically, you need to work out those numbers.

1413
01:10:40,420 --> 01:10:43,420
And then, how to find the minimum?

1414
01:10:43,420 --> 01:10:44,420
So here is very simple.

1415
01:10:44,420 --> 01:10:49,420
For a1, I have my 0, of course, is the point itself.

1416
01:10:49,420 --> 01:10:52,420
So 0 will be the minimum out of the three.

1417
01:10:52,420 --> 01:10:56,420
So I can determine that a1 goes to cluster 1, right?

1418
01:10:56,420 --> 01:11:01,420
And then a2, I got distance with 5, with 4.4, 3.1.

1419
01:11:01,420 --> 01:11:05,420
So 3.16 is the minimum out of the three.

1420
01:11:05,420 --> 01:11:09,420
So a2 should go to the third cluster, right?

1421
01:11:09,420 --> 01:11:13,420
You are comparing the number here in the same row

1422
01:11:13,420 --> 01:11:17,420
and find out which is the minimum.

1423
01:11:17,420 --> 01:11:21,420
And that corresponding centroid is the membership.

1424
01:11:21,420 --> 01:11:23,420
All right, so let me repeat.

1425
01:11:23,420 --> 01:11:25,420
When you do the question like this,

1426
01:11:25,420 --> 01:11:27,420
you need to work out the distance

1427
01:11:27,420 --> 01:11:31,420
between every sample point to every centroid.

1428
01:11:31,420 --> 01:11:35,420
Here, this a1, b1, c1 are the three centroids.

1429
01:11:35,420 --> 01:11:38,420
Every row represents a sample point.

1430
01:11:38,420 --> 01:11:40,420
So you need to work out all the distance

1431
01:11:40,420 --> 01:11:45,420
and compare the numbers in every row.

1432
01:11:45,420 --> 01:11:48,420
And you figure out which is the smallest number

1433
01:11:48,420 --> 01:11:51,420
out of the three in every row.

1434
01:11:51,420 --> 01:11:57,420
And that cluster is the one the point belongs to.

1435
01:11:57,420 --> 01:11:59,420
So you can do this for every sample.

1436
01:11:59,420 --> 01:12:02,420
You have to do the decision for every row.

1437
01:12:02,420 --> 01:12:06,420
So a goes to first cluster, a2 goes to cluster 3,

1438
01:12:06,420 --> 01:12:09,420
a3 goes to cluster 2, b1 goes to cluster 2.

1439
01:12:09,420 --> 01:12:10,420
So something like that.

1440
01:12:10,420 --> 01:12:13,420
Every sample makes a decision on the membership.

1441
01:12:13,420 --> 01:12:15,420
So you can end up with something like this.

1442
01:12:15,420 --> 01:12:18,420
So you can see that this is my cluster 1,

1443
01:12:18,420 --> 01:12:21,420
this is my cluster 2, this is my cluster 3.

1444
01:12:21,420 --> 01:12:26,420
So here, the blue point, red point, green point

1445
01:12:26,420 --> 01:12:28,420
are the new centers.

1446
01:12:28,420 --> 01:12:30,420
So once I do the membership assignment,

1447
01:12:30,420 --> 01:12:35,420
the step two will be updating the centroid.

1448
01:12:35,420 --> 01:12:41,420
So original centroid was a1, b1, and c1.

1449
01:12:41,420 --> 01:12:44,420
So these are the initial centroid.

1450
01:12:44,420 --> 01:12:48,420
But now I finish my step one, I can proceed to step two.

1451
01:12:48,420 --> 01:12:52,420
Step two is nothing but recompute the centroid.

1452
01:12:52,420 --> 01:12:54,420
How to recompute the centroid?

1453
01:12:54,420 --> 01:12:58,420
I just take all the points in the cluster, find average.

1454
01:12:58,420 --> 01:13:00,420
The average will be the new center.

1455
01:13:00,420 --> 01:13:02,420
So you can see that the center actually

1456
01:13:02,420 --> 01:13:04,420
shifted from b1 to here.

1457
01:13:04,420 --> 01:13:08,420
And the center for three shifted from c1 to here.

1458
01:13:08,420 --> 01:13:10,420
So that means we have to keep doing it,

1459
01:13:10,420 --> 01:13:11,420
because it's not finished.

1460
01:13:11,420 --> 01:13:14,420
You do observe a change in the centroid.

1461
01:13:14,420 --> 01:13:15,420
So it's not stopped.

1462
01:13:15,420 --> 01:13:18,420
You have to redo the first step.

1463
01:13:18,420 --> 01:13:20,420
How to redo the step?

1464
01:13:20,420 --> 01:13:21,420
Now this is actually step two.

1465
01:13:21,420 --> 01:13:23,420
You just compute the centroid.

1466
01:13:23,420 --> 01:13:26,420
The new centroid will be c1, c2, and c3.

1467
01:13:26,420 --> 01:13:30,420
So the new c, these are the new centers.

1468
01:13:30,420 --> 01:13:31,420
Now when you finish step two, you

1469
01:13:31,420 --> 01:13:34,420
have to redo step one, which is going to determine the membership.

1470
01:13:34,420 --> 01:13:38,420
But right now, the centroids are changed.

1471
01:13:38,420 --> 01:13:40,420
So you have to work out the new table.

1472
01:13:40,420 --> 01:13:42,420
So in the new table, these three will

1473
01:13:42,420 --> 01:13:45,420
be changed to the new center.

1474
01:13:45,420 --> 01:13:47,420
You have to update the table again

1475
01:13:47,420 --> 01:13:52,420
and re-decide which is the belongingness for every sample.

1476
01:13:52,420 --> 01:13:56,420
You have to redo this until you reach the stopping criteria,

1477
01:13:56,420 --> 01:14:00,420
which is either there's no change on membership

1478
01:14:00,420 --> 01:14:03,420
or no change on the centroid, either one.

1479
01:14:03,420 --> 01:14:06,420
Then you can stop and output the current membership

1480
01:14:06,420 --> 01:14:09,420
as the final output.

1481
01:14:09,420 --> 01:14:11,420
All right?

1482
01:14:11,420 --> 01:14:13,420
Questions?

1483
01:14:13,420 --> 01:14:14,420
Make sense?

1484
01:14:14,420 --> 01:14:16,420
All right, so one thing I can promise

1485
01:14:16,420 --> 01:14:19,420
is that in the exam, I will make this number simpler.

1486
01:14:19,420 --> 01:14:22,420
So you won't get a big table, which takes a lot of time.

1487
01:14:22,420 --> 01:14:26,420
But I will make maybe points fewer or numbers simpler.

1488
01:14:26,420 --> 01:14:27,420
So you won't really have a lot of time

1489
01:14:27,420 --> 01:14:29,420
computing the number, which doesn't make sense.

1490
01:14:29,420 --> 01:14:31,420
But you need to know the principle.

1491
01:14:31,420 --> 01:14:33,420
How can you do this thing step by step?

1492
01:14:33,420 --> 01:14:35,420
And I need to check your procedure.

1493
01:14:35,420 --> 01:14:36,420
So not only the final result, you

1494
01:14:36,420 --> 01:14:39,420
have to show me the step by step procedure.

1495
01:14:39,420 --> 01:14:40,420
When do you do step one?

1496
01:14:40,420 --> 01:14:41,420
When do you do step two?

1497
01:14:41,420 --> 01:14:42,420
When do you stop?

1498
01:14:42,420 --> 01:14:44,420
Every step counts.

1499
01:14:44,420 --> 01:14:48,420
All right, so this is important for algorithm kind of question.

1500
01:14:48,420 --> 01:14:51,420
OK, so this is actually one example.

1501
01:14:51,420 --> 01:14:57,420
So I use another example to show that how the centroid can

1502
01:14:57,420 --> 01:15:01,420
gradually change until we reach the stopping criteria.

1503
01:15:01,420 --> 01:15:06,420
So here, all of the green points are my samples.

1504
01:15:06,420 --> 01:15:09,420
I don't know how they belong to, but just some samples.

1505
01:15:09,420 --> 01:15:13,420
And I initialize the tool center as the blue cross

1506
01:15:13,420 --> 01:15:15,420
and the red cross.

1507
01:15:15,420 --> 01:15:17,420
So I run my first iteration.

1508
01:15:17,420 --> 01:15:19,420
I will try to assign a membership.

1509
01:15:19,420 --> 01:15:22,420
So I use color coding to denote the membership.

1510
01:15:22,420 --> 01:15:25,420
So red goes to red, blue goes to blue.

1511
01:15:25,420 --> 01:15:30,420
So this is the status after I finish my first step one.

1512
01:15:30,420 --> 01:15:34,420
And of course, you know that we need to recompute the center.

1513
01:15:34,420 --> 01:15:38,420
So this is how I do my first step two.

1514
01:15:38,420 --> 01:15:40,420
So after you finish the membership,

1515
01:15:40,420 --> 01:15:44,420
you take the average to update the center.

1516
01:15:44,420 --> 01:15:48,420
Then the center clearly change from here to here.

1517
01:15:48,420 --> 01:15:50,420
The blue one goes from here to here.

1518
01:15:50,420 --> 01:15:53,420
So you can see that they can gradually

1519
01:15:53,420 --> 01:15:57,420
move the center to be something more meaningful

1520
01:15:57,420 --> 01:15:58,420
from the initial guess.

1521
01:15:58,420 --> 01:15:59,420
And now I'm not finished.

1522
01:15:59,420 --> 01:16:01,420
You have to do one more step one.

1523
01:16:01,420 --> 01:16:05,420
So you really do step one using the new centers.

1524
01:16:05,420 --> 01:16:07,420
And the membership will be changed.

1525
01:16:07,420 --> 01:16:10,420
Initially, it was separating this way,

1526
01:16:10,420 --> 01:16:12,420
and now I'm going to separate it this way.

1527
01:16:12,420 --> 01:16:17,420
So the point will be re-clustered.

1528
01:16:17,420 --> 01:16:19,420
And now my membership updated.

1529
01:16:19,420 --> 01:16:22,420
I can update my centroids.

1530
01:16:22,420 --> 01:16:25,420
But this time, you see that the change of the center

1531
01:16:25,420 --> 01:16:28,420
becomes much smaller.

1532
01:16:28,420 --> 01:16:31,420
You don't have a huge change from here to here.

1533
01:16:31,420 --> 01:16:34,420
It's because the algorithm is trying to converge.

1534
01:16:34,420 --> 01:16:36,420
So the algorithm will most likely

1535
01:16:36,420 --> 01:16:38,420
converge to eventually stop.

1536
01:16:38,420 --> 01:16:40,420
And the phenomenon is that every time

1537
01:16:40,420 --> 01:16:42,420
from iteration to iteration, the change

1538
01:16:42,420 --> 01:16:44,420
will be smaller and smaller.

1539
01:16:44,420 --> 01:16:47,420
So this is the evidence showing that now I'm actually

1540
01:16:47,420 --> 01:16:49,420
closing to the convergence.

1541
01:16:49,420 --> 01:16:51,420
But still, there's something changed.

1542
01:16:51,420 --> 01:16:56,420
I have to redo my central update.

1543
01:16:56,420 --> 01:16:57,420
So this is my update.

1544
01:16:57,420 --> 01:16:59,420
And then I redo my membership assignment.

1545
01:16:59,420 --> 01:17:04,420
So from E to F, only a few points changed.

1546
01:17:04,420 --> 01:17:06,420
Only here changed.

1547
01:17:06,420 --> 01:17:09,420
But still, as long as there's a change, you have to keep doing it.

1548
01:17:09,420 --> 01:17:14,420
You can only stop if there's absolutely no change.

1549
01:17:14,420 --> 01:17:17,420
So you do one more time to update the centroid

1550
01:17:17,420 --> 01:17:19,420
and do one more step.

1551
01:17:19,420 --> 01:17:20,420
And then you do another step.

1552
01:17:20,420 --> 01:17:23,420
So here, if you observe no change between two

1553
01:17:23,420 --> 01:17:26,420
consecutive iterations, you can stop.

1554
01:17:26,420 --> 01:17:29,420
So this will be your final output.

1555
01:17:29,420 --> 01:17:32,420
So the final output will include both the centroid and also

1556
01:17:32,420 --> 01:17:33,420
the membership.

1557
01:17:36,420 --> 01:17:38,420
So this is an illustrative way to show

1558
01:17:38,420 --> 01:17:42,420
how the algorithm can evolve from step by step

1559
01:17:42,420 --> 01:17:43,420
through the iterations.

1560
01:17:43,420 --> 01:17:46,420
And you understand why we stop only when there's no change.

1561
01:17:46,420 --> 01:17:48,420
Because everything becomes so stable.

1562
01:17:48,420 --> 01:17:49,420
Everything converges.

1563
01:17:52,420 --> 01:17:55,420
Questions?

1564
01:17:55,420 --> 01:17:57,420
Make sense?

1565
01:17:57,420 --> 01:17:58,420
OK.

1566
01:17:58,420 --> 01:18:00,420
So here I show you one example.

1567
01:18:00,420 --> 01:18:01,420
Then we can take a break.

1568
01:18:01,420 --> 01:18:02,420
All right?

1569
01:18:02,420 --> 01:18:06,420
So just now, what I'm showing you are just some points,

1570
01:18:06,420 --> 01:18:10,420
two-dimensional, binary class.

1571
01:18:10,420 --> 01:18:12,420
It doesn't mean the algorithm doesn't

1572
01:18:12,420 --> 01:18:14,420
look like a working practice.

1573
01:18:14,420 --> 01:18:16,420
But I want to show you that actually clustering

1574
01:18:16,420 --> 01:18:19,420
can work in practice.

1575
01:18:19,420 --> 01:18:21,420
So just use k-means.

1576
01:18:21,420 --> 01:18:24,420
So suppose, for example, I have a photo like this.

1577
01:18:24,420 --> 01:18:27,420
I have no idea what I have in my photo.

1578
01:18:27,420 --> 01:18:29,420
But I want to try to separate, let's say,

1579
01:18:29,420 --> 01:18:32,420
the castle from the background, or maybe from the sky,

1580
01:18:32,420 --> 01:18:34,420
from the forest.

1581
01:18:34,420 --> 01:18:37,420
But I have no idea what is castle, what is the forest.

1582
01:18:37,420 --> 01:18:39,420
I can actually run k-means and see whether I can figure them

1583
01:18:39,420 --> 01:18:40,420
out.

1584
01:18:40,420 --> 01:18:43,420
So if I run k-means, I can use the very similar idea

1585
01:18:44,420 --> 01:18:46,420
as I showed you before for flower.

1586
01:18:46,420 --> 01:18:49,420
I use the RGB intensity as my feature.

1587
01:18:49,420 --> 01:18:51,420
I will first plot the diagram.

1588
01:18:51,420 --> 01:18:55,420
Let's say I plot the gray scale diagram as the distribution,

1589
01:18:55,420 --> 01:18:57,420
as a histogram.

1590
01:18:57,420 --> 01:19:00,420
Now I can actually try to, let's say,

1591
01:19:00,420 --> 01:19:04,420
put this part as my cluster one, maybe this part

1592
01:19:04,420 --> 01:19:07,420
as my cluster two, this part as my cluster three.

1593
01:19:07,420 --> 01:19:09,420
I can form clustering results.

1594
01:19:10,420 --> 01:19:14,420
Let's say this is k equal to 2, this is k equal to 3.

1595
01:19:14,420 --> 01:19:17,420
So as you can see that, actually, roughly, they

1596
01:19:17,420 --> 01:19:20,420
can actually capture the foreground of the castle

1597
01:19:20,420 --> 01:19:21,420
and maybe the forest.

1598
01:19:21,420 --> 01:19:25,420
Because intensity-wise, they're very close to each other.

1599
01:19:25,420 --> 01:19:27,420
If you increase the k, they can further

1600
01:19:27,420 --> 01:19:32,420
separate the mountain area or the lake behind and also

1601
01:19:32,420 --> 01:19:34,420
the foreground forest.

1602
01:19:34,420 --> 01:19:37,420
So you can see that even without labels, intrinsically,

1603
01:19:37,420 --> 01:19:40,420
there are some differentiable features just in the intensity

1604
01:19:40,420 --> 01:19:41,420
space.

1605
01:19:41,420 --> 01:19:44,420
But this was just like gray scale.

1606
01:19:44,420 --> 01:19:47,420
Gray scale means I haven't even considered the color.

1607
01:19:47,420 --> 01:19:50,420
I just considered how bright the pixel is.

1608
01:19:50,420 --> 01:19:52,420
I can do something like this.

1609
01:19:52,420 --> 01:19:54,420
Let's further consider the RGB.

1610
01:19:54,420 --> 01:19:57,420
I just take RGB into it, and every point

1611
01:19:57,420 --> 01:20:00,420
I have three-dimensional space, three-dimensional vector.

1612
01:20:00,420 --> 01:20:02,420
I run k-means.

1613
01:20:02,420 --> 01:20:05,420
I can actually get pretty good results.

1614
01:20:05,420 --> 01:20:07,420
All the forest actually has been separated.

1615
01:20:07,420 --> 01:20:10,420
The castle foreground will be separated and then the background.

1616
01:20:10,420 --> 01:20:13,420
So this is one example showing you that actually k-means

1617
01:20:13,420 --> 01:20:16,420
can be applied to more practical examples

1618
01:20:16,420 --> 01:20:18,420
rather than just poor examples.

1619
01:20:18,420 --> 01:20:20,420
And if you don't have labels, that

1620
01:20:20,420 --> 01:20:24,420
could be something not too bad for you to give a try.

1621
01:20:24,420 --> 01:20:26,420
So this is actually something I want to show.

1622
01:20:26,420 --> 01:20:28,420
But of course, just for illustration, in the exam,

1623
01:20:28,420 --> 01:20:31,420
I will not give you this case because it's hard to confuse.

1624
01:20:31,420 --> 01:20:33,420
For exam, it's always a poor example.

1625
01:20:33,420 --> 01:20:36,420
But I wish to let you know that they can be used in practice.

1626
01:20:40,420 --> 01:20:41,420
All right.

1627
01:20:41,420 --> 01:20:42,420
Any questions?

1628
01:20:42,420 --> 01:20:44,420
If not, let's take a break.

1629
01:20:44,420 --> 01:20:48,420
And we'll come back around 2.05.

1630
01:20:48,420 --> 01:20:49,420
OK, thank you.

1631
01:20:49,420 --> 01:20:50,420
Let's take a break.

1632
01:21:03,420 --> 01:21:04,420
Thank you.

1633
01:21:33,420 --> 01:21:34,420
Thank you.

1634
01:22:03,420 --> 01:22:04,420
Thank you.

1635
01:22:33,420 --> 01:22:34,420
Thank you.

1636
01:23:03,420 --> 01:23:04,420
Thank you.

1637
01:23:33,420 --> 01:23:34,420
Thank you.

1638
01:24:03,420 --> 01:24:04,420
Thank you.

1639
01:24:33,420 --> 01:24:34,420
Thank you.

1640
01:25:03,420 --> 01:25:04,420
Thank you.

1641
01:25:33,420 --> 01:25:34,420
Thank you.

1642
01:26:03,420 --> 01:26:06,420
Can we do a little test of the code when we put it on our data?

1643
01:26:12,420 --> 01:26:18,420
We need to test the code to put it on our back.

1644
01:26:18,420 --> 01:26:20,420
Yeah, we'd better test it.

1645
01:26:20,420 --> 01:26:24,420
And the reason is that, let's say

1646
01:26:24,420 --> 01:26:27,420
if there's something you didn't experiment with in your computer,

1647
01:26:27,420 --> 01:26:30,420
we can't do rock, but you make sure the code can be rock.

1648
01:26:30,420 --> 01:26:31,420
We give a lot of BTs.

1649
01:26:31,420 --> 01:26:37,420
We give a lot of bugs, but if your code can be reproducible, then you have a better path.

1650
01:26:37,420 --> 01:26:41,420
What's the way you would recommend Python?

1651
01:26:41,420 --> 01:26:46,420
I mean, we would recommend Python, but it's up to you.

1652
01:26:46,420 --> 01:26:48,420
We don't specify the language.

1653
01:26:48,420 --> 01:26:51,420
But it's because we're coming from a very diverse background.

1654
01:26:51,420 --> 01:26:56,420
For our undergraduates, normally they do Python because they have a Python course.

1655
01:26:56,420 --> 01:26:58,420
So everybody knows how to do Python.

1656
01:26:58,420 --> 01:27:05,420
But if you find that Python is hard to read, and you are equipped for other, for example, C or Mac apps, find the key.

1657
01:27:05,420 --> 01:27:06,420
I'm OK with that.

1658
01:27:06,420 --> 01:27:08,420
OK, thank you.

1659
01:27:08,420 --> 01:27:09,420
Thank you.

1660
01:27:14,420 --> 01:27:16,420
Oh, you just take the average of all the points.

1661
01:27:16,420 --> 01:27:19,420
C1, B1, B3, B2, A3, C4.

1662
01:27:19,420 --> 01:27:22,420
Average. Sum them together and divide by 5.

1663
01:27:22,420 --> 01:27:23,420
Oh, yeah.

1664
01:27:23,420 --> 01:27:24,420
All right.

1665
01:27:28,420 --> 01:27:29,420
All right.

1666
01:27:58,420 --> 01:27:59,420
All right.

1667
01:28:28,420 --> 01:28:29,420
All right.

1668
01:28:58,420 --> 01:28:59,420
All right.

1669
01:29:28,420 --> 01:29:29,420
All right.

1670
01:29:58,420 --> 01:29:59,420
All right.

1671
01:30:28,420 --> 01:30:29,420
All right.

1672
01:30:58,420 --> 01:30:59,420
All right.

1673
01:31:28,420 --> 01:31:29,420
All right.

1674
01:31:58,420 --> 01:31:59,420
All right.

1675
01:32:29,420 --> 01:32:30,420
All right.

1676
01:32:30,420 --> 01:32:31,420
All right.

1677
01:32:31,420 --> 01:32:32,420
All right.

1678
01:32:32,420 --> 01:32:33,420
All right.

1679
01:32:33,420 --> 01:32:34,420
All right.

1680
01:32:34,420 --> 01:32:35,420
All right.

1681
01:32:35,420 --> 01:32:36,420
All right.

1682
01:32:36,420 --> 01:32:37,420
All right.

1683
01:32:37,420 --> 01:32:38,420
All right.

1684
01:32:38,420 --> 01:32:39,420
All right.

1685
01:32:39,420 --> 01:32:40,420
All right.

1686
01:32:40,420 --> 01:32:41,420
All right.

1687
01:32:41,420 --> 01:32:42,420
All right.

1688
01:32:42,420 --> 01:32:43,420
All right.

1689
01:32:43,420 --> 01:32:44,420
All right.

1690
01:32:44,420 --> 01:32:45,420
All right.

1691
01:32:45,420 --> 01:32:46,420
All right.

1692
01:32:46,420 --> 01:32:47,420
All right.

1693
01:32:47,420 --> 01:32:48,420
All right.

1694
01:32:48,420 --> 01:32:49,420
All right.

1695
01:32:49,420 --> 01:32:50,420
All right.

1696
01:32:50,420 --> 01:32:51,420
Yeah?

1697
01:32:51,420 --> 01:32:52,420
OK.

1698
01:32:52,420 --> 01:32:53,420
Thank you.

1699
01:32:53,420 --> 01:32:54,420
Thank you.

1700
01:32:54,420 --> 01:32:55,420
OK.

1701
01:32:55,420 --> 01:32:56,420
Thank you.

1702
01:32:56,420 --> 01:32:57,420
Thank you.

1703
01:32:57,420 --> 01:32:58,420
Thank you.

1704
01:32:58,420 --> 01:32:59,420
Thank you.

1705
01:32:59,420 --> 01:33:00,420
Thank you.

1706
01:33:00,420 --> 01:33:01,420
Thank you.

1707
01:33:01,420 --> 01:33:02,420
Good.

1708
01:33:02,420 --> 01:33:03,420
Thank you.

1709
01:33:03,420 --> 01:33:04,420
Bye.

1710
01:33:04,420 --> 01:33:05,420
Bye.

1711
01:33:05,420 --> 01:33:06,420
Thank you.

1712
01:33:06,420 --> 01:33:07,420
Good.

1713
01:33:07,420 --> 01:33:08,420
Bye.

1714
01:33:08,420 --> 01:33:09,420
Good.

1715
01:33:09,420 --> 01:33:10,420
Thank you.

1716
01:33:10,420 --> 01:33:11,420
Bye.

1717
01:33:11,420 --> 01:33:12,420
Bye.

1718
01:33:12,420 --> 01:33:13,420
Revelation sequence.

1719
01:33:13,420 --> 01:33:14,420
Hello, everyone.

1720
01:33:14,420 --> 01:33:16,420
,

1721
01:33:50,420 --> 01:33:58,180
All right, I think since most of us are here, we can maybe start.

1722
01:33:58,180 --> 01:34:00,340
Is it okay to start now?

1723
01:34:00,340 --> 01:34:06,180
Okay, so we start earlier, then we can finish these earlier, right, so you can go for dinner,

1724
01:34:06,180 --> 01:34:08,180
I don't know whether somebody else.

1725
01:34:08,180 --> 01:34:13,740
Okay, anyway, let's start, and let's just resume at the place we ended just now, right?

1726
01:34:13,740 --> 01:34:19,100
So just now I showed you some examples showing that K-means can be used not for toy example,

1727
01:34:19,100 --> 01:34:23,740
but also practical case, image processing, image segmentation.

1728
01:34:23,740 --> 01:34:29,900
But heaven say that, actually every learning method, every method we're going to talk about

1729
01:34:29,900 --> 01:34:32,300
for this course is based on learning, right?

1730
01:34:32,300 --> 01:34:37,820
So if it's a learning method, the first question you ask yourself is always, what are the methods

1731
01:34:37,820 --> 01:34:40,460
trying to minimize or maximize, right?

1732
01:34:40,460 --> 01:34:45,740
So in principle, all the learning method can be somehow formulated as an optimization problem.

1733
01:34:45,740 --> 01:34:49,860
So I mean, I will not test you on this, but just for information, just always say that

1734
01:34:49,860 --> 01:34:55,660
K-means is trying to minimize the summation of the distance between every sample to the

1735
01:34:55,660 --> 01:34:56,820
centroid, right?

1736
01:34:56,820 --> 01:35:01,600
I was saying that in words, but I didn't really formulate it in math.

1737
01:35:01,600 --> 01:35:06,180
So if you want to be serious about the objective function, this is what the K-means is trying

1738
01:35:06,180 --> 01:35:07,180
to minimize, right?

1739
01:35:07,180 --> 01:35:10,300
So basically, we're talking about every sample, right?

1740
01:35:10,300 --> 01:35:14,780
So here, it's going to sum from i equal to 1 to n.

1741
01:35:14,780 --> 01:35:21,940
Every sample, we care about the distance to v corresponding mean, right, the centroid.

1742
01:35:21,940 --> 01:35:26,320
So we want to find out the correct centroid, the correct membership.

1743
01:35:26,320 --> 01:35:31,980
This is a centroid, this is a membership, such that the entire summation of the distance

1744
01:35:31,980 --> 01:35:38,620
between the sample to the associated centroid is together minimized, right?

1745
01:35:38,620 --> 01:35:45,860
So mathematically, we are solving the following minimization problem to figure out what is

1746
01:35:45,860 --> 01:35:50,480
the best mean and what is the best membership.

1747
01:35:50,480 --> 01:35:55,180
And because of that, we are alternatively updating the mu and r.

1748
01:35:55,180 --> 01:35:59,180
That was the two steps we're talking about in K-means, right?

1749
01:35:59,180 --> 01:36:04,340
So all of this iterative process is coming from the fact that we're minimizing this guy

1750
01:36:04,340 --> 01:36:08,700
alternatively over mu and over r, all right?

1751
01:36:08,700 --> 01:36:14,220
So it's like we fix mu, find what's the minimum of r, we fix r, find what's the minimum of

1752
01:36:14,220 --> 01:36:15,460
mu, right?

1753
01:36:15,460 --> 01:36:20,820
So this is actually the formal formulation we're trying to minimize.

1754
01:36:20,820 --> 01:36:24,100
For those of you who know optimization, this is what the function looks like, right?

1755
01:36:24,100 --> 01:36:28,940
But of course, I'm not going to test on this, just to give you some idea, right, rigorously

1756
01:36:28,940 --> 01:36:32,740
what are we talking about, all right?

1757
01:36:32,740 --> 01:36:37,900
So just when I say that, knowing the algorithm, of course, you need to know the algorithm.

1758
01:36:37,900 --> 01:36:43,100
But we also wish to understand what is good about the algorithm and what is bad about

1759
01:36:43,100 --> 01:36:44,100
the algorithm.

1760
01:36:44,100 --> 01:36:49,180
So in other words, the pros and cons of every algorithm we learn.

1761
01:36:49,180 --> 01:36:53,940
So for K-means, people love K-means, it's because it's simple, right, as you see, just

1762
01:36:53,940 --> 01:36:54,940
two lines, right?

1763
01:36:54,940 --> 01:37:00,700
Single, but effective, we showed that they work sometimes for real business.

1764
01:37:00,700 --> 01:37:02,580
And it's very easy to implement, right?

1765
01:37:02,580 --> 01:37:07,180
So this is the reason why you always try this and see whether it works, right?

1766
01:37:07,180 --> 01:37:12,620
But it doesn't mean that this K-means is already a perfect algorithm, because there are lots

1767
01:37:12,620 --> 01:37:16,700
of drawbacks actually for K-means, I'll just name a few, right?

1768
01:37:16,700 --> 01:37:25,260
First one is the K. You have to know the K beforehand, because K here is the hyperparameter.

1769
01:37:25,260 --> 01:37:28,020
You cannot expect to learn the number, right?

1770
01:37:28,020 --> 01:37:30,020
But what happens if I don't know the K?

1771
01:37:30,740 --> 01:37:33,620
Right, so if you remember just now this example, right?

1772
01:37:33,620 --> 01:37:37,380
If you do K equal to 3, that sounds like a more meaningful classification.

1773
01:37:37,380 --> 01:37:45,140
But if K equal to 2, sorry, K equal to 2, you may not get exactly what you want, right?

1774
01:37:45,140 --> 01:37:51,180
So different K will get different class results, but some of them could make more sense.

1775
01:37:51,180 --> 01:37:56,860
So this is the problem for K-means, because this K looks like a mask for you to pick before

1776
01:37:56,860 --> 01:37:58,780
even running the algorithm, right?

1777
01:37:58,780 --> 01:38:04,220
So the K actually is something you have to tune, have to really try, but that's not the

1778
01:38:04,220 --> 01:38:05,220
worst part.

1779
01:38:05,220 --> 01:38:11,180
Actually, the worst part is that K-means have the risk of being stocked as some so-called

1780
01:38:11,180 --> 01:38:13,020
poor local minimum, all right?

1781
01:38:13,020 --> 01:38:17,820
So just now I asked you a question, like if I change my initialization of the clusters,

1782
01:38:17,820 --> 01:38:20,020
well, I get different results.

1783
01:38:20,020 --> 01:38:26,060
So the answer is yes, K-means is actually very sensitive to the initialization, and

1784
01:38:26,060 --> 01:38:30,380
some really terrible initialization can give you some poor local minimum.

1785
01:38:30,380 --> 01:38:34,660
In other words, you are stuck in some place where you are going to stop, but doesn't give

1786
01:38:34,660 --> 01:38:36,700
you anything meaningful, all right?

1787
01:38:36,700 --> 01:38:39,500
So let's see some example of what I'm talking about, right?

1788
01:38:39,500 --> 01:38:45,420
So think about this example where the ground truth is like that, right?

1789
01:38:45,420 --> 01:38:50,260
So the ground truth is like a green goes to green, red goes to red, blue goes to blue,

1790
01:38:50,260 --> 01:38:51,260
all right?

1791
01:38:51,260 --> 01:38:54,020
So if you get something like this, that's perfect.

1792
01:38:54,020 --> 01:39:00,140
But in practice, OK, you are looking for the optimal clustering result like this, but

1793
01:39:00,140 --> 01:39:05,060
if you can only do so with the right initialization, all right?

1794
01:39:05,060 --> 01:39:07,060
So here's one example.

1795
01:39:07,060 --> 01:39:14,460
So this is one example, but if you somehow get the initialization wrong or bad or inappropriate,

1796
01:39:14,460 --> 01:39:17,960
you could end up with something like this, all right?

1797
01:39:17,960 --> 01:39:24,480
So it still satisfies the definition of points in the cluster will be similar, and the point

1798
01:39:24,480 --> 01:39:29,360
outside the cluster will be dissimilar, but the cluster is somehow very different from

1799
01:39:29,360 --> 01:39:32,520
your ground truth, all right?

1800
01:39:32,520 --> 01:39:37,280
And all of this depends on how you initialize your clusters, right?

1801
01:39:37,280 --> 01:39:43,200
So here's one example, and with good initialization, for example, my cluster one is here, my cluster

1802
01:39:43,200 --> 01:39:47,520
two is here, my cluster three is here, all right, the black cross.

1803
01:39:47,520 --> 01:39:48,520
That's my initialization.

1804
01:39:48,520 --> 01:39:54,640
And then I run my k-means gradually from my initial points, blah, blah, blah, all the

1805
01:39:54,640 --> 01:39:58,760
way, I can somehow reach something quite ideal, right?

1806
01:39:58,760 --> 01:40:00,880
Something quite close to the ground truth.

1807
01:40:00,880 --> 01:40:05,960
So this is the ideal case, but nobody knows what's the right initialization, right?

1808
01:40:05,960 --> 01:40:12,280
So if I somehow accidentally pick this kind of initialization, which doesn't look so bad,

1809
01:40:12,280 --> 01:40:13,280
right?

1810
01:40:13,280 --> 01:40:14,280
It's still OK.

1811
01:40:14,280 --> 01:40:16,280
I mean, you still make the points quite close to the centroid.

1812
01:40:16,320 --> 01:40:22,160
But if you have those three as the centroids, if you run k-means multiple times, you're

1813
01:40:22,160 --> 01:40:24,920
going to stop, but stop at here, right?

1814
01:40:24,920 --> 01:40:26,720
You'll get something quite ridiculous.

1815
01:40:26,720 --> 01:40:31,920
The originally, the one cluster will be divided into two, and they still give you something

1816
01:40:31,920 --> 01:40:34,280
convergence, right?

1817
01:40:34,280 --> 01:40:39,240
So this is something we call the poor local minimum, or bad local minimum, because it's

1818
01:40:39,240 --> 01:40:43,940
somewhere you're stuck there, but you don't really reach the final answer, right?

1819
01:40:43,940 --> 01:40:50,620
So this is one of the risks of using the k-means, and the risk is picking the bad initialization,

1820
01:40:50,620 --> 01:40:53,060
and nobody knows what is bad, right?

1821
01:40:53,060 --> 01:40:55,540
That's the problem for k-means.

1822
01:40:55,540 --> 01:40:56,540
OK?

1823
01:40:56,540 --> 01:40:59,140
Another problem is the distance metric.

1824
01:40:59,140 --> 01:41:03,340
Remember, for every clustering method, you need a distance metric, right?

1825
01:41:03,340 --> 01:41:06,260
We talk about Euclidean, we talk about Manhattan, right?

1826
01:41:06,260 --> 01:41:09,700
So what is the good distance metric?

1827
01:41:09,700 --> 01:41:11,340
Still very unclear, right?

1828
01:41:11,340 --> 01:41:15,800
So we say that we use Euclidean, but Euclidean may not work in many cases.

1829
01:41:15,800 --> 01:41:23,140
So if this is my example where I want to separate the red points from the blue points, apparently

1830
01:41:23,140 --> 01:41:28,060
the red points close to each other, but the blue points is surrounding at the further

1831
01:41:28,060 --> 01:41:29,060
points, right?

1832
01:41:29,060 --> 01:41:37,820
But if I have to use Euclidean distance, I cannot find a clear linear decision boundary

1833
01:41:37,820 --> 01:41:43,820
to make the points separated, because these points are not linear separable if you use

1834
01:41:43,820 --> 01:41:44,820
Euclidean distance.

1835
01:41:44,820 --> 01:41:48,620
In other words, you cannot find the line to separate them, right?

1836
01:41:48,620 --> 01:41:51,580
But clearly, they are different.

1837
01:41:51,580 --> 01:41:55,940
So to solve that, you need to find this different distance metric.

1838
01:41:55,940 --> 01:42:02,540
For example, you can convert x and y into the polar coordinates, where the two coordinates

1839
01:42:02,540 --> 01:42:06,060
becomes the angle and the radius, right?

1840
01:42:06,060 --> 01:42:09,460
I hope most of you know what is polar coordinates, right?

1841
01:42:09,460 --> 01:42:14,020
So you can represent the points using x and y, you can also represent the point using

1842
01:42:14,020 --> 01:42:18,780
the angle towards the horizontal and also the distance, the radius, right?

1843
01:42:18,780 --> 01:42:24,620
So if you convert the x, y into polar coordinates, the blue and the red can be separable, right?

1844
01:42:24,620 --> 01:42:26,860
So you can find a line to separate them.

1845
01:42:26,860 --> 01:42:32,620
And if you use the second distance metric, you can find a meaningful k-means result.

1846
01:42:33,180 --> 01:42:37,420
But you cannot do that if you stick to the Euclidean distance.

1847
01:42:37,420 --> 01:42:38,420
Okay?

1848
01:42:38,420 --> 01:42:44,380
So this is one example to show that initialization matters, distance matters, k-matters, everything

1849
01:42:44,380 --> 01:42:45,540
actually is so sensitive.

1850
01:42:45,540 --> 01:42:51,180
So you can easily get it wrong with k-means if you do not pick the right choice, all right?

1851
01:42:51,180 --> 01:42:54,220
So those are the limitations of k-means.

1852
01:42:54,220 --> 01:43:00,060
If you find that your data are quite kind of risky and you don't have a good idea of

1853
01:43:00,060 --> 01:43:05,500
initialization, you better forget about k-means because you can get things wrong, all right?

1854
01:43:05,500 --> 01:43:10,060
So bear this in mind and that's the principle when you do project.

1855
01:43:10,060 --> 01:43:13,140
You need to tell me, for example, I give you the cannabis stock, right?

1856
01:43:13,140 --> 01:43:15,660
You need to decide what is the classifier.

1857
01:43:15,660 --> 01:43:19,020
So I want to see how you make the decision, right?

1858
01:43:19,020 --> 01:43:23,340
Are you going to do neural networks, are you going to do maybe random forest, right?

1859
01:43:23,340 --> 01:43:24,580
So you have to tell me the reason.

1860
01:43:24,580 --> 01:43:31,100
So here is one of the ways to identify whether the limitation of the method matters for my

1861
01:43:31,100 --> 01:43:32,800
dataset, right?

1862
01:43:32,800 --> 01:43:38,060
So this is one example which tells you how you can do analysis.

1863
01:43:38,060 --> 01:43:40,060
Okay, make sense?

1864
01:43:40,060 --> 01:43:41,060
Questions?

1865
01:43:41,060 --> 01:43:42,060
Any questions?

1866
01:43:42,060 --> 01:43:43,060
All right, great.

1867
01:43:43,060 --> 01:43:46,100
So, but this is all about k-means.

1868
01:43:46,100 --> 01:43:49,740
So so far we have learned, for example, what is k-means.

1869
01:43:49,740 --> 01:43:51,460
You know exactly everything.

1870
01:43:51,460 --> 01:43:55,780
We have seen some examples, visual examples, numerical examples, you know how to apply

1871
01:43:55,780 --> 01:43:57,700
them for a given point, right?

1872
01:43:57,700 --> 01:44:03,820
But of course, you need to know what is k, what is the initialization, and we see some

1873
01:44:03,820 --> 01:44:05,280
practical cases.

1874
01:44:05,280 --> 01:44:06,820
We also see the planned parts, right?

1875
01:44:06,820 --> 01:44:09,140
But this is what you need to know for k-means.

1876
01:44:09,140 --> 01:44:15,000
All right, but that's the method for partition-based clustering.

1877
01:44:15,000 --> 01:44:17,700
We also need to talk about the hierarchical clustering, right?

1878
01:44:17,700 --> 01:44:23,020
So hierarchical clustering will be different because we're not going to partition a sample

1879
01:44:23,020 --> 01:44:24,020
in one shot.

1880
01:44:24,020 --> 01:44:28,020
We're going to progressively merge or separate the points.

1881
01:44:28,020 --> 01:44:34,340
And the one algorithm we learned for this type is called HAC, hierarchical agglomerative

1882
01:44:34,340 --> 01:44:35,560
algorithm.

1883
01:44:35,560 --> 01:44:41,720
So it's going to be very, quite different from k-means because there are two approaches.

1884
01:44:41,740 --> 01:44:51,040
So for HAC, the idea is that you do not really partition that, but you start it by treating

1885
01:44:51,040 --> 01:44:55,520
every point as an individual cluster first.

1886
01:44:55,520 --> 01:45:01,040
The starting point is that you treat every point, every sample itself, as an individual

1887
01:45:01,040 --> 01:45:02,040
cluster.

1888
01:45:02,040 --> 01:45:09,480
And progressively, at every step, you try to merge the pair of the column two clusters.

1889
01:45:10,200 --> 01:45:14,320
If you find that they are the closest to each other.

1890
01:45:14,320 --> 01:45:17,880
So it means that, okay, I can start with, let's say I have three points, A, B, C. I

1891
01:45:17,880 --> 01:45:22,120
treat A as cluster one, B as cluster two, C as cluster three.

1892
01:45:22,120 --> 01:45:28,840
And then I look at, okay, is AB the closest, or BC the closest, or AC are the closest?

1893
01:45:28,840 --> 01:45:31,680
If AB are the closest, I will group AB together.

1894
01:45:31,680 --> 01:45:35,560
So AB becomes the first cluster, C will be the second cluster.

1895
01:45:35,560 --> 01:45:41,680
You progressively merge them, but every time, you only merge one pair, which has the smallest

1896
01:45:41,680 --> 01:45:44,720
distance out of all the choices.

1897
01:45:44,720 --> 01:45:46,880
So this is how you do HAC progressively.

1898
01:45:46,880 --> 01:45:54,280
But of course, here the question again, how to merge the pair and how to define the minimum

1899
01:45:54,280 --> 01:45:55,480
distance.

1900
01:45:55,480 --> 01:45:59,120
So this is always a question, we ask the question for k-means as well.

1901
01:45:59,120 --> 01:46:05,680
So here, we have to introduce the distance between not the points, but the distance between

1902
01:46:05,680 --> 01:46:08,080
two clusters.

1903
01:46:08,080 --> 01:46:12,520
So the distance measured two points will be different from the distance measured in two

1904
01:46:12,520 --> 01:46:14,240
clusters.

1905
01:46:14,240 --> 01:46:16,680
And that's unique for HAC.

1906
01:46:16,680 --> 01:46:21,200
k-means, you don't have to measure this, but HAC, you need to know how to measure the distance

1907
01:46:21,200 --> 01:46:22,200
between two clusters.

1908
01:46:22,200 --> 01:46:23,920
So we're going to get there.

1909
01:46:23,920 --> 01:46:27,940
But let me show you the outline of the HAC algorithm.

1910
01:46:27,940 --> 01:46:33,900
It's also a very easy single algorithm, because everything can be explained in a few lines.

1911
01:46:33,900 --> 01:46:38,780
Basically, you need to initialize, but the initialization is always the same.

1912
01:46:38,780 --> 01:46:43,020
You treat every point as an individual cluster in the beginning.

1913
01:46:43,020 --> 01:46:46,460
So no randomness is always the same.

1914
01:46:46,460 --> 01:46:52,140
So unlike k-means, k-means, you have to do random initialization, but for HAC, it's fixed.

1915
01:46:52,140 --> 01:46:56,520
So now, you also need to do multiple iterations.

1916
01:46:56,520 --> 01:47:01,960
But here, the iteration is actually by progressively grouping the clusters.

1917
01:47:01,960 --> 01:47:08,200
So every iteration, you are going to merge two clusters with the minimum distance.

1918
01:47:08,200 --> 01:47:14,640
So here, suppose I'm starting at the point where every data is a single cluster.

1919
01:47:14,640 --> 01:47:16,800
I'm going to group, for example, AB together.

1920
01:47:16,800 --> 01:47:21,680
So in step one, I'm going to group the pair of AB.

1921
01:47:21,680 --> 01:47:25,520
And I move on to group, say, hey, here, the DE is the second closest.

1922
01:47:25,520 --> 01:47:28,000
So I'm going to group DE.

1923
01:47:28,000 --> 01:47:31,200
And now, I realize that C and DE are the closest.

1924
01:47:31,200 --> 01:47:33,280
I would just group C and DE together.

1925
01:47:33,280 --> 01:47:35,200
So I form two clusters.

1926
01:47:35,200 --> 01:47:37,140
And finally, I group everything together.

1927
01:47:37,140 --> 01:47:41,320
So this is the four steps we need to group everything.

1928
01:47:41,320 --> 01:47:44,960
So the stopping criteria will be either of the two.

1929
01:47:44,960 --> 01:47:48,860
So it's either that all the points are merged into one.

1930
01:47:48,860 --> 01:47:52,440
So this is actually the scenario where you don't have anything to merge.

1931
01:47:52,440 --> 01:47:54,560
You only have one cluster left.

1932
01:47:54,560 --> 01:47:56,000
So you can stop there.

1933
01:47:56,000 --> 01:48:00,840
Or if the question tells you that I want to group everything into a k cluster, if they

1934
01:48:00,840 --> 01:48:06,800
already specify that I need k cluster, you can stop at the step where only k's cluster

1935
01:48:06,800 --> 01:48:07,800
remains.

1936
01:48:07,800 --> 01:48:08,800
So it's either way.

1937
01:48:08,800 --> 01:48:09,800
But actually, it doesn't matter.

1938
01:48:09,800 --> 01:48:17,480
Later, we can see that you can always trace back from fewer clusters to more clusters.

1939
01:48:17,480 --> 01:48:19,240
That's one of the good points for HAC.

1940
01:48:19,240 --> 01:48:26,080
But this will be all about the algorithm in a nutshell.

1941
01:48:26,080 --> 01:48:35,880
So because we're doing this merging progressively, after you have done with your HAC, one can

1942
01:48:35,880 --> 01:48:42,080
come up with a tree structure to visualize the entire merging process.

1943
01:48:42,080 --> 01:48:47,360
So you can read this plot from the bottom to the top where every red point is a data

1944
01:48:47,360 --> 01:48:48,360
sample.

1945
01:48:48,480 --> 01:48:50,460
So you are merging this progressively.

1946
01:48:50,460 --> 01:48:54,800
So eventually, you are going to form a tree structure called dendrogram.

1947
01:48:54,800 --> 01:49:01,180
So this diagram explains how progressively those data points are merged from more cluster

1948
01:49:01,180 --> 01:49:02,400
to fewer clusters.

1949
01:49:02,400 --> 01:49:07,720
So it's going to be a tree-like diagram, records the sequence of merging.

1950
01:49:07,720 --> 01:49:14,560
So this diagram, you probably find that sometimes two merges happen at the same time.

1951
01:49:14,560 --> 01:49:17,400
In practice, this is possible for HAC.

1952
01:49:17,400 --> 01:49:23,320
But for this course, only for this course, let's make it very simple.

1953
01:49:23,320 --> 01:49:24,640
Single merge every time.

1954
01:49:24,640 --> 01:49:25,640
So this won't be happening.

1955
01:49:25,640 --> 01:49:30,880
So for example questions, let's stick to the merging that once at a time.

1956
01:49:30,880 --> 01:49:33,280
Because I don't want to make it complicated.

1957
01:49:33,280 --> 01:49:37,880
But you should know that in practice, it's possible to merge multiple in one time.

1958
01:49:37,880 --> 01:49:41,440
For example, if they are the same distance, you should merge them together.

1959
01:49:41,480 --> 01:49:46,960
In general, you will have a tree structure like this, which records the sequence of merging.

1960
01:49:46,960 --> 01:49:52,560
And a good point is that if you have this diagram, I can name any k.

1961
01:49:52,560 --> 01:49:55,680
I can say I have three clusters, I want four clusters.

1962
01:49:55,680 --> 01:50:01,900
You can simply do a cut at the level that I have only k cluster remains.

1963
01:50:01,900 --> 01:50:07,960
So you can always trace back to any arbitrary number of clusters as the results.

1964
01:50:07,960 --> 01:50:11,840
So here, if I want two clusters, I just need to cut at this place.

1965
01:50:11,840 --> 01:50:18,640
So I will know that, well, this six will be one cluster, this three will be one cluster.

1966
01:50:18,640 --> 01:50:23,520
So you can do arbitrary number of k between one and the total number of samples.

1967
01:50:23,520 --> 01:50:26,640
You will know the exact membership.

1968
01:50:26,640 --> 01:50:29,620
So this is actually one thing good for HAC.

1969
01:50:29,620 --> 01:50:34,000
So in this case, you will know that the three will be the cluster one, the final six will

1970
01:50:34,000 --> 01:50:36,360
be the cluster two.

1971
01:50:36,360 --> 01:50:41,320
So let's see that for HAC, there will be the following advantages that k-means doesn't

1972
01:50:41,320 --> 01:50:42,320
have.

1973
01:50:42,320 --> 01:50:47,360
The first one is that you do not have to predefine the number of clusters to k.

1974
01:50:47,360 --> 01:50:51,280
Because like I mentioned, you can arbitrarily just do a cut.

1975
01:50:51,280 --> 01:50:56,800
You can always merge into one and do another kind of re-selection of the cluster without

1976
01:50:56,800 --> 01:50:59,080
re-running the whole algorithm.

1977
01:50:59,080 --> 01:51:01,020
k-means, you have to re-run.

1978
01:51:01,020 --> 01:51:04,360
So this is the advantages in the first place.

1979
01:51:04,360 --> 01:51:11,280
Really any clustering result will be obtained if you have this dendrogram, so you can cut

1980
01:51:11,280 --> 01:51:12,880
it anywhere.

1981
01:51:12,880 --> 01:51:14,880
And the result will be deterministic.

1982
01:51:14,880 --> 01:51:19,800
So no randomness, unlike k-means, you get different results.

1983
01:51:19,800 --> 01:51:23,360
For HAC, the result is always the same.

1984
01:51:23,360 --> 01:51:29,320
Once you determine the distance metric, same distance metric, same data, you always get

1985
01:51:29,320 --> 01:51:31,540
the same result.

1986
01:51:31,540 --> 01:51:33,360
So that's something good for HAC.

1987
01:51:33,360 --> 01:51:40,160
So now, when we say distance metric, we haven't really defined what is the distance metric

1988
01:51:40,160 --> 01:51:43,720
not between points, but between two clusters.

1989
01:51:43,720 --> 01:51:48,040
This is something we have to specify for HAC, which is very important.

1990
01:51:48,040 --> 01:51:54,720
So for this class, we're going to learn four, there are many, but we're going to learn four

1991
01:51:54,720 --> 01:51:59,160
different kind of distance metrics between two clusters.

1992
01:51:59,160 --> 01:52:00,160
So what are the four?

1993
01:52:00,160 --> 01:52:04,360
The first one is called a single linkage, or we call it a mean distance.

1994
01:52:04,360 --> 01:52:05,360
So what is the mean distance?

1995
01:52:05,360 --> 01:52:09,200
Well, imagine you have multiple points in single clusters, right?

1996
01:52:09,200 --> 01:52:12,280
I want to measure the distance between these two clusters.

1997
01:52:12,280 --> 01:52:19,120
I can always use the minimum distance between any pair of two samples coming from each cluster

1998
01:52:19,120 --> 01:52:23,000
to represent the distance between the two clusters.

1999
01:52:23,000 --> 01:52:27,960
So in this case, I got four points in each cluster, the yellow distance will be my mean

2000
01:52:27,960 --> 01:52:28,960
distance.

2001
01:52:28,960 --> 01:52:31,280
Make sense?

2002
01:52:31,280 --> 01:52:36,640
So we can specify that H, let's take the mean distance to represent a cluster distance.

2003
01:52:36,640 --> 01:52:37,800
They were called single linkage.

2004
01:52:37,800 --> 01:52:42,440
Of course, we can do the opposite, we can say, well, I want to use the maximum, right?

2005
01:52:42,440 --> 01:52:48,000
So the maximum distance will be the maximum out of all choices of two points from each

2006
01:52:48,000 --> 01:52:49,000
cluster, right?

2007
01:52:49,000 --> 01:52:54,760
So this will be the max distance, or we could complete linkage, either way.

2008
01:52:54,760 --> 01:52:56,600
All right?

2009
01:52:56,600 --> 01:52:57,600
Make sense?

2010
01:52:57,640 --> 01:52:59,560
Easy to understand, right?

2011
01:52:59,560 --> 01:53:00,560
And then you can do something else.

2012
01:53:00,560 --> 01:53:07,360
I can say, well, I want to take the average, so I try all the possible distance, okay?

2013
01:53:07,360 --> 01:53:10,720
You just pair up every possible pairs, right?

2014
01:53:10,720 --> 01:53:14,920
And I take average, which is called the average linkage.

2015
01:53:14,920 --> 01:53:21,520
So average is taken by all of the pairs coming from each of the cluster, which is also possible,

2016
01:53:21,520 --> 01:53:24,360
and this will be the average linkage.

2017
01:53:24,360 --> 01:53:31,360
Or you can see that, okay, I just find, I just compute a centroid for each cluster first,

2018
01:53:31,360 --> 01:53:35,320
and I directly measure the distance between the two centroids.

2019
01:53:35,320 --> 01:53:38,680
This will be the centroid distance, okay?

2020
01:53:38,680 --> 01:53:45,080
So we just need to learn the four, max, minimum, average, or center, right?

2021
01:53:45,080 --> 01:53:48,760
One of the four, well, could be appears in the exam, right?

2022
01:53:48,760 --> 01:53:54,360
So basically, all of the four are qualified distance metric, but which one to use, it

2023
01:53:54,360 --> 01:53:55,360
depends on the questions.

2024
01:53:55,360 --> 01:53:59,560
I will specify the distance metric, if I give you a question, like, accuracy, all right?

2025
01:53:59,560 --> 01:54:02,200
Okay, but let's see how we can use that, right?

2026
01:54:02,200 --> 01:54:07,080
So just now, I showed the algorithms, I defined the distance metric, but let's see some example,

2027
01:54:07,080 --> 01:54:08,080
right?

2028
01:54:08,080 --> 01:54:10,360
So let's start with the visual example first.

2029
01:54:10,360 --> 01:54:15,560
Here, if you do two-dimensional space, you can always plot them in the 2D space.

2030
01:54:15,960 --> 01:54:21,520
Suppose I have these five points, represented as the ABCDE, right?

2031
01:54:21,520 --> 01:54:29,280
So if this is my five points, I should always start it by treating every point as an individual

2032
01:54:29,280 --> 01:54:30,280
cluster, right?

2033
01:54:30,280 --> 01:54:32,760
So that's the starting point of HHC.

2034
01:54:32,760 --> 01:54:38,880
And now, all of all the five points, right, I should pick the pair which gave me the minimum

2035
01:54:38,880 --> 01:54:40,760
distance, all right?

2036
01:54:40,760 --> 01:54:43,560
So that's how you do merging every time, right?

2037
01:54:43,560 --> 01:54:50,120
So here, the minimum distance will be the AC, so AC will be the first merge I make in

2038
01:54:50,120 --> 01:54:51,120
my first iteration.

2039
01:54:51,120 --> 01:54:57,120
So A and C will be merged first, because they are the closest to each other.

2040
01:54:57,120 --> 01:55:02,520
And after that, I have the D, right, D is the second closest pair I have here, right?

2041
01:55:02,520 --> 01:55:07,960
So D will be the second merge, and then AC and B will be my third merge.

2042
01:55:07,960 --> 01:55:12,920
You see, it's like progressive merging, and every time you only merge a pair, right?

2043
01:55:12,920 --> 01:55:15,800
So you know, AC and B will be the next one.

2044
01:55:15,800 --> 01:55:19,220
So then, eventually, everything will become a single cluster, right?

2045
01:55:19,220 --> 01:55:27,280
So these will be so-called nested clusters as the progressive merging generated by HHC,

2046
01:55:27,280 --> 01:55:28,280
right?

2047
01:55:28,280 --> 01:55:34,680
So until, you can finish until only one cluster is left, or if the question specify one K

2048
01:55:34,680 --> 01:55:40,360
cluster, you can stop when you reach the K cluster remaining, all right?

2049
01:55:40,360 --> 01:55:45,640
So this is like a visual example, which we plot in 2D to show how HHC works for five

2050
01:55:45,640 --> 01:55:47,200
points, right?

2051
01:55:47,200 --> 01:55:54,160
So in the end, after you're done with HHC, you can either show this kind of a membership

2052
01:55:54,160 --> 01:55:56,160
diagram or a dendrogram, right?

2053
01:55:56,160 --> 01:56:02,640
So dendrogram will be equivalent to represent how you progressively merge the pairs of clusters,

2054
01:56:02,640 --> 01:56:03,640
right?

2055
01:56:03,640 --> 01:56:08,480
You can see the correspondence here, like the blue here is blue, red is red, yellow,

2056
01:56:08,480 --> 01:56:09,620
sorry, green is green.

2057
01:56:09,620 --> 01:56:14,420
So you can tell from the dendrogram that blue is the first merge, you know, green is

2058
01:56:14,420 --> 01:56:19,640
the second, red is the third, and the dashed black is the last merge, right?

2059
01:56:19,640 --> 01:56:27,100
So dendrogram will contain every information about this progressive merging, and once you

2060
01:56:27,100 --> 01:56:29,940
show it, you can do arbitrary K, right?

2061
01:56:29,940 --> 01:56:37,060
If you want to know the cluster result for any K, you can do a cut.

2062
01:56:37,060 --> 01:56:38,500
Make sense?

2063
01:56:38,500 --> 01:56:41,100
So far, so good?

2064
01:56:41,100 --> 01:56:42,100
Any questions?

2065
01:56:42,100 --> 01:56:44,100
Okay, great.

2066
01:56:44,100 --> 01:56:47,020
Now, that was the visual, right?

2067
01:56:47,020 --> 01:56:51,580
Let's think about if I don't, if you don't draw the diagram, but just getting some numbers

2068
01:56:51,580 --> 01:56:54,420
numerically, how can you do HHC, right?

2069
01:56:54,420 --> 01:57:01,860
So here's one example, I make it simple, just four points, but every point will have two

2070
01:57:01,860 --> 01:57:02,860
dimensional, right?

2071
01:57:02,860 --> 01:57:06,580
Every point will have feature one, X, and feature two, Y.

2072
01:57:06,700 --> 01:57:11,420
So our job is to do HHC for the following four points.

2073
01:57:11,420 --> 01:57:14,840
So what's the step one?

2074
01:57:14,840 --> 01:57:18,420
The step one will be figure out the first merge, right?

2075
01:57:18,420 --> 01:57:20,260
Because initialization is best straightforward.

2076
01:57:20,260 --> 01:57:21,620
Every sample is an individual cluster.

2077
01:57:21,620 --> 01:57:27,260
So no discussion on initialization, but step one will be determine the first merge.

2078
01:57:27,260 --> 01:57:29,540
So how can I determine my first merge?

2079
01:57:29,540 --> 01:57:36,540
Closest, yes, but how can I determine which pair is the closest pair?

2080
01:57:37,500 --> 01:57:38,620
Distance, yes.

2081
01:57:38,620 --> 01:57:45,540
So what are the distance I need to compute and make a comparison?

2082
01:57:45,540 --> 01:57:48,980
So all of you are right, I'm just trying to push the limit.

2083
01:57:48,980 --> 01:57:52,980
So now we know that we need to find the closest, we need the distance, but how many distance

2084
01:57:52,980 --> 01:57:57,500
we need to find?

2085
01:57:57,500 --> 01:58:01,240
You need every pair, basically, you need every pair of the points, right?

2086
01:58:01,240 --> 01:58:07,520
So which basically will give you a, oh, sorry, before that, okay, before that, remember

2087
01:58:07,520 --> 01:58:10,480
that we need to determine the distance metric, sorry, I forgot this.

2088
01:58:10,480 --> 01:58:15,760
So before you run everything, anything you need to first figure out what's the distance

2089
01:58:15,760 --> 01:58:16,820
metric, all right?

2090
01:58:16,820 --> 01:58:18,420
So I almost forgot this.

2091
01:58:18,420 --> 01:58:20,360
So we see that we have learned four, right?

2092
01:58:20,360 --> 01:58:26,460
We only learned four, which is the mean, the max, the central distance, and the average

2093
01:58:26,460 --> 01:58:27,640
distance, right?

2094
01:58:27,640 --> 01:58:32,800
So for this example, let's just pick the central distance, right?

2095
01:58:32,800 --> 01:58:36,840
Different distance will give you different kind of table, for sure, right?

2096
01:58:36,840 --> 01:58:42,520
So if it's a central distance, what you need is a distance table like this.

2097
01:58:42,520 --> 01:58:48,480
If it's four points, you are going to have a four by four matrix, okay?

2098
01:58:48,480 --> 01:58:51,560
But actually, many things are just repeating, right?

2099
01:58:51,560 --> 01:58:55,480
So one thing can tell is that the diagonal doesn't matter, because diagonal is always

2100
01:58:55,480 --> 01:58:56,480
zero, right?

2101
01:58:56,520 --> 01:58:59,240
From one to one, two to two, it doesn't matter.

2102
01:58:59,240 --> 01:59:01,520
So I don't care about the diagonal.

2103
01:59:01,520 --> 01:59:05,500
The other thing you figure out is that this matrix is actually symmetric, right?

2104
01:59:05,500 --> 01:59:12,000
So 0.14 equal to 0.14, here equal to here, because distance between two and three is

2105
01:59:12,000 --> 01:59:14,000
always equal to three and two, right?

2106
01:59:14,000 --> 01:59:22,360
So actually, the entire matrix will be symmetric, and to this end, you only need half of that,

2107
01:59:22,360 --> 01:59:23,360
right?

2108
01:59:23,600 --> 01:59:28,840
So in this case, you only need, how many, six numbers, right?

2109
01:59:28,840 --> 01:59:32,940
Except for the diagonal, it's called the half.

2110
01:59:32,940 --> 01:59:42,120
Those numbers are the distance you need, and you need to find out the minimum, make sense?

2111
01:59:42,120 --> 01:59:47,040
Okay, but for central distance, you can treat every point at the center, because there's

2112
01:59:47,040 --> 01:59:49,480
a single point in the cluster, right?

2113
01:59:49,480 --> 01:59:50,720
So you need to have this table.

2114
01:59:50,720 --> 01:59:57,600
So now, all of the six distance, you need to find out which one is the smallest, all

2115
01:59:57,600 --> 01:59:59,960
of the six, right?

2116
01:59:59,960 --> 02:00:03,760
And here, 0.14 is the minimum, right?

2117
02:00:03,760 --> 02:00:10,200
And if this is a minimum, that means my first merge will be one and two.

2118
02:00:10,200 --> 02:00:11,200
Just find the position, right?

2119
02:00:11,200 --> 02:00:16,280
This position is actually at the second row, first column, that means one and two must

2120
02:00:16,280 --> 02:00:19,160
be merged first, all right?

2121
02:00:19,160 --> 02:00:24,600
So my first merge will be one and two, okay?

2122
02:00:24,600 --> 02:00:25,600
Because it's the smallest, right?

2123
02:00:25,600 --> 02:00:26,600
All choices.

2124
02:00:26,600 --> 02:00:27,600
Okay.

2125
02:00:27,600 --> 02:00:34,080
Now, if it's a central distance, one thing you have to do is recompute the new center,

2126
02:00:34,080 --> 02:00:35,080
right?

2127
02:00:35,080 --> 02:00:39,320
Because when you merge things, the center will change, all right?

2128
02:00:39,320 --> 02:00:40,640
So you have to recompute the center.

2129
02:00:40,640 --> 02:00:46,440
So in this case, how to compute the center of one and two, just take average, right?

2130
02:00:46,440 --> 02:00:52,480
So you take average, you will get a new center, and then this will be your new center, right?

2131
02:00:52,480 --> 02:00:55,800
And then you can update this table, all right?

2132
02:00:55,800 --> 02:00:59,600
But you update the table, you probably don't need to update everything.

2133
02:00:59,600 --> 02:01:03,240
For example, the 0.5 will be the same, right?

2134
02:01:03,240 --> 02:01:07,720
Because it's still between three and four, so you need to update these two numbers.

2135
02:01:07,720 --> 02:01:12,920
Any distance involve the new centroid, we have to update, recompute them.

2136
02:01:12,920 --> 02:01:15,840
So using this distance, right?

2137
02:01:15,840 --> 02:01:20,080
So this will be the new table you got, and your table will be smaller because you have

2138
02:01:20,080 --> 02:01:22,240
fewer clusters.

2139
02:01:22,240 --> 02:01:28,560
So now in the second merge, you need to compare the three numbers here and figure out which

2140
02:01:28,560 --> 02:01:31,200
one is the smallest, all right?

2141
02:01:31,200 --> 02:01:33,840
So here, 0.5 is the smallest.

2142
02:01:33,840 --> 02:01:39,560
So you know that the second merge will be between three and four, right?

2143
02:01:39,560 --> 02:01:44,160
So between three and four will be the second merge, and you need to compute the new center,

2144
02:01:44,160 --> 02:01:46,480
which is actually here.

2145
02:01:46,480 --> 02:01:50,200
And now the final one is just the single merge, right?

2146
02:01:50,200 --> 02:01:52,920
And you just merge the remaining.

2147
02:01:52,920 --> 02:01:56,000
So you get the final centroid, then drop down.

2148
02:01:56,000 --> 02:01:59,140
You have this kind of dendrogram, right?

2149
02:01:59,140 --> 02:02:07,280
So this is how you can do the HAC if it is a centroid distance, all right?

2150
02:02:07,280 --> 02:02:09,040
Make sense?

2151
02:02:09,040 --> 02:02:14,240
So basically, it's like compute the distance matrix, forget about half, forget about the

2152
02:02:14,240 --> 02:02:21,960
diagonal line, look at the upper or lower triangle, find the minima, determine the merge,

2153
02:02:21,960 --> 02:02:24,680
and then update the centroid after you do the merge.

2154
02:02:24,680 --> 02:02:27,200
Update the table, redo everything, right?

2155
02:02:27,200 --> 02:02:31,060
Find the minima, determine the second merge, and do that again, right?

2156
02:02:31,060 --> 02:02:36,000
So this is actually the standard way of doing HAC, but the order matters.

2157
02:02:36,000 --> 02:02:37,480
Which one merge first?

2158
02:02:37,480 --> 02:02:38,480
Which one merge second?

2159
02:02:38,920 --> 02:02:39,920
This one matters.

2160
02:02:39,920 --> 02:02:42,360
You have to make sure this is correct, OK?

2161
02:02:42,360 --> 02:02:46,160
Now, this is actually an example for centroid distance, right?

2162
02:02:46,160 --> 02:02:49,600
Let's try another possibility, which is the average distance.

2163
02:02:49,600 --> 02:02:55,920
So if it's average distance, it's actually a little bit simpler, I would say.

2164
02:02:55,920 --> 02:02:57,640
So starting point is the same.

2165
02:02:57,640 --> 02:03:00,500
You're just treating everything as an individual cluster.

2166
02:03:00,500 --> 02:03:02,500
The very first step will be no change.

2167
02:03:02,500 --> 02:03:05,040
You still merge 0.14, all right?

2168
02:03:05,600 --> 02:03:09,520
So you merge that, you will get the new table.

2169
02:03:09,520 --> 02:03:16,760
But here the thing is that if you do the merge of 1 and 2, now we are using the average distance.

2170
02:03:16,760 --> 02:03:21,360
You don't have to compute the centroid, because the centroid doesn't matter, right?

2171
02:03:21,360 --> 02:03:24,640
What I care about is the average distance.

2172
02:03:24,640 --> 02:03:31,020
So what will be the new average distance between the new cluster and other points?

2173
02:03:31,020 --> 02:03:39,360
So for example, I want to compute the distance between the new cluster, 1 and 2, with 3, right?

2174
02:03:39,360 --> 02:03:48,180
So the average distance is actually the average between 1 and 3, and the average between 2 and 3.

2175
02:03:48,180 --> 02:03:54,460
In fact, you can directly take the average, 1, 3 is here, and 2, 3 is here.

2176
02:03:54,460 --> 02:03:58,860
So you can take the average of these two numbers and update the table directly.

2177
02:04:01,960 --> 02:04:02,460
Make sense?

2178
02:04:02,460 --> 02:04:06,140
Because it's no longer centroid distance, it's average distance.

2179
02:04:06,140 --> 02:04:14,660
So mathematically, the average distance between the new center to adding point will be the average between 1, 3 and 2, 3, right?

2180
02:04:14,660 --> 02:04:16,900
That's the definition.

2181
02:04:16,900 --> 02:04:19,540
And you can get this number from your old table.

2182
02:04:19,540 --> 02:04:24,580
You just need to do update without recomputing the centroid, because there's no centroid.

2183
02:04:24,580 --> 02:04:26,860
You don't have to use centroid, right?

2184
02:04:26,860 --> 02:04:31,060
So be careful about the distance metric specified by the question.

2185
02:04:31,060 --> 02:04:36,100
It determines the way you update the table, right?

2186
02:04:36,100 --> 02:04:38,660
OK, any questions here?

2187
02:04:41,820 --> 02:04:42,460
Make sense?

2188
02:04:42,460 --> 02:04:46,940
OK, average distance is defined by all the pairs, right?

2189
02:04:46,940 --> 02:04:51,900
If you have one cluster which is 1, 2, the other cluster is 3, you only have two pairs, right?

2190
02:04:51,900 --> 02:04:53,660
1, 3 and 2, 3.

2191
02:04:53,660 --> 02:04:55,740
So you just need to take average over 2.

2192
02:04:55,740 --> 02:04:59,860
You kind of figure out the new average distance, right?

2193
02:04:59,860 --> 02:05:04,100
So that's how you update the table, but once the table is updated, you'll find again.

2194
02:05:04,100 --> 02:05:05,460
Same thing for the other, right?

2195
02:05:05,460 --> 02:05:12,220
So if it's like 1, 2 with 4, you will be the average of 1, 4, and 2, 4, right?

2196
02:05:12,220 --> 02:05:13,620
Very similar.

2197
02:05:13,620 --> 02:05:20,060
But that's how you update the table to this one, OK?

2198
02:05:20,060 --> 02:05:25,700
Now, out of three numbers, again, you have to determine which is the second merge, right?

2199
02:05:25,700 --> 02:05:28,740
But of course, this is still here.

2200
02:05:28,740 --> 02:05:31,340
So the second merge is like 3 and 4.

2201
02:05:31,340 --> 02:05:36,740
You just need to merge 3 and 4, and then the last merge will be everything together, right?

2202
02:05:36,740 --> 02:05:44,580
But if you want to work out this number, you can actually do 1, 2, and 3, 4 as the average

2203
02:05:44,580 --> 02:05:48,900
of 1, 2, and 3, and then 1, 2, and 4.

2204
02:05:48,900 --> 02:05:52,300
But actually, you don't need this because the final merge will always be unique, right?

2205
02:05:52,300 --> 02:05:55,260
Just merge everything together, all right?

2206
02:05:55,260 --> 02:05:59,260
But the update of the table, you must be careful, right?

2207
02:05:59,260 --> 02:06:03,700
You stick to the definition of average link, and then update the table correspondingly,

2208
02:06:03,700 --> 02:06:04,700
right?

2209
02:06:04,700 --> 02:06:09,540
So this is what you need, and that's how you can figure out the final merge.

2210
02:06:09,540 --> 02:06:10,540
OK?

2211
02:06:10,540 --> 02:06:16,220
I hope these two examples help, right?

2212
02:06:16,220 --> 02:06:18,700
Of course, you can try the min and max.

2213
02:06:18,700 --> 02:06:20,300
Actually min and max are even simpler, right?

2214
02:06:20,300 --> 02:06:23,860
If you tried it, you will find that, you know, you don't have to take average, you take the

2215
02:06:23,860 --> 02:06:24,860
min, all right?

2216
02:06:25,460 --> 02:06:30,220
But no matter what distance metric specified by the question, I wish you can do the same

2217
02:06:30,220 --> 02:06:35,220
thing to complete the dendrogram and make sure the merging are in order.

2218
02:06:35,220 --> 02:06:36,220
OK?

2219
02:06:36,220 --> 02:06:39,180
All right, try it.

2220
02:06:39,180 --> 02:06:41,460
If you have anything unclear, you can get back to me.

2221
02:06:41,460 --> 02:06:43,140
I'm happy to explain this again.

2222
02:06:43,140 --> 02:06:47,660
All right, but this is all you need to know for HVAC.

2223
02:06:47,660 --> 02:06:53,000
And again, the very last thing for HVAC is understanding the pro and cons, right?

2224
02:06:53,000 --> 02:06:59,280
So if you compare k-means and HVAC, both of them can do clustering, OK?

2225
02:06:59,280 --> 02:07:05,240
Both of them are unsupervised, but if you compare the two, k-means will be simpler and

2226
02:07:05,240 --> 02:07:07,240
cheaper, right?

2227
02:07:07,240 --> 02:07:11,360
Because HVAC, we have to store all the, you know, intermediate clustering result, we have

2228
02:07:11,360 --> 02:07:12,920
to construct the whole dendrogram.

2229
02:07:12,920 --> 02:07:17,840
So comparing in terms of the efficiency, k-means is simpler.

2230
02:07:17,840 --> 02:07:22,720
But k-means has a lot of problems, like, you know, results, sensitive to initialization,

2231
02:07:22,720 --> 02:07:26,720
number of cluster must be predefined, all of them are pretty risky, right?

2232
02:07:26,720 --> 02:07:28,520
HVAC is the opposite.

2233
02:07:28,520 --> 02:07:34,520
It will be deterministic, no randomness if the distance is determined, and you don't

2234
02:07:34,520 --> 02:07:35,520
really care about the k.

2235
02:07:35,520 --> 02:07:38,500
You can actually determine the k later, right?

2236
02:07:38,500 --> 02:07:40,840
So it's the exact opposite of k-means.

2237
02:07:40,840 --> 02:07:46,720
But, of course, it's going to be more expensive, right, because there's no free lunch.

2238
02:07:46,720 --> 02:07:48,800
If you want to enjoy something, you have to pay the price.

2239
02:07:48,800 --> 02:07:54,320
The price is that it's going to be more expensive in terms of memory, in terms of computation,

2240
02:07:54,320 --> 02:07:57,320
compared to k-means, right?

2241
02:07:57,320 --> 02:08:02,600
So it's going to trade-off between how much you want to pay and how much you want to enjoy,

2242
02:08:02,600 --> 02:08:03,600
right?

2243
02:08:03,600 --> 02:08:07,040
So you have to understand the trade-off between the two methods, OK?

2244
02:08:07,040 --> 02:08:11,720
So this is actually the comparison, and also the pro and cons for HVAC, right?

2245
02:08:11,720 --> 02:08:13,360
So but that's all.

2246
02:08:13,360 --> 02:08:16,680
For clustering, you only need to know the two, right?

2247
02:08:16,680 --> 02:08:22,440
k-means, HVAC, and you need to make sure you understand what is the definition, right?

2248
02:08:22,440 --> 02:08:24,200
And what is the distance metric?

2249
02:08:24,200 --> 02:08:27,240
How do you apply k-means and HVAC?

2250
02:08:27,240 --> 02:08:28,240
What's the goal?

2251
02:08:28,240 --> 02:08:30,900
And what are you expecting at the end, right?

2252
02:08:30,900 --> 02:08:36,180
So these are the takeaway for unsupervised learning, all right?

2253
02:08:36,180 --> 02:08:39,120
So remember we have some questions in the beginning, right?

2254
02:08:39,120 --> 02:08:40,520
Now we should be able to answer, right?

2255
02:08:40,520 --> 02:08:41,520
So cluster.

2256
02:08:41,520 --> 02:08:42,520
What is cluster?

2257
02:08:42,520 --> 02:08:43,520
What is clustering?

2258
02:08:43,520 --> 02:08:48,960
So the difference between clustering and classification, main difference is supervised versus unsupervised,

2259
02:08:48,960 --> 02:08:49,960
right?

2260
02:08:49,960 --> 02:08:56,480
And k-means, clearly, you know that picking up the k, initialization of the cluster, all

2261
02:08:56,480 --> 02:08:58,240
of them need a metric.

2262
02:08:58,240 --> 02:09:05,120
All of them will be something undetermined, right, which depends on your heuristics.

2263
02:09:05,120 --> 02:09:08,920
And HVAC, the problem is actually the expensive cost.

2264
02:09:08,920 --> 02:09:15,000
In terms of memory, in terms of commutational, both of them are more expensive than k-means.

2265
02:09:15,000 --> 02:09:17,000
All right?

2266
02:09:17,000 --> 02:09:20,600
Okay, so that's all for unsupervised learning.

2267
02:09:20,600 --> 02:09:23,600
Sorry, that's all for clustering.

2268
02:09:23,600 --> 02:09:27,960
Any question before I move on to the very last topic, regression, which we're going

2269
02:09:27,960 --> 02:09:29,960
to cover today?

2270
02:09:29,960 --> 02:09:32,240
Everybody's clear?

2271
02:09:32,240 --> 02:09:34,160
Okay, good.

2272
02:09:34,160 --> 02:09:39,960
Now, we have 20 minutes, or maybe 25 minutes, I'm going to finish the regression.

2273
02:09:39,960 --> 02:09:45,480
So regression, well, some people can treat that as the supervised learning problem, right?

2274
02:09:45,480 --> 02:09:51,720
If you want to predict, say, the labels using the variables, but if you are treating this

2275
02:09:51,720 --> 02:09:58,000
as a line-feeding problem, or you want to feed a point with some kind of model, right?

2276
02:09:58,000 --> 02:10:01,680
This one is actually unsupervised, so it depends on how you view that.

2277
02:10:01,680 --> 02:10:03,760
So let's first talk about the concept.

2278
02:10:03,760 --> 02:10:08,040
One is regression, and I'm going to introduce you to the most simple regression, like linear

2279
02:10:08,040 --> 02:10:13,600
regression, and see how we can do it, and then some examples.

2280
02:10:13,600 --> 02:10:18,160
And I'm also going to, if I have time, I'm also going to show you the derivation of linear

2281
02:10:18,160 --> 02:10:21,180
regression, but this part is for interest, right?

2282
02:10:21,180 --> 02:10:25,600
So I won't test it on that, but I want to show you that linear regression and learning

2283
02:10:25,600 --> 02:10:26,840
is very simple.

2284
02:10:26,840 --> 02:10:29,280
You don't have to do a gradient descent.

2285
02:10:29,280 --> 02:10:32,680
There's a closed form solution for linear regression, all right?

2286
02:10:32,680 --> 02:10:39,700
So concept-wise, you need to understand the difference, again, between regression and

2287
02:10:39,700 --> 02:10:42,520
the classification, okay?

2288
02:10:42,520 --> 02:10:47,480
And then for training the regressor, what is the training loss, or in other words, what

2289
02:10:47,480 --> 02:10:50,160
are the objectives we are minimizing here?

2290
02:10:50,160 --> 02:10:55,100
So these are the two important things to understand the linear regression.

2291
02:10:55,100 --> 02:10:57,800
So let's recap on the classification, right?

2292
02:10:57,800 --> 02:11:02,480
We say that classification is nothing but let the computer decide whether the image

2293
02:11:02,480 --> 02:11:03,480
is capable, right?

2294
02:11:03,480 --> 02:11:04,480
The simple example.

2295
02:11:04,480 --> 02:11:11,680
But one thing you realize is that it's always a mapping from data to discrete labels.

2296
02:11:11,680 --> 02:11:17,420
Discrete means yes or no, class one, class two, class three, right?

2297
02:11:17,420 --> 02:11:21,520
Every output is either one or two, either zero or one.

2298
02:11:21,520 --> 02:11:23,960
So that's like discrete labels.

2299
02:11:23,960 --> 02:11:26,440
But sometimes we want to do something final, right?

2300
02:11:26,440 --> 02:11:31,520
We want to do something, say, well, I'm not interested in whether it's a cat, but I wish

2301
02:11:31,520 --> 02:11:35,560
to know what's the likelihood the image is a cat, right?

2302
02:11:35,560 --> 02:11:38,140
How likely is this cat, how likely is the dog?

2303
02:11:38,140 --> 02:11:43,960
So what I need is a percentage, or it's a likelihood, which is represented not as a

2304
02:11:43,960 --> 02:11:49,720
discrete label, but as some percentage, some kind of real number, right?

2305
02:11:49,720 --> 02:11:52,560
So this is also needed in practice.

2306
02:11:52,560 --> 02:11:58,660
But if you are looking for a likelihood which is a continuous quantity, this problem is

2307
02:11:58,660 --> 02:12:02,080
no longer a classification problem, all right?

2308
02:12:02,080 --> 02:12:07,200
So the difference is that classification output is always discrete.

2309
02:12:07,200 --> 02:12:12,280
Now if you're looking for a continuous quantity as the output, it's going to be a regression

2310
02:12:12,280 --> 02:12:13,280
problem, all right?

2311
02:12:13,280 --> 02:12:19,720
So the difference here is that classification always predicts a discrete label.

2312
02:12:19,720 --> 02:12:22,400
Regression is going to predict a continuous quantity, all right?

2313
02:12:22,440 --> 02:12:24,200
That's the major difference.

2314
02:12:24,200 --> 02:12:33,240
So of course, in some sense, right, you can actually also form or maybe formulate a classification

2315
02:12:33,240 --> 02:12:35,640
indirectly as a regression.

2316
02:12:35,640 --> 02:12:43,280
For example, I can say that, well, I can determine the class based on, let's say, the probability

2317
02:12:43,280 --> 02:12:48,960
for a discrete label, right, which is also something we do in practice, but the intermediate

2318
02:12:48,960 --> 02:12:49,960
result doesn't matter.

2319
02:12:49,960 --> 02:12:54,440
Whether it's a classification or regression depends on the final result.

2320
02:12:54,440 --> 02:12:58,720
So finally if I need a label, discrete label, that's a classification.

2321
02:12:58,720 --> 02:13:04,200
Finally if I need a continuous quantity, that's going to be regression, no matter what I do

2322
02:13:04,200 --> 02:13:05,580
in middle, right?

2323
02:13:05,580 --> 02:13:07,600
So this is the principle.

2324
02:13:07,600 --> 02:13:14,200
And the way we evaluate the effectiveness for classification for regression is also

2325
02:13:14,200 --> 02:13:15,860
different, all right?

2326
02:13:15,860 --> 02:13:22,300
So if you're doing a classification, normally we use like, say, accuracy to quantify or

2327
02:13:22,300 --> 02:13:26,100
to evaluate whether my results are good or bad, right?

2328
02:13:26,100 --> 02:13:31,700
So the personage can be, the accuracy can be the person page of the correctly identified

2329
02:13:31,700 --> 02:13:35,180
sample out of all predictions, right?

2330
02:13:35,180 --> 02:13:38,000
So this is also something you need for your mini-project, right?

2331
02:13:38,000 --> 02:13:42,420
So when you have some results, when you have some validation result, you need to tell me

2332
02:13:42,420 --> 02:13:43,740
what's the accuracy, right?

2333
02:13:43,740 --> 02:13:48,660
So accuracy will be the measurement for classification, all right?

2334
02:13:48,660 --> 02:13:51,420
But for regression, it's different.

2335
02:13:51,420 --> 02:13:54,620
Regression because you are outputting something continuous.

2336
02:13:54,620 --> 02:13:58,140
You cannot say this is true or false, right or wrong, right?

2337
02:13:58,140 --> 02:14:04,340
Because the only way you can measure is like, how close my prediction is comparing to the

2338
02:14:04,340 --> 02:14:05,820
ground truth.

2339
02:14:05,820 --> 02:14:10,380
So you need also a continuous quantity to evaluate the correctness.

2340
02:14:10,380 --> 02:14:15,260
So the commonly used accuracy is something like root mean square error, right?

2341
02:14:15,260 --> 02:14:21,580
Something like the L2 long distance between my prediction and the ground truth.

2342
02:14:21,580 --> 02:14:23,420
Of course, it's the smaller the better, right?

2343
02:14:23,420 --> 02:14:25,100
But it won't be person page.

2344
02:14:25,100 --> 02:14:30,780
It will be some quantity that measures the deviation from your output and ground truth.

2345
02:14:30,780 --> 02:14:36,460
So you can see that because the output is different, the way we evaluate the results

2346
02:14:36,460 --> 02:14:38,040
also different.

2347
02:14:38,040 --> 02:14:40,240
So be careful when you do the project, right?

2348
02:14:40,240 --> 02:14:45,160
Make sure you are reporting the right metric to evaluate you more.

2349
02:14:45,160 --> 02:14:47,600
Depends on what problem you're solving, okay?

2350
02:14:47,600 --> 02:14:49,440
So this is one of the difference.

2351
02:14:49,440 --> 02:14:53,120
So then what is the regression problem look like?

2352
02:14:53,120 --> 02:14:57,920
Well, for regression, we are predicting a scalar, right?

2353
02:14:57,920 --> 02:15:01,820
So basically, I want to output some kind of quantity continuous.

2354
02:15:01,820 --> 02:15:06,040
So the output will be a number that something like a real number.

2355
02:15:06,040 --> 02:15:08,320
So the output will be the scalar.

2356
02:15:08,320 --> 02:15:10,020
The input may not be a scalar.

2357
02:15:10,020 --> 02:15:11,920
The input can be a vector, right?

2358
02:15:11,920 --> 02:15:15,800
So just now we say that you can talk about d-dimensional vector.

2359
02:15:15,800 --> 02:15:18,220
So your sample will have d numbers, okay?

2360
02:15:18,220 --> 02:15:21,100
But the output will always be a scalar.

2361
02:15:21,100 --> 02:15:24,220
The input can be either a scalar or a vector.

2362
02:15:24,220 --> 02:15:26,960
Depends on your data dimension, right?

2363
02:15:26,960 --> 02:15:31,320
So a linear regression, so we only learn linear regression, by the way.

2364
02:15:31,320 --> 02:15:34,020
There could be other more complicated regression, right?

2365
02:15:34,020 --> 02:15:41,260
But if it's a linear regression, the way we predict the output is by using a linear

2366
02:15:41,260 --> 02:15:43,180
function, something like this.

2367
02:15:43,180 --> 02:15:49,260
We use a linear function to multiply my w, which is the weights of the linear regressor,

2368
02:15:49,260 --> 02:15:54,340
with my x plus the offset or bias term, right?

2369
02:15:54,340 --> 02:15:58,340
So this is how you can formulate a regressor to predict my y.

2370
02:15:58,340 --> 02:16:04,420
So here, the w is also a d-dimensional vector, okay?

2371
02:16:04,420 --> 02:16:08,660
So I hope all of you know what is this t, superscript t, right?

2372
02:16:08,660 --> 02:16:10,820
It's called transpose, okay?

2373
02:16:10,820 --> 02:16:16,460
So if you do w transpose x, it's like inner product of the two, okay?

2374
02:16:16,460 --> 02:16:19,540
So all of you know what is inner product, like you have two vectors.

2375
02:16:19,540 --> 02:16:22,620
You want to do inner product, how do you do that?

2376
02:16:22,620 --> 02:16:28,780
It's like you do w1 times x1 plus w2 times x2 plus w3 times x3, right?

2377
02:16:28,780 --> 02:16:34,460
You sum together, the output will be a scalar, okay?

2378
02:16:34,460 --> 02:16:36,380
So do you know this?

2379
02:16:36,380 --> 02:16:40,900
If you don't know this, I can explain, but I just want you to know that, are you familiar

2380
02:16:40,900 --> 02:16:43,500
with the notation here?

2381
02:16:43,500 --> 02:16:49,260
t stands for transpose, means that I'm going to make my column vector to a row vector,

2382
02:16:49,260 --> 02:16:50,260
right?

2383
02:16:50,820 --> 02:16:57,460
And because of that, the output of the w transpose x will be a scalar, okay?

2384
02:16:57,460 --> 02:17:03,900
So w transpose x is equal to w inner product x, it's the same thing, right, but I use this

2385
02:17:03,900 --> 02:17:09,700
as a proper notation to represent a linear regression, okay?

2386
02:17:09,700 --> 02:17:16,820
So by doing so, I'm going to get a scalar plus a scalar that will make the y also a

2387
02:17:16,820 --> 02:17:17,820
scalar, right?

2388
02:17:17,820 --> 02:17:21,060
This is the linear regression form, all right?

2389
02:17:21,060 --> 02:17:26,060
But of course, we do not know exactly in the beginning what is w and b, right?

2390
02:17:26,060 --> 02:17:31,500
w and b will be the parameter of my model, my kind of linear regressor.

2391
02:17:31,500 --> 02:17:39,420
So there will be a training process to determine what is w and what is b, okay?

2392
02:17:39,420 --> 02:17:44,240
So the training process basically is to find the best w based on the training data.

2393
02:17:44,240 --> 02:17:50,240
So here the training data means you will have n points, where you use x, sorry, x,

2394
02:17:50,240 --> 02:17:55,920
y, x2, all the way to xn, where is the label y, y1, y2, and y, yn, right?

2395
02:17:55,920 --> 02:18:02,480
So this is actually what we need for training, or we need to do fitting of the line, right?

2396
02:18:02,480 --> 02:18:08,800
But later I can show you that, in fact, if you treat y together with x, right, this can

2397
02:18:08,800 --> 02:18:15,000
be formulated as unsupervised learning, because you can form the data as the, how should I

2398
02:18:15,000 --> 02:18:19,120
say, you can represent data as the merge of both x and y, right?

2399
02:18:19,120 --> 02:18:21,640
So there are two ways to view the regression, right?

2400
02:18:21,640 --> 02:18:27,920
Okay, but basically the training process refers to the process which we determine the w, or

2401
02:18:27,920 --> 02:18:30,680
we learn the w, right?

2402
02:18:30,680 --> 02:18:37,000
So here's like a few kind of progress showing that how we can do the training, right?

2403
02:18:37,000 --> 02:18:42,560
So like I said, every training process is always about minimize something, or maximize

2404
02:18:42,560 --> 02:18:45,880
something to find the right parameter, okay?

2405
02:18:45,880 --> 02:18:54,240
So here's the same thing, I want to find the parameter of w and b by minimizing the following

2406
02:18:54,240 --> 02:18:55,700
mean square error, right?

2407
02:18:55,700 --> 02:18:56,700
So what is this error?

2408
02:18:56,700 --> 02:19:04,440
This error is called the mean square error, which is between the ground truth y and the

2409
02:19:04,440 --> 02:19:06,440
predicted y, okay?

2410
02:19:06,880 --> 02:19:12,080
Just like we said that linear regression is going to predict the y using this linear function,

2411
02:19:12,080 --> 02:19:13,080
okay?

2412
02:19:13,080 --> 02:19:19,320
But what I want to do is that I want to find the best w and b such that my prediction is

2413
02:19:19,320 --> 02:19:22,520
very close to the ground truth.

2414
02:19:22,520 --> 02:19:27,000
That's why here you can see that the term here is exactly measuring the distance between

2415
02:19:27,000 --> 02:19:31,680
the prediction, this is my prediction, and the ground truth, okay?

2416
02:19:31,680 --> 02:19:36,420
I take the square, it still measures the distance, right, the L2 distance, I sum them up at the

2417
02:19:36,420 --> 02:19:37,900
average, right?

2418
02:19:37,900 --> 02:19:41,980
So this is why they call it mean, mean means the average.

2419
02:19:41,980 --> 02:19:44,520
Square error means the error is somehow squared, right?

2420
02:19:44,520 --> 02:19:45,780
So this is where the name comes from.

2421
02:19:45,780 --> 02:19:52,300
So basically, I want to minimize the overall error, all the samples, overall error, comparing

2422
02:19:52,300 --> 02:19:58,860
the prediction and the ground truth, I want to minimize it by finding the best w and b,

2423
02:19:58,860 --> 02:19:59,860
right?

2424
02:19:59,860 --> 02:20:03,620
So this is basically what linear regression learning is happening, right?

2425
02:20:03,620 --> 02:20:07,300
Why, what we're going to do, what we're going to minimize.

2426
02:20:07,300 --> 02:20:10,060
This is actually the mean square error we're going to minimize.

2427
02:20:10,060 --> 02:20:12,700
All right, so let's make it clear.

2428
02:20:12,700 --> 02:20:18,580
For exam purpose, what you need to know is that if I ask you, say, hey, what is the loss

2429
02:20:18,580 --> 02:20:20,100
we're using for linear regression?

2430
02:20:20,100 --> 02:20:24,340
You need to tell me, okay, the loss is mean square error, okay?

2431
02:20:24,340 --> 02:20:27,980
This is actually the form we're trying to minimize, and what are we learning?

2432
02:20:27,980 --> 02:20:33,100
We're learning the w and b, but that's it, okay?

2433
02:20:33,100 --> 02:20:34,860
How are we learning the w and b?

2434
02:20:34,860 --> 02:20:38,660
I'm going to talk about it, but that's not something we're going to test you, all right?

2435
02:20:38,660 --> 02:20:44,060
But you are more than welcome to learn because it's very interesting, I would say, all right?

2436
02:20:44,060 --> 02:20:48,380
So but before that, let's see one example, right?

2437
02:20:48,380 --> 02:20:55,040
What is actually happening when we do linear regression in the visual, visual example?

2438
02:20:55,040 --> 02:20:56,700
So consider the following data, right?

2439
02:20:56,700 --> 02:20:59,220
So this is actually the data we have.

2440
02:20:59,220 --> 02:21:02,860
We only consider x, y, two-dimensional, okay?

2441
02:21:02,860 --> 02:21:06,300
So every row here represents a data point.

2442
02:21:06,300 --> 02:21:10,540
So the data 1 is like 1, 1, data 2 is 2, 2, data 3 is 3, 1.3.

2443
02:21:10,540 --> 02:21:16,820
So every sample has one-dimensional x and one-dimensional y, okay?

2444
02:21:16,820 --> 02:21:21,100
Now I want to do linear regression over these data.

2445
02:21:21,100 --> 02:21:26,940
So like I mentioned, right, if it's two-dimensional data, I can plot that in a 2D space.

2446
02:21:26,940 --> 02:21:32,220
So now I plot these five points in the x, y plane, right?

2447
02:21:32,220 --> 02:21:36,020
The horizontal is actually the x, vertical is the y.

2448
02:21:36,020 --> 02:21:41,160
Every point will be plotted here, 1, 2, 3, 4, 5, right?

2449
02:21:41,160 --> 02:21:43,140
So this is my five points.

2450
02:21:43,140 --> 02:21:49,820
Now if I run linear regression to find a regressor for that, what I'm doing here is actually,

2451
02:21:49,820 --> 02:21:54,000
okay, of course, I need to first define, right?

2452
02:21:54,000 --> 02:21:58,260
I'm minimizing my mean square arrow and find a line such that the mean square arrow is

2453
02:21:58,260 --> 02:21:59,740
somehow minimized.

2454
02:21:59,740 --> 02:22:06,260
So here what I'm doing here is actually finding a line, the black one, which is fully determined

2455
02:22:06,260 --> 02:22:08,780
by w and b, okay?

2456
02:22:08,780 --> 02:22:09,780
Why?

2457
02:22:09,780 --> 02:22:13,860
Because in this case, w will be one-dimensional, it's a single number.

2458
02:22:13,860 --> 02:22:15,740
b is also a single number, right?

2459
02:22:15,740 --> 02:22:22,940
So the w will be the slope of an ingredient of this line, b will be the offset here of

2460
02:22:22,940 --> 02:22:24,300
the line.

2461
02:22:24,300 --> 02:22:28,140
So two numbers can uniquely determine a line in the plane, right?

2462
02:22:28,180 --> 02:22:35,260
So basically, I need to find the w and b such that I determine this line and this line should

2463
02:22:35,260 --> 02:22:43,220
minimize the mean square arrow, which are denoted as the distance, color distance in

2464
02:22:43,220 --> 02:22:44,620
the space.

2465
02:22:44,620 --> 02:22:52,540
Okay, so here's one thing very, very critical, this distance is not the distance between

2466
02:22:52,540 --> 02:22:57,260
the point and the line, but it's the distance vertically.

2467
02:22:57,260 --> 02:23:04,940
Okay, very important because later, I mean, not later, I mean, in week 12, okay, actually

2468
02:23:04,940 --> 02:23:08,300
very, very late, week 12, we're going to learn PCA, okay?

2469
02:23:08,300 --> 02:23:12,940
PCA, you will see something very similar, like reducing dimensionality.

2470
02:23:12,940 --> 02:23:19,180
We also want to minimize the distance, but that distance is different from this one because

2471
02:23:19,180 --> 02:23:23,140
that distance is the perpendicular distance.

2472
02:23:23,140 --> 02:23:24,420
I want to minimize the projection.

2473
02:23:24,420 --> 02:23:31,900
So here, the distance is only on y, that's why the distance is vertical, okay?

2474
02:23:31,900 --> 02:23:37,180
I wish to highlight this and that's the only, that's the major difference between regression

2475
02:23:37,180 --> 02:23:41,460
and also principle component analysis, all right?

2476
02:23:41,460 --> 02:23:46,700
Okay, so then the question is why, why it's vertical, why not perpendicular?

2477
02:23:46,700 --> 02:23:49,900
We have to go back to the loss function, right?

2478
02:23:49,900 --> 02:23:53,620
Remember, this is our loss function, okay?

2479
02:23:53,620 --> 02:23:54,620
What is this loss function?

2480
02:23:54,620 --> 02:24:02,780
The loss function is the difference between the predicted y and the ground truth y, okay?

2481
02:24:02,780 --> 02:24:06,340
Both sides are y, nothing to do with x, are y's.

2482
02:24:06,340 --> 02:24:13,780
That means the distance here are measured in the y domain, y-axis, okay?

2483
02:24:13,780 --> 02:24:15,820
And vertical is actually the y-axis.

2484
02:24:15,820 --> 02:24:20,540
That's why here the entire distance is defined as the vertical distance.

2485
02:24:20,540 --> 02:24:28,340
So in other words, in this plot, right, this y value is the ground truth y for my point

2486
02:24:28,340 --> 02:24:31,820
5, okay, because this is actually my point 5, right?

2487
02:24:31,820 --> 02:24:39,900
The line intersection here, this is the predicted y using my linear regressor, okay?

2488
02:24:39,900 --> 02:24:48,640
In other words, this point is my w times x5 plus b, this is my wi-fi.

2489
02:24:48,640 --> 02:24:54,460
So the distance between my wi-fi and w times x5 plus b, that's the distance we're talking

2490
02:24:54,460 --> 02:24:56,620
about, vertical distance.

2491
02:24:56,620 --> 02:25:01,220
So be very clear about that and I could give you some tricky question and if you get it

2492
02:25:01,220 --> 02:25:04,220
wrong then, you know, you have to think about this example, all right?

2493
02:25:04,220 --> 02:25:08,740
This is going to tell you that, you know, what is the distance, what is the right distance

2494
02:25:08,740 --> 02:25:11,620
to visualize in the 2D space, okay?

2495
02:25:11,620 --> 02:25:17,020
Think about it and, you know, convince yourself and then later we can talk about another case,

2496
02:25:17,020 --> 02:25:18,580
which is the PCA, all right?

2497
02:25:18,580 --> 02:25:22,220
Okay, but this is important and this is the only thing I need to know, okay?

2498
02:25:22,220 --> 02:25:26,740
You do have to think about the derivation, but I'm going to tell you the derivation of

2499
02:25:26,740 --> 02:25:27,740
linear regression.

2500
02:25:27,740 --> 02:25:32,900
Okay, so, okay, this is like, this point is actually w times xi plus b.

2501
02:25:32,900 --> 02:25:34,740
In this case, it's x5, right?

2502
02:25:34,740 --> 02:25:37,980
And this is the point which is yi, same for every point.

2503
02:25:37,980 --> 02:25:43,220
So this is my y4 and this will be w times x4 plus b, right?

2504
02:25:43,220 --> 02:25:45,580
And this is my y3.

2505
02:25:45,580 --> 02:25:52,420
So all of the arrows are arrowing y, that's why all of the distance are vertical, okay?

2506
02:25:52,420 --> 02:25:58,620
All right, so that's all for the things that could be tested, all right?

2507
02:25:58,620 --> 02:26:02,340
So from now on, the things are just for your knowledge.

2508
02:26:02,340 --> 02:26:05,820
So I won't really give you anything exammable, all right?

2509
02:26:05,820 --> 02:26:10,700
So I'm going to show you how one can derive the w and b, right?

2510
02:26:10,700 --> 02:26:16,540
So just now we see that if we train the regressor, we will know w and b.

2511
02:26:16,540 --> 02:26:20,700
But we haven't talked about how exactly we can obtain the w and b, right?

2512
02:26:20,700 --> 02:26:26,820
So in practice, there's an easy way, I mean, not so easy, but like a reasonably easy way

2513
02:26:26,820 --> 02:26:31,540
to obtain the w and b by using some simple linear algebra, all right?

2514
02:26:31,540 --> 02:26:34,720
No gradient descent, just linear algebra, we'll do it.

2515
02:26:34,720 --> 02:26:36,600
So here's actually the starting point, right?

2516
02:26:36,600 --> 02:26:42,080
We're talking about having a group of data, which is the x and y.

2517
02:26:42,080 --> 02:26:44,940
We want to obtain the w and b, right?

2518
02:26:44,940 --> 02:26:48,920
So this is actually the linear regressor we have, all right?

2519
02:26:48,920 --> 02:26:54,680
But for making things simpler, we can do one possible simplification.

2520
02:26:54,680 --> 02:27:01,080
That is writing the w transpose x plus b as the expanded vector.

2521
02:27:01,080 --> 02:27:02,960
Okay, how I expand it?

2522
02:27:03,000 --> 02:27:07,160
I'm cascading the w with b.

2523
02:27:07,160 --> 02:27:08,280
Okay, here's what I mean, right?

2524
02:27:08,280 --> 02:27:11,360
So maybe I just draw things a little bit.

2525
02:27:11,360 --> 02:27:19,100
So suppose my, let's say my w is, I don't know, let's say 1 and 2, all right?

2526
02:27:19,100 --> 02:27:21,640
So my w and b is equal to 3.

2527
02:27:21,640 --> 02:27:26,800
So in this case, I can do w transpose x plus b.

2528
02:27:26,840 --> 02:27:38,680
I can also make it as 1, 2, 3 multiplied by x1.

2529
02:27:38,680 --> 02:27:39,400
Make sense?

2530
02:27:39,400 --> 02:27:48,640
Because if you do vector-vector multiplication, right, this is equal to 1, 2 times x plus 3 times 1.

2531
02:27:48,640 --> 02:27:49,880
So it's the same thing, right?

2532
02:27:49,880 --> 02:27:54,480
What I'm doing here is I'm just making it simpler, making two terms into one term.

2533
02:27:54,520 --> 02:27:58,000
But nothing changes, it's an equivalent change, everything is the same.

2534
02:27:58,000 --> 02:28:04,120
I just make the notation simpler by merging the b into my w.

2535
02:28:04,120 --> 02:28:05,920
Because anyway, I'm going to learn both, right?

2536
02:28:05,920 --> 02:28:09,920
So I don't want to differentiate them, I make them the same, into one vector.

2537
02:28:09,920 --> 02:28:10,880
All right, okay?

2538
02:28:10,880 --> 02:28:16,880
So I hope this explains the small trick here, erase, all right.

2539
02:28:16,880 --> 02:28:25,360
So after that, I can simplify my notation by calling the entire vector as w.

2540
02:28:25,360 --> 02:28:27,560
I'm actually abusing the notation here.

2541
02:28:27,560 --> 02:28:35,480
I want to redefine the w as the cascading concatenation of the old w with b together.

2542
02:28:35,480 --> 02:28:40,920
It's still a vector, but vector is one more dimension with b appended there, right?

2543
02:28:40,960 --> 02:28:48,440
So in this case, I also update my x with my old x plus one more 1, right?

2544
02:28:48,440 --> 02:28:52,880
I'm going to expand my x with one more dimension, but the number is 1.

2545
02:28:52,880 --> 02:29:00,520
The reason is that I want to simplify the linear regression as simply a w times x.

2546
02:29:00,520 --> 02:29:02,440
But the things are still the same.

2547
02:29:02,440 --> 02:29:07,640
All of the parameters are embedded in the new dimension, in the new notation.

2548
02:29:07,640 --> 02:29:13,280
But the dimension is upgrading from d-dimensional to d plus 1-dimensional.

2549
02:29:13,280 --> 02:29:14,800
Same for w, right?

2550
02:29:14,800 --> 02:29:17,480
Upgrading from d-dimensional to d plus 1-dimensional.

2551
02:29:17,480 --> 02:29:21,640
It's a simple algebra, but it's going to make the notation simpler.

2552
02:29:21,640 --> 02:29:24,520
All right, so now I reformulated the problem.

2553
02:29:24,520 --> 02:29:27,320
I just now remembered we have this mean square error, right?

2554
02:29:27,320 --> 02:29:33,800
The mean square error is actually between w times x plus b minus y, remember?

2555
02:29:33,800 --> 02:29:36,240
That was the old mean square error.

2556
02:29:36,240 --> 02:29:43,840
After I do this upgrading, the mean square error will become simply w times x minus y.

2557
02:29:43,840 --> 02:29:46,680
But here, the x and w are updated, all right?

2558
02:29:46,680 --> 02:29:52,640
It's the same thing, but I changed the notation a little bit, OK?

2559
02:29:52,640 --> 02:29:59,040
The reason for doing that is I can simplify this matrix form into a simple L2 norm.

2560
02:29:59,040 --> 02:30:04,960
OK, L2 norm is actually the Euclidean distance measuring between x, w minus y.

2561
02:30:04,960 --> 02:30:12,960
So here, I do a change of the notation where every x1, x2, x3 are vectors, remember?

2562
02:30:12,960 --> 02:30:14,640
Every sample is a vector.

2563
02:30:14,640 --> 02:30:16,960
But I have entirely d numbers, right?

2564
02:30:16,960 --> 02:30:18,480
Sorry, d points.

2565
02:30:18,480 --> 02:30:19,680
Oh, sorry, n points.

2566
02:30:19,680 --> 02:30:20,960
I have n samples.

2567
02:30:20,960 --> 02:30:25,440
So I'm going to have x1, x2, x3, all the way to xn, right?

2568
02:30:25,440 --> 02:30:30,240
But instead of writing a summation, I can define a matrix, right?

2569
02:30:30,240 --> 02:30:34,400
The matrix means it's going to be like a height and width, right?

2570
02:30:34,400 --> 02:30:39,360
So here, every row of the big matrix will be a specific xi.

2571
02:30:40,560 --> 02:30:46,080
Basically means my x1 is the first row, my x2 is the second row, my x3 is the third row.

2572
02:30:46,080 --> 02:30:54,400
So you are going to concatenate in all of x one by one to form this big matrix x, uppercase x, right?

2573
02:30:54,400 --> 02:31:02,640
So in this case, the mean square error can be written as uppercase x times w and minus y.

2574
02:31:03,600 --> 02:31:09,200
Because this multiplication will give you the predicted y, but all of them together.

2575
02:31:09,200 --> 02:31:12,240
So this will be my y1, y2, y3, all the way to yn.

2576
02:31:13,440 --> 02:31:14,400
Still, nothing changed.

2577
02:31:14,400 --> 02:31:20,880
I just simplified the notation, upgrading the summation to a matrix vector multiplication, right?

2578
02:31:21,840 --> 02:31:23,120
So this is what I'm doing.

2579
02:31:23,120 --> 02:31:30,720
But by doing so, I can use this form to denote the mean square error I'm trying to minimize, all right?

2580
02:31:31,680 --> 02:31:32,480
So now what happened?

2581
02:31:33,840 --> 02:31:35,280
This is the thing I'm going to minimize.

2582
02:31:36,160 --> 02:31:37,280
I know all of the x.

2583
02:31:38,000 --> 02:31:39,200
I also know all of the y.

2584
02:31:40,160 --> 02:31:41,760
I want to find what is w.

2585
02:31:42,320 --> 02:31:46,080
So w here is the variable I want to figure out, right?

2586
02:31:46,080 --> 02:31:48,720
Remember, w includes the old w plus the b.

2587
02:31:50,160 --> 02:31:54,640
Now, how can you solve the minimization problem?

2588
02:31:55,200 --> 02:31:57,760
Many ways, but one way to do it, you can find the gradient, right?

2589
02:31:57,760 --> 02:32:03,040
You can take the gradient with respect to w and make the gradient equal to zero, right?

2590
02:32:03,040 --> 02:32:04,480
That's the first order solution.

2591
02:32:04,480 --> 02:32:07,360
Sorry, second order solution for a linear equation, right?

2592
02:32:07,360 --> 02:32:11,520
So if you do so, you can find the gradient of this linear term,

2593
02:32:12,080 --> 02:32:18,320
and the gradient will be something like 2 transpose x times w minus 2 times x transpose y.

2594
02:32:18,320 --> 02:32:19,280
You can break the bracket.

2595
02:32:19,840 --> 02:32:26,320
Now, if this guy equal to zero, which means this term should equal to the second term.

2596
02:32:26,400 --> 02:32:27,520
So this term will be equal.

2597
02:32:28,640 --> 02:32:32,480
All right, now I can inverse the x transpose x,

2598
02:32:33,040 --> 02:32:36,800
make it to the right-hand side, now only have w on the left-hand side.

2599
02:32:37,680 --> 02:32:41,600
Now, this is the closed formula for my w.

2600
02:32:41,600 --> 02:32:48,480
And the w can be written as the matrix inverse of x transpose x multiplied by x transpose y.

2601
02:32:49,120 --> 02:32:52,720
So just in the one line, if you know the entire x and y,

2602
02:32:52,720 --> 02:32:56,080
you can use this one line equation to compute the entire w.

2603
02:32:56,160 --> 02:32:59,200
Which is the parameter of your linear regression.

2604
02:33:00,800 --> 02:33:03,200
OK, so this is like some, how should I say,

2605
02:33:04,000 --> 02:33:06,560
least square solution for linear regression.

2606
02:33:07,520 --> 02:33:12,400
This is only true if the loss function is mean square error, right?

2607
02:33:12,400 --> 02:33:14,800
Other error function may end up with different solutions,

2608
02:33:14,800 --> 02:33:20,320
but if it's a mean square error, we will use the b square term solution,

2609
02:33:21,200 --> 02:33:24,640
which can find easily the parameters of your linear regression.

2610
02:33:26,240 --> 02:33:30,640
OK, one thing to keep in mind is that if you want to find,

2611
02:33:30,640 --> 02:33:32,800
let's say, a d-dimensional linear regression,

2612
02:33:33,360 --> 02:33:37,600
you have to have at least d sample points in the space.

2613
02:33:38,800 --> 02:33:41,360
OK, in other words, you want to have five unknowns.

2614
02:33:41,360 --> 02:33:44,320
You want to determine, you need to have five equations, right?

2615
02:33:44,320 --> 02:33:46,560
This is very simple, solve linear equation, right?

2616
02:33:46,560 --> 02:33:51,200
The reason is that only when you have d rows in your x matrix,

2617
02:33:51,760 --> 02:33:53,760
this matrix inverse will exist.

2618
02:33:54,320 --> 02:33:56,960
Otherwise, you cannot invert a matrix.

2619
02:33:58,080 --> 02:34:01,600
But for most of linear regression, you can assume that this one exists.

2620
02:34:02,160 --> 02:34:05,760
But if not, this w may not be unique.

2621
02:34:05,760 --> 02:34:08,160
You can have infinitely many solutions for your w.

2622
02:34:10,240 --> 02:34:11,520
OK, that makes sense?

2623
02:34:11,520 --> 02:34:14,080
Just like you want to determine a line, you need two points, right?

2624
02:34:14,080 --> 02:34:15,600
Two points can determine a line.

2625
02:34:15,600 --> 02:34:18,240
One point, you have many, many lines, which can fit the points.

2626
02:34:19,040 --> 02:34:21,760
OK, so this is actually the co-sponsor.

2627
02:34:21,760 --> 02:34:24,960
But I'm not going to pass on this, but I want to give you some idea.

2628
02:34:24,960 --> 02:34:28,320
When we say training the linear regressor, how are we training?

2629
02:34:29,040 --> 02:34:31,520
The training is actually using the following way.

2630
02:34:31,520 --> 02:34:35,760
You can figure out the matrix, do the inversion, and work out the w.

2631
02:34:35,760 --> 02:34:39,040
And those w will determine the regressor you're talking about.

2632
02:34:39,680 --> 02:34:40,960
So this is one way to do it.

2633
02:34:42,000 --> 02:34:44,240
And of course, you feel free to try this for your project.

2634
02:34:44,240 --> 02:34:48,240
But you won't see the question asking about deriving it in the exam.

2635
02:34:48,240 --> 02:34:51,040
So don't worry about it if you feel like it's hard to read.

2636
02:34:51,760 --> 02:34:58,240
OK, so like I mentioned, we are minimizing the mean square error.

2637
02:34:58,240 --> 02:35:02,880
So this closed form is called a least square estimator of the linear regression.

2638
02:35:03,840 --> 02:35:09,280
OK, so also I explained to you that, well, if you visualize a point in 2D,

2639
02:35:09,840 --> 02:35:12,480
we are basically fitting a point with a line.

2640
02:35:13,040 --> 02:35:13,360
Right?

2641
02:35:13,360 --> 02:35:16,080
You want to minimize the vertical distance,

2642
02:35:16,080 --> 02:35:20,400
such that you minimize the vertical distance to find the line fitting all the points.

2643
02:35:21,040 --> 02:35:28,880
But you see, sometimes you can find a good fit.

2644
02:35:29,520 --> 02:35:34,880
But there could be a case that maybe your points are not distributing so closely.

2645
02:35:34,880 --> 02:35:40,720
So you may fail to find a good line fitting them, which is possible.

2646
02:35:40,720 --> 02:35:41,840
Practice is possible.

2647
02:35:41,840 --> 02:35:43,200
So what should we do?

2648
02:35:43,200 --> 02:35:46,480
I mean, what should one deal with the regression problem

2649
02:35:47,120 --> 02:35:51,360
with those points are quite kind of sparsely distributed.

2650
02:35:52,080 --> 02:35:53,440
So I have two options.

2651
02:35:53,440 --> 02:35:57,040
The first option is that, well, maybe I don't have to use a line, right?

2652
02:35:57,040 --> 02:35:59,440
I can use a polynomial to fit the points.

2653
02:36:00,000 --> 02:36:02,160
So for example, here, I want to fit the points.

2654
02:36:02,720 --> 02:36:06,960
Instead of using a line, which is the red line, you can use a polynomial,

2655
02:36:06,960 --> 02:36:08,960
like high order polynomial to fit the points.

2656
02:36:10,400 --> 02:36:14,720
In this case, as you can see, the arrow here is the vertical distance.

2657
02:36:15,360 --> 02:36:18,880
Comparing the line with the polynomial, polynomial is actually much better, right?

2658
02:36:18,880 --> 02:36:20,800
You fit the points in a much better way.

2659
02:36:21,600 --> 02:36:26,320
But do you think this is a good choice for regression?

2660
02:36:32,480 --> 02:36:32,880
No?

2661
02:36:32,880 --> 02:36:33,200
Why?

2662
02:36:35,200 --> 02:36:36,000
Exactly, overfitting.

2663
02:36:36,000 --> 02:36:37,040
So some of you may notice, right?

2664
02:36:37,040 --> 02:36:41,360
So in fact, if you look at the literature in regression,

2665
02:36:41,360 --> 02:36:43,680
very few people talk about polynomial fitting, right?

2666
02:36:43,680 --> 02:36:46,800
Even though they fit very well, the question is that,

2667
02:36:46,800 --> 02:36:51,200
you only fit well for those kind of points you know, right?

2668
02:36:51,200 --> 02:36:53,920
But those kind of training samples may have noise.

2669
02:36:53,920 --> 02:36:56,480
Nobody knows whether the points themselves are very clean

2670
02:36:56,480 --> 02:36:58,400
and coming from the actual distribution.

2671
02:36:58,400 --> 02:37:02,480
So use polynomial to fit the points will give you smaller error,

2672
02:37:03,120 --> 02:37:06,480
but you won't be able to generalize that to future data,

2673
02:37:06,480 --> 02:37:11,040
because the future data won't really be exactly the same as your training data, right?

2674
02:37:11,040 --> 02:37:15,920
So that's why even though linear regressor will give you some error in the training,

2675
02:37:15,920 --> 02:37:18,880
people still love it, because the structure of the line

2676
02:37:18,880 --> 02:37:21,680
will help you to overcome overfitting recognition, right?

2677
02:37:21,680 --> 02:37:25,600
So we're going to get to that in week 11, unfortunately.

2678
02:37:25,600 --> 02:37:30,000
But in 11, we're going to talk about what is overfitting or even underfitting

2679
02:37:30,000 --> 02:37:33,280
and how to overcome them if you observe that, right?

2680
02:37:33,280 --> 02:37:36,320
What's the way we can maybe constrain our model

2681
02:37:36,320 --> 02:37:40,160
or change the way we train to prevent overfitting, right?

2682
02:37:40,160 --> 02:37:43,280
But this is one example that overfitting is a problem,

2683
02:37:43,280 --> 02:37:45,360
and you should not just minimize the error.

2684
02:37:45,360 --> 02:37:49,120
You should also consider the generalization to testing data, right?

2685
02:37:49,120 --> 02:37:50,480
So the answer is no.

2686
02:37:50,480 --> 02:37:53,200
Nobody wants to do the polynomial fitting due to overfitting.

2687
02:37:53,200 --> 02:37:56,160
People still love linear regression, all right?

2688
02:37:57,040 --> 02:37:59,920
But instead, another way people are trying to do

2689
02:38:00,640 --> 02:38:06,000
if you find a really bad fit is by replacing x with some better feature.

2690
02:38:06,560 --> 02:38:09,840
So we don't have to do fitting the original xy domain.

2691
02:38:10,240 --> 02:38:12,400
We can actually do the fitting maybe in a better domain.

2692
02:38:12,960 --> 02:38:16,880
So this is actually all about this kind of feature embedding and also deep learning, right?

2693
02:38:16,880 --> 02:38:21,200
Deep learning is nothing but getting the right feature and we do a fitting again, right?

2694
02:38:21,200 --> 02:38:26,080
So possibly learning the feature will help you to fit the data better

2695
02:38:26,640 --> 02:38:29,040
even if you still use linear regression, right?

2696
02:38:29,040 --> 02:38:31,440
This is actually the better solution in practice

2697
02:38:31,440 --> 02:38:34,560
rather than upgrading the linear to polynomial, all right?

2698
02:38:35,440 --> 02:38:38,720
OK, so I think that's pretty much the end I will talk about.

2699
02:38:38,720 --> 02:38:40,640
So just summarize, right?

2700
02:38:40,640 --> 02:38:45,920
For regression part, you need to know the difference between regression and classification.

2701
02:38:45,920 --> 02:38:49,040
The major difference is what are you predicting, right?

2702
02:38:49,040 --> 02:38:51,120
Regression is going to be continuous quantity.

2703
02:38:51,680 --> 02:38:53,440
Classification is always discrete.

2704
02:38:53,440 --> 02:38:57,840
So it depends on the finger prediction will make a difference

2705
02:38:57,840 --> 02:39:00,240
whether it's classification or regression.

2706
02:39:00,240 --> 02:39:04,720
So for linear regression, you need to know what is the training loss, OK?

2707
02:39:04,720 --> 02:39:08,000
The training loss in our course will be mean square error

2708
02:39:08,000 --> 02:39:10,720
between the prediction and the ground truth.

2709
02:39:11,280 --> 02:39:17,360
And also I wish to understand why visually we are minimizing the vertical distance

2710
02:39:17,360 --> 02:39:19,040
rather than the perpendicular distance.

2711
02:39:19,680 --> 02:39:21,360
The reason is actually at the loss function.

2712
02:39:21,360 --> 02:39:24,640
This is the loss in the y domain.

2713
02:39:24,640 --> 02:39:26,000
Nothing to do with x.

2714
02:39:26,000 --> 02:39:31,520
That's why we are always talking about minimizing the vertical offset, OK?

2715
02:39:31,520 --> 02:39:34,400
So these are something kind of takeaway for regression part.

2716
02:39:35,040 --> 02:39:37,040
So with that, that's the end for today.

2717
02:39:37,040 --> 02:39:42,080
And I hope I will see you again like after several weeks, so week 11.

2718
02:39:42,080 --> 02:39:44,000
All right, so don't forget the grouping, right?

2719
02:39:44,000 --> 02:39:46,320
I hope to see the response by everyone.

2720
02:39:46,320 --> 02:39:49,680
Either you find a group or not by the end of recess, OK?

2721
02:39:49,680 --> 02:39:53,360
So by the end of recess, I'm going to help those who cannot find a group.

2722
02:39:53,360 --> 02:39:55,120
I'm going to announce the grouping.

2723
02:39:55,120 --> 02:39:56,720
Then you can proceed to the project.

2724
02:39:57,760 --> 02:39:58,960
All right, OK, thank you so much.

2725
02:39:58,960 --> 02:40:00,480
And see you in week 11.

2726
02:40:30,480 --> 02:40:34,240
Thank you so much.

2727
02:41:00,480 --> 02:41:04,240
Yes, yes.

2728
02:41:31,440 --> 02:41:36,240
No, no, this is n.

2729
02:41:42,240 --> 02:41:48,240
Yeah, yeah, x, I mean, the weight is like 80 plus 1, 100 plus 1, yes.

2730
02:41:49,040 --> 02:41:54,240
1, yes, it's not a matrix.

2731
02:42:00,480 --> 02:42:05,120
Either way, but I need to see a reason.

2732
02:42:05,840 --> 02:42:07,200
For example, you want to propose your own.

2733
02:42:07,840 --> 02:42:10,160
What's wrong with the existing one, right?

2734
02:42:10,160 --> 02:42:13,120
Or you want to use something at the baseline y.

2735
02:42:13,120 --> 02:42:14,720
Why this one specifically?

2736
02:42:14,720 --> 02:42:16,080
Why not other baseline?

2737
02:42:16,080 --> 02:42:22,880
So that's why you need to do a summary first and show me a clear kind of a thinking.

2738
02:42:22,880 --> 02:42:25,360
So how you end up with your choice.

2739
02:42:26,160 --> 02:42:34,640
Can we select about three to four models based on the model that you just told me to propose

2740
02:42:34,640 --> 02:42:36,240
with final notion of this?

2741
02:42:36,240 --> 02:42:37,920
Yeah, I mean, that's something you can do.

2742
02:42:37,920 --> 02:42:43,440
But I wish to remind you that, OK, you're talking about numerical verification.

2743
02:42:43,440 --> 02:42:44,640
You say, hey, I have five.

2744
02:42:44,640 --> 02:42:49,920
I want to numerically test how well they work in the empirical experiment.

2745
02:42:49,920 --> 02:42:54,640
But that will only verify the performance over a specific set.

2746
02:42:55,840 --> 02:43:01,520
I also wish to know the reason if you find A is better than B as a fact.

2747
02:43:01,520 --> 02:43:07,040
But why do you think that's possibly what kind of generate this kind of result?

2748
02:43:07,040 --> 02:43:09,280
Yeah, so like to explain.

2749
02:43:10,160 --> 02:43:10,880
Yes, yes.

2750
02:43:10,880 --> 02:43:12,240
Or at least give you some intuition.

2751
02:43:12,800 --> 02:43:16,240
I know for deep learning, maybe it's hard to theoretically justify.

2752
02:43:16,240 --> 02:43:20,880
But it's always better to give me some of the heuristics.

2753
02:43:20,880 --> 02:43:23,600
For example, maybe to do is the complexity of the method.

2754
02:43:23,680 --> 02:43:25,520
Maybe something to do is like overfitting.

2755
02:43:25,520 --> 02:43:26,800
Something to do is optimizer.

2756
02:43:26,800 --> 02:43:27,360
Right.

2757
02:43:27,360 --> 02:43:29,120
So that was your understanding.

2758
02:43:29,120 --> 02:43:29,620
Right.

2759
02:43:29,620 --> 02:43:33,520
You know how they are actually formulating.

2760
02:43:33,520 --> 02:43:34,800
Oh, OK.

2761
02:43:34,800 --> 02:43:35,920
OK, good.

2762
02:43:53,600 --> 02:44:09,440
So no more questions, right?

2763
02:44:10,080 --> 02:44:10,320
OK.

2764
02:44:23,600 --> 02:44:37,440
OK.

2765
02:44:53,600 --> 02:45:11,440
OK.

2766
02:45:23,600 --> 02:45:41,440
OK.

2767
02:45:53,600 --> 02:46:11,440
OK.

2768
02:46:23,600 --> 02:46:41,440
OK.

2769
02:46:53,600 --> 02:47:11,440
OK.

2770
02:47:23,600 --> 02:47:47,440
OK.

2771
02:47:53,600 --> 02:48:17,440
OK.

2772
02:48:23,600 --> 02:48:35,440
OK.

2773
02:48:53,600 --> 02:49:17,440
OK.

2774
02:49:23,600 --> 02:49:35,440
OK.

2775
02:49:53,600 --> 02:50:17,440
OK.

2776
02:50:23,600 --> 02:50:47,440
OK.

2777
02:50:53,600 --> 02:51:17,440
OK.

2778
02:51:23,600 --> 02:51:47,440
OK.

2779
02:51:53,600 --> 02:52:17,440
OK.

2780
02:52:23,600 --> 02:52:47,440
OK.

2781
02:52:53,600 --> 02:53:17,440
OK.

2782
02:53:23,600 --> 02:53:47,440
OK.

2783
02:53:53,600 --> 02:54:17,440
OK.

2784
02:54:23,600 --> 02:54:47,440
OK.

2785
02:54:53,600 --> 02:55:17,440
OK.

2786
02:55:23,600 --> 02:55:47,440
OK.

2787
02:55:53,600 --> 02:56:17,440
OK.

2788
02:56:23,600 --> 02:56:47,440
OK.

2789
02:56:53,600 --> 02:57:17,440
OK.

2790
02:57:23,600 --> 02:57:47,440
OK.

2791
02:57:47,440 --> 02:57:49,280
Bye-bye.

2792
02:58:17,440 --> 02:58:41,280
OK.

2793
02:58:47,440 --> 02:59:11,280
OK.

2794
02:59:17,440 --> 02:59:41,280
OK.

2795
02:59:47,440 --> 02:59:59,280
OK.

