1
00:00:00,000 --> 00:00:30,000
 ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

2
00:02:30,000 --> 00:03:00,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
00:03:00,000 --> 00:03:30,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4
00:03:30,000 --> 00:04:00,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5
00:04:00,000 --> 00:04:07,060
åœapplause

6
00:04:17,399 --> 00:04:19,240
 Continue

7
00:04:22,140 --> 00:04:23,740
 we have

8
00:04:26,000 --> 00:04:27,640
 great

9
00:04:27,640 --> 00:04:28,640
 several days.

10
00:04:29,640 --> 00:04:32,539
 And of course now you have to come back.

11
00:04:32,539 --> 00:04:35,640
 This is the second half of the semester.

12
00:04:35,640 --> 00:04:37,340
 So there also kind of

13
00:04:38,640 --> 00:04:39,479
 ingenious,

14
00:04:40,539 --> 00:04:43,979
 kind of remind students that you have to start to now

15
00:04:45,280 --> 00:04:46,780
 prepare for the final exam.

16
00:04:47,539 --> 00:04:49,020
 Okay.

17
00:04:49,020 --> 00:04:52,039
 For those if you have not taken final exam in NTU,

18
00:04:52,039 --> 00:04:54,280
 you know, right, the final exam,

19
00:04:54,280 --> 00:04:56,479
 they all come together and

20
00:04:58,140 --> 00:05:01,539
 I don't know why the projector is not on.

21
00:05:01,539 --> 00:05:02,539
 Let me try to see.

22
00:05:02,539 --> 00:05:03,539
 Yeah.

23
00:05:03,539 --> 00:05:04,539
 On, on.

24
00:05:10,240 --> 00:05:11,240
 Okay.

25
00:05:11,240 --> 00:05:12,240
 Okay. Good.

26
00:05:13,039 --> 00:05:17,740
 So the final exam, check the schedule, check the timing,

27
00:05:17,740 --> 00:05:19,340
 and those should be confirmed.

28
00:05:19,340 --> 00:05:22,240
 It would not be changed anymore.

29
00:05:22,240 --> 00:05:25,840
 So you look at those timing and some of you,

30
00:05:25,840 --> 00:05:27,340
 if you are lucky or unlucky,

31
00:05:27,340 --> 00:05:30,940
 you may have a few exam together back to back.

32
00:05:30,940 --> 00:05:35,739
 So you have to plan accordingly how to prepare for,

33
00:05:35,739 --> 00:05:37,140
 for your final exams.

34
00:05:37,140 --> 00:05:38,640
 Okay.

35
00:05:38,640 --> 00:05:42,940
 And of course, the other thing is the assignment were out.

36
00:05:42,940 --> 00:05:43,739
 I was

37
00:05:45,440 --> 00:05:46,640
 trying to

38
00:05:50,539 --> 00:05:52,840
 think about what would be the good question for you.

39
00:05:52,840 --> 00:05:54,440
 So I said these two questions.

40
00:05:54,440 --> 00:05:55,940
 I hope that you have time.

41
00:05:55,940 --> 00:05:58,040
 Of course the marks are, of course,

42
00:05:58,040 --> 00:06:02,340
 the whole assignment whole world one will carry only five marks

43
00:06:02,340 --> 00:06:05,540
 of your final so-called total marks.

44
00:06:05,540 --> 00:06:06,840
 So it's not a lot.

45
00:06:06,840 --> 00:06:07,740
 Okay.

46
00:06:07,740 --> 00:06:10,740
 But more importantly for you to try out,

47
00:06:10,740 --> 00:06:12,340
 don't just copy from others.

48
00:06:12,340 --> 00:06:15,540
 Actually, even you don't submit, just lose five marks only.

49
00:06:15,540 --> 00:06:17,140
 But try out

50
00:06:18,440 --> 00:06:22,040
 and then you likely would be surprised, right?

51
00:06:22,040 --> 00:06:25,140
 Oh, in the lecture, everything seemed very easy,

52
00:06:25,140 --> 00:06:26,840
 very logical, but when you try out,

53
00:06:26,840 --> 00:06:32,040
 hey, some, some, there was some confusion.

54
00:06:32,040 --> 00:06:33,840
 So there's part of you to make decision.

55
00:06:33,840 --> 00:06:36,940
 You have to think carefully, put your comments,

56
00:06:36,940 --> 00:06:38,840
 justify your answer, right?

57
00:06:38,840 --> 00:06:40,640
 That's why I always ask this.

58
00:06:40,640 --> 00:06:42,840
 Justify your answer.

59
00:06:42,840 --> 00:06:47,140
 They could have more than one possible solution.

60
00:06:47,140 --> 00:06:49,340
 Not all of them are correct, of course,

61
00:06:49,340 --> 00:06:51,040
 but then you provide the reason.

62
00:06:51,140 --> 00:06:52,140
 Okay.

63
00:06:52,140 --> 00:06:54,140
 Immediately I would have got the students say that,

64
00:06:54,140 --> 00:06:57,040
 sir, your question is wrong.

65
00:06:57,040 --> 00:06:58,040
 Okay.

66
00:06:59,140 --> 00:07:00,140
 Okay.

67
00:07:00,140 --> 00:07:01,640
 Actually that is exactly the purpose.

68
00:07:01,640 --> 00:07:03,140
 I said the question like that.

69
00:07:03,140 --> 00:07:06,840
 For example, the student highlighted that.

70
00:07:06,840 --> 00:07:07,840
 Let me see.

71
00:07:07,840 --> 00:07:08,840
 Where is it?

72
00:07:21,040 --> 00:07:22,040
 Okay.

73
00:07:40,340 --> 00:07:42,640
 So when you try to attempt the question,

74
00:07:42,640 --> 00:07:47,640
 you'll find that there are some which are confusing, right?

75
00:07:47,640 --> 00:07:54,039
 Whether you would decide to indicate this or the other.

76
00:07:54,039 --> 00:07:57,240
 And that is exactly why I make the question.

77
00:07:57,240 --> 00:08:02,840
 So look into this and then let me see.

78
00:08:02,840 --> 00:08:03,840
 Yeah.

79
00:08:03,840 --> 00:08:06,240
 Where is that question?

80
00:08:17,640 --> 00:08:18,640
 Yeah.

81
00:08:18,640 --> 00:08:31,940
 So basically the question actually you have so-called eight training samples.

82
00:08:31,940 --> 00:08:36,439
 You have to build a decision tree using ID tree algorithm.

83
00:08:36,439 --> 00:08:37,439
 Okay.

84
00:08:37,439 --> 00:08:40,439
 Based on this training sample.

85
00:08:40,439 --> 00:08:46,340
 And I do not say that all these training samples are so-called error free.

86
00:08:46,340 --> 00:08:52,840
 So when you build your tree, therefore at the end, I ask a question, can you prune the

87
00:08:52,840 --> 00:09:01,640
 tree so that to achieve similar performance or even same performance so that the tree

88
00:09:01,640 --> 00:09:02,640
 will be easier.

89
00:09:02,640 --> 00:09:05,240
 So these are the questions that you look into it.

90
00:09:05,240 --> 00:09:07,840
 So each question is one mark.

91
00:09:07,840 --> 00:09:10,440
 So look into that.

92
00:09:10,440 --> 00:09:11,940
 Try to practice yourself.

93
00:09:11,940 --> 00:09:13,440
 And this is another question.

94
00:09:13,440 --> 00:09:15,040
 I thought about it.

95
00:09:15,040 --> 00:09:17,699
 I think this would be a good exercise.

96
00:09:17,699 --> 00:09:23,099
 So I give you six training samples, right?

97
00:09:23,099 --> 00:09:27,339
 Tree belong to this solid class.

98
00:09:27,339 --> 00:09:30,540
 The other tree belong to M.E. class.

99
00:09:30,540 --> 00:09:35,260
 So my question is just to make sure you understand the question correctly.

100
00:09:35,260 --> 00:09:40,140
 If I were to so-called put any point here, right?

101
00:09:40,140 --> 00:09:48,439
 So if I have a, okay, I cannot draw here.

102
00:09:48,439 --> 00:09:49,439
 Okay.

103
00:09:49,439 --> 00:09:50,439
 Here.

104
00:09:50,439 --> 00:09:51,439
 Insert.

105
00:09:51,439 --> 00:09:58,880
 Maybe I put a cross here.

106
00:09:58,880 --> 00:10:07,200
 So if I have a data point here, okay, this test point does it belong to the solid or

107
00:10:07,200 --> 00:10:09,760
 the empty class, right?

108
00:10:09,780 --> 00:10:12,300
 I can move this point around, right?

109
00:10:12,300 --> 00:10:19,460
 And this point does it belong to solid or empty class using one nearest neighbor, right?

110
00:10:19,460 --> 00:10:22,540
 So maybe I give you a simple example.

111
00:10:22,540 --> 00:10:28,140
 Let me put out a simple example.

112
00:10:28,140 --> 00:10:37,080
 I have this one.

113
00:10:37,080 --> 00:10:43,720
 And that's not one to fill.

114
00:10:43,720 --> 00:10:48,520
 So I have two classes, researched.

115
00:10:48,520 --> 00:10:57,180
 One of them is solid.

116
00:10:57,180 --> 00:11:05,020
 And then the other one is here, assuming the symmetric.

117
00:11:05,020 --> 00:11:12,020
 Here.

118
00:11:12,020 --> 00:11:29,120
 And then I want to have this.

119
00:11:29,120 --> 00:11:36,120
 This is the one I just draw.

120
00:11:36,120 --> 00:11:39,120
 Disappear already.

121
00:11:39,120 --> 00:11:42,120
 Maybe easier.

122
00:11:42,120 --> 00:11:45,120
 I just draw here.

123
00:11:45,120 --> 00:11:46,120
 Okay.

124
00:11:46,120 --> 00:12:00,120
 Let's see.

125
00:12:00,120 --> 00:12:04,120
 Coming up.

126
00:12:04,120 --> 00:12:06,120
 Yeah.

127
00:12:07,120 --> 00:12:18,120
 Let's say I have a region here which I put a solid class and an empty class.

128
00:12:18,120 --> 00:12:19,120
 Okay.

129
00:12:19,120 --> 00:12:21,120
 Just illustrate this.

130
00:12:21,120 --> 00:12:26,120
 So now I want you to shape the area state.

131
00:12:26,120 --> 00:12:34,120
 If any point lengths on that area will belong to this solid class, which will be the region

132
00:12:34,120 --> 00:12:40,120
 that if any point I point here will belong to this rather than this.

133
00:12:40,120 --> 00:12:41,120
 Right?

134
00:12:41,120 --> 00:12:42,120
 Another question?

135
00:12:42,120 --> 00:12:43,120
 Okay.

136
00:12:43,120 --> 00:12:45,120
 Anyone know the answer for this?

137
00:12:45,120 --> 00:12:47,120
 It's very straightforward.

138
00:12:47,120 --> 00:12:57,120
 The area that you have a point located in it, it will belong to the solid class.

139
00:12:57,120 --> 00:13:03,120
 Anyone?

140
00:13:03,120 --> 00:13:09,120
 What the assignment is slightly more challenging than this?

141
00:13:09,120 --> 00:13:14,120
 Anyone?

142
00:13:14,120 --> 00:13:17,120
 This area.

143
00:13:17,120 --> 00:13:19,120
 Right?

144
00:13:19,120 --> 00:13:24,120
 Because any point here will be closer to this rather than this.

145
00:13:24,120 --> 00:13:30,120
 Any point here you draw a Euclidean distance, this will be shorter than this.

146
00:13:30,120 --> 00:13:31,120
 Right?

147
00:13:31,120 --> 00:13:36,120
 So this area will all belong to the solid class.

148
00:13:36,120 --> 00:13:37,120
 Okay?

149
00:13:37,120 --> 00:13:41,120
 So using that, do your assignment.

150
00:13:41,120 --> 00:13:47,120
 For this, you have to draw the area that which any point by using one nearest neighbor belong

151
00:13:47,120 --> 00:13:56,120
 to solid class by using three nearest neighbor, slightly more challenging than by using five

152
00:13:56,120 --> 00:13:58,120
 nearest neighbor.

153
00:13:58,120 --> 00:13:59,120
 Okay?

154
00:13:59,120 --> 00:14:06,120
 So any point by using one nearest neighbor, slightly more challenging than by using five

155
00:14:06,120 --> 00:14:07,120
 nearest neighbor.

156
00:14:07,120 --> 00:14:08,120
 Okay?

157
00:14:08,120 --> 00:14:11,120
 If you want to think about it, will the result change?

158
00:14:11,120 --> 00:14:15,120
 I thought about setting this, but already too many questions, I just give you.

159
00:14:15,120 --> 00:14:20,120
 Will the result change if you use a Manhattan distance?

160
00:14:20,120 --> 00:14:21,120
 Okay.

161
00:14:21,120 --> 00:14:23,120
 So think about it.

162
00:14:23,120 --> 00:14:24,120
 Okay?

163
00:14:24,120 --> 00:14:25,120
 So these are two questions.

164
00:14:25,120 --> 00:14:28,120
 Shouldn't be difficult based on what we have covered in the lecture.

165
00:14:28,120 --> 00:14:35,120
 You should be able to do it.

166
00:14:58,120 --> 00:15:05,120
 Okay?

167
00:15:05,120 --> 00:15:20,120
 Let's continue with the question.

168
00:15:20,120 --> 00:15:25,120
 Okay?

169
00:15:25,120 --> 00:15:48,320
 Let's continue with lectures.

170
00:15:48,320 --> 00:15:54,320
 So last lecture, we talked about nearest neighbor classifiers.

171
00:15:54,320 --> 00:16:05,320
 And then again, we basically talked about the so-called matrix that you need to use or

172
00:16:05,320 --> 00:16:06,320
 measure.

173
00:16:06,320 --> 00:16:19,680
 You need to use to measure the distance between the two so-called points, the training sample

174
00:16:19,680 --> 00:16:28,240
 and the nearest neighbor so that you can decide which class the test sample belong to.

175
00:16:28,240 --> 00:16:33,400
 So today we're going to look at the other classifier, which is called support vector

176
00:16:33,400 --> 00:16:35,199
 machine.

177
00:16:35,199 --> 00:16:43,400
 And support vector machine, actually, you think carefully if it is just a binary classifier,

178
00:16:43,400 --> 00:16:44,400
 okay?

179
00:16:44,400 --> 00:16:51,120
 The simplest one, the basic one, which basically give you just like the nearest neighbor that

180
00:16:51,120 --> 00:16:53,640
 we show, right?

181
00:16:53,640 --> 00:16:57,000
 You could operate in a 2D like this, right?

182
00:16:57,000 --> 00:17:02,680
 So if I give you some training sample, positive and negative, right?

183
00:17:02,680 --> 00:17:09,280
 How can you find the decision boundary so that like this one, we call it hyperplanned?

184
00:17:09,280 --> 00:17:13,319
 It could separate the positive from the negative classes.

185
00:17:13,319 --> 00:17:15,760
 So this is called support vector machine.

186
00:17:15,760 --> 00:17:20,440
 So in 1D, it will be just a single threshold, right?

187
00:17:20,440 --> 00:17:25,879
 For example, if you want to decide whether this number is positive or negative, then

188
00:17:25,880 --> 00:17:33,680
 you will choose to ask whether the value is larger or smaller than zero, right?

189
00:17:33,680 --> 00:17:38,560
 So it's larger than zero, it's a positive value, less than zero is negative.

190
00:17:38,560 --> 00:17:44,840
 So in 1D, the hyperplane become a value, a threshold, okay?

191
00:17:44,840 --> 00:17:51,800
 In 2D, you have a two-dimensional, so you can only draw a line, assuming this is a linear

192
00:17:51,800 --> 00:17:52,800
 classifier.

193
00:17:53,200 --> 00:17:58,560
 Of course, you could have nonlinear, but let's start with a simple linear classifier that

194
00:17:58,560 --> 00:18:01,840
 you can separate by drawing a line.

195
00:18:01,840 --> 00:18:05,320
 For such cases, that seems to be possible.

196
00:18:05,320 --> 00:18:11,600
 And then for 3D, then your decision boundary will be a plan.

197
00:18:11,600 --> 00:18:17,879
 Normally the decision is 1D lower than your training data.

198
00:18:18,840 --> 00:18:24,760
 So 2D is one line, 3D is a two-dimensional plan, okay?

199
00:18:24,760 --> 00:18:30,840
 So all this we call the hyperplane for you to make decision.

200
00:18:30,840 --> 00:18:41,600
 And SPM basically is trying to find the hyperplane which allows so-called some possible error,

201
00:18:41,600 --> 00:18:45,520
 because bear in mind that these samples are training samples.

202
00:18:46,480 --> 00:18:49,800
 They are not all the samples that you are going to see, right?

203
00:18:49,800 --> 00:18:54,400
 They are just a small set of samples that you happen to have.

204
00:18:54,400 --> 00:18:59,360
 You try to find a boundary that can separate these two classes.

205
00:18:59,360 --> 00:19:04,040
 So there are many such possible hyperplanes, okay?

206
00:19:04,040 --> 00:19:10,280
 In this case, one line, which could separate the positive from the negative class.

207
00:19:10,280 --> 00:19:12,840
 For example, here's another one, right?

208
00:19:12,840 --> 00:19:18,120
 And you can also separate the two classes without any error, okay?

209
00:19:18,120 --> 00:19:20,639
 Here's another one, okay?

210
00:19:20,639 --> 00:19:26,199
 So among all these possible lines, which one is better?

211
00:19:26,199 --> 00:19:28,840
 So there is basically an engineering problem.

212
00:19:28,840 --> 00:19:40,280
 So for SPM, basically you'll try to find the line such that the distance from the positive

213
00:19:40,720 --> 00:19:44,879
 to this line will be maximum, right?

214
00:19:44,879 --> 00:19:51,720
 To allow maximum margins of error or margin of noise, okay?

215
00:19:51,720 --> 00:19:55,600
 So for example, so how then how you formulate this, right?

216
00:19:55,600 --> 00:20:02,560
 Assuming for example, you are looking for this decision boundary, D, okay?

217
00:20:03,000 --> 00:20:13,360
 Now you can consider that it's actually a margin from D1 to D2, okay?

218
00:20:13,360 --> 00:20:21,600
 Which you currently, based on trained data, none of these data is located within this margin.

219
00:20:21,600 --> 00:20:28,840
 Ideally, you want to find the line such that the margin is as wide as possible, right?

220
00:20:28,919 --> 00:20:34,800
 So that means if you have a test data which could have some noise or error,

221
00:20:34,800 --> 00:20:43,879
 turn out that it will be very so-called different form.

222
00:20:43,879 --> 00:20:48,480
 I mean, it will not be, even if they are different from this training sample,

223
00:20:48,480 --> 00:20:51,639
 you can still make a correct decision.

224
00:20:51,639 --> 00:20:57,360
 So as we then choose the maximum margin plan as the decision boundary.

225
00:20:57,360 --> 00:21:00,840
 For example, here there are two possible such plans.

226
00:21:00,840 --> 00:21:04,240
 One is a D1, the other is a D2, right?

227
00:21:04,240 --> 00:21:10,199
 So based on this, if you define the margin, you can see that the distance between D1,

228
00:21:10,199 --> 00:21:16,360
 1 to D1, 2 is larger than D2, 1 to D2, 2.

229
00:21:16,360 --> 00:21:24,199
 So therefore, this D1 is likely chosen for SSSVM, right?

230
00:21:24,200 --> 00:21:31,760
 So there are many, many such so-called possible decision boundary,

231
00:21:31,760 --> 00:21:37,760
 but SPM is choosing the one which maximizes the margin,

232
00:21:37,760 --> 00:21:42,040
 which also maximizes the tolerance of errors or noise, okay?

233
00:21:43,520 --> 00:21:45,440
 So that is just a concept.

234
00:21:45,440 --> 00:21:52,040
 Then how do you so-called formulate this problem so that you can,

235
00:21:52,040 --> 00:21:56,280
 even a set of training sample, you can solve this problem, okay?

236
00:21:56,280 --> 00:22:01,399
 So of course, we use a 2D data as an example,

237
00:22:01,399 --> 00:22:06,320
 which you are finding the line here, but the whole set of

238
00:22:06,320 --> 00:22:11,080
 mathematics can be applied to multi-dimensional data.

239
00:22:11,080 --> 00:22:11,879
 Could be very high.

240
00:22:11,879 --> 00:22:16,320
 It's just a vector, so-called manipulation.

241
00:22:17,360 --> 00:22:22,000
 So let's consider they are end training sample, right?

242
00:22:22,000 --> 00:22:27,400
 Each sample is denoted by the feature xi.

243
00:22:27,400 --> 00:22:33,240
 This is the vector of attributes or feature vector that we mentioned.

244
00:22:33,240 --> 00:22:35,120
 Yi is the label.

245
00:22:35,120 --> 00:22:38,880
 So in this case, there's label them as minus one and one, right?

246
00:22:38,880 --> 00:22:42,800
 And one and zero, because for the formulation, and

247
00:22:42,800 --> 00:22:48,080
 this later on you'll see that it will be easier to put the mathematics into a very

248
00:22:48,080 --> 00:22:52,159
 nice formula for you to so-called find the solution.

249
00:22:53,360 --> 00:22:57,760
 So if you define all these points, in this case,

250
00:22:57,760 --> 00:23:01,879
 they are 2D assuming you have a zero, zero coordinate here.

251
00:23:01,879 --> 00:23:09,360
 Then this one will be x1, 1, x1, 2, x2, 1, x2, 2, all these data points, right?

252
00:23:09,360 --> 00:23:13,879
 If Yi is part one, they belong to the positive class,

253
00:23:13,879 --> 00:23:15,639
 minus one belong to negative class.

254
00:23:16,320 --> 00:23:19,840
 So then your problem now can be formulated.

255
00:23:19,840 --> 00:23:24,560
 The decision boundary you're looking for, the green line here,

256
00:23:24,560 --> 00:23:29,680
 will be the w dot x plus b equal to zero.

257
00:23:29,680 --> 00:23:32,640
 This is a line equation, right?

258
00:23:32,640 --> 00:23:38,920
 In 2D, will be w1 x1 plus w2 x2 plus b equal to zero, right?

259
00:23:38,920 --> 00:23:42,720
 Which is, again, your line equation here.

260
00:23:46,080 --> 00:23:52,080
 If this x1 is x2, so any line here you can write it as

261
00:23:52,080 --> 00:23:57,680
 w1 x1 plus w2 x2 plus b equal to zero.

262
00:24:00,240 --> 00:24:05,440
 Just for, I'm not sure which one will be captured by the camera, but

263
00:24:05,440 --> 00:24:11,480
 any line here, if you change the w1, w2, and b, b is basically the

264
00:24:11,520 --> 00:24:17,840
 intersection when x1 equal to zero, right?

265
00:24:17,840 --> 00:24:21,640
 And b will be equal to, x1 equal to zero, b,

266
00:24:21,640 --> 00:24:28,200
 this point will be equal, x2 will be equal to minus b, divided by w2, right?

267
00:24:28,200 --> 00:24:31,160
 So, similarly, you can find the point.

268
00:24:31,160 --> 00:24:37,320
 So any line here can be described by different value of w1, w2, and b, okay?

269
00:24:38,320 --> 00:24:43,520
 So that is basically the line equation here.

270
00:24:44,720 --> 00:24:51,320
 Even this train of data, you can find such a line such that this line is w,

271
00:24:51,320 --> 00:24:55,240
 dot product with x plus b equal to zero.

272
00:24:55,240 --> 00:24:59,720
 There's w, for 2D, it's w1 x1 plus w2 x2 plus b equal to zero.

273
00:25:00,919 --> 00:25:03,560
 And then you can imagine that there's another line here.

274
00:25:03,560 --> 00:25:06,360
 Let's assume this is wx plus b equal to one.

275
00:25:07,760 --> 00:25:13,360
 And there's another line, wx plus b equal to minus one, right?

276
00:25:13,360 --> 00:25:17,520
 So if you define your class label as minus one plus one,

277
00:25:17,520 --> 00:25:22,840
 that means you only have to plug in any of this xi.

278
00:25:22,840 --> 00:25:28,800
 If this is your decision boundary, xi, dot product with w plus b,

279
00:25:28,800 --> 00:25:32,200
 if this is on this side, then this value will be positive.

280
00:25:32,200 --> 00:25:35,439
 On the other side, it will be negative, right?

281
00:25:35,440 --> 00:25:42,120
 So in your classifier, you just want to check by putting in any sample xi,

282
00:25:42,120 --> 00:25:47,440
 what will be the value, whether it's a positive or negative of this so-called

283
00:25:48,640 --> 00:25:50,920
 form expression, right?

284
00:25:50,920 --> 00:25:56,360
 So all your train data should be larger than one, larger than or equal to one, right?

285
00:25:56,360 --> 00:25:59,200
 Testing data less than or equal to minus one.

286
00:26:01,720 --> 00:26:04,560
 So this is basically the boundary that you are looking for.

287
00:26:05,440 --> 00:26:10,120
 And then how do you find this w and b?

288
00:26:10,120 --> 00:26:16,800
 You want to find the w such that the distance between these two,

289
00:26:16,800 --> 00:26:20,840
 one to minus one line, will be maximum, right?

290
00:26:20,840 --> 00:26:26,880
 So now you imagine there are two such a point along this line, w, right?

291
00:26:26,880 --> 00:26:32,160
 Then because as long as the point located at this line,

292
00:26:32,160 --> 00:26:35,600
 you have to satisfy this equation, wx plus b equal to 0.

293
00:26:35,600 --> 00:26:37,320
 There's a line equation.

294
00:26:37,320 --> 00:26:40,920
 So we have this x1, x2, right?

295
00:26:40,920 --> 00:26:45,480
 So these are the ones that because they are on the decision boundary,

296
00:26:45,480 --> 00:26:48,440
 they have to fulfill this line equation.

297
00:26:48,440 --> 00:26:50,160
 Then you subtract one from the other.

298
00:26:50,160 --> 00:26:56,160
 That means this x2 minus x1 is this vector in this direction.

299
00:26:56,160 --> 00:26:59,120
 This one, dot product with w equal to 0.

300
00:26:59,120 --> 00:27:03,280
 That means the w must be perpendicular to this line.

301
00:27:03,280 --> 00:27:04,520
 You have this.

302
00:27:04,520 --> 00:27:07,879
 And x2 minus x1 will be the vector which

303
00:27:07,879 --> 00:27:13,600
 is running across along this so-called decision boundary.

304
00:27:13,600 --> 00:27:18,919
 So that is basically a very simple so-called mathematics

305
00:27:18,919 --> 00:27:20,879
 manipulation.

306
00:27:20,879 --> 00:27:24,600
 Then if any of the xa is on the right-hand side,

307
00:27:24,600 --> 00:27:27,879
 then you know that wx plus b larger than 0.

308
00:27:28,320 --> 00:27:31,280
 So we talked about the size earlier on.

309
00:27:31,280 --> 00:27:33,720
 And then this is less than 0.

310
00:27:33,720 --> 00:27:38,360
 Any of the red lines exceed below this.

311
00:27:38,360 --> 00:27:44,200
 Then if we label all this square as a plus one, the red one

312
00:27:44,200 --> 00:27:49,800
 as a minus one, then the y label now

313
00:27:49,800 --> 00:27:53,560
 can the decision cross y now will be.

314
00:27:53,560 --> 00:27:55,360
 If this is large and 0, the label

315
00:27:56,120 --> 00:27:57,840
 is less than 0, negative 1.

316
00:27:57,840 --> 00:28:00,360
 So again, we're just using this charge

317
00:28:00,360 --> 00:28:05,879
 that you can formulate the problem like this.

318
00:28:05,879 --> 00:28:11,439
 So then assuming these two hyperplanes, which

319
00:28:11,439 --> 00:28:14,320
 are parallel to the decision boundary here,

320
00:28:14,320 --> 00:28:17,040
 why is xa plus b equal to k?

321
00:28:17,040 --> 00:28:18,679
 Originally, they may not be equal to 1.

322
00:28:18,679 --> 00:28:20,760
 Any k.

323
00:28:20,760 --> 00:28:23,360
 The other is k minus 0.

324
00:28:23,360 --> 00:28:25,320
 So this is our k prime.

325
00:28:25,320 --> 00:28:30,879
 Anything on the left-hand side is less than 0.

326
00:28:30,879 --> 00:28:33,600
 Then you can move this line.

327
00:28:33,600 --> 00:28:35,919
 You can adjust this line.

328
00:28:35,919 --> 00:28:41,439
 And then these two parallel lines stay such that this wxa

329
00:28:41,439 --> 00:28:42,840
 plus b equal to 1.

330
00:28:42,840 --> 00:28:45,360
 Now this line is exactly 1 rather than k.

331
00:28:45,360 --> 00:28:48,719
 And this is minus 1.

332
00:28:49,160 --> 00:28:54,720
 So these two points, xa and xc, are the nearest

333
00:28:54,720 --> 00:28:57,240
 to this decision boundary, one from positive,

334
00:28:57,240 --> 00:28:59,400
 one from negative class.

335
00:28:59,400 --> 00:29:01,000
 Assuming you have this.

336
00:29:01,000 --> 00:29:04,640
 Now xa, this line is 1 and minus 1.

337
00:29:04,640 --> 00:29:08,200
 You can always do that because w and b and k,

338
00:29:08,200 --> 00:29:12,760
 they are just a scalar, integer.

339
00:29:12,760 --> 00:29:15,000
 You can always move the line slightly

340
00:29:15,000 --> 00:29:18,040
 and then the class 1 minus 1 are equal distance

341
00:29:18,040 --> 00:29:19,920
 to the decision boundary.

342
00:29:19,920 --> 00:29:21,080
 Then you normalize it.

343
00:29:21,080 --> 00:29:22,600
 So the one is class 1.

344
00:29:22,600 --> 00:29:24,760
 The other is minus 1.

345
00:29:24,760 --> 00:29:31,000
 And b will be just specifying the line in the middle.

346
00:29:31,000 --> 00:29:32,400
 Then you subtract these two.

347
00:29:32,400 --> 00:29:36,440
 You are going to find that this d, which is the distance,

348
00:29:36,440 --> 00:29:38,120
 now is the margin.

349
00:29:38,120 --> 00:29:43,160
 If you look at this one, xa, xc, dog product with w, w

350
00:29:43,920 --> 00:29:45,480
 is perpendicular to the line.

351
00:29:45,480 --> 00:29:49,760
 So this is xa minus xc is this one.

352
00:29:49,760 --> 00:29:54,600
 xa minus xc is this dog product with w.

353
00:29:54,600 --> 00:29:59,360
 It will be the distance, which is the margin d.

354
00:29:59,360 --> 00:30:05,000
 Again, this is a simple so-called algebra vector

355
00:30:05,000 --> 00:30:05,800
 manipulation.

356
00:30:05,800 --> 00:30:09,560
 If you have forgotten this, go back to check up.

357
00:30:09,560 --> 00:30:12,760
 So then the margin, now we go to two nearby.

358
00:30:13,520 --> 00:30:21,600
 The magnitude of w, which is the length of this w vector.

359
00:30:21,600 --> 00:30:25,480
 So that will be the margin.

360
00:30:25,480 --> 00:30:29,160
 So xa and xc, these are the two points closer

361
00:30:29,160 --> 00:30:30,440
 to the decision boundary.

362
00:30:30,440 --> 00:30:36,200
 We call this support vector, support vector, xa and xc.

363
00:30:36,960 --> 00:30:43,240
 And then you can find the w so that it

364
00:30:43,240 --> 00:30:47,840
 allows you to maximize the distance d2 here.

365
00:30:47,840 --> 00:30:56,120
 And one thing that you can notice, the w

366
00:30:56,120 --> 00:31:01,440
 is the vector which is perpendicular to this decision

367
00:31:01,440 --> 00:31:03,760
 boundary, w.

368
00:31:04,320 --> 00:31:09,680
 With any line which is above this plus one,

369
00:31:09,680 --> 00:31:11,320
 this will be your training sample.

370
00:31:11,320 --> 00:31:13,920
 This is your negative sample.

371
00:31:13,920 --> 00:31:17,920
 So your goal now is to learn the parameter w and b

372
00:31:17,920 --> 00:31:22,560
 such that for all the training sample yi

373
00:31:22,560 --> 00:31:25,480
 equal to one positive class, this is larger than one.

374
00:31:25,480 --> 00:31:28,200
 All the point here, larger than one.

375
00:31:28,200 --> 00:31:31,240
 And this is less than one, minus one.

376
00:31:31,800 --> 00:31:34,280
 So you can see if yi is minus one,

377
00:31:34,280 --> 00:31:38,760
 because all the red point will have a label minus one.

378
00:31:38,760 --> 00:31:43,120
 And then this is wx plus b equal to minus one line.

379
00:31:43,120 --> 00:31:46,760
 Then the rest will be less than minus one.

380
00:31:46,760 --> 00:31:50,400
 Then you can merge these two equations into one.

381
00:31:50,400 --> 00:31:55,560
 And positive and negative now for all the point one to n.

382
00:31:55,560 --> 00:31:57,720
 Because yi is positive or negative,

383
00:31:57,720 --> 00:32:00,600
 if you multiply this, if yi is minus one,

384
00:32:01,480 --> 00:32:04,240
 then this exactly will become this formula.

385
00:32:04,240 --> 00:32:07,000
 So now for all your training data,

386
00:32:07,000 --> 00:32:10,199
 you just have to focus on this equation.

387
00:32:10,199 --> 00:32:12,879
 You make sure all your training data with the label

388
00:32:12,879 --> 00:32:16,199
 plus one and minus one, they all have to fulfill

389
00:32:16,199 --> 00:32:21,199
 this one constraint, which is yi multiplied wxi plus b

390
00:32:23,040 --> 00:32:24,800
 has to be larger than equal to one.

391
00:32:24,960 --> 00:32:29,960
 And then you want to find the margin such that

392
00:32:31,159 --> 00:32:34,480
 this is maximum, and that means w,

393
00:32:34,480 --> 00:32:39,480
 the square of the distance, the length,

394
00:32:40,399 --> 00:32:42,120
 divided by two will be minimum.

395
00:32:42,120 --> 00:32:43,399
 It's just the opposite.

396
00:32:45,680 --> 00:32:50,680
 So SVM now, basically giving you a set of training data,

397
00:32:50,840 --> 00:32:55,680
 xi and yi label, you now can find the SVM,

398
00:32:55,680 --> 00:33:00,680
 which is the decision boundary w dot x plus b equal to zero.

399
00:33:02,600 --> 00:33:06,360
 And such that you'll find all the possible w and b

400
00:33:06,360 --> 00:33:08,560
 that satisfy this two set of equation.

401
00:33:08,560 --> 00:33:13,240
 One is this equation for all the training sample,

402
00:33:13,240 --> 00:33:16,120
 and then this one has to minimize.

403
00:33:16,120 --> 00:33:17,960
 Because if you want to maximize the margin,

404
00:33:17,960 --> 00:33:21,360
 you have to minimize this opposite of this.

405
00:33:22,600 --> 00:33:24,440
 And this one, you can solve this problem

406
00:33:24,440 --> 00:33:27,960
 by using a Lagrange multiplier method.

407
00:33:27,960 --> 00:33:30,960
 So for those who have studied engineering mathematics,

408
00:33:30,960 --> 00:33:33,120
 and they are packets for you to do that,

409
00:33:33,120 --> 00:33:36,840
 every problem you formulate at this so-called

410
00:33:36,840 --> 00:33:41,840
 constraint equation, which you are given a set of xi,

411
00:33:42,680 --> 00:33:47,680
 you want to find w and b such that all these xi, yi,

412
00:33:47,920 --> 00:33:50,200
 fulfill this constraint.

413
00:33:50,200 --> 00:33:53,920
 And then among this, you want to minimize some of these

414
00:33:53,920 --> 00:33:58,300
 so-called variable that you are looking for the answer.

415
00:33:58,300 --> 00:34:01,760
 And that is a standard problem you can always solve

416
00:34:01,760 --> 00:34:05,080
 by using Lagrange multiplier method.

417
00:34:06,160 --> 00:34:07,840
 So I leave it to you for you to go and check

418
00:34:07,840 --> 00:34:09,520
 the engineering mathematics.

419
00:34:09,520 --> 00:34:11,840
 Once you formulate the problem like this,

420
00:34:11,840 --> 00:34:15,520
 you can always find a solution to that problem.

421
00:34:16,199 --> 00:34:21,199
 However, the word is not so simple.

422
00:34:22,639 --> 00:34:26,040
 What if now you have two new training sample,

423
00:34:26,040 --> 00:34:27,719
 n and p here?

424
00:34:27,719 --> 00:34:30,840
 Before then, you actually find this is your boundary,

425
00:34:30,840 --> 00:34:34,040
 but now you find that these two points were

426
00:34:34,040 --> 00:34:36,520
 on the opposite side of the boundary.

427
00:34:36,520 --> 00:34:41,040
 This suppose belong to the red class minus one.

428
00:34:41,040 --> 00:34:45,239
 This suppose belong to the positive plus one class.

429
00:34:45,279 --> 00:34:48,600
 But now with either with the noisy data

430
00:34:48,600 --> 00:34:53,600
 or is the actual data that it's just not so perfect.

431
00:34:53,759 --> 00:34:57,399
 You cannot always expect there is a decision boundary

432
00:34:57,399 --> 00:35:02,399
 separate the so-called two classes so nicely.

433
00:35:04,359 --> 00:35:07,200
 Then if you, with these two new sample,

434
00:35:07,200 --> 00:35:10,979
 if you still try to find the maximum margin,

435
00:35:10,979 --> 00:35:12,560
 let me erase the...

436
00:35:15,759 --> 00:35:21,000
 So you can find this.

437
00:35:21,000 --> 00:35:25,240
 Now the original D one no longer possible

438
00:35:25,240 --> 00:35:28,319
 to separate all the positive negative.

439
00:35:28,319 --> 00:35:31,580
 And you can find the D two here.

440
00:35:31,580 --> 00:35:35,160
 Now the margin is smaller, the D is smaller.

441
00:35:36,279 --> 00:35:39,640
 And it can do the job, but the question now is,

442
00:35:39,640 --> 00:35:44,240
 is the D one or D two a better decision boundary?

443
00:35:44,240 --> 00:35:47,879
 So pay in mind that this could be noisy data.

444
00:35:48,799 --> 00:35:51,959
 It could be actually original here,

445
00:35:51,959 --> 00:35:53,040
 but because of noise,

446
00:35:53,040 --> 00:35:55,799
 it kind of come over to the decision boundary.

447
00:35:55,799 --> 00:35:58,439
 Although you try to fix the problem for these two,

448
00:35:58,439 --> 00:36:00,680
 but for many other unseen data,

449
00:36:00,680 --> 00:36:02,240
 you may make more errors.

450
00:36:03,700 --> 00:36:07,600
 So if your goal is to still find the maximum margin,

451
00:36:07,600 --> 00:36:11,439
 while you accept some possible noisy data,

452
00:36:11,440 --> 00:36:14,160
 so D two may not be the best solution.

453
00:36:15,600 --> 00:36:20,000
 So then you can actually twist the problem formulation

454
00:36:20,000 --> 00:36:25,000
 into a new one, which allow you to have some,

455
00:36:25,360 --> 00:36:29,000
 this error, okay?

456
00:36:29,000 --> 00:36:34,000
 This side, the side, the small error that originally here,

457
00:36:34,960 --> 00:36:38,480
 but allow this point to move slightly over

458
00:36:38,480 --> 00:36:41,000
 to the positive direction.

459
00:36:41,000 --> 00:36:44,160
 And this one move to the negative direction.

460
00:36:44,160 --> 00:36:48,660
 So we call this a slack variable psi, okay?

461
00:36:49,440 --> 00:36:52,640
 This one now, G, why I still positive one,

462
00:36:52,640 --> 00:36:54,400
 before then is larger than one, right?

463
00:36:54,400 --> 00:36:59,400
 But you allow a small error slack variable minus Xi.

464
00:37:00,200 --> 00:37:02,480
 So you move to this direction.

465
00:37:02,480 --> 00:37:05,520
 And this one originals will be less than minus one,

466
00:37:05,520 --> 00:37:09,720
 but now you allow this move to positive direction psi.

467
00:37:11,080 --> 00:37:12,800
 You don't know how much they will move,

468
00:37:12,800 --> 00:37:14,760
 but based on the train data,

469
00:37:14,760 --> 00:37:17,400
 you, with this so-called slack variable,

470
00:37:17,400 --> 00:37:21,000
 you can still fit them all the data

471
00:37:21,000 --> 00:37:23,720
 into one single constraint equation.

472
00:37:25,760 --> 00:37:28,240
 We call this a soft margin approach,

473
00:37:29,080 --> 00:37:33,120
 applicable for car cases that which may not have a complete

474
00:37:33,120 --> 00:37:36,320
 or clean separation by the boundary.

475
00:37:37,320 --> 00:37:41,800
 Then you can now formulate the problem as this, right?

476
00:37:41,800 --> 00:37:45,120
 You still subject to this constraint with Xi,

477
00:37:45,120 --> 00:37:48,960
 the so-called slack variable here, right?

478
00:37:48,960 --> 00:37:52,680
 With this for all the possible sample,

479
00:37:52,680 --> 00:37:56,560
 while you minimize this summation of this Xi,

480
00:37:56,560 --> 00:37:58,880
 sometimes they are zero because they are not really

481
00:37:58,880 --> 00:37:59,880
 across the boundary.

482
00:37:59,880 --> 00:38:02,040
 For those really move across the boundary,

483
00:38:02,040 --> 00:38:04,280
 then you have a small value here.

484
00:38:04,400 --> 00:38:08,080
 So rather you minimize this, which maximize the margin,

485
00:38:08,080 --> 00:38:11,360
 you also have another slack variable,

486
00:38:11,360 --> 00:38:14,080
 you want to minimize this, right?

487
00:38:14,080 --> 00:38:15,400
 Minimum tolerance.

488
00:38:16,280 --> 00:38:19,560
 And of course you can put the weightage Xi

489
00:38:19,560 --> 00:38:23,600
 to see which one you want to keep more weightage.

490
00:38:23,600 --> 00:38:24,800
 If you make Xi small,

491
00:38:24,800 --> 00:38:29,520
 then you'll try to focus on maximizing the margin.

492
00:38:29,520 --> 00:38:32,440
 If you make this so-called Xi large,

493
00:38:32,440 --> 00:38:36,760
 that means you'll focus on maximizing the W, the Xi,

494
00:38:36,760 --> 00:38:40,880
 the all the so-called slack variable, right?

495
00:38:40,880 --> 00:38:43,360
 Then your margin may be smaller, right?

496
00:38:43,360 --> 00:38:45,920
 Rather than maximizing, which is basically,

497
00:38:45,920 --> 00:38:47,400
 if you put a lot of weightage,

498
00:38:47,400 --> 00:38:49,320
 you don't want to have any error,

499
00:38:49,320 --> 00:38:52,720
 you may actually end up with this D2, right?

500
00:38:52,720 --> 00:38:55,560
 When the C, the weightage, you start to change,

501
00:38:55,560 --> 00:38:58,680
 you may move from D1 to D2.

502
00:38:58,680 --> 00:39:01,920
 Then you can kind of decide which one

503
00:39:02,120 --> 00:39:04,520
 you want to give more weightage.

504
00:39:04,520 --> 00:39:05,600
 And with this formulation,

505
00:39:05,600 --> 00:39:08,720
 you can still solve this problem, right?

506
00:39:08,720 --> 00:39:11,360
 And again, go and check how to solve this.

507
00:39:11,360 --> 00:39:12,900
 And then at the end, you'll find

508
00:39:12,900 --> 00:39:17,360
 what is the best value of W and B, right?

509
00:39:17,360 --> 00:39:20,240
 This is still your classifier

510
00:39:20,240 --> 00:39:24,240
 for get about all the past one minus one, the Xi.

511
00:39:25,400 --> 00:39:29,760
 Ultimately, what you care about is the W and B, right?

512
00:39:29,760 --> 00:39:32,000
 So when you have a new test sample X,

513
00:39:32,000 --> 00:39:36,040
 you just plug into WX plus B.

514
00:39:36,040 --> 00:39:37,800
 If this sign is positive,

515
00:39:37,800 --> 00:39:42,800
 then belong to the square class.

516
00:39:43,160 --> 00:39:46,480
 Otherwise, it's the circle class, okay?

517
00:39:46,480 --> 00:39:51,480
 So that is how you compute the support vector machine.

518
00:39:52,280 --> 00:39:54,280
 All of this is only for linear case.

519
00:39:54,280 --> 00:39:55,800
 They are also more complex,

520
00:39:55,800 --> 00:39:58,080
 like nonlinear support vector machine,

521
00:39:58,080 --> 00:40:03,000
 which I leave it to you for you to explore more

522
00:40:03,000 --> 00:40:04,520
 if you are interested in that.

523
00:40:06,480 --> 00:40:08,319
 Then the next question is,

524
00:40:08,319 --> 00:40:10,720
 this only work for two classes,

525
00:40:10,720 --> 00:40:15,360
 but in the world of many, many potential application,

526
00:40:15,360 --> 00:40:18,319
 there are more than two classes, right?

527
00:40:18,319 --> 00:40:21,420
 Like the C-far, 10, C-far, 100.

528
00:40:21,420 --> 00:40:24,920
 You could have 10 or even 100 classes, right?

529
00:40:24,920 --> 00:40:29,920
 So how could you so-called extend the SVM

530
00:40:30,160 --> 00:40:34,040
 to cope with all these multiple classes?

531
00:40:35,200 --> 00:40:40,120
 So of course, you can do that by first,

532
00:40:40,120 --> 00:40:42,440
 there are a few possible way to do it.

533
00:40:42,440 --> 00:40:47,440
 One is you can perform this one against the other, right?

534
00:40:49,400 --> 00:40:52,720
 Let's say you have K classes, right?

535
00:40:52,839 --> 00:40:56,839
 Now you can design K different support vector machine.

536
00:40:56,839 --> 00:41:01,439
 One is whether it belongs to class one or non-class one.

537
00:41:01,439 --> 00:41:05,240
 Class two or non-class two, et cetera, et cetera.

538
00:41:06,080 --> 00:41:07,600
 Then for every test sample,

539
00:41:07,600 --> 00:41:12,600
 you test against all these K support vector machine, right?

540
00:41:13,640 --> 00:41:17,160
 And then the one produce the highest value,

541
00:41:17,160 --> 00:41:21,600
 WX plus B for vector machine, one, two, three, four.

542
00:41:21,640 --> 00:41:25,480
 So if it's the largest value, positive value,

543
00:41:25,480 --> 00:41:28,600
 that means that it's very strong evidence,

544
00:41:28,600 --> 00:41:31,960
 very far away from this decision boundary.

545
00:41:31,960 --> 00:41:36,440
 So you can say that belong to class number one, for example.

546
00:41:36,440 --> 00:41:38,200
 Or, right?

547
00:41:38,200 --> 00:41:41,520
 Because the other is non-class number one, right?

548
00:41:41,520 --> 00:41:44,120
 You can find out all these to see

549
00:41:44,120 --> 00:41:47,600
 which one give you the highest possible positive value

550
00:41:47,600 --> 00:41:49,000
 that you belong to.

551
00:41:49,000 --> 00:41:52,520
 This is either class one or class two, and you'll class K.

552
00:41:55,840 --> 00:41:58,760
 And of course there are other options.

553
00:41:58,760 --> 00:42:02,760
 And this one you have more classifier,

554
00:42:02,760 --> 00:42:07,760
 but it built in some kind of so-called wisdom of crowd.

555
00:42:09,960 --> 00:42:13,440
 They kind of idea when we talk about the random forest, right?

556
00:42:13,440 --> 00:42:18,440
 So rather than K, now you have NC2, right?

557
00:42:18,800 --> 00:42:22,480
 Which is all the possible pair of classes,

558
00:42:22,480 --> 00:42:26,320
 class one versus two, class one versus three,

559
00:42:26,320 --> 00:42:29,840
 under class one versus K, class K, right?

560
00:42:29,840 --> 00:42:33,520
 Then two versus class one, or class three,

561
00:42:33,520 --> 00:42:35,040
 you're going to repeat again.

562
00:42:35,040 --> 00:42:37,000
 Two versus class four, right?

563
00:42:37,000 --> 00:42:40,120
 And you are going to have K times K minus one,

564
00:42:40,120 --> 00:42:44,080
 thereby two of such support vector machine, right?

565
00:42:44,960 --> 00:42:49,960
 Then you evaluate which one class I

566
00:42:50,000 --> 00:42:51,640
 has the highest number of vote.

567
00:42:51,640 --> 00:42:54,000
 Because after you go through all this,

568
00:42:54,000 --> 00:42:56,279
 at this one, it is, oh, this is a class one.

569
00:42:56,279 --> 00:42:57,640
 Oh, this is class three.

570
00:42:57,640 --> 00:43:00,920
 And this is class, maybe class three again, right?

571
00:43:00,920 --> 00:43:03,000
 And this is class K, right?

572
00:43:03,000 --> 00:43:06,000
 So at the end you'll find that among all the K times K

573
00:43:06,000 --> 00:43:08,880
 minus one divided by two, many of them say

574
00:43:08,880 --> 00:43:11,279
 it belong to class three, right?

575
00:43:11,280 --> 00:43:14,800
 The one who appeared the most number of choices

576
00:43:14,800 --> 00:43:17,880
 by the classifier, then you say, okay,

577
00:43:17,880 --> 00:43:20,760
 it is the class three, right?

578
00:43:20,760 --> 00:43:24,200
 So for this one, you have more classifier,

579
00:43:24,200 --> 00:43:26,920
 but likely that with the multiple one,

580
00:43:26,920 --> 00:43:31,920
 sometimes you can actually allow more error tolerance

581
00:43:32,280 --> 00:43:36,000
 because if you build all this by using this

582
00:43:36,000 --> 00:43:40,360
 so-called non-separable class, you can do that.

583
00:43:40,400 --> 00:43:42,080
 Here I have one website.

584
00:43:42,080 --> 00:43:45,160
 If in case you are interested, you can go to the website,

585
00:43:46,160 --> 00:43:50,040
 okay, to play with this support vector machine,

586
00:43:50,920 --> 00:43:54,920
 control copy, see whether I can still access from here.

587
00:44:11,200 --> 00:44:16,200
 So this is the tool that here you can try to enlarge this.

588
00:44:20,760 --> 00:44:21,920
 It's a support vector machine.

589
00:44:21,920 --> 00:44:25,840
 You can start to specify the classes, right?

590
00:44:25,840 --> 00:44:27,880
 This is a positive class here,

591
00:44:27,880 --> 00:44:29,800
 and then change to the other class.

592
00:44:32,080 --> 00:44:33,440
 Okay, then you run.

593
00:44:33,440 --> 00:44:37,200
 So this is a support vector machine, right?

594
00:44:37,200 --> 00:44:38,720
 It is not a linear.

595
00:44:38,720 --> 00:44:43,240
 It's a kind of a, you can look at the parameters,

596
00:44:43,240 --> 00:44:47,200
 T2, so T1, run again.

597
00:44:48,960 --> 00:44:50,600
 I can put this a number.

598
00:44:52,879 --> 00:44:57,000
 You got this one is not a clear, clean separation.

599
00:45:01,279 --> 00:45:02,759
 You can go and check the value,

600
00:45:02,759 --> 00:45:05,680
 the definition of all these parameters.

601
00:45:05,680 --> 00:45:10,680
 The one is using the mu SVM, one class SVM,

602
00:45:10,680 --> 00:45:13,720
 epsilon SVM, different kind of a linear,

603
00:45:13,720 --> 00:45:15,560
 zero is a linear class.

604
00:45:15,560 --> 00:45:16,919
 One is a polynomial.

605
00:45:16,919 --> 00:45:18,799
 I put the zero is a linear.

606
00:45:18,799 --> 00:45:20,520
 Let's try it again.

607
00:45:21,359 --> 00:45:23,560
 This is another class,

608
00:45:23,560 --> 00:45:26,200
 and then change to the other class here.

609
00:45:28,359 --> 00:45:30,240
 So this is a linear classifier,

610
00:45:30,240 --> 00:45:32,480
 the one we cover in the lecture.

611
00:45:32,480 --> 00:45:34,680
 But even to use the polynomial,

612
00:45:34,680 --> 00:45:39,680
 you can put one, they'll give you a boundary, right?

613
00:45:40,480 --> 00:45:43,120
 It's not linear, but slightly the curve,

614
00:45:43,120 --> 00:45:45,520
 but there are some error here, right?

615
00:45:45,520 --> 00:45:49,680
 So you can play around with this support vector machine,

616
00:45:49,680 --> 00:45:52,279
 and you can have more than two classes.

617
00:45:52,279 --> 00:45:54,279
 Let's see, clear?

618
00:45:54,279 --> 00:45:56,359
 Okay, this is the first class,

619
00:45:56,359 --> 00:45:58,640
 then I change to another class,

620
00:46:00,000 --> 00:46:01,879
 and then change to third class.

621
00:46:02,880 --> 00:46:03,880
 Okay, right.

622
00:46:05,200 --> 00:46:08,280
 So now you can separate into three area

623
00:46:08,280 --> 00:46:12,200
 based on the region is similar to the nearest neighbor, right?

624
00:46:12,200 --> 00:46:13,960
 You pick any test point,

625
00:46:13,960 --> 00:46:16,880
 then you test belong to these three classes,

626
00:46:16,880 --> 00:46:18,280
 which one they belong to,

627
00:46:18,280 --> 00:46:22,560
 then you shape the area belong to these three.

628
00:46:23,400 --> 00:46:24,240
 Right, okay?

629
00:46:27,080 --> 00:46:30,440
 Yeah, if I try to use another zero here,

630
00:46:31,320 --> 00:46:36,320
 run, so this is the linear classifier

631
00:46:36,920 --> 00:46:39,400
 for the support vector machine.

632
00:46:44,080 --> 00:46:46,360
 Okay, so before we go,

633
00:46:46,360 --> 00:46:47,720
 let's, I have achieved,

634
00:46:47,720 --> 00:46:48,800
 so you have a few,

635
00:46:48,800 --> 00:46:51,240
 how do you compute the support vector machine?

636
00:46:51,240 --> 00:46:54,760
 If you have to do the hang calculation,

637
00:46:54,760 --> 00:46:56,000
 which is something that,

638
00:46:56,000 --> 00:46:57,520
 of course, just now we show you the two,

639
00:46:57,520 --> 00:46:59,560
 you don't have to really calculate yourself,

640
00:46:59,560 --> 00:47:02,520
 you just have to know how to prepare the data,

641
00:47:02,520 --> 00:47:05,080
 fit into the formula,

642
00:47:05,080 --> 00:47:07,520
 then you can get your solution.

643
00:47:07,520 --> 00:47:10,000
 But if you were to want to do these,

644
00:47:10,000 --> 00:47:14,400
 okay, let's look into how do you really compute

645
00:47:14,400 --> 00:47:16,279
 using the Lagrange multiplier

646
00:47:16,279 --> 00:47:19,680
 to compute the w and the b, right?

647
00:47:22,120 --> 00:47:24,120
 So this is optional,

648
00:47:24,120 --> 00:47:25,799
 in case you don't like the mathematics,

649
00:47:25,799 --> 00:47:28,920
 you can skip, but since SVM is so important,

650
00:47:28,920 --> 00:47:32,080
 in fact SVM, you can think about the perceptron

651
00:47:32,080 --> 00:47:33,800
 on neural network,

652
00:47:33,800 --> 00:47:34,720
 it's kind of a,

653
00:47:37,200 --> 00:47:39,680
 a more kind of expanded SVM,

654
00:47:39,680 --> 00:47:44,680
 because each of the neurons is also a classifier,

655
00:47:45,000 --> 00:47:46,960
 binary classifier.

656
00:47:46,960 --> 00:47:49,440
 So in case you probably did not hear,

657
00:47:49,440 --> 00:47:50,460
 early this week,

658
00:47:52,160 --> 00:47:55,200
 there were two Nobel Prize awarded,

659
00:47:55,480 --> 00:47:56,799
 right?

660
00:47:56,799 --> 00:48:01,240
 Jeff Hinton, the guy who started the neural network,

661
00:48:01,240 --> 00:48:04,799
 he got the Nobel Prize in physics, right?

662
00:48:04,799 --> 00:48:05,640
 Computer science,

663
00:48:05,640 --> 00:48:08,839
 there is a Nobel Prize in computer science,

664
00:48:08,839 --> 00:48:10,080
 so they give him one,

665
00:48:10,080 --> 00:48:12,960
 together with another professor in Princeton,

666
00:48:12,960 --> 00:48:16,600
 they got this year Nobel Prize in physics,

667
00:48:16,600 --> 00:48:20,480
 then the next day there's a Nobel Prize in chemistry,

668
00:48:21,759 --> 00:48:24,399
 by the one the alpha fold, right?

669
00:48:24,440 --> 00:48:27,440
 So they're doing the so-called DNA,

670
00:48:28,960 --> 00:48:30,040
 they recognize,

671
00:48:30,040 --> 00:48:34,120
 I think I haven't seen such a case so fast,

672
00:48:34,120 --> 00:48:37,760
 and then turned out they reward the two to Nobel Prize,

673
00:48:37,760 --> 00:48:38,600
 right?

674
00:48:38,600 --> 00:48:39,680
 And my daughter tell me,

675
00:48:39,680 --> 00:48:42,040
 they have nothing to do with chemistry,

676
00:48:42,040 --> 00:48:44,480
 it's a computer science problem, right?

677
00:48:44,480 --> 00:48:46,200
 The other physics is, right?

678
00:48:46,200 --> 00:48:47,680
 It's a computer science, okay?

679
00:48:49,080 --> 00:48:50,080
 So anyway,

680
00:48:51,480 --> 00:48:53,640
 the constraint optimization problem

681
00:48:53,640 --> 00:48:56,720
 is the one we saw earlier, right?

682
00:48:56,720 --> 00:48:58,759
 The Lagrange multiplier basically,

683
00:48:58,759 --> 00:49:00,560
 you can group these two together,

684
00:49:00,560 --> 00:49:03,799
 remember we want to minimize this, right?

685
00:49:03,799 --> 00:49:08,279
 And then we want to make sure this equal to this

686
00:49:08,279 --> 00:49:11,240
 for all the training sample, right?

687
00:49:11,240 --> 00:49:13,200
 So, but you have two constraints,

688
00:49:13,200 --> 00:49:14,400
 to solve a constraint problem,

689
00:49:14,400 --> 00:49:16,759
 you cannot just do that,

690
00:49:16,759 --> 00:49:20,400
 you can combine the two together, right?

691
00:49:20,400 --> 00:49:23,080
 Then it become optimization problem

692
00:49:23,080 --> 00:49:25,759
 by introducing this lambda i,

693
00:49:25,759 --> 00:49:28,000
 which are called Lagrange multiplier.

694
00:49:29,840 --> 00:49:32,360
 Again, if you don't like the mathematics,

695
00:49:32,360 --> 00:49:35,440
 you can close your eyes, forget all this,

696
00:49:35,440 --> 00:49:38,200
 but it will help you to have a deeper understanding

697
00:49:38,200 --> 00:49:42,799
 about how you compute those WMP.

698
00:49:42,799 --> 00:49:46,600
 So lambda i's are Lagrange multiplier,

699
00:49:46,600 --> 00:49:48,520
 then your objective now is,

700
00:49:48,520 --> 00:49:52,000
 early on we want to minimize this number, right?

701
00:49:52,040 --> 00:49:56,560
 Minimizing this actually will be maximizing your margin, right?

702
00:49:56,560 --> 00:50:00,840
 You want this equal to the constraint equation,

703
00:50:00,840 --> 00:50:03,320
 larger than equal to i, one, right?

704
00:50:03,320 --> 00:50:07,280
 This one, you want this to be larger equal to one, right?

705
00:50:07,280 --> 00:50:08,320
 So that mean,

706
00:50:12,120 --> 00:50:14,280
 if you minimize this,

707
00:50:14,280 --> 00:50:17,120
 you want this all to be larger than one,

708
00:50:17,120 --> 00:50:18,600
 because it's negative here,

709
00:50:18,600 --> 00:50:20,880
 it will be minimized as well, right?

710
00:50:20,880 --> 00:50:22,920
 So you also satisfy all the equation

711
00:50:22,920 --> 00:50:26,120
 if you try to minimize this, okay?

712
00:50:26,120 --> 00:50:31,120
 So then your goal now is to find WB and the Lagrange

713
00:50:31,120 --> 00:50:36,120
 multiplier, which are the new parameters you introduce.

714
00:50:38,440 --> 00:50:41,000
 First you will minimizing this,

715
00:50:41,000 --> 00:50:42,960
 we call this Lagrangian,

716
00:50:42,960 --> 00:50:47,360
 the new so-called cost value function.

717
00:50:47,360 --> 00:50:49,280
 But to minimize any of those things,

718
00:50:49,280 --> 00:50:51,760
 you take the partial differentiation

719
00:50:51,760 --> 00:50:54,600
 with respect to the unknown W,

720
00:50:54,600 --> 00:50:55,720
 which if you do this,

721
00:50:55,720 --> 00:50:59,280
 you'll find that W will be equal to this formula,

722
00:50:59,280 --> 00:51:02,880
 W and this, lambda i, yi xi.

723
00:51:04,200 --> 00:51:06,560
 Remember, yi is the cost labor,

724
00:51:06,560 --> 00:51:09,480
 plus one minus one, so easy to compute.

725
00:51:09,480 --> 00:51:13,460
 Xi is the value of the attribute, the vector, right?

726
00:51:13,460 --> 00:51:16,360
 For 2D, just x1, x2,

727
00:51:16,360 --> 00:51:20,240
 but it could be a multi-dimensional vector.

728
00:51:20,240 --> 00:51:21,960
 Then if you have your lambda,

729
00:51:21,960 --> 00:51:23,160
 assuming you find your lambda,

730
00:51:23,160 --> 00:51:28,160
 then W can be compute by summing up all the training sample,

731
00:51:29,120 --> 00:51:33,360
 xi, yi, multiply with lambda, right?

732
00:51:33,360 --> 00:51:36,560
 It's such a very simple calculation.

733
00:51:36,560 --> 00:51:39,640
 Because you know your yi, you know your xi,

734
00:51:39,640 --> 00:51:42,320
 once you have computed all your lambda i,

735
00:51:42,320 --> 00:51:44,600
 you just multiply these together,

736
00:51:45,040 --> 00:51:46,000
 and sum them up,

737
00:51:46,000 --> 00:51:47,680
 that will be your value of W.

738
00:51:49,520 --> 00:51:52,839
 And then of course, you still have another variable B.

739
00:51:52,839 --> 00:51:56,560
 If you again, partial differentiate with W to B,

740
00:51:56,560 --> 00:52:00,360
 you'll find that lambda i, yi has to be equal to zero

741
00:52:00,360 --> 00:52:01,200
 of the summation.

742
00:52:01,200 --> 00:52:02,799
 This is another new constraint.

743
00:52:04,720 --> 00:52:09,720
 So this two set of equation will be the so-called,

744
00:52:10,560 --> 00:52:12,279
 the computation you need later on

745
00:52:12,279 --> 00:52:14,520
 to use them to compute your W,

746
00:52:14,520 --> 00:52:16,680
 then lambda i, right?

747
00:52:17,680 --> 00:52:21,440
 If you do that, you can practice W back to here.

748
00:52:21,440 --> 00:52:23,080
 You will get this so-called,

749
00:52:23,080 --> 00:52:25,320
 the dual optimization problem.

750
00:52:25,320 --> 00:52:26,920
 Now W disappear.

751
00:52:26,920 --> 00:52:30,920
 The new variable will be lambda i, lambda j, right?

752
00:52:31,820 --> 00:52:36,080
 And then this Khrush-Kuhn-Tucker condition.

753
00:52:36,080 --> 00:52:39,200
 Whereby, I'll call this one, you can go and check.

754
00:52:39,200 --> 00:52:40,480
 By solving this problem,

755
00:52:40,480 --> 00:52:45,480
 you need to make sure lambda i are all equal or larger than zero.

756
00:52:46,040 --> 00:52:49,160
 And then this one now, in order to minimize this,

757
00:52:49,160 --> 00:52:53,560
 this is actually lambda i times this will be equal to zero.

758
00:52:53,560 --> 00:52:56,960
 So this will be the two set of condition together

759
00:52:56,960 --> 00:53:00,920
 with this, this, those in the square box,

760
00:53:00,920 --> 00:53:03,280
 will be given yi and xi.

761
00:53:03,280 --> 00:53:06,920
 You have to find all the lambda i,

762
00:53:06,920 --> 00:53:09,440
 and then if you fill all this equation

763
00:53:09,480 --> 00:53:13,880
 into the box that I draw there,

764
00:53:13,880 --> 00:53:16,800
 that would allow you to solve this problem.

765
00:53:16,800 --> 00:53:18,960
 It becomes solvable, right?

766
00:53:21,240 --> 00:53:23,720
 And once you have that,

767
00:53:23,720 --> 00:53:25,600
 and this condition also tell you

768
00:53:25,600 --> 00:53:30,600
 that the multiplier must be equal to larger than zero.

769
00:53:31,000 --> 00:53:33,760
 And then it will be zero

770
00:53:33,760 --> 00:53:37,880
 unless the training sample satisfies the condition.

771
00:53:38,160 --> 00:53:43,160
 So if the training samples are exactly on this plus one line,

772
00:53:46,600 --> 00:53:50,240
 and yi will be non-zero, okay?

773
00:53:51,280 --> 00:53:54,840
 Otherwise, for those, if this is not equal to zero,

774
00:53:54,840 --> 00:53:56,720
 this one is not equal to zero,

775
00:53:56,720 --> 00:53:59,080
 and lambda i will be equal to zero.

776
00:54:00,680 --> 00:54:05,280
 So the sample that when this is satisfied,

777
00:54:05,280 --> 00:54:07,040
 they are called support vector.

778
00:54:08,040 --> 00:54:09,880
 If you go back to the earlier on,

779
00:54:09,880 --> 00:54:12,080
 when we decide the boundary here,

780
00:54:12,080 --> 00:54:17,080
 something that we can pay attention to,

781
00:54:18,800 --> 00:54:20,280
 when you decide this w,

782
00:54:20,280 --> 00:54:25,280
 on these two points here are so-called necessary

783
00:54:25,680 --> 00:54:28,640
 for you to decide what is the value of w.

784
00:54:28,640 --> 00:54:30,400
 The rest of them are not important.

785
00:54:30,400 --> 00:54:33,600
 I can add all new training sample here,

786
00:54:33,600 --> 00:54:35,720
 or training sample here.

787
00:54:35,720 --> 00:54:38,160
 They will not change my decision boundary.

788
00:54:39,000 --> 00:54:41,880
 They only depend on those positive and negative points

789
00:54:41,880 --> 00:54:44,240
 which are nearest to the boundary.

790
00:54:45,160 --> 00:54:49,440
 And such points are known as support vectors.

791
00:54:49,440 --> 00:54:52,279
 Therefore, this is called support vector machine.

792
00:54:52,279 --> 00:54:54,240
 And those points on the boundary,

793
00:54:54,240 --> 00:54:57,720
 they will have lambda i larger than zero.

794
00:54:58,920 --> 00:55:02,680
 Those points, they are not on these two boundary,

795
00:55:02,680 --> 00:55:07,040
 above this, they will not have any inference

796
00:55:07,040 --> 00:55:08,600
 on where the boundaries are.

797
00:55:09,640 --> 00:55:14,640
 So in some sense, support vector machine also robust

798
00:55:14,960 --> 00:55:17,600
 to noisy training sample,

799
00:55:17,600 --> 00:55:19,720
 as long as they are not really near

800
00:55:19,720 --> 00:55:21,720
 to the decision boundary.

801
00:55:21,720 --> 00:55:23,160
 You could have outlier.

802
00:55:24,399 --> 00:55:27,080
 If you think about other training sample

803
00:55:27,080 --> 00:55:28,480
 like the neural network,

804
00:55:28,480 --> 00:55:30,000
 if you have outlier,

805
00:55:30,000 --> 00:55:33,480
 if you still want to so-called fit the data well,

806
00:55:33,480 --> 00:55:36,240
 and then they have to change, train the parameter

807
00:55:36,240 --> 00:55:39,120
 to include that particular outlier.

808
00:55:40,000 --> 00:55:42,600
 But support vector machine will just ignore the outlier,

809
00:55:42,600 --> 00:55:46,560
 as long as they are not really along the decision boundary.

810
00:55:46,560 --> 00:55:51,560
 So the boundary will not be affected by those outliers.

811
00:55:52,800 --> 00:55:54,440
 So those training sample,

812
00:55:54,440 --> 00:55:58,520
 they are not along the d1, i1, d2.

813
00:55:58,520 --> 00:56:02,840
 They would have the nunch multiplier equal to zero.

814
00:56:02,840 --> 00:56:06,200
 That means they are not involved in making the decision.

815
00:56:06,200 --> 00:56:10,759
 Because all the here, w, right?

816
00:56:10,759 --> 00:56:13,000
 Actually, if lambda i equal to zero,

817
00:56:13,000 --> 00:56:17,600
 those point y and xi would not have any say about this w.

818
00:56:18,800 --> 00:56:23,400
 Those non-zero lambda i only apply to those point

819
00:56:23,400 --> 00:56:27,480
 on the so-called two, so-called boundary,

820
00:56:27,520 --> 00:56:30,480
 hyperplane, and they are called support vectors.

821
00:56:35,160 --> 00:56:36,480
 You can solve this problem,

822
00:56:36,480 --> 00:56:38,800
 the dual optimization problem,

823
00:56:38,800 --> 00:56:42,360
 by using this Karushkun Tucker condition,

824
00:56:42,360 --> 00:56:44,960
 one, two, three, and then you can solve it.

825
00:56:45,920 --> 00:56:47,520
 And once you solve this problem,

826
00:56:47,520 --> 00:56:50,000
 any of these are test sample z here,

827
00:56:50,000 --> 00:56:51,560
 and this is actually your w.

828
00:56:51,560 --> 00:56:54,680
 You just lambda i, y, xi sum them up.

829
00:56:54,680 --> 00:56:59,680
 This is actually the w that we compute.z plus b.

830
00:57:01,839 --> 00:57:04,839
 Then you can solve this problem.

831
00:57:10,120 --> 00:57:12,640
 So here's, let's look at a few example here.

832
00:57:15,359 --> 00:57:19,359
 Now, if I give you this eight training sample,

833
00:57:19,359 --> 00:57:23,960
 one belong to the class red one minus one,

834
00:57:23,960 --> 00:57:26,720
 and then this is a positive one plus one.

835
00:57:26,720 --> 00:57:30,400
 So x one, x two, x three, x four,

836
00:57:30,400 --> 00:57:34,400
 then x five, x six, x seven, x eight.

837
00:57:36,040 --> 00:57:38,560
 So this case, of course, if you just look at it,

838
00:57:38,560 --> 00:57:40,240
 you know that your decision boundary

839
00:57:40,240 --> 00:57:42,960
 likely to be somewhere here, right?

840
00:57:42,960 --> 00:57:45,560
 Somewhere in the middle, okay?

841
00:57:45,560 --> 00:57:49,560
 So if this is here, then,

842
00:57:51,440 --> 00:57:53,720
 which are likely the support vector, okay?

843
00:57:53,720 --> 00:57:57,919
 So this x one and x five and x six, right?

844
00:57:57,919 --> 00:57:59,040
 Will be the support vector.

845
00:57:59,040 --> 00:58:01,919
 If you choose them accordingly,

846
00:58:01,919 --> 00:58:04,120
 then lambda one, lambda five, lambda six

847
00:58:04,120 --> 00:58:07,080
 will not be equal to zero, right?

848
00:58:07,080 --> 00:58:10,640
 And then you know y one, y equal to minus one,

849
00:58:10,640 --> 00:58:12,319
 because it belong to the red class.

850
00:58:12,319 --> 00:58:15,680
 Y five, y six is a positive class one here.

851
00:58:15,680 --> 00:58:17,720
 So then you can compute your w.

852
00:58:19,399 --> 00:58:22,720
 W will be equal to the summation early on here.

853
00:58:22,759 --> 00:58:23,759
 W will be equal to this,

854
00:58:23,759 --> 00:58:28,120
 a summation of one to n lambda i, y, x, i.

855
00:58:28,120 --> 00:58:29,879
 Because they're only three point,

856
00:58:29,879 --> 00:58:33,240
 which lambda i not equal to zero.

857
00:58:33,240 --> 00:58:38,240
 So you only have lambda one, time vector one, one zero.

858
00:58:38,799 --> 00:58:42,319
 Lambda five, time three one vector,

859
00:58:42,319 --> 00:58:46,339
 and then lambda six by three minus one, okay?

860
00:58:46,339 --> 00:58:49,600
 And then your lambda has to fulfill this equation,

861
00:58:50,600 --> 00:58:55,360
 which is the minus one,

862
00:58:55,360 --> 00:59:00,360
 because for x one, the class is y one is minus one, right?

863
00:59:00,400 --> 00:59:04,080
 Minus lambda one plus lambda five, lambda six equal to zero.

864
00:59:04,080 --> 00:59:08,640
 So here's another set of equation, and this is w, right?

865
00:59:08,640 --> 00:59:11,279
 Then you can plug into this formula.

866
00:59:11,279 --> 00:59:12,940
 This one you need to,

867
00:59:15,360 --> 00:59:18,960
 these three will be either on the support vector machine, right?

868
00:59:18,960 --> 00:59:23,240
 So here, so x one had to satisfy w,

869
00:59:23,240 --> 00:59:26,040
 x one plus b equal to minus one,

870
00:59:26,040 --> 00:59:30,840
 because it's on this so-called w,

871
00:59:30,840 --> 00:59:33,480
 x plus b equal to minus one.

872
00:59:33,480 --> 00:59:36,680
 And then the other two is equal to plus one.

873
00:59:36,680 --> 00:59:39,000
 So then you can write down the formula here.

874
00:59:39,000 --> 00:59:43,560
 This is one, and then, sorry, here,

875
00:59:43,560 --> 00:59:45,280
 you can write down the formula.

876
00:59:45,280 --> 00:59:48,760
 W one is here, okay?

877
00:59:49,560 --> 00:59:52,880
 So w, you can put this value here,

878
00:59:52,880 --> 00:59:55,880
 plus b equal to minus one.

879
00:59:55,880 --> 00:59:58,040
 Then you can write down all the equation.

880
00:59:58,040 --> 01:00:00,440
 You are going to have this set of equation,

881
01:00:00,440 --> 01:00:04,320
 solving lambda one, lambda five, lambda six, right?

882
01:00:04,320 --> 01:00:07,320
 You have, and b, you are going to have four equations

883
01:00:07,320 --> 01:00:09,160
 to solve this four unknown.

884
01:00:10,680 --> 01:00:12,200
 So which is quite straightforward.

885
01:00:12,200 --> 01:00:14,160
 For example, you have study, needed algebra,

886
01:00:14,160 --> 01:00:17,120
 you can solve it, as long as they are not really

887
01:00:17,120 --> 01:00:20,200
 redundant, you can find lambda one, lambda five,

888
01:00:20,200 --> 01:00:22,600
 lambda six, and b equal to minus two.

889
01:00:24,720 --> 01:00:29,720
 Then your w will be equal to one zero, right?

890
01:00:29,839 --> 01:00:32,000
 I think there is one, let me see,

891
01:00:32,000 --> 01:00:34,359
 actually, should be this one, okay?

892
01:00:34,359 --> 01:00:36,160
 Somehow this one appear wrongly.

893
01:00:36,160 --> 01:00:38,440
 So plug in all the formula here,

894
01:00:38,440 --> 01:00:41,560
 you can find that x one is three, two.

895
01:00:41,560 --> 01:00:44,720
 There is a decision boundary, one divided by five.

896
01:00:45,560 --> 01:00:48,680
 This decision will tell you that x one is equal to one point

897
01:00:48,680 --> 01:00:51,279
 five, I will move this one to earlier, okay?

898
01:00:54,200 --> 01:00:56,720
 But how do you know which one are support vector?

899
01:00:56,720 --> 01:01:01,279
 Before, of course, in here you can eyeball the data,

900
01:01:01,279 --> 01:01:03,200
 you know that it will be there.

901
01:01:03,200 --> 01:01:06,240
 But in the reality, you could have many training samples.

902
01:01:06,240 --> 01:01:09,520
 You might not be able to tell, right?

903
01:01:09,520 --> 01:01:13,919
 Of course, you can try to group them,

904
01:01:13,960 --> 01:01:16,960
 try to group a positive and negative one.

905
01:01:16,960 --> 01:01:20,320
 If you actually choose wrong one, for example, in this case,

906
01:01:20,320 --> 01:01:23,920
 if you do not know, you select x one, x two are support vector.

907
01:01:23,920 --> 01:01:25,640
 Of course, in reality, you will not do that,

908
01:01:25,640 --> 01:01:28,560
 but you will select some from positive classes.

909
01:01:28,560 --> 01:01:30,560
 Your support vectors will come from two sides,

910
01:01:30,560 --> 01:01:33,920
 but just assume that you randomly choose w, x one, x two.

911
01:01:34,840 --> 01:01:38,680
 Then lambda one, lambda two will not equal to zero, right?

912
01:01:38,680 --> 01:01:42,360
 So there is a condition that once there are support vector,

913
01:01:42,480 --> 01:01:45,040
 lambda will not be equal to zero.

914
01:01:45,040 --> 01:01:47,040
 Then both y will be minus one.

915
01:01:47,040 --> 01:01:51,400
 So you plug into this formula and also this formula,

916
01:01:51,400 --> 01:01:55,600
 you find that minus lambda one, minus lambda two equal to zero,

917
01:01:55,600 --> 01:01:59,240
 lambda one must be equal to minus lambda two.

918
01:01:59,240 --> 01:02:01,920
 And then there's another condition, lambda i must larger

919
01:02:01,920 --> 01:02:03,480
 than equal to zero.

920
01:02:03,480 --> 01:02:07,040
 So therefore it's not possible because first,

921
01:02:07,040 --> 01:02:10,800
 they are not equal to zero, and then both of them need

922
01:02:10,800 --> 01:02:13,480
 to be larger than or equal to zero.

923
01:02:13,480 --> 01:02:16,920
 And then they cannot be one is the negative of the other.

924
01:02:16,920 --> 01:02:20,560
 So therefore, this is not really support vector pair.

925
01:02:21,680 --> 01:02:25,040
 So then you can choose other like x two and x five.

926
01:02:25,040 --> 01:02:28,760
 Now you select x two here and x five here.

927
01:02:30,000 --> 01:02:33,360
 Then you plug into the formula, you'll find that at the end,

928
01:02:33,360 --> 01:02:35,080
 there will be some confection.

929
01:02:35,080 --> 01:02:38,360
 Some point will not satisfy larger than equal to one.

930
01:02:39,280 --> 01:02:43,320
 So there are a set of rules tell you how to choose

931
01:02:43,320 --> 01:02:45,560
 a possible support vector.

932
01:02:45,560 --> 01:02:48,920
 Then you can compute the boundary accordingly.

933
01:02:48,920 --> 01:02:52,280
 So there is basically the solution to support vector machine.

934
01:02:52,280 --> 01:02:54,600
 Of course, the two I showed you earlier on,

935
01:02:54,600 --> 01:02:56,040
 they already program all this.

936
01:02:56,040 --> 01:02:59,440
 You can just use the tool to compute

937
01:02:59,440 --> 01:03:02,280
 for a linear support vector machine.

938
01:03:03,360 --> 01:03:07,440
 And beside linear, you could also use other form

939
01:03:07,480 --> 01:03:09,440
 by parametric,

940
01:03:11,320 --> 01:03:13,280
 polynomial support vector,

941
01:03:13,280 --> 01:03:15,440
 or even nonlinear support vector machine.

942
01:03:17,440 --> 01:03:21,000
 Okay, this give you a very brief introduction

943
01:03:21,000 --> 01:03:23,960
 to the support vector machine.

944
01:03:23,960 --> 01:03:29,160
 It's learn a decision boundary for binary classification

945
01:03:29,160 --> 01:03:33,080
 by maximizing the margin between two hyper plans.

946
01:03:34,040 --> 01:03:39,040
 If you allow some error, you can have a soft margin approach

947
01:03:40,759 --> 01:03:45,040
 which allow you to minimize the so-called

948
01:03:45,040 --> 01:03:49,600
 the possible variation while maximizing your margin

949
01:03:49,600 --> 01:03:52,640
 by having some weight, the C and the K.

950
01:03:54,680 --> 01:03:57,880
 And solution to this will be

951
01:03:57,880 --> 01:04:01,600
 Lagrangian dual optimization problem.

952
01:04:01,640 --> 01:04:04,720
 You can also extend this to multiple class cases

953
01:04:04,720 --> 01:04:07,640
 that we mentioned earlier.

954
01:04:08,520 --> 01:04:12,080
 So this is support vector machine is another classifier.

955
01:04:12,080 --> 01:04:15,279
 And bear in mind this was a classifier

956
01:04:15,279 --> 01:04:17,040
 that when it came out,

957
01:04:17,040 --> 01:04:22,040
 it actually performed quite comparatively

958
01:04:24,040 --> 01:04:26,520
 compared to this neural network

959
01:04:26,520 --> 01:04:29,240
 that one time even the young LeCoultre learned.

960
01:04:30,160 --> 01:04:34,799
 And because it has a very solid mathematic behind,

961
01:04:34,799 --> 01:04:38,240
 I can think about the logic, try to maximize the margin,

962
01:04:38,240 --> 01:04:40,479
 assuming your data they all behave.

963
01:04:40,479 --> 01:04:44,359
 So you want to make sure that you allow a large margin

964
01:04:44,359 --> 01:04:48,000
 that even you have some error, some noisy data,

965
01:04:48,000 --> 01:04:50,839
 they likely will still perform

966
01:04:50,839 --> 01:04:53,600
 while to classify those the data.

967
01:04:53,600 --> 01:04:56,799
 So that is the logic behind the support vector machine.

968
01:05:00,240 --> 01:05:03,240
 Okay, with that, we come to the last lecture

969
01:05:03,240 --> 01:05:06,240
 on the neural network.

970
01:05:11,240 --> 01:05:12,839
 Of course, neural network is the network

971
01:05:12,839 --> 01:05:17,839
 that which now been used by many, many applications.

972
01:05:22,799 --> 01:05:25,959
 The one we are going to introduce is very basic,

973
01:05:26,000 --> 01:05:28,200
 but then you can really expand it

974
01:05:28,200 --> 01:05:33,200
 to many, many other variation or more advanced network,

975
01:05:34,240 --> 01:05:39,160
 which have the same so-called principle behind,

976
01:05:39,160 --> 01:05:43,280
 including your transformer, just a training data

977
01:05:43,280 --> 01:05:48,280
 and try to perform all these so-called regression

978
01:05:48,280 --> 01:05:50,320
 optimization problem, right?

979
01:05:50,320 --> 01:05:53,040
 Using gradient descents to find the solution.

980
01:05:54,040 --> 01:05:57,759
 So we will spend some time to go into the fundamental

981
01:05:57,759 --> 01:06:01,320
 in the early part, the neural network,

982
01:06:01,320 --> 01:06:05,000
 the media classifier, like SVM that we thought about it,

983
01:06:05,000 --> 01:06:08,160
 and then the perceptrons, then some of the basic,

984
01:06:08,160 --> 01:06:10,960
 like the activation function,

985
01:06:10,960 --> 01:06:15,360
 back propagation, gradient descents, some of this updating,

986
01:06:15,360 --> 01:06:18,040
 and these are very, very fundamental, right?

987
01:06:18,040 --> 01:06:19,680
 And although they are fundamental,

988
01:06:20,240 --> 01:06:23,919
 a lot of them, they really do not have a very clear

989
01:06:23,919 --> 01:06:27,799
 cut theory behind why you do that, right?

990
01:06:27,799 --> 01:06:31,319
 And there are a lot of engineering, optimization,

991
01:06:31,319 --> 01:06:35,440
 so-called some experience, tricking the data,

992
01:06:35,440 --> 01:06:40,440
 learning in order to get this, what, where we are today, right?

993
01:06:40,560 --> 01:06:44,879
 Then later on we'll introduce a large number of the variation

994
01:06:44,880 --> 01:06:49,720
 of the so-called multi-layer perceptrons, right?

995
01:06:49,720 --> 01:06:54,720
 Including the more reason one, the auto encoder,

996
01:06:54,920 --> 01:06:59,920
 the generative adversarial network for gen AI,

997
01:07:00,240 --> 01:07:03,880
 and even the transformer, right?

998
01:07:03,880 --> 01:07:06,560
 We will not have time to go into detail how those work,

999
01:07:06,560 --> 01:07:09,200
 but once you understand the basics,

1000
01:07:09,200 --> 01:07:10,600
 if you have the patience,

1001
01:07:10,600 --> 01:07:14,840
 you should be able to learn the rest from internet, right?

1002
01:07:14,840 --> 01:07:16,800
 And there are a lot of video,

1003
01:07:16,800 --> 01:07:20,520
 try to explain to you all the different concepts, okay?

1004
01:07:22,560 --> 01:07:25,760
 So, one, two, three, yeah?

1005
01:07:25,760 --> 01:07:27,600
 Let's introduce a few more

1006
01:07:27,600 --> 01:07:30,440
 then we can have a break about two.

1007
01:07:31,400 --> 01:07:33,680
 So what are neural networks?

1008
01:07:36,960 --> 01:07:39,440
 If you recall that earlier in the introduction,

1009
01:07:39,440 --> 01:07:41,040
 talk about the Rosenblatt,

1010
01:07:41,040 --> 01:07:46,040
 they tried to design a simple so-called logical gate, right?

1011
01:07:47,040 --> 01:07:52,120
 Allow you to learn and then make decision, right?

1012
01:07:52,120 --> 01:07:55,960
 So why they have the confidence

1013
01:07:55,960 --> 01:08:00,960
 that that small unit of logical so-called decision can work?

1014
01:08:01,800 --> 01:08:03,080
 They're always an assumption

1015
01:08:03,080 --> 01:08:06,880
 because your brain somehow operate in the way.

1016
01:08:06,880 --> 01:08:10,080
 So a lot of so-called even the new network,

1017
01:08:10,080 --> 01:08:12,520
 they always see, oh, try to make analogy

1018
01:08:12,520 --> 01:08:14,160
 with the human brain.

1019
01:08:14,160 --> 01:08:16,040
 Oh, because the brain operate this,

1020
01:08:16,040 --> 01:08:18,880
 so we would like to propose something like this

1021
01:08:18,880 --> 01:08:21,200
 in the neural network.

1022
01:08:21,200 --> 01:08:24,359
 Some of them it work, some of them they don't work, right?

1023
01:08:24,359 --> 01:08:28,859
 So they keep exploring all these possible so-called network.

1024
01:08:30,680 --> 01:08:32,600
 So basically the neural network,

1025
01:08:32,600 --> 01:08:37,600
 the goal is try to mimic the functioning of your brain, right?

1026
01:08:40,559 --> 01:08:44,040
 Biological so-called neural system

1027
01:08:44,040 --> 01:08:46,120
 to solve classification problem.

1028
01:08:47,319 --> 01:08:49,160
 So there was the original goal.

1029
01:08:50,319 --> 01:08:52,559
 Then you look at the human brain.

1030
01:08:52,559 --> 01:08:55,760
 Human brain actually have many of these

1031
01:08:55,760 --> 01:08:58,600
 so-called neuron like this, right?

1032
01:08:58,600 --> 01:09:01,160
 They are 10 to the power of 11 of them.

1033
01:09:02,080 --> 01:09:05,279
 And they are connected through so-called exons

1034
01:09:05,279 --> 01:09:06,960
 and dendrites.

1035
01:09:06,960 --> 01:09:09,960
 These are the so-called dendrites, right?

1036
01:09:10,600 --> 01:09:15,399
 This dendrite will receive stimuli input

1037
01:09:15,399 --> 01:09:18,640
 from the other neurons, right?

1038
01:09:18,640 --> 01:09:22,080
 And they combine all the input for the other neuron.

1039
01:09:22,080 --> 01:09:24,399
 You can see there's another one here.

1040
01:09:24,399 --> 01:09:26,279
 They connect to the dendrite.

1041
01:09:26,279 --> 01:09:27,960
 Actually between the two connection point,

1042
01:09:27,960 --> 01:09:32,960
 the dendrite actually send chemical so-called signal

1043
01:09:32,960 --> 01:09:36,120
 or chemical so-called reaction.

1044
01:09:36,120 --> 01:09:38,899
 So that is pass some information to the next one.

1045
01:09:38,899 --> 01:09:40,679
 You all happen very, very fast.

1046
01:09:42,859 --> 01:09:44,939
 And then if you correct a lot of signal,

1047
01:09:44,939 --> 01:09:47,899
 then you will send the signal further down

1048
01:09:47,899 --> 01:09:50,219
 through its own dendrite.

1049
01:09:50,219 --> 01:09:52,740
 And these exons will be the one collect

1050
01:09:52,740 --> 01:09:54,299
 all the signal more to the next one.

1051
01:09:54,299 --> 01:09:58,660
 And you have 10 to the power of 11 of them.

1052
01:09:58,660 --> 01:10:00,219
 Later on we will see this.

1053
01:10:00,219 --> 01:10:04,299
 Such operation is nothing but a binary classification

1054
01:10:04,299 --> 01:10:07,059
 that we thought about it in the SVM, right?

1055
01:10:07,060 --> 01:10:09,380
 You have to collect a lot of signal here.

1056
01:10:10,740 --> 01:10:12,620
 If there are enough signal,

1057
01:10:12,620 --> 01:10:16,120
 then it will send the signal to the next one.

1058
01:10:18,620 --> 01:10:21,300
 You're learning when you were born,

1059
01:10:21,300 --> 01:10:23,620
 you have many, many these neurons.

1060
01:10:23,620 --> 01:10:25,500
 And this connection, dendrite,

1061
01:10:25,500 --> 01:10:27,660
 how the neuron connect to the other neurons

1062
01:10:27,660 --> 01:10:30,980
 are all very flexible at the beginning.

1063
01:10:30,980 --> 01:10:33,560
 So when you were born for the first few months,

1064
01:10:33,560 --> 01:10:35,700
 you have many of these dendrites.

1065
01:10:35,740 --> 01:10:38,420
 They are not very well connected.

1066
01:10:38,420 --> 01:10:40,019
 But after you start to observe

1067
01:10:40,019 --> 01:10:42,580
 and the dendrite trying to connect, right?

1068
01:10:42,580 --> 01:10:44,300
 When there's a signal coming out,

1069
01:10:44,300 --> 01:10:47,660
 there's some response and you start to learn, right?

1070
01:10:47,660 --> 01:10:51,220
 So the way is actually decide

1071
01:10:51,220 --> 01:10:56,220
 how you connect the output to the next neuron, right?

1072
01:10:56,420 --> 01:10:59,660
 If you study, if you learn a lot,

1073
01:10:59,660 --> 01:11:03,860
 some of these neurons could have an additional seal

1074
01:11:03,860 --> 01:11:06,559
 which allow your signal to travel very fast.

1075
01:11:07,540 --> 01:11:09,460
 I believe some of you have this experience

1076
01:11:09,460 --> 01:11:11,799
 if you study very hard for your exam,

1077
01:11:11,799 --> 01:11:14,280
 like practice, practice a lot, right?

1078
01:11:14,280 --> 01:11:17,580
 Every question come you can solve immediately.

1079
01:11:17,580 --> 01:11:20,259
 Because your brain have been trained to operate

1080
01:11:20,259 --> 01:11:23,420
 such that when you see this signal, you respond, right?

1081
01:11:23,420 --> 01:11:26,900
 Therefore, learning, practicing is important, right?

1082
01:11:26,900 --> 01:11:29,500
 But for those, if they are new to you,

1083
01:11:29,500 --> 01:11:31,380
 if you do not practice your dendrite,

1084
01:11:31,380 --> 01:11:33,259
 they are not connected.

1085
01:11:33,260 --> 01:11:36,580
 Then it become harder to transmit, right?

1086
01:11:36,580 --> 01:11:38,120
 You don't know where to connect.

1087
01:11:39,140 --> 01:11:41,920
 Your learning is a biological process.

1088
01:11:42,780 --> 01:11:45,700
 Therefore, it cannot happen overnight.

1089
01:11:45,700 --> 01:11:47,100
 You have to learn regularly

1090
01:11:47,100 --> 01:11:50,620
 if you trust the theory of how your brain function.

1091
01:11:50,620 --> 01:11:54,100
 Every time you learn, you try to connect, right?

1092
01:11:54,100 --> 01:11:58,140
 And therefore, they say that if your early years

1093
01:11:58,140 --> 01:12:02,340
 like secondary or your high school, you learn a lot,

1094
01:12:02,340 --> 01:12:04,100
 now it become easier for you to learn

1095
01:12:04,100 --> 01:12:07,020
 because your network have been connected, right?

1096
01:12:07,020 --> 01:12:09,460
 So in undergraduate, when you are still young,

1097
01:12:09,460 --> 01:12:12,820
 you should try to learn as many different things as possible

1098
01:12:12,820 --> 01:12:16,380
 so that your wire, your brains are wired, right?

1099
01:12:16,380 --> 01:12:19,540
 Sometimes you learn some knowledge

1100
01:12:19,540 --> 01:12:22,220
 which might be very difficult, right?

1101
01:12:22,220 --> 01:12:25,100
 But a few years later, when you come and look at it,

1102
01:12:25,100 --> 01:12:28,060
 you find that, oh, it's easier for you to pick up

1103
01:12:28,060 --> 01:12:30,020
 because your brain have been configured

1104
01:12:30,020 --> 01:12:33,180
 to send all the signal in that way, right?

1105
01:12:34,140 --> 01:12:38,020
 And there are also times that you may have forgotten someone

1106
01:12:38,020 --> 01:12:40,980
 because the connection have not been activated.

1107
01:12:40,980 --> 01:12:44,100
 But suddenly, because the other thing happened,

1108
01:12:44,100 --> 01:12:45,900
 you bring you back those memories

1109
01:12:45,900 --> 01:12:48,740
 because they are all there,

1110
01:12:48,740 --> 01:12:51,900
 just that you need time to exercise that.

1111
01:12:51,900 --> 01:12:56,420
 So this connection are the one that really help your brain

1112
01:12:56,420 --> 01:13:01,420
 to train your brain, to learn, to optimize it.

1113
01:13:01,860 --> 01:13:04,780
 So therefore, when they decide the neural network,

1114
01:13:04,780 --> 01:13:07,020
 they try to formulate that.

1115
01:13:07,020 --> 01:13:08,660
 They do know how the brain learn,

1116
01:13:08,660 --> 01:13:10,660
 how the brain connect with each other,

1117
01:13:10,660 --> 01:13:13,380
 but they know how each of the neurons might function

1118
01:13:13,380 --> 01:13:17,580
 based on what we just explained, right?

1119
01:13:19,140 --> 01:13:23,740
 So a perceptron, this is the one that based on the Rosenblatt

1120
01:13:24,179 --> 01:13:26,500
 that you have this quick to adjust.

1121
01:13:26,500 --> 01:13:30,179
 Now you can mimic one neuron like this,

1122
01:13:30,179 --> 01:13:34,019
 assuming you have the input signal X1 to Xn.

1123
01:13:34,019 --> 01:13:37,380
 This will be the dense drive from the other neuron.

1124
01:13:37,380 --> 01:13:39,260
 They send the signal to, right?

1125
01:13:40,300 --> 01:13:43,780
 To this neuron, then you have the weight W1 to Wn,

1126
01:13:43,780 --> 01:13:47,540
 which decide how strongly you want to react to the stimuli.

1127
01:13:48,500 --> 01:13:50,660
 Then you connect them together.

1128
01:13:50,660 --> 01:13:52,019
 I'll explain this B later.

1129
01:13:52,020 --> 01:13:53,460
 This is the bias,

1130
01:13:53,460 --> 01:13:55,660
 basically how strong the signal you need before

1131
01:13:55,660 --> 01:13:59,660
 you send the signal to the next set of a layer neuron.

1132
01:14:00,940 --> 01:14:02,340
 So if you look closely,

1133
01:14:02,340 --> 01:14:06,940
 you can write the formula, this signal input X,

1134
01:14:06,940 --> 01:14:09,660
 we call the net input here,

1135
01:14:09,660 --> 01:14:14,540
 is the Wj times Xj,

1136
01:14:14,540 --> 01:14:18,140
 you sum them up plus your bias B, right?

1137
01:14:18,140 --> 01:14:20,220
 So this is the summation here.

1138
01:14:21,220 --> 01:14:26,220
 You collect all the input, multiply with the weight,

1139
01:14:26,620 --> 01:14:28,860
 plus the bias, right?

1140
01:14:28,860 --> 01:14:32,780
 They give you the signal input to this particular neuron.

1141
01:14:33,780 --> 01:14:34,940
 But before you send it out,

1142
01:14:34,940 --> 01:14:37,620
 this is something very interesting.

1143
01:14:37,620 --> 01:14:39,420
 You know, even for your brain, right?

1144
01:14:39,420 --> 01:14:43,100
 When the signal is too soft, too small,

1145
01:14:43,100 --> 01:14:44,980
 it will not trigger the signal

1146
01:14:44,980 --> 01:14:46,980
 to send to the other foreign neuron.

1147
01:14:46,980 --> 01:14:49,500
 It will just stop there, just die there.

1148
01:14:49,500 --> 01:14:52,820
 So it's not like any signal can both send through.

1149
01:14:52,820 --> 01:14:55,380
 If it's not strong enough, you'll just stop.

1150
01:14:55,380 --> 01:14:59,620
 So therefore there's this so-called activation function,

1151
01:14:59,620 --> 01:15:01,340
 we call it sigma.

1152
01:15:01,340 --> 01:15:03,380
 It actually take all your input.

1153
01:15:03,380 --> 01:15:06,780
 If the activation function, this input, right?

1154
01:15:06,780 --> 01:15:08,780
 So this will be sigma.

1155
01:15:08,780 --> 01:15:11,180
 Sigma X is larger than,

1156
01:15:12,540 --> 01:15:14,260
 if this signal is larger than zero,

1157
01:15:14,260 --> 01:15:17,020
 then the sigma X is one, otherwise it's zero.

1158
01:15:17,060 --> 01:15:19,980
 So change this to sigma here.

1159
01:15:19,980 --> 01:15:23,900
 So why now become your input signal,

1160
01:15:23,900 --> 01:15:28,900
 but go through some so-called activation function

1161
01:15:28,940 --> 01:15:33,460
 and which is defined is if this X is larger than zero,

1162
01:15:33,460 --> 01:15:35,500
 then your output is one.

1163
01:15:35,500 --> 01:15:39,660
 Then trigger one signal, so the other one is zero.

1164
01:15:39,660 --> 01:15:44,160
 So this X is all the input from your so-called,

1165
01:15:45,160 --> 01:15:48,400
 the accumulation for all your input multiplied

1166
01:15:48,400 --> 01:15:50,040
 to weight with the bias.

1167
01:15:50,880 --> 01:15:54,040
 So this is basically the Rosenberg neuron.

1168
01:15:54,040 --> 01:15:55,320
 You write in the formula.

1169
01:15:56,680 --> 01:15:59,599
 Then of course, to simplify this,

1170
01:16:00,720 --> 01:16:04,559
 because this B is a bit annoying, right?

1171
01:16:04,559 --> 01:16:09,559
 You could consider B is nothing but another feature input,

1172
01:16:10,080 --> 01:16:14,080
 X zero, where the value is always one, right?

1173
01:16:15,360 --> 01:16:19,560
 And then this weight now become right in B zero, W zero,

1174
01:16:19,560 --> 01:16:21,400
 this B equal to W zero, right?

1175
01:16:22,880 --> 01:16:24,840
 W zero is now equal to B.

1176
01:16:26,140 --> 01:16:28,880
 So then you can write this formula as such.

1177
01:16:28,880 --> 01:16:31,120
 It becomes the dog product, WX.

1178
01:16:33,040 --> 01:16:37,600
 So when you talk about SVM, we had the WX plus B, okay?

1179
01:16:37,600 --> 01:16:40,440
 And that B is this bias, okay?

1180
01:16:40,440 --> 01:16:45,440
 Now I could also absorb the B into my X and W, right?

1181
01:16:45,920 --> 01:16:48,720
 Assuming for that particular X zero,

1182
01:16:48,720 --> 01:16:50,840
 the input is always equal to one.

1183
01:16:50,840 --> 01:16:53,200
 Then the W zero equal to B.

1184
01:16:53,200 --> 01:16:57,240
 Then I could write this without the B, okay?

1185
01:16:57,240 --> 01:17:01,960
 And this is, we call this W, the B is the bias.

1186
01:17:01,960 --> 01:17:04,320
 It's the same bias as what we saw

1187
01:17:04,360 --> 01:17:09,360
 in the support vector machine, WX plus B equal to zero, right?

1188
01:17:12,799 --> 01:17:15,559
 So which is a decision boundary?

1189
01:17:15,559 --> 01:17:18,639
 For a cube like this is a plan, right?

1190
01:17:18,639 --> 01:17:22,200
 This is nothing but now you decide whether they are larger

1191
01:17:22,200 --> 01:17:23,960
 or equal to zero.

1192
01:17:23,960 --> 01:17:28,960
 If the larger to zero positive, less than zero negative.

1193
01:17:29,440 --> 01:17:30,840
 So rather than one and minus one,

1194
01:17:30,840 --> 01:17:33,360
 now I decide whether it's a positive or negative.

1195
01:17:33,400 --> 01:17:38,080
 So a support, a perceptron is nothing but a support

1196
01:17:38,080 --> 01:17:38,920
 vector machine.

1197
01:17:38,920 --> 01:17:42,920
 It's just slightly different kind of formulation, right?

1198
01:17:44,519 --> 01:17:47,240
 And that is what a perceptron is doing.

1199
01:17:47,240 --> 01:17:51,120
 And X now become your attribute, your input.

1200
01:17:51,120 --> 01:17:54,880
 B, we call this a bias, a bias, okay?

1201
01:17:54,880 --> 01:17:59,880
 In this case, because if the B, the value is too small,

1202
01:18:00,640 --> 01:18:03,080
 then you will not send the signal,

1203
01:18:03,080 --> 01:18:06,760
 Y will not have any output, will be equal to zero, right?

1204
01:18:06,760 --> 01:18:11,760
 If this dot product of WXJ plus B, you sum them up,

1205
01:18:14,040 --> 01:18:17,840
 is very small, less than zero,

1206
01:18:17,840 --> 01:18:19,800
 then Y will not have any output.

1207
01:18:20,660 --> 01:18:24,720
 So a perceptron can act as a linear classifier,

1208
01:18:24,720 --> 01:18:29,720
 try to differentiate the space into one,

1209
01:18:30,160 --> 01:18:31,820
 and zero class, okay?

1210
01:18:33,400 --> 01:18:38,400
 So with that, we can try to define a perceptron,

1211
01:18:38,520 --> 01:18:42,360
 which allow you to do the basic logical operation,

1212
01:18:42,360 --> 01:18:44,600
 which was the original so-called idea

1213
01:18:44,600 --> 01:18:47,800
 when this PICS and Rosenblatt,

1214
01:18:47,800 --> 01:18:50,280
 they come out with the neuron, right?

1215
01:18:50,280 --> 01:18:54,840
 They want to first mimic the logical operation.

1216
01:18:54,840 --> 01:18:55,920
 So in this case,

1217
01:18:56,920 --> 01:19:01,920
 you can formulate W1X1, W1X1, W2X2 plus B.

1218
01:19:03,000 --> 01:19:08,000
 Now you have this assuming that X10, X20, Y is equal to zero,

1219
01:19:09,320 --> 01:19:13,400
 which is the n logical n operation.

1220
01:19:13,400 --> 01:19:18,400
 If any of them is zero, output always zero,

1221
01:19:18,400 --> 01:19:21,560
 if both of them are one, output is one, okay?

1222
01:19:21,560 --> 01:19:26,480
 So in this so-called plot, X1, X, right?

1223
01:19:26,480 --> 01:19:29,280
 This three point belong to zero class,

1224
01:19:30,160 --> 01:19:33,040
 and this one, X1 equal to one, X2 equal to one

1225
01:19:33,040 --> 01:19:34,500
 belong to this class.

1226
01:19:34,500 --> 01:19:37,520
 So you want to find the decision boundary,

1227
01:19:37,520 --> 01:19:38,720
 in this case, a line here,

1228
01:19:38,720 --> 01:19:42,800
 such that you can separate the one from zero class.

1229
01:19:44,320 --> 01:19:49,320
 So in this case, what you are looking for is W1, W2, and B.

1230
01:19:49,519 --> 01:19:54,519
 So for this problem, you can use SVM to solve it, right?

1231
01:19:54,639 --> 01:19:56,679
 You could try to try yourself to find

1232
01:19:56,679 --> 01:19:59,480
 what is SVM for this, right?

1233
01:19:59,480 --> 01:20:02,559
 And then, like this will be the support vector.

1234
01:20:05,160 --> 01:20:09,960
 So this one, this one, this three will be the support vector

1235
01:20:09,960 --> 01:20:12,599
 for you to find the boundary here, right?

1236
01:20:12,599 --> 01:20:16,000
 To separate one from zero or one from minus one.

1237
01:20:16,000 --> 01:20:17,719
 But if you use a perceptron,

1238
01:20:17,720 --> 01:20:22,720
 now you plug into the formula, W1, W2, and B are unknown

1239
01:20:23,240 --> 01:20:24,080
 you want to find.

1240
01:20:24,080 --> 01:20:27,440
 When X1 equal to zero, X2 equal to zero,

1241
01:20:27,440 --> 01:20:31,440
 and then this should be less than equal to zero, right?

1242
01:20:31,440 --> 01:20:35,500
 Because this is the class zero, right?

1243
01:20:35,500 --> 01:20:39,160
 Similarly, when X1 is zero, one, you have this formula,

1244
01:20:39,160 --> 01:20:40,940
 one zero, you have this constraint,

1245
01:20:40,940 --> 01:20:43,180
 one one, you have this constraint.

1246
01:20:43,180 --> 01:20:47,000
 So then you have this few constraint here,

1247
01:20:47,000 --> 01:20:52,000
 B less than equal to zero, W2 plus B less than equal to zero

1248
01:20:52,440 --> 01:20:56,240
 or W2 less than equal to minus B, okay?

1249
01:20:56,240 --> 01:21:00,360
 So you are going to have one, two, three, four condition

1250
01:21:00,360 --> 01:21:04,200
 constraint, which you need to find W1, W2, and B

1251
01:21:04,200 --> 01:21:07,120
 satisfy all these four constraints, right?

1252
01:21:07,120 --> 01:21:09,840
 Then you can do this logical A.

1253
01:21:09,840 --> 01:21:11,760
 Apparently you can find such thing.

1254
01:21:11,760 --> 01:21:16,120
 One is X, W1 equal to one, W2 equal to one,

1255
01:21:16,120 --> 01:21:20,960
 and B equal to minus 1.5, right?

1256
01:21:20,960 --> 01:21:24,120
 So then you can satisfy all these four conditions.

1257
01:21:24,120 --> 01:21:27,760
 So this is the line, okay?

1258
01:21:27,760 --> 01:21:29,760
 And in fact, there are more than one line,

1259
01:21:29,760 --> 01:21:31,360
 if you just satisfy this,

1260
01:21:31,360 --> 01:21:34,559
 you can actually even use a minus 1.2,

1261
01:21:34,559 --> 01:21:38,960
 you're still satisfied, just the line now become here, right?

1262
01:21:38,960 --> 01:21:41,559
 It's no longer the support vector machine.

1263
01:21:41,559 --> 01:21:45,200
 So you can adjust the B, just like support vector machine,

1264
01:21:45,200 --> 01:21:49,080
 you move the line, but it may not be the maximum margin,

1265
01:21:49,080 --> 01:21:53,760
 but it will still be the solution for this perceptron.

1266
01:21:53,760 --> 01:21:57,440
 Therefore, in your training, you know, network,

1267
01:21:57,440 --> 01:22:00,360
 you could end up with a different set of network.

1268
01:22:00,360 --> 01:22:02,220
 They all satisfy the condition,

1269
01:22:02,220 --> 01:22:06,920
 but it may not be always the so-called maximum margin.

1270
01:22:09,040 --> 01:22:12,000
 So similarly, you can look at the logical all function,

1271
01:22:12,000 --> 01:22:16,520
 which is 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1.

1272
01:22:16,520 --> 01:22:18,720
 Okay, plug into this formula.

1273
01:22:18,720 --> 01:22:20,160
 Again, this is B, right?

1274
01:22:20,160 --> 01:22:21,960
 Last and theta, this is B.

1275
01:22:23,200 --> 01:22:24,480
 Less than equal to zero.

1276
01:22:24,480 --> 01:22:26,960
 Then you can find another set of formula,

1277
01:22:26,960 --> 01:22:31,320
 X, W1, W2 equal to one, B equal to minus 0.5.

1278
01:22:31,320 --> 01:22:34,480
 Then the line here can separate from positive to negative.

1279
01:22:37,080 --> 01:22:39,360
 And then this is the problem

1280
01:22:39,360 --> 01:22:42,599
 that they cannot solve by using one perceptron.

1281
01:22:42,599 --> 01:22:45,120
 Logical XOR, exclusive all.

1282
01:22:46,900 --> 01:22:49,120
 You have very, output equal to one,

1283
01:22:50,000 --> 01:22:55,000
 with any one of them equal to one, but not both, right?

1284
01:22:55,480 --> 01:23:00,480
 If X1, X2, they are the same, 0, 0, 1, 1, output equal to zero.

1285
01:23:00,960 --> 01:23:03,519
 This is called exclusive all.

1286
01:23:03,519 --> 01:23:04,719
 They apply this formula,

1287
01:23:04,719 --> 01:23:07,160
 you have to satisfy this four condition,

1288
01:23:07,200 --> 01:23:10,720
 and these are the simplified version of the,

1289
01:23:10,720 --> 01:23:13,280
 I mean, you simplify, you get this one.

1290
01:23:13,280 --> 01:23:18,120
 You find that you will not be able to find a set of W1, W2,

1291
01:23:18,120 --> 01:23:20,559
 and B such that you can separate,

1292
01:23:20,559 --> 01:23:23,160
 you can satisfy this four condition.

1293
01:23:23,160 --> 01:23:24,680
 It's quite easy to see here.

1294
01:23:24,680 --> 01:23:26,680
 There's no way you can draw a line,

1295
01:23:27,960 --> 01:23:32,960
 which can separate the positive from the X from the zero class.

1296
01:23:33,200 --> 01:23:36,480
 Therefore, you don't have a solution for here, right?

1297
01:23:36,480 --> 01:23:41,160
 So this is the problem that,

1298
01:23:41,160 --> 01:23:45,120
 when the perceptron actually run into problem,

1299
01:23:46,919 --> 01:23:48,360
 talk about this, okay,

1300
01:23:48,360 --> 01:23:50,400
 it is not really something possible

1301
01:23:50,400 --> 01:23:55,400
 by using a perceptron to do a simple operation

1302
01:23:57,400 --> 01:24:00,000
 like exclusive all, right?

1303
01:24:00,000 --> 01:24:03,280
 That created the so-called AI winter, right?

1304
01:24:03,280 --> 01:24:05,000
 One of the reasons to argue.

1305
01:24:05,800 --> 01:24:08,320
 But what actually could be done is,

1306
01:24:08,320 --> 01:24:10,440
 rather than one new perceptron,

1307
01:24:10,440 --> 01:24:13,560
 you can build three of them.

1308
01:24:13,560 --> 01:24:18,000
 In this case, you have this X1, X2 input still like this,

1309
01:24:18,000 --> 01:24:19,760
 but rather than you create Y directly,

1310
01:24:19,760 --> 01:24:22,120
 you could have built two of them.

1311
01:24:22,120 --> 01:24:25,640
 In fact, it's here, one, you create a boundary H1,

1312
01:24:25,640 --> 01:24:28,320
 which separate these three from here,

1313
01:24:28,320 --> 01:24:32,080
 the other H2 separate one from these three, right?

1314
01:24:32,080 --> 01:24:34,320
 Then you have two, you can combine this together,

1315
01:24:34,320 --> 01:24:39,320
 another neuron, then you can have this decision boundary

1316
01:24:40,000 --> 01:24:43,440
 that allow you to classify X from zero.

1317
01:24:43,440 --> 01:24:46,000
 So rather than one, now you have two.

1318
01:24:46,000 --> 01:24:47,639
 You combine them together,

1319
01:24:47,639 --> 01:24:51,240
 Y can actually solve the exclusive all.

1320
01:24:51,240 --> 01:24:54,240
 So I will leave this to you as an exercise.

1321
01:24:54,240 --> 01:24:56,120
 Go back to practice.

1322
01:24:56,120 --> 01:24:58,040
 What would be the value you need

1323
01:24:58,040 --> 01:25:02,480
 for this perceptron one, two, three,

1324
01:25:03,400 --> 01:25:08,120
 such that it can separate this 4.2 classes.

1325
01:25:09,400 --> 01:25:12,639
 And there are a lot of online solution you can find,

1326
01:25:12,639 --> 01:25:16,120
 but before then I would like you to hand calculate.

1327
01:25:16,120 --> 01:25:18,240
 And there are multiple solution, for example.

1328
01:25:18,240 --> 01:25:22,919
 Rather than this, you can also find the two line here, right?

1329
01:25:22,919 --> 01:25:25,280
 Which also can do the same thing.

1330
01:25:25,280 --> 01:25:29,480
 Allow you to separate the 4.2 classes

1331
01:25:29,480 --> 01:25:32,160
 by using three perceptron.

1332
01:25:32,160 --> 01:25:34,200
 Use this set for formula, think about it,

1333
01:25:34,200 --> 01:25:36,200
 how to get the perceptron.

1334
01:25:38,240 --> 01:25:39,080
 Yes?

1335
01:25:44,639 --> 01:25:47,240
 Denial regression model, it depends on output.

1336
01:25:47,240 --> 01:25:51,720
 If your output is a so-called not a class, classes,

1337
01:25:51,720 --> 01:25:53,240
 yeah, there is a denial equation.

1338
01:25:53,240 --> 01:25:56,200
 You can actually, yeah, think for bring up this point.

1339
01:25:56,200 --> 01:25:58,920
 So this one, of course, in here we talk about

1340
01:25:58,920 --> 01:26:02,000
 the Y is zero and one, right?

1341
01:26:02,000 --> 01:26:05,560
 But in fact, your Y could be a real number, right?

1342
01:26:05,560 --> 01:26:08,240
 You estimate the height, then you have a lot of input.

1343
01:26:08,240 --> 01:26:11,760
 Then you become a regression problem, okay?

1344
01:26:11,760 --> 01:26:14,760
 You try to minimize the prediction

1345
01:26:14,760 --> 01:26:16,040
 from your target value.

1346
01:26:16,040 --> 01:26:19,960
 So it is, when we, we will recall that we talk about

1347
01:26:19,960 --> 01:26:24,320
 the supervised learning, right?

1348
01:26:24,320 --> 01:26:27,160
 You could have two class of supervised learning.

1349
01:26:27,160 --> 01:26:29,120
 One is a classification, right?

1350
01:26:29,120 --> 01:26:30,720
 One is a regression.

1351
01:26:30,720 --> 01:26:32,640
 So they are for two different purpose.

1352
01:26:32,640 --> 01:26:33,840
 You all depend on your output.

1353
01:26:33,840 --> 01:26:38,200
 If your output are binary, so-called those are labor classes,

1354
01:26:38,200 --> 01:26:40,680
 then it's a classification problem.

1355
01:26:40,680 --> 01:26:43,600
 If your output is to predict, for example,

1356
01:26:43,600 --> 01:26:45,800
 what is the score of your exam?

1357
01:26:45,800 --> 01:26:47,640
 What is the temperature tomorrow?

1358
01:26:47,640 --> 01:26:49,560
 So that may not be just a class,

1359
01:26:49,560 --> 01:26:51,440
 you want to have a value.

1360
01:26:51,440 --> 01:26:53,840
 Then it become a regression problem.

1361
01:26:53,840 --> 01:26:57,200
 But the electron can be train for both, right?

1362
01:26:57,200 --> 01:27:00,599
 It's just a training how you want to minimize the error.

1363
01:27:02,720 --> 01:27:04,519
 Of course, there is just one.

1364
01:27:04,519 --> 01:27:05,880
 Then you think about it,

1365
01:27:05,880 --> 01:27:09,000
 you can now connect them together, right?

1366
01:27:09,880 --> 01:27:12,679
 Here I input, just now we look at a tree, right?

1367
01:27:12,679 --> 01:27:17,679
 And then this is one perceptron to one layer, okay?

1368
01:27:18,360 --> 01:27:20,160
 Early on we have two to one, right?

1369
01:27:20,160 --> 01:27:21,720
 Now you have another layer,

1370
01:27:21,720 --> 01:27:23,800
 you can continue to have many layer.

1371
01:27:24,000 --> 01:27:25,080
 Okay?

1372
01:27:25,080 --> 01:27:27,440
 So then this is a neural network.

1373
01:27:27,440 --> 01:27:29,360
 It's a multi-layer perceptron.

1374
01:27:29,360 --> 01:27:32,520
 You have one, two hidden layer,

1375
01:27:32,520 --> 01:27:35,400
 one input layer, one output layer.

1376
01:27:35,400 --> 01:27:36,240
 Okay?

1377
01:27:36,240 --> 01:27:37,080
 When you combine them together,

1378
01:27:37,080 --> 01:27:39,280
 these two are called hidden layer

1379
01:27:39,280 --> 01:27:41,320
 because you cannot access them directly

1380
01:27:41,320 --> 01:27:43,920
 from the input or from the output.

1381
01:27:43,920 --> 01:27:44,760
 Okay?

1382
01:27:44,760 --> 01:27:45,960
 You have to go through this layer,

1383
01:27:45,960 --> 01:27:49,680
 input layer, output layer before you can reach these two.

1384
01:27:49,680 --> 01:27:50,560
 Right?

1385
01:27:50,600 --> 01:27:54,680
 So this is a fit forward neural network

1386
01:27:54,680 --> 01:27:57,760
 because the signal will only propagate to one direction.

1387
01:27:57,760 --> 01:27:59,200
 It will not come back.

1388
01:27:59,200 --> 01:28:00,240
 Fit forward.

1389
01:28:01,080 --> 01:28:01,920
 Okay?

1390
01:28:01,920 --> 01:28:06,920
 And sometimes we call this to topology is the N, M, M, C.

1391
01:28:07,600 --> 01:28:11,640
 N meaning you have a N input, feature input,

1392
01:28:11,640 --> 01:28:16,640
 followed with M perceptron, M perceptron up to M.

1393
01:28:17,480 --> 01:28:20,080
 Then you have output one to C.

1394
01:28:20,080 --> 01:28:21,640
 Last one you have C.

1395
01:28:21,640 --> 01:28:22,480
 Okay?

1396
01:28:22,480 --> 01:28:26,200
 So for the C far 10, then C will be equal to 10.

1397
01:28:26,200 --> 01:28:27,320
 You have 10 classes.

1398
01:28:28,280 --> 01:28:29,320
 Okay?

1399
01:28:29,320 --> 01:28:31,040
 So of course, if you have 10 classes,

1400
01:28:31,040 --> 01:28:34,800
 you can also think of a neural network.

1401
01:28:34,800 --> 01:28:37,240
 The output could be one output,

1402
01:28:37,240 --> 01:28:40,920
 but the value could be one, two, three, four, until 10.

1403
01:28:40,920 --> 01:28:42,720
 But which turned out to be not as good

1404
01:28:42,720 --> 01:28:44,840
 as you have a 10 output.

1405
01:28:44,840 --> 01:28:47,440
 Each one is either one or zero.

1406
01:28:47,440 --> 01:28:48,280
 Right?

1407
01:28:48,280 --> 01:28:49,680
 The one with the value equal to one

1408
01:28:49,800 --> 01:28:52,200
 or the largest value will be your classes.

1409
01:28:53,240 --> 01:28:56,600
 So these are a lot of so-called expedient try and error

1410
01:28:56,600 --> 01:28:57,440
 to compare.

1411
01:28:57,440 --> 01:29:00,280
 They find that this will be the number of classes

1412
01:29:00,280 --> 01:29:04,640
 will be better for you to train the network

1413
01:29:04,640 --> 01:29:05,880
 easier for a train network.

1414
01:29:08,080 --> 01:29:08,920
 Okay?

1415
01:29:08,920 --> 01:29:11,800
 Just to summarize, before we go for break,

1416
01:29:13,000 --> 01:29:15,400
 actually we can go to three more slides

1417
01:29:15,400 --> 01:29:17,400
 for most like before the demo.

1418
01:29:17,440 --> 01:29:19,639
 So multi-layer perceptron basically,

1419
01:29:19,639 --> 01:29:23,759
 you have one input layer, one or more hidden layers

1420
01:29:23,759 --> 01:29:25,879
 and one output layer.

1421
01:29:25,879 --> 01:29:29,679
 They are connected by links with adjustable weights.

1422
01:29:29,679 --> 01:29:31,320
 So you come back to here,

1423
01:29:31,320 --> 01:29:35,160
 all these links would have weight W, W1, W2.

1424
01:29:35,160 --> 01:29:36,000
 Right?

1425
01:29:36,000 --> 01:29:38,120
 And some of them may not be connected.

1426
01:29:38,120 --> 01:29:38,960
 Right?

1427
01:29:38,960 --> 01:29:39,799
 It will be zero.

1428
01:29:39,799 --> 01:29:42,599
 So each of them would have a bias.

1429
01:29:42,599 --> 01:29:43,440
 Right?

1430
01:29:43,440 --> 01:29:45,040
 Normally you don't put the bias,

1431
01:29:45,120 --> 01:29:47,600
 you can assume they all have a bias B.

1432
01:29:47,600 --> 01:29:50,040
 If you really want to show the bias, you can do this.

1433
01:29:50,040 --> 01:29:54,400
 You can have this bias unit that you connect here.

1434
01:29:56,600 --> 01:29:57,440
 Connect here.

1435
01:29:59,160 --> 01:30:00,000
 Okay?

1436
01:30:00,000 --> 01:30:01,200
 You can connect here.

1437
01:30:01,200 --> 01:30:02,800
 Connect here.

1438
01:30:02,800 --> 01:30:06,280
 So the bias unit, remember the X zero,

1439
01:30:06,280 --> 01:30:09,000
 this one output always equal to one.

1440
01:30:09,000 --> 01:30:11,040
 And then the weight here becomes your bias

1441
01:30:11,040 --> 01:30:12,560
 to that particular unit.

1442
01:30:12,560 --> 01:30:13,400
 Right?

1443
01:30:13,400 --> 01:30:18,400
 So weight, let's say one to B.

1444
01:30:19,559 --> 01:30:20,400
 Right?

1445
01:30:20,400 --> 01:30:24,200
 And this is a weight, maybe two to B.

1446
01:30:24,200 --> 01:30:26,080
 So all this, this will become,

1447
01:30:26,080 --> 01:30:28,120
 the weight will become your bias.

1448
01:30:30,920 --> 01:30:34,360
 Each perceptron unit take the input

1449
01:30:34,360 --> 01:30:38,040
 from the previous output layers

1450
01:30:38,040 --> 01:30:43,040
 and then the input and then go to output layer.

1451
01:30:43,400 --> 01:30:48,400
 Add a bias, then apply the input to an activation function,

1452
01:30:48,480 --> 01:30:51,719
 the sigma to create the output of the unit.

1453
01:30:52,679 --> 01:30:53,960
 Okay?

1454
01:30:53,960 --> 01:30:56,160
 And this nonlinear activation function

1455
01:30:56,160 --> 01:30:57,799
 turned out to be a key.

1456
01:30:57,799 --> 01:31:01,200
 This sigma, the function sigma X.

1457
01:31:01,200 --> 01:31:04,200
 If you think carefully without nonlinear,

1458
01:31:04,200 --> 01:31:07,160
 if this one I just take the X input,

1459
01:31:07,160 --> 01:31:10,280
 everything can be collected into one neuron

1460
01:31:10,280 --> 01:31:12,719
 because you just have W multiplied by X,

1461
01:31:12,720 --> 01:31:15,280
 everything you can actually collect

1462
01:31:15,280 --> 01:31:17,880
 into one neuron with many, many input.

1463
01:31:17,880 --> 01:31:20,560
 So this sigma turned out to be very important.

1464
01:31:20,560 --> 01:31:22,800
 It has to be in nonlinear.

1465
01:31:23,920 --> 01:31:24,840
 Right?

1466
01:31:24,840 --> 01:31:29,480
 And there are a few choices normally that we will use

1467
01:31:29,480 --> 01:31:31,680
 and which allow you to mimic how your brain

1468
01:31:31,680 --> 01:31:34,920
 to decide whether to send the signal further down or not.

1469
01:31:34,920 --> 01:31:36,000
 Okay?

1470
01:31:36,000 --> 01:31:38,840
 And if you have many, many of these circle unit

1471
01:31:38,840 --> 01:31:42,800
 and you have used a proper nonlinear activation function

1472
01:31:43,800 --> 01:31:45,120
 and you can basically,

1473
01:31:45,120 --> 01:31:47,720
 it has been shown that you can approximate

1474
01:31:47,720 --> 01:31:49,640
 any classification function.

1475
01:31:49,640 --> 01:31:52,040
 You just have this huge enough.

1476
01:31:52,040 --> 01:31:53,840
 Earlier we talked about the classification

1477
01:31:53,840 --> 01:31:56,280
 could be very complex, right?

1478
01:31:56,280 --> 01:32:00,640
 Like transformer recognize all your input question,

1479
01:32:00,640 --> 01:32:02,760
 predict what will be the next one.

1480
01:32:02,760 --> 01:32:03,600
 Right?

1481
01:32:03,600 --> 01:32:05,400
 And there's a very complex so-called boundary,

1482
01:32:05,400 --> 01:32:08,440
 not just like what we show you a binary

1483
01:32:08,440 --> 01:32:09,540
 could be very complex.

1484
01:32:10,719 --> 01:32:15,200
 But you can show that with a proper connection

1485
01:32:15,200 --> 01:32:19,440
 you can practically mimic any kind of

1486
01:32:19,440 --> 01:32:23,120
 so-called function, boundary function you want.

1487
01:32:23,120 --> 01:32:25,080
 For example, here is one example.

1488
01:32:25,080 --> 01:32:30,080
 You have this X, Y, 2D input and there's a bias.

1489
01:32:30,120 --> 01:32:34,719
 The first layer, the first layer basically is,

1490
01:32:34,719 --> 01:32:38,280
 every unit is a binary classification, right?

1491
01:32:38,960 --> 01:32:40,040
 Because WX plus B.

1492
01:32:40,040 --> 01:32:44,360
 So the first layer basically will separate a 2D into half,

1493
01:32:44,360 --> 01:32:45,880
 two half.

1494
01:32:45,880 --> 01:32:48,960
 But the next layer, the second layer basically

1495
01:32:48,960 --> 01:32:52,880
 try to combine this into a region.

1496
01:32:54,059 --> 01:32:56,280
 Just like when we do the exclusive OR,

1497
01:32:56,280 --> 01:32:59,480
 you have two neurons, each one separate one half half.

1498
01:32:59,480 --> 01:33:02,200
 Where combined then you can actually have a region.

1499
01:33:02,519 --> 01:33:03,599
 Right?

1500
01:33:03,599 --> 01:33:08,559
 So now you can start to classify more complex region.

1501
01:33:08,559 --> 01:33:10,240
 And then if you go further, further up,

1502
01:33:10,240 --> 01:33:13,639
 you can actually break the area into very,

1503
01:33:13,639 --> 01:33:16,320
 very so-called precise boundary.

1504
01:33:16,320 --> 01:33:21,320
 When you have more layer, more so-called perceptron per layer.

1505
01:33:22,240 --> 01:33:27,240
 Which allow you to describe any kind of a very tricky

1506
01:33:27,320 --> 01:33:32,040
 so-called distribution of your input data, training data.

1507
01:33:32,040 --> 01:33:33,560
 As long as they are separable,

1508
01:33:33,560 --> 01:33:36,480
 you can always find this very subtle boundary

1509
01:33:36,480 --> 01:33:37,700
 to separate them.

1510
01:33:37,700 --> 01:33:39,920
 We will show some demo later how to do that.

1511
01:33:41,420 --> 01:33:44,620
 And then the activation function, the sigma X, right?

1512
01:33:44,620 --> 01:33:48,200
 If you have this sigma X equal to X, which is a linear.

1513
01:33:48,200 --> 01:33:50,840
 But this is very useless.

1514
01:33:50,840 --> 01:33:53,400
 If you use this one basically the whole,

1515
01:33:53,400 --> 01:33:55,320
 even you have million of parameters,

1516
01:33:55,320 --> 01:33:56,760
 it's just a single neuron.

1517
01:33:56,760 --> 01:33:59,940
 It doesn't really do more complicated than that.

1518
01:34:00,940 --> 01:34:02,860
 So here is another sigmoid function.

1519
01:34:02,860 --> 01:34:06,019
 This was popular at the early day.

1520
01:34:06,860 --> 01:34:08,500
 Rather than a line,

1521
01:34:08,500 --> 01:34:13,500
 you basically compress all the output to one to zero.

1522
01:34:13,500 --> 01:34:16,780
 If the X is very large value,

1523
01:34:16,780 --> 01:34:19,299
 this one becomes zero, then the output will be one,

1524
01:34:19,299 --> 01:34:20,299
 approaching one.

1525
01:34:20,299 --> 01:34:24,460
 If X is a negative value, this becomes infinity.

1526
01:34:24,460 --> 01:34:25,940
 And then the output equal to zero.

1527
01:34:25,940 --> 01:34:29,440
 So this is the sigmoid function.

1528
01:34:30,740 --> 01:34:33,299
 You also have a tanged X function.

1529
01:34:33,299 --> 01:34:35,700
 I think this one was used by Young LeCun.

1530
01:34:35,700 --> 01:34:38,500
 He's the learning network in one of the version.

1531
01:34:38,500 --> 01:34:43,500
 So you could have output from one to minus one.

1532
01:34:45,099 --> 01:34:46,740
 And this is the one we saw earlier.

1533
01:34:46,740 --> 01:34:51,139
 Either it's larger than zero,

1534
01:34:51,139 --> 01:34:54,980
 you put plus one, minus one for the SVM, right?

1535
01:34:54,980 --> 01:34:58,740
 Or you can have a shift is a bit plus one and zero.

1536
01:35:00,500 --> 01:35:04,660
 The third one, this is called rectify linear unit,

1537
01:35:04,660 --> 01:35:06,179
 rect rule.

1538
01:35:06,179 --> 01:35:08,139
 This is the most common one now.

1539
01:35:08,139 --> 01:35:09,179
 It's like this.

1540
01:35:10,299 --> 01:35:13,580
 If X is larger than zero, you keep X.

1541
01:35:13,580 --> 01:35:16,980
 If X is less than zero, you zero out.

1542
01:35:16,980 --> 01:35:21,259
 So basically the operation maximum value of zero and X.

1543
01:35:21,259 --> 01:35:23,500
 So this is a sigma X.

1544
01:35:23,500 --> 01:35:27,179
 And this is the one now very commonly used

1545
01:35:27,179 --> 01:35:28,820
 in many of the network.

1546
01:35:28,820 --> 01:35:29,660
 Why?

1547
01:35:29,660 --> 01:35:31,660
 Because it is easy to compute.

1548
01:35:31,660 --> 01:35:34,259
 And later on when you look at the back propagation,

1549
01:35:34,259 --> 01:35:36,820
 you need to find the differentiation of this,

1550
01:35:36,820 --> 01:35:40,299
 which is always equal to one when X is larger than zero.

1551
01:35:40,299 --> 01:35:42,299
 The differentiation of this, right?

1552
01:35:42,299 --> 01:35:44,620
 The sigma prime X.

1553
01:35:44,620 --> 01:35:47,139
 This is something that when you do the back propagation,

1554
01:35:47,139 --> 01:35:51,380
 you are going to compute this sigma prime X.

1555
01:35:51,380 --> 01:35:55,780
 And the value will be equal to one and zero.

1556
01:35:56,780 --> 01:36:00,179
 So you simplify your calculation a lot.

1557
01:36:00,179 --> 01:36:01,980
 So there were a lot of question,

1558
01:36:01,980 --> 01:36:04,099
 oh, why you throw away all the signal,

1559
01:36:04,099 --> 01:36:05,740
 which is less than zero?

1560
01:36:05,740 --> 01:36:08,740
 Shouldn't that actually the information will be useful?

1561
01:36:09,780 --> 01:36:11,139
 But when you do the training,

1562
01:36:11,139 --> 01:36:12,540
 if the signal is very useful,

1563
01:36:12,540 --> 01:36:15,980
 you just have a weight, which is a negative value.

1564
01:36:15,980 --> 01:36:17,580
 All the negative will come positive.

1565
01:36:17,580 --> 01:36:19,179
 You can have many, many neural.

1566
01:36:20,740 --> 01:36:23,099
 And then there are people come up with this,

1567
01:36:23,099 --> 01:36:25,460
 in order to keep the negative so-called value,

1568
01:36:25,460 --> 01:36:28,860
 they come up with this leaky lag rule.

1569
01:36:28,860 --> 01:36:30,740
 When X is less than zero,

1570
01:36:30,740 --> 01:36:35,740
 they put a alpha X where alpha is smaller than one.

1571
01:36:36,100 --> 01:36:37,420
 If alpha equal to one,

1572
01:36:37,420 --> 01:36:40,700
 then will be the linear function, which is really useless.

1573
01:36:40,700 --> 01:36:44,820
 So they put another so-called slope here,

1574
01:36:44,820 --> 01:36:49,180
 alpha X with alpha, very, very small value.

1575
01:36:49,180 --> 01:36:51,700
 And there's called leaky lag rule.

1576
01:36:51,700 --> 01:36:53,140
 So there are some studies show you

1577
01:36:53,180 --> 01:36:56,020
 that leaky lag rule may perform slightly better

1578
01:36:56,020 --> 01:36:58,300
 for some of the test data set.

1579
01:36:58,300 --> 01:37:02,220
 But in general, many people just focus on the red pool

1580
01:37:02,220 --> 01:37:05,100
 because of the simplification of the calculation.

1581
01:37:06,260 --> 01:37:07,460
 Okay, let's take a break here.

1582
01:37:07,460 --> 01:37:09,740
 We'll come back to show you some demo and continue.

1583
01:37:09,740 --> 01:37:12,340
 Come back by, let's say 15 minutes.

1584
01:37:15,300 --> 01:37:16,740
 By 2.25.

1585
01:37:53,140 --> 01:37:53,980
 By 2.25.

1586
01:38:23,140 --> 01:38:23,980
 By 2.25.

1587
01:38:53,220 --> 01:38:55,020
 We'll go to retry here.

1588
01:39:00,540 --> 01:39:02,160
 Yes.

1589
01:39:02,160 --> 01:39:03,920
 This one is exact number.

1590
01:39:04,820 --> 01:39:06,540
 Now we go check back.

1591
01:39:16,460 --> 01:39:17,180
 Okay.

1592
01:39:18,860 --> 01:39:19,700
 Okay.

1593
01:39:53,140 --> 01:39:56,140
 and calculate the distance like this.

1594
01:39:56,140 --> 01:39:57,420
 How should I apply?

1595
01:39:57,420 --> 01:39:59,740
 A point where on the line.

1596
01:39:59,740 --> 01:40:00,620
 Yes, yes.

1597
01:40:00,620 --> 01:40:02,020
 But we don't do which class?

1598
01:40:02,020 --> 01:40:03,540
 Positive or negative?

1599
01:40:03,540 --> 01:40:04,020
 Nothing.

1600
01:40:04,020 --> 01:40:05,740
 It's in the in between.

1601
01:40:05,740 --> 01:40:08,060
 Then this in between way?

1602
01:40:08,060 --> 01:40:10,140
 Then it will be 0 equal to 0.

1603
01:40:10,140 --> 01:40:12,140
 Then?

1604
01:40:12,140 --> 01:40:14,460
 But if this is like very important one,

1605
01:40:14,460 --> 01:40:17,020
 why don't we take it as a z plane

1606
01:40:17,020 --> 01:40:20,820
 and calculate the axis like this?

1607
01:40:20,820 --> 01:40:21,780
 No, no.

1608
01:40:21,780 --> 01:40:25,500
 Is it a training data or testing data?

1609
01:40:25,500 --> 01:40:26,860
 This is my testing one.

1610
01:40:26,860 --> 01:40:27,460
 Testing one.

1611
01:40:27,460 --> 01:40:29,019
 Testing will just make decision.

1612
01:40:29,019 --> 01:40:32,300
 If it is larger than 0, then it's, yeah,

1613
01:40:32,300 --> 01:40:34,380
 you are not going to change your W and B.

1614
01:40:34,380 --> 01:40:36,059
 You are already trained.

1615
01:40:36,059 --> 01:40:39,179
 If it's a testing data, just put into WX plus B,

1616
01:40:39,179 --> 01:40:42,019
 see whether it's positive or negative.

1617
01:40:42,019 --> 01:40:44,099
 If it's exactly on the line, it will be 0.

1618
01:40:44,099 --> 01:40:46,099
 Then you have to just flip a coin

1619
01:40:46,099 --> 01:40:49,420
 to decide where's the positive or negative.

1620
01:40:49,420 --> 01:40:52,980
 But then I want to convert some of the data

1621
01:40:52,980 --> 01:40:56,500
 into training because I have to work between all the things.

1622
01:40:56,500 --> 01:40:58,180
 Then I should construct this data.

1623
01:40:58,180 --> 01:41:00,180
 So if this is your training data, then of course,

1624
01:41:00,180 --> 01:41:01,660
 then your boundary will not be here.

1625
01:41:01,660 --> 01:41:03,060
 If your boundary will be moved here,

1626
01:41:03,060 --> 01:41:07,220
 if this is something you do not allow the slack variable,

1627
01:41:07,220 --> 01:41:10,380
 then your new boundary will, if this is a training data,

1628
01:41:10,380 --> 01:41:12,060
 then your boundary should not be here

1629
01:41:12,060 --> 01:41:14,100
 or it will be closer to here.

1630
01:41:14,100 --> 01:41:18,500
 Unless you allow the slack so-called non separable function.

1631
01:41:18,580 --> 01:41:22,540
 But then to set up the boundary set,

1632
01:41:22,540 --> 01:41:24,700
 so we take this linearly one by one,

1633
01:41:24,700 --> 01:41:27,660
 so there are like multiple cases where we check that.

1634
01:41:27,660 --> 01:41:29,460
 No, no, if you have, okay,

1635
01:41:29,460 --> 01:41:31,300
 they only will give you one boundary

1636
01:41:31,300 --> 01:41:33,220
 with all your training data.

1637
01:41:33,220 --> 01:41:35,980
 It's not multiple one, right?

1638
01:41:37,060 --> 01:41:38,340
 So what is your question?

1639
01:41:38,340 --> 01:41:39,660
 Eve?

1640
01:41:39,660 --> 01:41:41,660
 Sorry, let me put this down.

1641
01:51:48,500 --> 01:51:50,500
 Before you make a move Ð¯

1642
01:55:18,500 --> 01:55:44,500
 Okay.

1643
01:55:44,500 --> 01:55:46,900
 Let's continue.

1644
01:55:46,900 --> 01:55:53,500
 So earlier on I was trying to tell you that the exercise, right, and some of them point

1645
01:55:53,500 --> 01:55:59,299
 out that number one and number four sample, they have exactly the same input but the

1646
01:55:59,299 --> 01:56:02,099
 outputs are different.

1647
01:56:02,099 --> 01:56:04,540
 Okay.

1648
01:56:04,540 --> 01:56:10,179
 So it was done purposely, so you had to build a tree by considering this because you could

1649
01:56:10,179 --> 01:56:13,299
 have an outlier or noisy sample, right?

1650
01:56:13,299 --> 01:56:15,780
 You could classify wrongly.

1651
01:56:15,780 --> 01:56:20,620
 So you build your decision tree, how are you going to deal with such situation?

1652
01:56:20,620 --> 01:56:21,620
 Okay.

1653
01:56:21,620 --> 01:56:27,580
 So this is something that I want you to think about in your assignment.

1654
01:56:27,580 --> 01:56:29,099
 Okay.

1655
01:56:29,099 --> 01:56:34,300
 Let's come back to neural network.

1656
01:56:34,300 --> 01:56:39,700
 Okay in here, let me show you the, okay here.

1657
01:56:39,700 --> 01:56:40,700
 Okay here.

1658
01:56:40,700 --> 01:56:42,219
 Let's look at a very simple example.

1659
01:56:42,220 --> 01:56:50,180
 If you go to this website, you can find there are input x1, x2, the two dimensional data

1660
01:56:50,180 --> 01:56:55,660
 that we mentioned, then we use one perceptron, right?

1661
01:56:55,660 --> 01:56:56,820
 And this is the output.

1662
01:56:56,820 --> 01:56:57,820
 Okay.

1663
01:56:57,820 --> 01:57:00,540
 I just want hidden layer, okay?

1664
01:57:00,540 --> 01:57:01,540
 And this is output.

1665
01:57:01,540 --> 01:57:02,540
 Okay.

1666
01:57:02,540 --> 01:57:05,420
 Two input, one perceptron output.

1667
01:57:05,420 --> 01:57:10,060
 I want to separate the positive from negative class, right?

1668
01:57:10,060 --> 01:57:12,660
 The blue and the orange class.

1669
01:57:12,660 --> 01:57:17,140
 This is almost the same as the exclusive of the one that we just showed you.

1670
01:57:17,140 --> 01:57:21,700
 You cannot use a single neuron to do that, right?

1671
01:57:21,700 --> 01:57:22,700
 But you can do the training.

1672
01:57:22,700 --> 01:57:24,860
 I select here, this is the input.

1673
01:57:24,860 --> 01:57:30,780
 I select the rectify linear unit as activation function.

1674
01:57:30,780 --> 01:57:32,580
 I do not use regularization.

1675
01:57:32,580 --> 01:57:36,460
 I'll talk about this in the later part of the, why is it regularization?

1676
01:57:36,460 --> 01:57:38,580
 You don't want to overfit the data.

1677
01:57:38,580 --> 01:57:39,580
 Okay.

1678
01:57:39,580 --> 01:57:41,340
 So this is your training data.

1679
01:57:41,340 --> 01:57:46,140
 Your goal is to find the boundary can separate blue from red.

1680
01:57:46,140 --> 01:57:47,140
 Okay.

1681
01:57:47,140 --> 01:57:48,140
 Let's train.

1682
01:57:48,140 --> 01:57:54,300
 As you can expect, by using one neuron, you cannot do the job.

1683
01:57:54,300 --> 01:57:57,500
 You can only separate them into two half.

1684
01:57:57,500 --> 01:57:59,059
 And then the error will be high.

1685
01:57:59,059 --> 01:58:04,260
 You see, test, training error and test error are almost the same, right?

1686
01:58:04,260 --> 01:58:08,100
 Regardless of how long you train, you just cannot do the job.

1687
01:58:08,100 --> 01:58:11,540
 You can change your activation function like sigmoid.

1688
01:58:11,540 --> 01:58:13,500
 You will still be the same.

1689
01:58:13,500 --> 01:58:18,380
 You still find you one line, which, right, it will still not be able.

1690
01:58:18,380 --> 01:58:19,380
 Okay.

1691
01:58:19,380 --> 01:58:22,380
 I can discredit side output one and zero.

1692
01:58:22,380 --> 01:58:26,620
 So this is zero and one.

1693
01:58:26,620 --> 01:58:27,620
 Okay.

1694
01:58:27,620 --> 01:58:31,580
 So the way to do it, as I mentioned, you can have two neuron.

1695
01:58:31,580 --> 01:58:32,580
 Okay.

1696
01:58:32,580 --> 01:58:34,860
 This one, the first one I have two.

1697
01:58:34,860 --> 01:58:39,980
 And then I have this one output.

1698
01:58:39,980 --> 01:58:56,820
 Let's see whether this one can train.

1699
01:58:56,820 --> 01:58:58,660
 You see now, there are two neuron.

1700
01:58:58,660 --> 01:58:59,700
 I can have two line.

1701
01:58:59,700 --> 01:59:01,179
 I can separate the region.

1702
01:59:01,180 --> 01:59:07,140
 Of course, you still cannot separate them well, but in terms of the training and testing,

1703
01:59:07,140 --> 01:59:08,460
 now it dropped to 0.1.

1704
01:59:08,460 --> 01:59:10,460
 Early on, it was 0.4.

1705
01:59:10,460 --> 01:59:11,460
 Right?

1706
01:59:11,460 --> 01:59:14,580
 At least what you can do with the two neuron.

1707
01:59:14,580 --> 01:59:18,860
 Of course, you can think about you have four.

1708
01:59:18,860 --> 01:59:25,060
 The septron, then you train again.

1709
01:59:25,060 --> 01:59:28,860
 You take a while because it may not start at the right parameter.

1710
01:59:29,219 --> 01:59:35,860
 Keep finding the solution and now it will come in.

1711
01:59:35,860 --> 01:59:48,700
 You can see the parameter here, W1, W2.

1712
01:59:48,700 --> 01:59:50,620
 These are the weights.

1713
01:59:50,620 --> 01:59:51,620
 Right?

1714
01:59:51,620 --> 01:59:56,700
 You can see this one, it does poorer than the earlier one.

1715
01:59:56,820 --> 01:59:58,420
 You have two more neuron.

1716
01:59:58,420 --> 01:59:59,420
 It doesn't mean you're improved.

1717
01:59:59,420 --> 02:00:00,420
 Early on, you have two.

1718
02:00:00,420 --> 02:00:02,420
 You can drop to 0.1.

1719
02:00:02,420 --> 02:00:05,980
 Now you have two.

1720
02:00:05,980 --> 02:00:09,700
 By right, four should be better than two.

1721
02:00:09,700 --> 02:00:12,540
 Because it's a more complicated network.

1722
02:00:12,540 --> 02:00:15,340
 The two is just a subset of four.

1723
02:00:15,340 --> 02:00:20,540
 But based on your starting point, you may not learn that network that you want.

1724
02:00:20,540 --> 02:00:24,700
 So therefore, some type of regularization coming would help.

1725
02:00:24,700 --> 02:00:28,340
 Let's try to create two outputs.

1726
02:00:28,340 --> 02:00:32,300
 That might help you to do the job.

1727
02:00:32,300 --> 02:00:41,420
 Two outputs, I could have now one and zero.

1728
02:00:41,420 --> 02:00:48,540
 So this is the boundary if I for the four.

1729
02:00:48,540 --> 02:00:52,340
 So the beauty is I don't have to worry about what platform.

1730
02:00:52,340 --> 02:00:55,140
 I just use the same network to train.

1731
02:00:55,140 --> 02:00:59,460
 I don't have to worry about the mathematics.

1732
02:00:59,460 --> 02:01:00,460
 So anyone can do.

1733
02:01:00,460 --> 02:01:04,660
 You collect your data, what kind of boundary you want.

1734
02:01:04,660 --> 02:01:06,260
 Now I only use X1, X2.

1735
02:01:06,260 --> 02:01:08,780
 You can think of some kind of kernel trick.

1736
02:01:08,780 --> 02:01:15,460
 You can talk about X1 squared, X2 squared, X1 times X2, sine X1, sine X2.

1737
02:01:15,460 --> 02:01:16,820
 But normally, these are not so.

1738
02:01:16,820 --> 02:01:20,740
 You just use the data as a draw data as input.

1739
02:01:20,740 --> 02:01:22,620
 Liddy will help.

1740
02:01:22,620 --> 02:01:26,179
 Let's look at other more complicated cases like this.

1741
02:01:26,179 --> 02:01:27,540
 This is simple.

1742
02:01:27,540 --> 02:01:29,259
 This is, okay, let's look at this one.

1743
02:01:29,259 --> 02:01:32,259
 This one, see whether you can do a job.

1744
02:01:32,259 --> 02:01:33,259
 Unlikely.

1745
02:01:33,259 --> 02:01:44,099
 Yeah, this is the best you can do.

1746
02:01:44,099 --> 02:01:45,820
 You see the error coming down.

1747
02:01:45,820 --> 02:01:50,420
 It's all by learning, right?

1748
02:01:50,420 --> 02:01:55,580
 Sometimes it may trap in the local so-called minimum.

1749
02:01:55,580 --> 02:01:59,179
 But you see the error keep coming down.

1750
02:01:59,179 --> 02:02:02,219
 It's a training loss.

1751
02:02:02,219 --> 02:02:07,380
 And by using one hidden layer, two hidden layer, this is output.

1752
02:02:07,380 --> 02:02:08,900
 And you can get it.

1753
02:02:08,900 --> 02:02:11,219
 And this is of course the more challenging library.

1754
02:02:11,220 --> 02:02:16,020
 You cannot separate the data.

1755
02:02:16,020 --> 02:02:17,780
 It's very complex.

1756
02:02:31,780 --> 02:02:33,820
 Error is 0.4.

1757
02:02:33,820 --> 02:02:35,860
 Almost 0.5 is quite a bit.

1758
02:02:35,860 --> 02:02:38,460
 So this is something that cannot.

1759
02:02:38,580 --> 02:02:56,580
 You can try to increase the layer, increase this, see whether it helps.

1760
02:02:56,580 --> 02:03:04,860
 So it's very hard to fit the training data, the classification.

1761
02:03:04,860 --> 02:03:13,660
 See whether it helps.

1762
02:03:13,660 --> 02:03:15,700
 So this is what happens when they design neural networks.

1763
02:03:15,700 --> 02:03:17,700
 They keep trying an error.

1764
02:03:17,700 --> 02:03:19,940
 How complex your network is.

1765
02:03:19,940 --> 02:03:25,059
 You can think about including other data like this and this.

1766
02:03:25,060 --> 02:03:27,260
 Even this, see whether it helps.

1767
02:03:49,660 --> 02:03:51,620
 So drop to 0.1.

1768
02:03:51,620 --> 02:03:52,620
 Looks better.

1769
02:03:55,860 --> 02:04:01,060
 See this is all learning process.

1770
02:04:01,060 --> 02:04:05,060
 You can try to fit the data.

1771
02:04:05,060 --> 02:04:07,060
 This is probably overfit.

1772
02:04:07,060 --> 02:04:09,060
 It's not very regular.

1773
02:04:09,060 --> 02:04:12,060
 Ideally you want to just along the count line.

1774
02:04:12,060 --> 02:04:17,060
 But this could be the one solution to tell you that this would be the classifier.

1775
02:04:18,060 --> 02:04:27,060
 It may not be really reflecting the original so-called concept, the situation.

1776
02:04:27,060 --> 02:04:30,060
 So you can play around with this.

1777
02:04:30,060 --> 02:04:37,060
 And there are many so-called options after that you can use other kind of...

1778
02:04:37,060 --> 02:04:38,060
 You look at the linear.

1779
02:04:38,060 --> 02:04:40,060
 This one would not help.

1780
02:04:40,060 --> 02:04:43,060
 Whatever linear, they give you just one line.

1781
02:04:44,060 --> 02:04:48,060
 Earlier I said that you have more so-called perceptron.

1782
02:04:48,060 --> 02:04:52,060
 But if you use the linear function, it's just a binary classification.

1783
02:04:52,060 --> 02:04:58,060
 Even you have a million of perceptron, everything can collapse into one perceptron one line.

1784
02:04:58,060 --> 02:05:01,060
 So it would not really help you.

1785
02:05:01,060 --> 02:05:03,060
 So let's do...

1786
02:05:05,060 --> 02:05:08,060
 See whether you converge faster.

1787
02:05:08,060 --> 02:05:10,060
 You can see the computation wise.

1788
02:05:10,060 --> 02:05:12,060
 That rule is...

1789
02:05:12,060 --> 02:05:13,060
 Wow.

1790
02:05:13,060 --> 02:05:14,060
 Not bad.

1791
02:05:14,060 --> 02:05:16,060
 0.15.

1792
02:05:16,060 --> 02:05:18,060
 Easier to calculate.

1793
02:05:18,060 --> 02:05:23,060
 You can separate the function.

1794
02:05:23,060 --> 02:05:25,060
 0.14.

1795
02:05:31,060 --> 02:05:34,060
 There are still certain parts that it doesn't really do the job.

1796
02:05:34,060 --> 02:05:36,060
 Maybe you have to increase the...

1797
02:05:36,060 --> 02:05:39,060
 Maybe more hidden layer.

1798
02:05:39,060 --> 02:05:42,060
 And more neuron, perhaps.

1799
02:05:47,060 --> 02:05:49,060
 This is what they do when they train the network.

1800
02:05:49,060 --> 02:05:51,060
 They just keep changing.

1801
02:05:51,060 --> 02:05:53,060
 Those number of neurons over here are caused.

1802
02:05:53,060 --> 02:05:55,060
 These are hyperparameters.

1803
02:05:55,060 --> 02:05:57,060
 Something that you can select.

1804
02:05:57,060 --> 02:06:00,060
 You can choose to see whether you'll give you better results.

1805
02:06:00,060 --> 02:06:04,060
 It may not have any so-called real signs behind.

1806
02:06:04,060 --> 02:06:06,060
 Just try an error.

1807
02:06:10,060 --> 02:06:11,060
 Yeah.

1808
02:06:11,060 --> 02:06:13,060
 And then these are the parameters.

1809
02:06:13,060 --> 02:06:15,060
 You can see all the weights here.

1810
02:06:19,060 --> 02:06:20,060
 Okay.

1811
02:06:20,060 --> 02:06:22,060
 This is the perceptron.

1812
02:06:22,060 --> 02:06:24,060
 Let's continue the lecture.

1813
02:06:24,060 --> 02:06:32,060
 Now you can go back and play with it if you are interested to find out.

1814
02:06:32,060 --> 02:06:36,060
 Then I can configure any topology.

1815
02:06:36,060 --> 02:06:39,060
 But how do you find the weights?

1816
02:06:39,060 --> 02:06:41,060
 Certainly not the one I showed you.

1817
02:06:41,060 --> 02:06:46,060
 You plug in the formula, the sigma, then find out all the conditions.

1818
02:06:46,060 --> 02:06:48,060
 Every data point, you have one formula.

1819
02:06:48,060 --> 02:06:52,060
 So that is almost very challenging.

1820
02:06:52,060 --> 02:06:55,060
 Even you want to solve it by some numerical method.

1821
02:06:55,060 --> 02:06:59,060
 So luckily, you have this big propagation.

1822
02:06:59,060 --> 02:07:11,060
 There is the one that actually hinted and they come up with this so-called paper in nature

1823
02:07:11,060 --> 02:07:21,060
 to tell you how you can do this big propagation to learn all the weights and bias of the multi-layer perceptron.

1824
02:07:21,060 --> 02:07:28,060
 So the idea is you have a training set of training data.

1825
02:07:28,060 --> 02:07:34,060
 So for each training data, you have the target output, which could be just a class labor.

1826
02:07:34,060 --> 02:07:39,060
 It could be also a regression problem, a value, a predictive value.

1827
02:07:39,060 --> 02:07:46,060
 For example, a classification task or a regression task, depending on what problem that you are trying to solve.

1828
02:07:46,060 --> 02:07:48,060
 Okay.

1829
02:07:48,060 --> 02:07:53,060
 And then the algorithm will adjust the weights, the w and the b.

1830
02:07:53,060 --> 02:08:06,060
 So if you start with some random number, as random as possible, then it will try to fit the data to the multi-layer perceptron one by one.

1831
02:08:06,060 --> 02:08:10,060
 And every time the data come in, you have the output.

1832
02:08:10,060 --> 02:08:12,060
 Compare the output with your target output.

1833
02:08:12,060 --> 02:08:19,060
 If the output are not the same, target and the actual output, then you have an error.

1834
02:08:19,060 --> 02:08:23,060
 And the error will help you to modify the weights and the bias.

1835
02:08:23,060 --> 02:08:27,060
 And then you do it in a backward direction.

1836
02:08:27,060 --> 02:08:30,060
 You fit the data in, create an error.

1837
02:08:30,060 --> 02:08:33,060
 The error actually will be propagated back.

1838
02:08:33,060 --> 02:08:38,060
 Because if the error is too large, look at, oh, this one actually contributes to the positive.

1839
02:08:38,060 --> 02:08:41,060
 You should adjust down the weight one by one.

1840
02:08:41,060 --> 02:08:53,060
 So if you do that, and for many, many training samples, if they all behave in the very so-called consistent way,

1841
02:08:53,060 --> 02:09:03,060
 you could find the network with a set of parameters that work well in classifying the data or predicting the value you want.

1842
02:09:03,060 --> 02:09:10,060
 Although such process may not guarantee you to converge to 100% accuracy,

1843
02:09:10,060 --> 02:09:14,060
 but if you do it right, it will always improve the result.

1844
02:09:14,060 --> 02:09:21,060
 Unless, like the homework I showed you, there are some inconsistencies, then there's never you can resolve this.

1845
02:09:21,060 --> 02:09:25,060
 But for most cases, you can always improve.

1846
02:09:25,060 --> 02:09:28,060
 So the trick it does is actually very simple.

1847
02:09:28,060 --> 02:09:33,060
 Assuming you have this gigantic network, at the end, you have this error function,

1848
02:09:33,060 --> 02:09:40,060
 which is basically the difference between the actual output and your target output.

1849
02:09:40,060 --> 02:09:45,060
 Of course, this function can be very complex, depending on how you formulate the problem.

1850
02:09:45,060 --> 02:09:52,060
 But assuming you have done that, you have this error function, which depends on the W.

1851
02:09:52,060 --> 02:10:01,060
 And W is the parameter to weights and the bias that you want to, so-called, find out which one give you the minimum error.

1852
02:10:01,060 --> 02:10:08,060
 Like the demo that we showed earlier, there was a weight then start to drop 0.4, 0.5, 0.1.

1853
02:10:08,060 --> 02:10:12,060
 That is the error function you want to minimize.

1854
02:10:12,060 --> 02:10:17,060
 So you start with some weight, then you compute the error function.

1855
02:10:17,060 --> 02:10:26,060
 If the error function is not zero, then you can always define the weights by using this gradient descent.

1856
02:10:27,060 --> 02:10:32,060
 The gradient descent, if you recall, assuming you have this error function, JW,

1857
02:10:32,060 --> 02:10:37,060
 and this JW could be very complex, and W could be millions of parameters.

1858
02:10:37,060 --> 02:10:42,060
 So there's no way you can visualize what is this error function like.

1859
02:10:42,060 --> 02:10:47,060
 But for its illustration purpose, let's look at the 2D.

1860
02:10:47,060 --> 02:10:52,060
 You have this error function here, and even 1D, we have weight.

1861
02:10:52,060 --> 02:11:02,060
 So just in this any value here, assuming you start from here, then you compute, you start with random weights.

1862
02:11:02,060 --> 02:11:07,060
 Then you put in your data, it will give you the error JW.

1863
02:11:07,060 --> 02:11:11,060
 This is the error that you get for the train data at this point.

1864
02:11:11,060 --> 02:11:14,060
 It certainly is not zero.

1865
02:11:15,060 --> 02:11:25,060
 But if you can find that the error is actually in the differentiation, you change the JW, DW.

1866
02:11:25,060 --> 02:11:27,060
 This is the weight.

1867
02:11:27,060 --> 02:11:37,060
 If you know this curve, this is the so-called gradient of this J over W at this position.

1868
02:11:37,060 --> 02:11:42,060
 Then you know that this is in the upward train, because the gradient is a gradient in positive.

1869
02:11:42,060 --> 02:11:48,060
 That means when you change the W, it increases, and the error function increases.

1870
02:11:48,060 --> 02:11:55,060
 So you should minus your original W with this negative of the gradient direction.

1871
02:11:55,060 --> 02:12:02,060
 If this is positive, that means if you move in this direction, the error is going to increase.

1872
02:12:02,060 --> 02:12:06,060
 So you should move in the negative direction of your gradient.

1873
02:12:06,060 --> 02:12:08,060
 But how much you have to move, you don't know.

1874
02:12:08,060 --> 02:12:16,060
 So you just multiply with the so-called eta, the learning weight, learning parameter.

1875
02:12:16,060 --> 02:12:20,060
 Multiplier gradient, you subtract this from the original weight.

1876
02:12:20,060 --> 02:12:24,060
 Let's say here, you subtract here, maybe the new weight.

1877
02:12:24,060 --> 02:12:25,060
 You can start with here.

1878
02:12:25,060 --> 02:12:28,060
 The weight is here, W0.

1879
02:12:28,060 --> 02:12:32,060
 Then you subtract, you get W1 here.

1880
02:12:32,060 --> 02:12:36,060
 Then you compute the error, JW1.

1881
02:12:36,060 --> 02:12:39,060
 Then in here, you compute the gradient.

1882
02:12:39,060 --> 02:12:44,060
 In this time, the gradient is actually in the negative direction.

1883
02:12:44,060 --> 02:12:47,060
 Then you should increase your W towards here.

1884
02:12:47,060 --> 02:12:53,060
 So you are going to find the W by moving down towards this valley.

1885
02:12:53,060 --> 02:13:00,060
 So the more iteration you do, if you select your so-called learning parameter properly,

1886
02:13:00,060 --> 02:13:06,060
 you can converge to a so-called local minimum or global minimum, depending on which one you are in.

1887
02:13:06,060 --> 02:13:09,060
 So there is called gradient descent.

1888
02:13:09,060 --> 02:13:14,060
 I believe some of you have learned this in your other engineering subject.

1889
02:13:14,060 --> 02:13:23,060
 If you can formulate any cost function, J, you start with a random initial value W.

1890
02:13:23,060 --> 02:13:27,060
 Then it may not be perfect, but your objective is to minimize the error function.

1891
02:13:27,060 --> 02:13:29,060
 You can do a small perturbation.

1892
02:13:29,060 --> 02:13:37,060
 You can increase W a bit, calculate the new weight, decrease a bit, calculate the new weight to see whether you are climbing or lowering.

1893
02:13:37,060 --> 02:13:43,060
 If the weight is lowering or you continue to move in the right direction, the error keeps decreasing.

1894
02:13:43,060 --> 02:13:44,060
 So that is the idea.

1895
02:13:44,060 --> 02:13:45,060
 It's very simple.

1896
02:13:45,060 --> 02:13:46,060
 Just try it.

1897
02:13:46,060 --> 02:13:50,060
 Just that this W could be very high-dimension.

1898
02:13:50,060 --> 02:13:52,060
 It's not 1D.

1899
02:13:52,060 --> 02:13:53,060
 Then you could have a million.

1900
02:13:53,060 --> 02:13:54,060
 Which one to change?

1901
02:13:54,060 --> 02:13:58,060
 You could have a W vector, which is one million of them.

1902
02:13:58,060 --> 02:14:03,060
 You can change one by one, change three of them, change four of them, change thousand of them.

1903
02:14:03,060 --> 02:14:05,060
 That becomes challenging.

1904
02:14:05,060 --> 02:14:09,060
 So this allows you to solve the problem.

1905
02:14:09,060 --> 02:14:14,060
 And this eta is a learning parameter, learning rate.

1906
02:14:14,060 --> 02:14:15,060
 We call it learning.

1907
02:14:15,060 --> 02:14:16,060
 How fast you want to learn?

1908
02:14:16,060 --> 02:14:18,060
 It's not always easy to choose.

1909
02:14:18,060 --> 02:14:23,060
 For example, in this case, look at the other key.

1910
02:14:24,060 --> 02:14:30,060
 So if you start with here, it keeps moving in this direction.

1911
02:14:30,060 --> 02:14:32,060
 It may be very slow.

1912
02:14:32,060 --> 02:14:35,060
 This error could be very slow.

1913
02:14:35,060 --> 02:14:38,060
 So just now you saw some demo at the beginning, no change.

1914
02:14:38,060 --> 02:14:40,060
 You keep iterating, but no change.

1915
02:14:40,060 --> 02:14:44,060
 You could wait for months before it starts to move.

1916
02:14:44,060 --> 02:14:48,060
 Or you could have this a very large value.

1917
02:14:48,060 --> 02:14:51,060
 If you are here, you point here, it keeps increasing.

1918
02:14:51,060 --> 02:14:54,060
 Your error keeps increasing rather than decreasing.

1919
02:14:54,060 --> 02:14:58,060
 Although you move in the right direction, but because of the valley,

1920
02:14:58,060 --> 02:15:03,060
 actually you move here, you assume it should be here, but it could go up.

1921
02:15:03,060 --> 02:15:06,060
 At the end, you may expose.

1922
02:15:06,060 --> 02:15:10,060
 So it depends on the learning parameter that you choose.

1923
02:15:10,060 --> 02:15:15,060
 So if you have a good learning rate, the yellow line, after iteration,

1924
02:15:15,060 --> 02:15:19,060
 you want this continue to decrease.

1925
02:15:19,060 --> 02:15:23,060
 If you start with here, the blue line is very slow.

1926
02:15:23,060 --> 02:15:26,060
 Until you lose your patience, you keep up.

1927
02:15:26,060 --> 02:15:31,060
 Or you run out of your budget, you cannot compute anymore.

1928
02:15:31,060 --> 02:15:35,060
 Or if you select the learning rate very hard, you may expose.

1929
02:15:35,060 --> 02:15:37,060
 This is the case, the error keeps increasing.

1930
02:15:37,060 --> 02:15:44,060
 So how to select the right learning parameters, it is not so straightforward.

1931
02:15:44,060 --> 02:15:49,060
 And likely they will not actually choose one learning rate for all the training.

1932
02:15:49,060 --> 02:15:51,060
 They start with some large number.

1933
02:15:51,060 --> 02:15:55,060
 Then based on the momentum, based on how fast it moves,

1934
02:15:55,060 --> 02:16:00,060
 you will start to reduce the learning rate until it converges.

1935
02:16:00,060 --> 02:16:06,060
 And there are papers about the atom optimizer to tell you how to compute.

1936
02:16:06,060 --> 02:16:12,060
 When you select one of the so-called learning approach, you can select this.

1937
02:16:12,060 --> 02:16:14,060
 This depends on where you start.

1938
02:16:14,060 --> 02:16:16,060
 Sometimes you can find solutions.

1939
02:16:16,060 --> 02:16:19,060
 Sometimes it traps in the local minimum.

1940
02:16:19,060 --> 02:16:22,060
 Earlier on, we showed that there were four and two.

1941
02:16:22,060 --> 02:16:26,060
 The four supposed to be more complex than two, but turned out the four cannot solve the problem

1942
02:16:26,060 --> 02:16:28,060
 because it was trapped in the local minimum.

1943
02:16:28,060 --> 02:16:34,060
 So when you have more neuron, it doesn't mean you always keep you a better solution.

1944
02:16:34,060 --> 02:16:37,060
 The function just becomes more complex.

1945
02:16:37,060 --> 02:16:40,060
 Easy for you to trap in the local minimum.

1946
02:16:41,059 --> 02:16:47,059
 Then you have fewer neuron, which you only have a very simple so-called error function curve.

1947
02:16:47,059 --> 02:16:51,059
 You may find the global minimum easier.

1948
02:16:51,059 --> 02:16:57,059
 Therefore, regularization, some of these tricks will come in to help you to find the global minimum.

1949
02:16:57,059 --> 02:16:59,059
 There is a gradient descent.

1950
02:16:59,059 --> 02:17:06,059
 And again, the whole idea of the network, assuming you construct this multi-layer perceptron

1951
02:17:06,059 --> 02:17:11,059
 with all the w, the weights, and the bias.

1952
02:17:11,059 --> 02:17:15,059
 And how do you find these w and weight bias?

1953
02:17:15,059 --> 02:17:17,059
 Basically, you define the output.

1954
02:17:17,059 --> 02:17:18,059
 What is your target output?

1955
02:17:18,059 --> 02:17:20,059
 What is your actual output?

1956
02:17:20,059 --> 02:17:26,059
 With all your trained data, you just keep doing this.

1957
02:17:26,059 --> 02:17:29,059
 With a set of initial w, you'll find your error.

1958
02:17:29,059 --> 02:17:34,059
 If it's not zero, you start to move the weights in a certain direction,

1959
02:17:34,059 --> 02:17:39,059
 which is the opposite of your gradient descent.

1960
02:17:39,059 --> 02:17:42,059
 It's called gradient descent algorithm.

1961
02:17:42,059 --> 02:17:52,059
 Then, liter by liter, you keep improving or reducing your error until you get to the point that you like the solution.

1962
02:17:52,059 --> 02:17:57,059
 That is basically the whole idea of the neural network training.

1963
02:17:57,059 --> 02:18:00,059
 Of course, how do you really implement it?

1964
02:18:00,059 --> 02:18:03,059
 There is a back propagation algorithm.

1965
02:18:03,059 --> 02:18:12,059
 There is considered one of the very important discovery.

1966
02:18:12,059 --> 02:18:17,059
 I didn't call it invention because it was before RuhmerHertt.

1967
02:18:17,059 --> 02:18:21,059
 It was already done by other.

1968
02:18:21,059 --> 02:18:23,059
 But how do you apply it in the neural network?

1969
02:18:23,059 --> 02:18:27,059
 It was Clinton and RuhmerHertt.

1970
02:18:27,059 --> 02:18:30,059
 They two of them work together.

1971
02:18:30,059 --> 02:18:38,059
 Let's look at one of these so-called multi-layer perceptron as an example.

1972
02:18:38,059 --> 02:18:41,059
 This is one of the things that we saw before.

1973
02:18:41,059 --> 02:18:51,059
 We can call this a 3, 4, 2 topology, 3 input neuron, one hidden layer with four neurons,

1974
02:18:51,059 --> 02:18:58,059
 and two output units in the output layer.

1975
02:18:58,059 --> 02:19:01,059
 We will define the Tk and Ok.

1976
02:19:01,059 --> 02:19:03,059
 In here, they are only c equal to 2.

1977
02:19:03,059 --> 02:19:05,059
 They are only two output neurons.

1978
02:19:05,059 --> 02:19:14,059
 The actual output and the target output and the actual output O.

1979
02:19:14,059 --> 02:19:20,059
 That is basically the square error, sum of the square error.

1980
02:19:21,059 --> 02:19:24,059
 Then how you adjust the weight?

1981
02:19:24,059 --> 02:19:25,059
 You start with W.

1982
02:19:25,059 --> 02:19:32,059
 This Wji is the weight between the neuron i and neuron j.

1983
02:19:32,059 --> 02:19:36,059
 You can label it 1, 2, 3, 4, 5, 6, 7, 8, and 9.

1984
02:19:36,059 --> 02:19:40,059
 Between, let's say, 3 and 7.

1985
02:19:40,059 --> 02:19:44,059
 W73 from 3 to 7.

1986
02:19:44,059 --> 02:19:51,060
 This is the bias to this neuron.

1987
02:19:51,060 --> 02:19:56,060
 With this, how many are no parameter that you have?

1988
02:19:56,060 --> 02:20:00,060
 It's just a simple 3, 4, 2 network.

1989
02:20:00,060 --> 02:20:07,060
 How many parameter that you can have?

1990
02:20:07,060 --> 02:20:08,060
 Any idea?

1991
02:20:08,060 --> 02:20:10,060
 Anyone can calculate?

1992
02:20:10,060 --> 02:20:13,060
 It could be an exam question.

1993
02:20:13,060 --> 02:20:16,060
 Each one would have a bias.

1994
02:20:16,060 --> 02:20:19,060
 Every connection had the weight.

1995
02:20:19,060 --> 02:20:31,060
 How many? Anyone?

1996
02:20:31,060 --> 02:20:32,060
 Anyone have a number?

1997
02:20:32,060 --> 02:20:34,060
 How many parameters?

1998
02:20:34,060 --> 02:20:37,060
 How would I need to find a solution?

1999
02:20:37,060 --> 02:20:50,060
 I need to adjust to solve this simple multi-perceptron network.

2000
02:20:50,060 --> 02:20:52,060
 First, how many weights here?

2001
02:20:52,060 --> 02:20:56,060
 Between 3 and 4, you have 3 times 4 different weights.

2002
02:20:56,060 --> 02:20:57,060
 Connection, right?

2003
02:20:57,060 --> 02:20:59,060
 There are 3 times 4.

2004
02:20:59,060 --> 02:21:01,060
 Each unit can connect to 4.

2005
02:21:01,060 --> 02:21:06,060
 So, it's a 3 times 4 here.

2006
02:21:06,060 --> 02:21:10,060
 Weight, 3 times 4.

2007
02:21:10,060 --> 02:21:11,060
 And how many here?

2008
02:21:11,060 --> 02:21:12,060
 4 and 2.

2009
02:21:12,060 --> 02:21:15,060
 4 times 2.

2010
02:21:15,060 --> 02:21:23,060
 So, 12 plus 8, 20, and plus the bias.

2011
02:21:23,060 --> 02:21:26,060
 You have 1, 2, 3, 4, 5, 6.

2012
02:21:26,060 --> 02:21:31,060
 So, total 26 parameters here.

2013
02:21:31,060 --> 02:21:35,060
 Assuming input, you don't put bias.

2014
02:21:35,060 --> 02:21:39,060
 But all this hidden and then output layer, you have bias.

2015
02:21:39,060 --> 02:21:45,060
 So, total, there are 26 parameters that you need to adjust.

2016
02:21:45,060 --> 02:21:50,060
 Some of them, assuming your error is E, so you use a gradient descent.

2017
02:21:50,060 --> 02:21:54,060
 Every time you adjust this, you take the differential error,

2018
02:21:54,060 --> 02:21:56,060
 you take the Jw, right?

2019
02:21:56,060 --> 02:22:00,060
 With respect to the W, JK, and this is a learning rate.

2020
02:22:00,060 --> 02:22:05,060
 Similarly for the B, the error, and then with respect to the BJ.

2021
02:22:05,060 --> 02:22:09,060
 Of course, your error had to express in terms of W, JK,

2022
02:22:09,060 --> 02:22:11,060
 express in terms of the BJ.

2023
02:22:11,060 --> 02:22:12,060
 We will show you how to do that.

2024
02:22:12,060 --> 02:22:17,060
 Because once you have a function which is a parameter of the unknown,

2025
02:22:17,060 --> 02:22:22,060
 then you can do the partial differentiation.

2026
02:22:22,060 --> 02:22:26,060
 So, the goal are the target and the network output vectors.

2027
02:22:26,060 --> 02:22:27,060
 So, how do you solve that?

2028
02:22:27,060 --> 02:22:32,060
 The training, first you initiate all the weights and bias.

2029
02:22:32,060 --> 02:22:37,060
 Of course, if everything you start with 0, there's nothing will happen.

2030
02:22:37,060 --> 02:22:41,060
 Your input, you multiply 0 or 0.

2031
02:22:41,060 --> 02:22:46,060
 Unless you use some kind of a so-called very strange activation function,

2032
02:22:46,060 --> 02:22:49,060
 otherwise output will not change.

2033
02:22:49,060 --> 02:22:57,060
 Only you will start this with a random so-called 0 mean Gaussian so-called number

2034
02:22:57,060 --> 02:23:00,060
 for all the weights and the bias.

2035
02:23:00,060 --> 02:23:06,060
 That means every, you think carefully, every neuron actually is now a different kind

2036
02:23:06,060 --> 02:23:14,060
 of a binary so-called hyperplane in this high dimensional space, right?

2037
02:23:14,060 --> 02:23:17,060
 We got the weights and the bias are different.

2038
02:23:17,060 --> 02:23:22,060
 They are actually pointing to different kind of partition.

2039
02:23:22,060 --> 02:23:25,060
 Then you have all the training samples, right?

2040
02:23:25,060 --> 02:23:27,060
 The inputs, you compute the input.

2041
02:23:27,060 --> 02:23:30,060
 Let's say you have an X1 sample here.

2042
02:23:30,060 --> 02:23:34,060
 You compute for each neuron, compute what is the net input,

2043
02:23:34,060 --> 02:23:38,060
 compute the old output, old Z, going through the activation function,

2044
02:23:38,060 --> 02:23:43,060
 which is something that we show you in the single neuron.

2045
02:23:44,060 --> 02:23:50,060
 Of course, once you compute all the output, the error may not be 0, most likely.

2046
02:23:50,060 --> 02:23:53,060
 This error, Tk minus OK.

2047
02:23:53,060 --> 02:24:00,060
 Tk is your target output, depending on whether it's 1 or 2, right?

2048
02:24:00,060 --> 02:24:04,060
 Tk1, Tk2, Tk1, T2.

2049
02:24:04,060 --> 02:24:06,060
 So T1 minus 01.

2050
02:24:06,060 --> 02:24:12,060
 So it is at this output here, each of these neurons you can compute

2051
02:24:12,060 --> 02:24:16,060
 the so-called error function delta k.

2052
02:24:16,060 --> 02:24:21,060
 I will tell you what is this delta k later on, how it comes from.

2053
02:24:21,060 --> 02:24:26,060
 But basically, when you compute this net k, right, for every neuron,

2054
02:24:26,060 --> 02:24:33,060
 you can compute this net j, j can be every of these network.

2055
02:24:34,060 --> 02:24:39,060
 Then in every of these neuron now, at the output layer first,

2056
02:24:39,060 --> 02:24:43,060
 output unit, right, you first, after the sample input compute,

2057
02:24:43,060 --> 02:24:45,060
 you get the old k.

2058
02:24:45,060 --> 02:24:50,060
 For each unit, then you subtract from the Tk, the target output,

2059
02:24:50,060 --> 02:24:57,060
 then you compute this value delta k, which is the differentiation

2060
02:24:57,060 --> 02:25:01,060
 of the activation function with the net k as your input,

2061
02:25:01,060 --> 02:25:04,060
 which is delta tk minus OK.

2062
02:25:04,060 --> 02:25:10,060
 We call this term, it's the error term at the output unit,

2063
02:25:10,060 --> 02:25:13,060
 each of them, you just learn this as the error function.

2064
02:25:13,060 --> 02:25:17,060
 Basically, because your tk minus OK is not zero,

2065
02:25:17,060 --> 02:25:23,060
 so every output unit would have a delta k, which is not zero.

2066
02:25:23,060 --> 02:25:26,060
 You just consider that it's the error created

2067
02:25:26,060 --> 02:25:29,060
 because the output, the target output and the actual output,

2068
02:25:29,060 --> 02:25:31,060
 are not the same.

2069
02:25:31,060 --> 02:25:35,060
 So you compute for all the output unit one by one,

2070
02:25:35,060 --> 02:25:38,060
 compute the different, which is the activation function,

2071
02:25:38,060 --> 02:25:43,060
 you take the derivative with the net k, multiply with this.

2072
02:25:43,060 --> 02:25:48,060
 Then for the hidden unit, this is the error that you use

2073
02:25:48,060 --> 02:25:52,060
 to compute all the delta j, j is the hidden layer.

2074
02:25:52,060 --> 02:25:55,060
 As long as there are no input and the output unit,

2075
02:25:55,060 --> 02:25:58,060
 they all belong to this hidden unit.

2076
02:25:58,060 --> 02:26:00,060
 So you can use this formula.

2077
02:26:00,060 --> 02:26:01,060
 So what is this?

2078
02:26:01,060 --> 02:26:05,060
 Earlier on, I mentioned this is a delta, let's say it's a delta,

2079
02:26:05,060 --> 02:26:09,060
 seven delta, one, two, three, four, five, six, seven,

2080
02:26:09,060 --> 02:26:11,060
 delta eight and delta nine.

2081
02:26:11,060 --> 02:26:13,060
 These are two outputs.

2082
02:26:13,060 --> 02:26:16,060
 Delta eight and delta nine.

2083
02:26:16,060 --> 02:26:20,060
 So now you want to compute this here, seven delta seven,

2084
02:26:20,060 --> 02:26:23,060
 which is a hidden unit.

2085
02:26:23,060 --> 02:26:28,060
 This delta j, the seven, is also the derivative

2086
02:26:28,060 --> 02:26:31,060
 of the activation function.

2087
02:26:31,060 --> 02:26:35,060
 Compute based on the net j input to this delta seven unit,

2088
02:26:35,060 --> 02:26:39,060
 plus the error here, delta eight and delta nine,

2089
02:26:39,060 --> 02:26:43,060
 multiply with the weight, w k j, you sum them up.

2090
02:26:43,060 --> 02:26:47,060
 Because if this is not zero, right, and there will be error.

2091
02:26:47,060 --> 02:26:50,060
 And these two errors actually come from here.

2092
02:26:50,060 --> 02:26:53,060
 So you also have to adjust all the weight here.

2093
02:26:53,060 --> 02:26:55,060
 So you have to compute the error now.

2094
02:26:55,060 --> 02:26:57,060
 At least all the hidden unit delta z.

2095
02:26:57,060 --> 02:26:59,060
 And this is how you compute.

2096
02:26:59,060 --> 02:27:02,060
 Looking at the error in the next layer,

2097
02:27:02,060 --> 02:27:05,060
 you do the back work weighted summation,

2098
02:27:05,060 --> 02:27:11,060
 compute here, multiply with this activation function.

2099
02:27:11,060 --> 02:27:12,060
 Why you multiply?

2100
02:27:12,060 --> 02:27:15,060
 Because remember this unit, right?

2101
02:27:15,060 --> 02:27:19,060
 You have the net input, then you do the activation function

2102
02:27:19,060 --> 02:27:21,060
 before you put the output.

2103
02:27:21,060 --> 02:27:24,060
 Now if you want to propagate error back work,

2104
02:27:24,060 --> 02:27:26,060
 you have to do the backward direction.

2105
02:27:26,060 --> 02:27:30,060
 Rather than you go out, you want to see the change of derivative,

2106
02:27:30,060 --> 02:27:32,060
 the gradient descent, where it moves.

2107
02:27:32,060 --> 02:27:35,060
 So when you move to the forward direction,

2108
02:27:35,060 --> 02:27:40,060
 you have to do the gradient of the activation function.

2109
02:27:40,060 --> 02:27:44,060
 For example, if you use this sigmoid,

2110
02:27:44,060 --> 02:27:47,060
 this is the derivative of the activation function.

2111
02:27:47,060 --> 02:27:51,060
 If you use the sigmoid, this is the activation of the sigmoid

2112
02:27:51,060 --> 02:27:57,060
 function, then your sigma derivative net k

2113
02:27:57,060 --> 02:28:00,060
 becomes OK1 minus OK.

2114
02:28:00,060 --> 02:28:03,060
 Which you can verify, you compute,

2115
02:28:03,060 --> 02:28:06,060
 because you have 1 plus e minus x,

2116
02:28:06,060 --> 02:28:09,060
 which is equal to your output.

2117
02:28:09,060 --> 02:28:14,060
 And the derivative of this becomes OK1 minus OK.

2118
02:28:14,060 --> 02:28:17,060
 And then the year becomes OJ1 minus OJ.

2119
02:28:17,060 --> 02:28:19,060
 This is the derivative of this.

2120
02:28:19,060 --> 02:28:22,060
 And this is still the same as the rest.

2121
02:28:22,060 --> 02:28:27,060
 This is only when your activation function is sigmoid.

2122
02:28:27,060 --> 02:28:35,060
 If this is a red rule, then this term will be either 1 or 0.

2123
02:28:35,060 --> 02:28:38,060
 It depends on what is the value of your net k.

2124
02:28:38,060 --> 02:28:42,060
 If the net k is a positive value, then the red rule,

2125
02:28:42,060 --> 02:28:44,060
 the derivative becomes 1.

2126
02:28:44,060 --> 02:28:48,060
 If it's a negative value, then the derivative becomes 0.

2127
02:28:48,060 --> 02:28:51,060
 Then for activation for the red rule,

2128
02:28:51,060 --> 02:28:55,060
 this delta k delta j will be easier to compute,

2129
02:28:55,060 --> 02:28:59,060
 because this term will be either 1 or 0.

2130
02:28:59,060 --> 02:29:02,060
 That's how you compute the error.

2131
02:29:02,060 --> 02:29:11,060
 Once you compute all this delta from delta 4, delta 5,

2132
02:29:11,060 --> 02:29:15,060
 delta 6, delta 7, delta 8, delta 9,

2133
02:29:15,060 --> 02:29:19,060
 then you can adjust the weight.

2134
02:29:19,060 --> 02:29:22,060
 This is plus delta w kj.

2135
02:29:22,060 --> 02:29:26,060
 Delta kj equal to, look at this, it's quite easy.

2136
02:29:26,060 --> 02:29:29,060
 For any of these weight adjustment,

2137
02:29:29,060 --> 02:29:32,060
 which will be the learning rate,

2138
02:29:32,060 --> 02:29:36,060
 eta w multiplied with delta k,

2139
02:29:36,060 --> 02:29:38,060
 the one at the following unit,

2140
02:29:38,060 --> 02:29:40,060
 the error you just compute,

2141
02:29:40,060 --> 02:29:45,060
 that's the output from the earlier unit, oj.

2142
02:29:45,060 --> 02:29:47,060
 You have compute delta k,

2143
02:29:47,060 --> 02:29:50,060
 then the adjustment will be the sigma,

2144
02:29:50,060 --> 02:29:56,060
 sorry, the eta w delta k and output oj.

2145
02:29:56,060 --> 02:30:01,060
 Similarly, for this weight will be delta z,

2146
02:30:01,060 --> 02:30:04,060
 oi multiplied with eta.

2147
02:30:04,060 --> 02:30:06,060
 This is the adjustment you make.

2148
02:30:06,060 --> 02:30:08,060
 And then for the bias,

2149
02:30:08,060 --> 02:30:13,060
 we mentioned that you have a unit with output 1,

2150
02:30:13,060 --> 02:30:15,060
 weight equal to bz,

2151
02:30:15,060 --> 02:30:17,060
 so because output equal to 1,

2152
02:30:17,060 --> 02:30:21,060
 so the adjustment is the learning rate for the bias,

2153
02:30:21,060 --> 02:30:25,060
 the sigma z, which is the error term,

2154
02:30:25,060 --> 02:30:28,060
 and then one output.

2155
02:30:28,060 --> 02:30:31,060
 And this will be the amount that you use

2156
02:30:31,060 --> 02:30:35,060
 to adjust your respective weight and bias.

2157
02:30:35,060 --> 02:30:40,060
 And that is the backward propagation formula.

2158
02:30:40,060 --> 02:30:42,060
 And that's it.

2159
02:30:42,060 --> 02:30:43,060
 It's not difficult.

2160
02:30:43,060 --> 02:30:46,060
 Once you define your error function,

2161
02:30:46,060 --> 02:30:50,060
 in this case, this error function is this, right?

2162
02:30:50,060 --> 02:30:53,060
 And you just follow this set of formula.

2163
02:30:53,060 --> 02:30:56,060
 You can, for every input you need,

2164
02:30:56,060 --> 02:31:00,060
 you can compute the difference between tk and ok,

2165
02:31:00,060 --> 02:31:02,060
 then you propagate the error back,

2166
02:31:02,060 --> 02:31:05,060
 and then you adjust the weight and bias.

2167
02:31:05,060 --> 02:31:09,060
 Then for the next sample, you compute again,

2168
02:31:09,060 --> 02:31:12,060
 you propagate the error back, adjust the weight.

2169
02:31:12,060 --> 02:31:17,060
 So one by one, you will move the weights to the right direction,

2170
02:31:17,060 --> 02:31:21,060
 reduce the error, like the demo that we showed earlier.

2171
02:31:21,060 --> 02:31:26,060
 OK, so you repeat the step for every sample

2172
02:31:26,060 --> 02:31:31,060
 until one of the following conditions is mapped

2173
02:31:31,060 --> 02:31:35,060
 for example, the error, the value of the error function,

2174
02:31:35,060 --> 02:31:42,060
 jw is very small, below a certain preset value.

2175
02:31:42,060 --> 02:31:48,060
 Or the delta w, delta b, becomes so small,

2176
02:31:48,060 --> 02:31:50,060
 you don't really adjust any further.

2177
02:31:50,060 --> 02:31:54,060
 But that one depends on a lot of big network training.

2178
02:31:54,060 --> 02:31:57,060
 Sometimes they actually run the whole training

2179
02:31:57,060 --> 02:31:59,060
 for two weeks, nothing happens.

2180
02:31:59,060 --> 02:32:02,060
 Then you stop too early, you will drop.

2181
02:32:02,060 --> 02:32:05,060
 You don't know what is the right learning rate.

2182
02:32:05,060 --> 02:32:08,060
 It moves slowly towards the direction.

2183
02:32:08,060 --> 02:32:12,060
 If you lose out of patience, you stop before it suddenly drops.

2184
02:32:12,060 --> 02:32:14,060
 Like some of the demo we showed,

2185
02:32:14,060 --> 02:32:16,060
 it's very, very slow, suddenly it drops up.

2186
02:32:16,060 --> 02:32:21,060
 So that is the so-called sometimes the problem

2187
02:32:21,060 --> 02:32:23,060
 that you have to deal with.

2188
02:32:23,060 --> 02:32:26,060
 Or until you run out of your time,

2189
02:32:26,060 --> 02:32:30,060
 a total number of updates has been reached.

2190
02:32:30,060 --> 02:32:36,060
 Many, many so-called months you'll be waiting, nothing changed.

2191
02:32:36,060 --> 02:32:39,060
 But the problem is you really didn't know which one is which.

2192
02:32:39,060 --> 02:32:41,060
 Sometimes you may terminate too early,

2193
02:32:41,060 --> 02:32:45,060
 sometimes you just wait for nothing will change.

2194
02:32:45,060 --> 02:32:50,060
 You just keep computing, wasting your power.

2195
02:32:50,060 --> 02:32:53,060
 Of course, the other way rather than one by one,

2196
02:32:53,060 --> 02:32:56,060
 as we mentioned earlier, every sample you adjust the weight,

2197
02:32:56,060 --> 02:33:01,060
 which could be very noisy because your sample,

2198
02:33:01,060 --> 02:33:04,060
 every sample could be a noise sample.

2199
02:33:04,060 --> 02:33:07,060
 It may push your weight to a wrong direction.

2200
02:33:07,060 --> 02:33:09,060
 And also very slow because for every sample,

2201
02:33:09,060 --> 02:33:11,060
 you have to do all the calculation

2202
02:33:11,060 --> 02:33:15,060
 for all the millions of parameters make adjustments.

2203
02:33:15,060 --> 02:33:20,060
 So one other way you can buy using the E-Port or A-Port,

2204
02:33:20,060 --> 02:33:23,060
 how you pronounce this, A-Port updating,

2205
02:33:23,060 --> 02:33:29,060
 which every time you do the so-called changes,

2206
02:33:29,060 --> 02:33:33,060
 you compute the error, you compute a delta, w, delta b,

2207
02:33:33,060 --> 02:33:35,060
 but you do not update the network.

2208
02:33:35,060 --> 02:33:40,060
 You use the same old network to input many, many samples,

2209
02:33:40,060 --> 02:33:42,060
 but you accumulate the delta,

2210
02:33:42,060 --> 02:33:45,060
 accumulate all the delta, delta, delta b,

2211
02:33:45,060 --> 02:33:49,060
 then you do the afters, you have seen all the data,

2212
02:33:49,060 --> 02:33:56,060
 you update once and for all, then you do it again.

2213
02:33:56,060 --> 02:34:00,060
 So rather than every sample you change the network,

2214
02:34:00,060 --> 02:34:05,060
 you use the same network to compute all the delta, w, delta b,

2215
02:34:05,060 --> 02:34:08,060
 then you accumulate all the changes, take the average,

2216
02:34:08,060 --> 02:34:10,060
 then you update the network at the end.

2217
02:34:10,060 --> 02:34:12,060
 This is called A-Port updating.

2218
02:34:12,060 --> 02:34:14,060
 So you can see that in some of the programs,

2219
02:34:14,060 --> 02:34:17,060
 I'll show you, you'll see that all one A-Port, two A-Port.

2220
02:34:17,060 --> 02:34:20,060
 Basically, every A-Port, they look at all the input data

2221
02:34:20,060 --> 02:34:23,060
 before the update, because you do not know the sequence

2222
02:34:23,060 --> 02:34:25,060
 of the data, it really makes a difference.

2223
02:34:25,060 --> 02:34:29,060
 If you actually update every time you've seen the data,

2224
02:34:29,060 --> 02:34:32,060
 imagine that if you randomize your input order,

2225
02:34:32,060 --> 02:34:35,060
 then the outcome may be different.

2226
02:34:35,060 --> 02:34:43,060
 Or you can do the kind of stochastic gradient descent.

2227
02:34:43,060 --> 02:34:46,060
 So the early two cases was one is every case you update,

2228
02:34:46,060 --> 02:34:47,060
 every sample you update.

2229
02:34:47,060 --> 02:34:50,060
 The other is the only update after you have seen all the samples.

2230
02:34:50,060 --> 02:34:53,060
 They are at the very true extreme case.

2231
02:34:53,060 --> 02:34:55,060
 So you can imagine that you can do that.

2232
02:34:55,060 --> 02:34:58,060
 You can select a small number of samples,

2233
02:34:58,060 --> 02:35:01,060
 input all the samples, accumulate all the changes,

2234
02:35:01,060 --> 02:35:02,060
 then you change it.

2235
02:35:02,060 --> 02:35:06,060
 Then you repeat the same process for another set of samples.

2236
02:35:06,060 --> 02:35:08,060
 That is so stochastic.

2237
02:35:08,060 --> 02:35:13,060
 Every time you just randomly select maybe 10% of your training samples,

2238
02:35:13,060 --> 02:35:18,060
 then you do this A-Port updating of the 10% of the training sample.

2239
02:35:18,060 --> 02:35:22,060
 You update, then you select another 10% of the sample.

2240
02:35:22,060 --> 02:35:26,060
 So this is more common, stochastic gradient descent.

2241
02:35:26,060 --> 02:35:29,060
 You select randomly, then you perform the update,

2242
02:35:29,060 --> 02:35:33,060
 then you repeat the same process until all the samples

2243
02:35:33,060 --> 02:35:37,060
 have been used for training.

2244
02:35:37,060 --> 02:35:39,060
 So now let's look at it.

2245
02:35:39,060 --> 02:35:41,060
 These are also optional slides.

2246
02:35:41,060 --> 02:35:42,060
 Now look at it.

2247
02:35:42,060 --> 02:35:45,060
 Why this set of formulas?

2248
02:35:45,060 --> 02:35:47,060
 I didn't just tell you how to compute,

2249
02:35:47,060 --> 02:35:52,060
 but I didn't tell you why delta k, delta z look like that.

2250
02:35:52,060 --> 02:35:54,060
 Why this is?

2251
02:35:54,060 --> 02:35:59,060
 And this is only true when your error is so-called

2252
02:35:59,060 --> 02:36:03,060
 sum of square error.

2253
02:36:03,060 --> 02:36:06,060
 If today you change your error to another type of error,

2254
02:36:06,060 --> 02:36:09,060
 then all this formula doesn't work.

2255
02:36:09,060 --> 02:36:13,060
 So you need to know exactly how to derive all these parameters.

2256
02:36:13,060 --> 02:36:17,060
 So this is what this optional slide shows you.

2257
02:36:17,060 --> 02:36:20,060
 Because how to compute?

2258
02:36:20,060 --> 02:36:23,060
 Although we still use this example,

2259
02:36:23,060 --> 02:36:26,060
 but at least this shows you how you can compute

2260
02:36:26,060 --> 02:36:28,060
 the formula that we show you.

2261
02:36:28,060 --> 02:36:30,060
 So early on in the Bayek prerogation,

2262
02:36:30,060 --> 02:36:33,060
 I told you that if you have this kind of sum of square error,

2263
02:36:33,060 --> 02:36:36,060
 this is the gradient descent.

2264
02:36:36,060 --> 02:36:40,060
 So you can move into negative gradient direction.

2265
02:36:40,060 --> 02:36:46,060
 And this is how you do the so-called forward and backward.

2266
02:36:46,060 --> 02:36:47,060
 This is the network.

2267
02:36:47,060 --> 02:36:52,060
 You have the net input for any, let's say, Bz.

2268
02:36:52,060 --> 02:36:56,060
 You actually have WzI multiplied with the OI.

2269
02:36:56,060 --> 02:36:57,060
 You sum them up.

2270
02:36:57,060 --> 02:37:00,060
 Then go through this activation function.

2271
02:37:00,060 --> 02:37:02,060
 You produce Oz.

2272
02:37:02,060 --> 02:37:06,060
 Then you continue to this until you get the Tk.

2273
02:37:06,060 --> 02:37:08,060
 And get the actual output.

2274
02:37:08,060 --> 02:37:10,060
 And this is your target output.

2275
02:37:10,060 --> 02:37:13,060
 So let's look at these two particular nodes.

2276
02:37:13,060 --> 02:37:16,060
 One of them is at the output layer.

2277
02:37:16,060 --> 02:37:19,060
 The other is at the Healen layer.

2278
02:37:19,060 --> 02:37:22,060
 So I showed you earlier that to compute the error

2279
02:37:22,060 --> 02:37:25,060
 at this output unit, you basically

2280
02:37:25,060 --> 02:37:30,060
 do this sigma prime net k Tk minus OK.

2281
02:37:30,060 --> 02:37:34,060
 So to compute the error of this is this formula.

2282
02:37:34,060 --> 02:37:37,060
 This is what I showed you earlier.

2283
02:37:37,060 --> 02:37:41,060
 But how do you get this formula?

2284
02:37:41,060 --> 02:37:45,060
 This is the adjustment that we make.

2285
02:37:45,060 --> 02:37:48,060
 So this has to summarize what we just

2286
02:37:48,060 --> 02:37:49,060
 kept what we covered.

2287
02:37:49,060 --> 02:37:52,060
 If your activation function is this,

2288
02:37:52,060 --> 02:37:56,060
 then this sigma net k become OK 1 minus OK.

2289
02:37:56,060 --> 02:37:59,060
 OK is the output of the unit.

2290
02:37:59,060 --> 02:38:02,060
 And then for the Healen unit, this is the same.

2291
02:38:02,060 --> 02:38:08,060
 Sigma derivative become Oj 1 minus Oj.

2292
02:38:08,060 --> 02:38:12,060
 Now let's show you how to get this formula.

2293
02:38:12,060 --> 02:38:15,060
 Sigma k equal to this.

2294
02:38:15,060 --> 02:38:20,060
 Sorry, delta k equal to this formula.

2295
02:38:20,060 --> 02:38:22,060
 So again, write down all what you know.

2296
02:38:22,060 --> 02:38:30,060
 And this is for delta k net k, delta, sorry, net k net j,

2297
02:38:30,060 --> 02:38:32,060
 OK Oj.

2298
02:38:32,060 --> 02:38:34,060
 And then this is what you know.

2299
02:38:34,060 --> 02:38:39,060
 Then to compute Wkj, you want to adjust this, right?

2300
02:38:39,060 --> 02:38:41,060
 It's basically, as we say earlier,

2301
02:38:41,060 --> 02:38:44,060
 you want to compute the derivative of error function

2302
02:38:44,060 --> 02:38:47,060
 with respect to Wjk.

2303
02:38:47,060 --> 02:38:50,060
 So this is what you need to compute.

2304
02:38:50,060 --> 02:38:56,060
 Because this E right now is written as Tk minus OK.

2305
02:38:56,060 --> 02:39:00,060
 So you cannot take the derivative with respect to Wkj,

2306
02:39:00,060 --> 02:39:01,060
 that I mean.

2307
02:39:01,060 --> 02:39:09,060
 So you have to express the E in terms of the parameter like OK.

2308
02:39:09,060 --> 02:39:13,060
 Then OK is a function of net k, which is here.

2309
02:39:13,060 --> 02:39:17,060
 OK is a function of net k, which is just the activation function.

2310
02:39:17,060 --> 02:39:25,060
 And then the net k is the function of this Wkj.

2311
02:39:25,060 --> 02:39:30,060
 And therefore, you can expand this derivative of E,

2312
02:39:30,060 --> 02:39:34,060
 which is Wkj, by using the change rule, right?

2313
02:39:34,060 --> 02:39:37,060
 Now you have the derivative E, first OK first.

2314
02:39:37,060 --> 02:39:39,060
 E is here.

2315
02:39:39,060 --> 02:39:42,060
 Then because E can be expressed as a function of OK,

2316
02:39:42,060 --> 02:39:44,060
 you take the derivative of OK.

2317
02:39:44,060 --> 02:39:47,060
 OK can be expressed as a function of net k.

2318
02:39:47,060 --> 02:39:50,060
 You take this OK, we'll add to net k.

2319
02:39:50,060 --> 02:39:53,060
 Then net k can be expressed as a function of Wkj.

2320
02:39:53,060 --> 02:39:56,060
 Then you can express this, right?

2321
02:39:56,060 --> 02:40:00,060
 So then each of these, you take the derivative.

2322
02:40:00,060 --> 02:40:04,060
 This E and OK is easy, because this is the derivative

2323
02:40:04,060 --> 02:40:06,060
 of this function with respect to OK.

2324
02:40:06,060 --> 02:40:11,060
 It becomes minus 2 divided by 2 Tk minus OK.

2325
02:40:11,060 --> 02:40:16,060
 So this term come from here, come from here, come from here.

2326
02:40:16,060 --> 02:40:17,060
 OK?

2327
02:40:17,060 --> 02:40:21,060
 Then this is nothing but the derivative of the activation function

2328
02:40:21,060 --> 02:40:24,060
 with respect to the input net k.

2329
02:40:24,060 --> 02:40:29,060
 And then this one, net k Wkj, there's only one term,

2330
02:40:29,060 --> 02:40:32,060
 particular term, depends on Wkj.

2331
02:40:32,060 --> 02:40:36,060
 And the derivative here, you only have the Oj, right?

2332
02:40:36,060 --> 02:40:40,060
 Therefore, you can write these changes as a minus delta koj,

2333
02:40:40,060 --> 02:40:43,060
 delta koj is actually here.

2334
02:40:43,060 --> 02:40:47,060
 Sigma net k Tk minus OK.

2335
02:40:47,060 --> 02:40:50,060
 So this is where you got the function.

2336
02:40:50,060 --> 02:40:51,060
 OK?

2337
02:40:51,060 --> 02:41:00,060
 Then the delta Wkj will be the eta W multiplied with delta koj.

2338
02:41:00,060 --> 02:41:04,060
 Then you have a minus term, because it's a gradient descent.

2339
02:41:04,060 --> 02:41:08,060
 So that's how you get these two terms, right?

2340
02:41:08,060 --> 02:41:13,060
 So this one, now, it becomes here, right?

2341
02:41:13,060 --> 02:41:18,060
 This one is Wk, you multiply here.

2342
02:41:18,060 --> 02:41:21,060
 Then you know the Oj, you know the eta W,

2343
02:41:21,060 --> 02:41:24,060
 then you can compute the changes.

2344
02:41:24,060 --> 02:41:25,060
 OK?

2345
02:41:25,060 --> 02:41:30,060
 So this is how you derive the gradient, the error term.

2346
02:41:30,060 --> 02:41:33,060
 And this one delta koj, we just call it an error term,

2347
02:41:33,060 --> 02:41:36,060
 it is a unique k.

2348
02:41:36,060 --> 02:41:39,060
 So this is true for all the output unit.

2349
02:41:39,060 --> 02:41:44,060
 But for the hidden unit, it's slightly more complex,

2350
02:41:44,060 --> 02:41:50,060
 because you still do this delta EWji, but this is hidden unit.

2351
02:41:50,060 --> 02:41:55,060
 Your delta E is here, which is far away from your Oj.

2352
02:41:55,060 --> 02:42:00,060
 So again, you have to apply the same rule, EOj,

2353
02:42:00,060 --> 02:42:07,060
 then Oj over net z, then net z over Wji.

2354
02:42:07,060 --> 02:42:14,060
 So this is something like before, but now you change the k to z.

2355
02:42:14,060 --> 02:42:18,060
 Then this is something that we are looking into here,

2356
02:42:18,060 --> 02:42:23,060
 because E and Oj, they do not connect together directly.

2357
02:42:23,060 --> 02:42:28,060
 But you know that E is a Tk minus Ok, so you can write this one.

2358
02:42:28,060 --> 02:42:32,060
 So you can further apply the change rule here,

2359
02:42:32,060 --> 02:42:40,060
 become sigma EOj, then delta Okoj.

2360
02:42:40,060 --> 02:42:43,060
 So this is a new term, you do a change rule,

2361
02:42:43,060 --> 02:42:45,060
 and this is what you know how to compute,

2362
02:42:45,060 --> 02:42:47,060
 you have done that before.

2363
02:42:47,060 --> 02:42:52,060
 Then this Okoj is here.

2364
02:42:52,060 --> 02:42:58,060
 So you'll find that it's only Wkz, which depends on Okoz.

2365
02:42:58,060 --> 02:43:00,060
 So you can write it here.

2366
02:43:00,060 --> 02:43:05,060
 Then this net k is the same, then you have this term.

2367
02:43:05,060 --> 02:43:09,060
 And this term now will become here.

2368
02:43:09,060 --> 02:43:11,060
 So at the end, you go through this,

2369
02:43:11,060 --> 02:43:14,060
 it's just application of a bunch of change rule,

2370
02:43:14,060 --> 02:43:19,060
 always from the E, but connect back to the parameter you want to change.

2371
02:43:19,060 --> 02:43:22,060
 So to express every of the intermediate term,

2372
02:43:22,060 --> 02:43:26,060
 we expect to the parameter that you want to optimize.

2373
02:43:26,060 --> 02:43:29,060
 So at the end, you get this for the hidden unit,

2374
02:43:29,060 --> 02:43:32,060
 this will be your error term,

2375
02:43:32,060 --> 02:43:38,060
 and this will be the adjustment of the W you want to make.

2376
02:43:38,060 --> 02:43:40,060
 And then for the bias, because bias is the same,

2377
02:43:40,060 --> 02:43:43,060
 you consider bias is a spatial unit of W,

2378
02:43:43,060 --> 02:43:45,060
 just the output is equal to 1,

2379
02:43:45,060 --> 02:43:52,060
 and then this will be the formula of the weight.

2380
02:43:52,060 --> 02:43:56,060
 Okay, then let's look at one specific example

2381
02:43:56,060 --> 02:44:02,060
 how you compute the back propagation.

2382
02:44:02,060 --> 02:44:06,060
 Again, it may look very straightforward to apply the formula.

2383
02:44:06,060 --> 02:44:09,060
 I strongly encourage you to do that.

2384
02:44:09,060 --> 02:44:12,060
 Before even you look at the answer,

2385
02:44:12,060 --> 02:44:17,060
 it's very tedious and often that you do not know which one to use.

2386
02:44:17,060 --> 02:44:19,060
 I'll give you an example now.

2387
02:44:19,060 --> 02:44:23,060
 I have a 3, 2, 1 network,

2388
02:44:23,060 --> 02:44:31,060
 which I have just one training sample, input 101.

2389
02:44:31,060 --> 02:44:36,060
 And then the output, class label 1,

2390
02:44:36,060 --> 02:44:40,060
 I want the target output equal to 1.

2391
02:44:40,060 --> 02:44:43,060
 The actual output may not be 1.

2392
02:44:43,060 --> 02:44:46,060
 So I initialize the network with all these W,

2393
02:44:46,060 --> 02:44:55,060
 W41, W51, W42, W52, 4353, 6465.

2394
02:44:55,060 --> 02:44:59,060
 These are the initial value, and this is the bias.

2395
02:44:59,060 --> 02:45:01,060
 I don't draw the bias here,

2396
02:45:01,060 --> 02:45:03,060
 but sometimes you will see that people do that.

2397
02:45:03,060 --> 02:45:08,060
 If you want to express a bias, you can have a bias unit B,

2398
02:45:08,060 --> 02:45:14,060
 and then connect this to 4, connect this to 5,

2399
02:45:14,060 --> 02:45:18,060
 connect this to 6.

2400
02:45:18,060 --> 02:45:21,060
 So this weight, the output will be 1,

2401
02:45:21,060 --> 02:45:24,060
 then this is bias 4 as a weight,

2402
02:45:24,060 --> 02:45:28,060
 bias 5, then bias 6.

2403
02:45:28,060 --> 02:45:33,060
 This is how you can expressly show that you have a bias unit.

2404
02:45:33,060 --> 02:45:36,060
 The output of the B is always 1,

2405
02:45:36,060 --> 02:45:41,060
 and the weight that you can consider it as a kind of spatial

2406
02:45:41,060 --> 02:45:46,060
 input with all the weights now become your bias.

2407
02:45:46,060 --> 02:45:54,060
 Assuming your eta, learning eta, W, and B are all equal to 0.9.

2408
02:45:54,060 --> 02:45:59,060
 The same, the same 0.9.

2409
02:45:59,060 --> 02:46:00,060
 Okay?

2410
02:46:00,060 --> 02:46:03,060
 So your job is to do one iteration,

2411
02:46:03,060 --> 02:46:07,060
 and you have to know how to adjust all the W and all the B

2412
02:46:07,060 --> 02:46:12,060
 so that the target output will be improved towards...

2413
02:46:12,060 --> 02:46:16,060
 So the actual output O will be closer to what you want,

2414
02:46:16,060 --> 02:46:17,060
 the target output.

2415
02:46:17,060 --> 02:46:18,060
 Okay?

2416
02:46:18,060 --> 02:46:19,060
 So this is how you compute.

2417
02:46:19,060 --> 02:46:23,060
 I just want to show you the step,

2418
02:46:23,060 --> 02:46:25,060
 but you should go back to compute,

2419
02:46:25,060 --> 02:46:28,060
 hand calculate yourself to convince yourself

2420
02:46:28,060 --> 02:46:30,060
 you can do the operation.

2421
02:46:31,060 --> 02:46:34,060
 First, you have to input 101.

2422
02:46:34,060 --> 02:46:38,060
 You propagate the unit, right?

2423
02:46:38,060 --> 02:46:40,060
 Put into all the unit here.

2424
02:46:40,060 --> 02:46:45,060
 This input 101, the first three units basically just put the 1 here,

2425
02:46:45,060 --> 02:46:46,060
 and 0 and 1.

2426
02:46:46,060 --> 02:46:47,060
 Okay?

2427
02:46:47,060 --> 02:46:48,060
 It doesn't do any...

2428
02:46:48,060 --> 02:46:50,060
 The input unit doesn't do anything.

2429
02:46:50,060 --> 02:46:54,060
 Just put forward the input x1, x2, x3 forward.

2430
02:46:55,060 --> 02:47:01,060
 Then the unit 4 here will be 1 times W41

2431
02:47:01,060 --> 02:47:08,060
 plus 0 times W42 plus 0 times W43 will be this one, right?

2432
02:47:08,060 --> 02:47:14,060
 You'll get the net 4 equal to minus 0.7.

2433
02:47:14,060 --> 02:47:17,060
 Then the output O4 will be...

2434
02:47:17,060 --> 02:47:22,060
 You put the net 4 minus 0.7 to this sigmoid function.

2435
02:47:22,060 --> 02:47:27,060
 You have to put the net 1, the other way, 1 plus e minus x.

2436
02:47:27,060 --> 02:47:30,060
 Your minus x is now minus 0.7.

2437
02:47:30,060 --> 02:47:31,060
 So you have this.

2438
02:47:31,060 --> 02:47:36,060
 So this is actual output 0.332, which is O4, right?

2439
02:47:36,060 --> 02:47:37,060
 For this unit.

2440
02:47:37,060 --> 02:47:41,060
 Then you compute for O5 equal to 0.525,

2441
02:47:41,060 --> 02:47:45,060
 compute for O6 equal to 0.474.

2442
02:47:45,060 --> 02:47:46,060
 This is output.

2443
02:47:46,060 --> 02:47:48,060
 Remember your target is 1.

2444
02:47:48,060 --> 02:47:56,060
 The base train sample with the current weights and bias is 0.474.

2445
02:47:56,060 --> 02:47:57,060
 So it is not perfect.

2446
02:47:57,060 --> 02:48:00,060
 So you have to now adjust your weight and bias

2447
02:48:00,060 --> 02:48:05,060
 so that the next iteration, the output O6 will be higher

2448
02:48:05,060 --> 02:48:09,060
 than 0.474 closer to 1.

2449
02:48:09,060 --> 02:48:11,060
 Okay?

2450
02:48:11,060 --> 02:48:15,060
 After you compute this, now you have to compute all the error

2451
02:48:15,060 --> 02:48:16,060
 at each unit.

2452
02:48:16,060 --> 02:48:17,060
 Now you are doing back...

2453
02:48:17,060 --> 02:48:22,060
 This one just do the forward network, forward propagation.

2454
02:48:22,060 --> 02:48:24,060
 Now you do the back work error.

2455
02:48:24,060 --> 02:48:27,060
 First compute error, the delta.

2456
02:48:27,060 --> 02:48:30,060
 The first one is to compute is this, okay?

2457
02:48:30,060 --> 02:48:32,060
 Delta 6.

2458
02:48:32,060 --> 02:48:36,060
 Delta 6, which is output unit is this formula.

2459
02:48:36,060 --> 02:48:40,060
 Tk is 1, target output.

2460
02:48:40,060 --> 02:48:42,060
 Ok is 0.474.

2461
02:48:42,060 --> 02:48:46,060
 So you have 1 minus 0.474.

2462
02:48:46,060 --> 02:48:50,060
 Then this one, sigma, because it's a sigma function,

2463
02:48:50,060 --> 02:48:53,060
 you have 1, T okay, 1 minus okay.

2464
02:48:53,060 --> 02:48:56,060
 Your okay is 0.474.

2465
02:48:56,060 --> 02:48:59,060
 So it's okay times 1 minus okay.

2466
02:48:59,060 --> 02:49:07,060
 You get the sigma, sorry, delta 6 is 0.1311 for this.

2467
02:49:07,060 --> 02:49:09,060
 Okay?

2468
02:49:09,060 --> 02:49:14,060
 Then your delta phi, delta phi is the, again,

2469
02:49:14,060 --> 02:49:18,060
 the derivative of this active function with the input net z.

2470
02:49:18,060 --> 02:49:22,060
 Net z now is 0.525.

2471
02:49:22,060 --> 02:49:23,060
 Okay?

2472
02:49:23,060 --> 02:49:27,060
 You have this 0.525, 1 minus 0.525,

2473
02:49:27,060 --> 02:49:29,060
 plus the summation of these two.

2474
02:49:29,060 --> 02:49:33,060
 Because you only have one output, you have this sigma 6,

2475
02:49:33,060 --> 02:49:37,060
 it goes to 0.1311 here.

2476
02:49:37,060 --> 02:49:42,060
 Then multiply with w for phi, right?

2477
02:49:42,060 --> 02:49:47,060
 Then you have w56, w56 is minus 0.2, so you get this.

2478
02:49:47,060 --> 02:49:49,060
 So you have to compute the delta 6 first,

2479
02:49:49,060 --> 02:49:52,060
 because you need it to compute delta 5.

2480
02:49:52,060 --> 02:49:54,060
 Backward.

2481
02:49:54,060 --> 02:49:57,060
 Then compute delta 4, you get this three term.

2482
02:49:57,060 --> 02:50:02,060
 This is the error at unit 456.

2483
02:50:02,060 --> 02:50:08,060
 Then you compute the so-called changes,

2484
02:50:08,060 --> 02:50:11,060
 compute the wkj based on this formula.

2485
02:50:11,060 --> 02:50:14,060
 You have all the delta, you have all the output.

2486
02:50:14,060 --> 02:50:18,060
 You need oj, oi, delta k, delta j.

2487
02:50:18,060 --> 02:50:21,060
 You know the learning rate is 0.9.

2488
02:50:21,060 --> 02:50:25,060
 Then you compute all this, including the bias.

2489
02:50:25,060 --> 02:50:29,060
 So this will be the new set of weights and bias

2490
02:50:29,060 --> 02:50:33,060
 after just one sample input.

2491
02:50:33,060 --> 02:50:35,060
 Okay?

2492
02:50:36,060 --> 02:50:43,060
 11 parameters that you have to adjust based on one sample.

2493
02:50:43,060 --> 02:50:45,060
 Okay?

2494
02:50:45,060 --> 02:50:50,060
 If you plug 101 input with the new set of weights and bias,

2495
02:50:50,060 --> 02:50:54,060
 you should see that the output now is closer to 1.

2496
02:50:54,060 --> 02:50:58,060
 Rather than originally, you have 0.474.

2497
02:50:58,060 --> 02:51:01,060
 It should be higher than 0.474.

2498
02:51:01,060 --> 02:51:04,060
 You can repeat this process again and again

2499
02:51:04,060 --> 02:51:07,060
 until the 0.474 is very close to 1.

2500
02:51:07,060 --> 02:51:09,060
 This is only for one sample unit.

2501
02:51:09,060 --> 02:51:12,060
 You can always do the iteration.

2502
02:51:12,060 --> 02:51:14,060
 Okay?

2503
02:51:14,060 --> 02:51:16,060
 Go back and try yourself to convey yourself

2504
02:51:16,060 --> 02:51:20,060
 that it is not really something mysterious.

2505
02:51:20,060 --> 02:51:24,060
 It's just very tedious for you to do that.

2506
02:51:24,060 --> 02:51:27,060
 Yeah, I think we'll stop here.

2507
02:51:27,060 --> 02:51:29,060
 In this lecture, when we come back,

2508
02:51:29,060 --> 02:51:32,060
 we will just tell you a couple of the network,

2509
02:51:32,060 --> 02:51:37,060
 like convolutional network and auto encoder.

2510
02:51:41,060 --> 02:51:44,060
 Going too much detail, I'll just tell you that they are similar like this.

2511
02:51:44,060 --> 02:51:46,060
 Just that they have a different configuration,

2512
02:51:46,060 --> 02:51:51,060
 different so-called parameters for you to train and optimize.

2513
02:51:51,060 --> 02:51:54,060
 I will see you next week.

2514
02:51:57,060 --> 02:51:59,060
 Thank you.

2515
02:52:27,060 --> 02:52:30,060
 Thank you.

2516
02:52:57,060 --> 02:53:00,060
 Thank you.

2517
02:53:27,060 --> 02:53:30,060
 Thank you.

2518
02:53:57,060 --> 02:54:00,060
 Thank you.

2519
02:54:27,060 --> 02:54:30,060
 Thank you.

2520
02:54:57,060 --> 02:55:00,060
 Thank you.

2521
02:55:27,060 --> 02:55:30,060
 Thank you.

2522
02:55:57,060 --> 02:56:00,060
 Thank you.

2523
02:56:27,060 --> 02:56:30,060
 Thank you.

2524
02:56:57,060 --> 02:57:00,060
 Thank you.

2525
02:57:27,060 --> 02:57:30,060
 Thank you.

2526
02:57:57,060 --> 02:58:00,060
 Thank you.

2527
02:58:27,060 --> 02:58:30,060
 Thank you.

2528
02:58:57,060 --> 02:59:00,060
 Thank you.

2529
02:59:27,060 --> 02:59:30,060
 Thank you.

2530
02:59:57,060 --> 03:00:00,060
 Thank you.

