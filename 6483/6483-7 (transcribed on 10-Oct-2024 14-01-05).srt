1
00:00:00,000 --> 00:00:30,000
 ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

2
00:01:30,000 --> 00:02:00,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
00:02:00,000 --> 00:02:30,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4
00:02:30,000 --> 00:03:00,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5
00:03:00,000 --> 00:03:30,000
 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Isabelle. So what happened to other students? What did they not do well in their quiz last Saturday? How do you do it with a quiz? Okay? Good. So we started the introduction to machine learning last week. And of course today we are going to look at some of the

6
00:03:30,000 --> 00:03:41,000
 basic classifiers. And I understand that some of you likely have seen . . .

7
00:03:41,000 --> 00:03:49,000
 Okay. I don't know why it doesn't come out. It should be on . . .

8
00:03:49,000 --> 00:03:52,000
 We try to see the projector.

9
00:04:19,000 --> 00:04:21,000
 Okay. Good.

10
00:04:32,000 --> 00:04:43,000
 So today we are going to look at the classification, one of the important applications of machine learning.

11
00:04:43,000 --> 00:04:58,000
 And start with a very specific classifier, a very old classifier. I'm not sure how can I . . .

12
00:04:58,000 --> 00:05:01,000
 I can switch off this light.

13
00:05:05,000 --> 00:05:10,000
 Is it here? No. This is a screen.

14
00:05:11,000 --> 00:05:22,000
 You know, for the video recording, I hope that the quality will be better. Now the contrast is likely not the best.

15
00:05:27,000 --> 00:05:30,000
 Anyone know how to switch off the light here?

16
00:05:32,000 --> 00:05:35,000
 Yeah. Very interesting.

17
00:05:36,000 --> 00:05:44,000
 Okay. Anyway. So I hope the recording you can see clearly. If there's any problem, let me know.

18
00:05:44,000 --> 00:05:52,000
 I will try to find out how to turn off the light in front in the next lecture.

19
00:05:54,000 --> 00:06:04,000
 So we are going to talk about this. And again, some of this topic might not be new to you, but it's good to also go through it together.

20
00:06:05,000 --> 00:06:09,000
 And to get the concept right.

21
00:06:09,000 --> 00:06:16,000
 Okay. No, no. Here. Here. Not the other. Okay. Not the other.

22
00:06:16,000 --> 00:06:23,000
 Yeah. I want to get this feel light because somehow the projection . . .

23
00:06:23,000 --> 00:06:25,000
 Okay. See. I think.

24
00:06:27,000 --> 00:06:30,000
 This one? Yeah. Good.

25
00:06:31,000 --> 00:06:35,000
 Yeah. I can list this better. Oh God, please do not fall into sleep.

26
00:06:39,000 --> 00:06:41,000
 Make sure it's right there.

27
00:06:42,000 --> 00:06:45,000
 So we look at classification. What is classification?

28
00:06:45,000 --> 00:06:58,000
 Although classification is the very simple process that you try to classify from the input data, how many, what type this data belongs to.

29
00:06:58,000 --> 00:07:09,000
 But in a lot of machine learning tasks, you can find that it will become very powerful too, depending on how you formulate your problem into a classification problem.

30
00:07:09,000 --> 00:07:15,000
 Which could include, like even for example, your . . .

31
00:07:15,000 --> 00:07:21,000
 Chatchity. Again, because you are familiar with that, so I will use it again.

32
00:07:21,000 --> 00:07:32,000
 So Chatchity, the way it works is like this, right? Based on your input, you try to project, predict what will be the next word or next token, right?

33
00:07:32,000 --> 00:07:39,000
 Out of the many possible tokens available. So it is a classification problem, right?

34
00:07:39,000 --> 00:07:47,000
 And you just have to continue to predict what will be the next word after this and this, until you hit the end of the sentence.

35
00:07:47,000 --> 00:07:53,000
 So it's a classification problem. So of course, it doesn't have to be a transformer.

36
00:07:53,000 --> 00:07:57,000
 It could be many, many other simple tools that you can use.

37
00:07:57,000 --> 00:08:04,000
 And Decision 3 turned out to be one of them, which have been around for many years.

38
00:08:04,000 --> 00:08:14,000
 And Conceptual-wise is actually similar to how we approach a problem, classification problem, by asking a sequence of questions.

39
00:08:14,000 --> 00:08:23,000
 So based on the answer of those questions, you decide ultimately this input data belongs to which class.

40
00:08:23,000 --> 00:08:33,000
 So we will go through that. And more importantly, we will look into some of the selection criteria because you can have many types of trees, right?

41
00:08:33,000 --> 00:08:43,000
 And in fact, when you build the trees, you can select different so-called selection measures to build the tree differently, right?

42
00:08:43,000 --> 00:08:49,000
 And there will be random forest. Random forest is just many, many trees.

43
00:08:49,000 --> 00:08:52,000
 It's nothing fancy. Just give them a name of.

44
00:08:52,000 --> 00:08:57,000
 You have many trees together. Then for each tree, you find out what is the output class.

45
00:08:57,000 --> 00:09:02,000
 And then you try to do a majority vote to make decision.

46
00:09:02,000 --> 00:09:08,000
 Then you also need to look into the performance measure, right?

47
00:09:08,000 --> 00:09:14,000
 You have many different classifiers, but which will give you the best result possible.

48
00:09:14,000 --> 00:09:19,000
 It will not be as simple as just accuracy, right?

49
00:09:19,000 --> 00:09:23,000
 And there will be many other factors like the speed, right?

50
00:09:23,000 --> 00:09:28,000
 The scalability, robustness to noise, right?

51
00:09:28,000 --> 00:09:37,000
 And whether it can be generalized to other similar data, so all these other things that you have to take into consideration.

52
00:09:37,000 --> 00:09:44,000
 So there are such measures like accuracy, error rates, sensitivity, specificity, right?

53
00:09:44,000 --> 00:09:49,000
 We call F measure and how do you evaluate those measures?

54
00:09:49,000 --> 00:09:57,000
 So we will go through that and hopefully, I think most likely we will finish the lecture on decision tree today.

55
00:09:57,000 --> 00:10:07,000
 If we still have time, we may start the next nearest neighborhood classifier, which is another type next.

56
00:10:07,000 --> 00:10:11,000
 And then perhaps even support vector machine after that, right?

57
00:10:11,000 --> 00:10:15,000
 And because I want to leave some time on the neural network.

58
00:10:15,000 --> 00:10:25,000
 Null network, of course, hopefully we will get the concept right so that you can learn some of the new so-called idea yourself, right?

59
00:10:26,000 --> 00:10:29,000
 So hopefully we would have more time on the neural network.

60
00:10:29,000 --> 00:10:33,000
 Okay, so what is a classification or classifier?

61
00:10:33,000 --> 00:10:37,000
 So again, just a word definition.

62
00:10:37,000 --> 00:10:44,000
 Classification basically you try to assign given an object, right?

63
00:10:44,000 --> 00:10:46,000
 Which you may not know what is it?

64
00:10:46,000 --> 00:10:49,000
 The object could be an email that you receive, right?

65
00:10:49,000 --> 00:10:57,000
 So your email mailbox will actually try to judge whether this email is spam, right?

66
00:10:57,000 --> 00:11:00,000
 Or non-spam emails or junk emails, right?

67
00:11:00,000 --> 00:11:10,000
 You try to move the email to a so-called junk folder automatically if you find that this email is suspicious, not really a genuine email.

68
00:11:10,000 --> 00:11:12,000
 So how do they make the judgment?

69
00:11:12,000 --> 00:11:14,000
 It's a classification process, right?

70
00:11:14,000 --> 00:11:24,000
 So you can have objects and often that you do not use everything of the object, you could try to look into the attributes of the objects.

71
00:11:24,000 --> 00:11:29,000
 Just now for the email example, you can look at some of the keywords.

72
00:11:29,000 --> 00:11:34,000
 Some of the so-called subject title or even the sender.

73
00:11:34,000 --> 00:11:38,000
 Are they from a particular so-called region of the group?

74
00:11:38,000 --> 00:11:41,000
 Or do they send to many, many recipients?

75
00:11:41,000 --> 00:11:45,000
 Are they the, is the sender in your so-called contact list?

76
00:11:45,000 --> 00:11:51,000
 All these are useful information features or sometimes we call it attributes, right?

77
00:11:51,000 --> 00:11:54,000
 An object would have a lot of so-called properties.

78
00:11:54,000 --> 00:11:56,000
 We call them attributes.

79
00:11:56,000 --> 00:12:07,000
 So with this, then your classifier basically take this feature or attribute, go to a function and which decide what is the class label, right?

80
00:12:07,000 --> 00:12:11,000
 Class label could be just a number, one, two, three, four.

81
00:12:11,000 --> 00:12:18,000
 It could be the transformer, charge APT output, the token, the word that we mentioned, right?

82
00:12:18,000 --> 00:12:26,000
 It could be an E. So this function F, it may not be as simple as those functions you have come across, right?

83
00:12:26,000 --> 00:12:33,000
 You look at the charge APT, it could be a billion or trillion of parameters,

84
00:12:33,000 --> 00:12:38,000
 and which is beyond human imagination, what kind of function is it?

85
00:12:38,000 --> 00:12:45,000
 With the input, which could be very complex, then output is one of the many, many types of classes.

86
00:12:45,000 --> 00:12:52,000
 And machine learning is basically find out as we introduced last week, what is this function?

87
00:12:52,000 --> 00:12:53,000
 Right?

88
00:12:53,000 --> 00:12:55,000
 Go through kind of training approach.

89
00:12:55,000 --> 00:13:02,000
 If you can collect a lot of data, then hopefully you can find a function which works well with all the data

90
00:13:02,000 --> 00:13:12,000
 that you use for training or even extend to those data that you have not seen during your training process.

91
00:13:12,000 --> 00:13:13,000
 Okay?

92
00:13:13,000 --> 00:13:14,000
 So the spam email is one of them.

93
00:13:14,000 --> 00:13:19,000
 And you can find another function which can do the speech recognition, right?

94
00:13:19,000 --> 00:13:20,000
 Think carefully.

95
00:13:20,000 --> 00:13:28,000
 Speech recognition is also try to recognize every words that you say, what is it, right?

96
00:13:28,000 --> 00:13:35,000
 So it's also a classification problem if you formulate the problem properly, right?

97
00:13:35,000 --> 00:13:45,000
 And many other things, image classification, whether the credit card expenses are fraudulence or legitimate,

98
00:13:45,000 --> 00:13:48,000
 or even this animal, whether it's a duck, right?

99
00:13:48,000 --> 00:13:54,000
 So of course, you can think of all these are so-called multi-moder feature, right?

100
00:13:54,000 --> 00:13:59,000
 And then of course, you may not know how to describe a duck,

101
00:13:59,000 --> 00:14:06,000
 but if you see something which works like a duck, quick like a duck, then it's slightly a duck.

102
00:14:06,000 --> 00:14:09,000
 There is a basic principle about classification.

103
00:14:09,000 --> 00:14:12,000
 So that also means that you could make mistakes, right?

104
00:14:12,000 --> 00:14:19,000
 If I somehow make a machine duck, right, it can work like a duck, quick like a duck, you say, oh, that is a duck,

105
00:14:19,000 --> 00:14:21,000
 but it could be a toy, right?

106
00:14:21,000 --> 00:14:22,000
 So it could happen.

107
00:14:22,000 --> 00:14:26,000
 So there is an error that you have to leave it.

108
00:14:26,000 --> 00:14:28,000
 Two main steps of classification.

109
00:14:28,000 --> 00:14:33,000
 So if you desire a classifier, you have to go through these two steps in general.

110
00:14:33,000 --> 00:14:37,000
 The first step is a so-called the learning step or training step,

111
00:14:37,000 --> 00:14:48,000
 which you try to construct a classification model based on the training data set given to you.

112
00:14:48,000 --> 00:14:56,000
 The training data set could be, right, like we thought about this image net data set, millions of images.

113
00:14:56,000 --> 00:14:59,000
 Each image, they give you a label.

114
00:14:59,000 --> 00:15:02,000
 What is the object inside this image?

115
00:15:02,000 --> 00:15:05,000
 So you need to have the data set.

116
00:15:05,000 --> 00:15:09,000
 You need to know what is the class label of the data set, right?

117
00:15:09,000 --> 00:15:13,000
 And that is a job that you have to collect, okay, those data.

118
00:15:13,000 --> 00:15:17,000
 So once you have that data set with the label, the ground through,

119
00:15:17,000 --> 00:15:21,000
 then you can select one of the classification model.

120
00:15:21,000 --> 00:15:27,000
 So today we will focus on decision three, but there will be many, many other models that you can choose from

121
00:15:27,000 --> 00:15:33,000
 based on your preferences, based on your experience, based on your computing facilities,

122
00:15:33,000 --> 00:15:41,000
 based on many, many other consideration factors that you need to take into consideration.

123
00:15:41,000 --> 00:15:44,000
 So after that, you have the classifier.

124
00:15:44,000 --> 00:15:49,000
 Then there is a so-called classification step, which is the step that you apply.

125
00:15:49,000 --> 00:15:52,000
 Sometimes it's called inference, right?

126
00:15:52,000 --> 00:15:54,000
 You use the classifier design.

127
00:15:54,000 --> 00:15:59,000
 Now you have a new input, the input, then you go through this classifier,

128
00:15:59,000 --> 00:16:02,000
 then you say, okay, this belongs to what class?

129
00:16:02,000 --> 00:16:03,000
 Okay.

130
00:16:03,000 --> 00:16:09,000
 So these two steps, okay, the first step is certainly generally very time consuming

131
00:16:09,000 --> 00:16:12,000
 because you have to go through a lot of iteration,

132
00:16:12,000 --> 00:16:17,000
 and it has to first start with a model, which may not work very well.

133
00:16:17,000 --> 00:16:20,000
 Then you'll find a way to tell the model, right?

134
00:16:20,000 --> 00:16:28,000
 And like the example I gave at Lawson Brad, when he designed this so-called classifier,

135
00:16:28,000 --> 00:16:31,000
 he used a hand to tell the note.

136
00:16:31,000 --> 00:16:36,000
 It is very much like for those, if you have the experience, try to fix some machine, right?

137
00:16:36,000 --> 00:16:41,000
 You just tilt until you find that it could be a Wi-Fi receiver,

138
00:16:41,000 --> 00:16:43,000
 it could be an old television.

139
00:16:43,000 --> 00:16:47,000
 You just select the so-called channel properly.

140
00:16:47,000 --> 00:16:50,000
 It's a kind of a training process.

141
00:16:50,000 --> 00:16:54,000
 So once you've done that, next time you just apply.

142
00:16:54,000 --> 00:16:56,000
 Apply, okay?

143
00:16:56,000 --> 00:17:02,000
 So therefore, a classification model is generally a function or mapping.

144
00:17:02,000 --> 00:17:12,000
 So is it a one-to-one mapping or multiple-to-one mapping or one-to-multiple mapping in general?

145
00:17:12,000 --> 00:17:15,000
 A classification process is what type of mapping?

146
00:17:15,000 --> 00:17:19,000
 I believe you've learned different kind of mapping before, right?

147
00:17:19,000 --> 00:17:23,000
 So what kind of mapping is it in general?

148
00:17:23,000 --> 00:17:31,000
 One-to-one, multiple-to-one.

149
00:17:31,000 --> 00:17:35,000
 Okay? It is generally a multiple-to-one mapping.

150
00:17:35,000 --> 00:17:40,000
 You could have different input data, but output belongs to a particular class.

151
00:17:40,000 --> 00:17:46,000
 You cannot have one input data, but it belongs to a number of different classes,

152
00:17:46,000 --> 00:17:50,000
 which in some applications you may want to do that,

153
00:17:50,000 --> 00:17:56,000
 but in general we want to have multiple-to-one mapping.

154
00:17:56,000 --> 00:18:01,000
 And more importantly, the model, this classifier that you learn,

155
00:18:01,000 --> 00:18:07,000
 it should fit well with the training data set, which you use to train the model.

156
00:18:07,000 --> 00:18:11,000
 So we call this a training error, right?

157
00:18:11,000 --> 00:18:14,000
 And which, of course, if you have a proper model,

158
00:18:14,000 --> 00:18:19,000
 you likely will get the error, continue to decrease.

159
00:18:19,000 --> 00:18:28,000
 If you have more and more data, or you have a model which can have many, many parameters.

160
00:18:28,000 --> 00:18:35,000
 And more importantly, that you also want the model to predict well to class labor,

161
00:18:35,000 --> 00:18:39,000
 which you have not seen the data, right?

162
00:18:39,000 --> 00:18:44,000
 If it can do well on the new data, predict their classes correctly,

163
00:18:44,000 --> 00:18:50,000
 then you say that this model has good generalization capabilities.

164
00:18:50,000 --> 00:18:53,000
 It can be generalized to unseen data.

165
00:18:53,000 --> 00:18:56,000
 Otherwise, the model will be only useful to the training data,

166
00:18:56,000 --> 00:19:00,000
 which is not really something interesting, right?

167
00:19:00,000 --> 00:19:02,000
 So then it comes with this problem like,

168
00:19:02,000 --> 00:19:05,000
 you probably heard about this, over-fifting, right?

169
00:19:05,000 --> 00:19:10,000
 If you do not have a powerful model,

170
00:19:10,000 --> 00:19:14,000
 you might overfit the data to the model,

171
00:19:14,000 --> 00:19:19,000
 turn out it may not behave well when you actually apply to unseen data.

172
00:19:19,000 --> 00:19:24,000
 We'll go through some of these processes, and how do you solve this kind of problem?

173
00:19:24,000 --> 00:19:25,000
 Okay?

174
00:19:25,000 --> 00:19:28,000
 So let's give you an example.

175
00:19:28,000 --> 00:19:31,000
 This is the conceptual flaw that we will go through.

176
00:19:31,000 --> 00:19:36,000
 So now, assuming that you are given this set of data,

177
00:19:36,000 --> 00:19:39,000
 okay, let's say this is your first job.

178
00:19:39,000 --> 00:19:43,000
 You start to work in a car dealer company.

179
00:19:43,000 --> 00:19:48,000
 Your job is to decide when a customer walks into your showroom,

180
00:19:48,000 --> 00:19:54,000
 you have to judge whether this customer is likely to buy a car from you, right?

181
00:19:54,000 --> 00:19:58,000
 Of course, you are not going to serve anyone walking into the room,

182
00:19:58,000 --> 00:20:01,000
 so you may compete with your colleagues,

183
00:20:01,000 --> 00:20:04,000
 try to get as high at the commission as possible.

184
00:20:04,000 --> 00:20:09,000
 So every time someone walks in, you can look at the age profile, right?

185
00:20:09,000 --> 00:20:11,000
 Look at the potential income.

186
00:20:11,000 --> 00:20:14,000
 You may not have the information, but if you ask a name,

187
00:20:14,000 --> 00:20:19,000
 it happens to be a member, then you might have such information about the income level.

188
00:20:19,000 --> 00:20:24,000
 Or look at their outfit, look at the car they come with,

189
00:20:24,000 --> 00:20:26,000
 look at their status, then you can make judgment,

190
00:20:26,000 --> 00:20:29,000
 okay, this person probably is quite okay.

191
00:20:29,000 --> 00:20:32,000
 Then, whether it's working or not, right?

192
00:20:32,000 --> 00:20:36,000
 If they only come after office hours, maybe like it's working,

193
00:20:36,000 --> 00:20:39,000
 it's coming during daytime, okay,

194
00:20:39,000 --> 00:20:44,000
 but not necessarily only those working will buy a car, right?

195
00:20:44,000 --> 00:20:46,000
 And credit rating, right?

196
00:20:46,000 --> 00:20:51,000
 You can check the record if they give you a credit card or you have the record.

197
00:20:51,000 --> 00:20:55,000
 And based on this, I've got to give you many, many other factors, right?

198
00:20:55,000 --> 00:20:57,000
 And you do not, these are attributes.

199
00:20:57,000 --> 00:21:04,000
 Before you train, you really do not know which attribute really will be the most useful factor

200
00:21:04,000 --> 00:21:06,000
 for you to make that decision, right?

201
00:21:06,000 --> 00:21:10,000
 For example, whether the person come alone, right?

202
00:21:10,000 --> 00:21:15,000
 Will he come with a family, and whether, what time he come in, right?

203
00:21:15,000 --> 00:21:20,000
 All this could be useful information, but before training, you would not know, right?

204
00:21:20,000 --> 00:21:26,000
 So the idea is actually before training, you don't want to make some kind of subjective judgment.

205
00:21:26,000 --> 00:21:33,000
 You want to collect as much data as many different types of data as possible

206
00:21:33,000 --> 00:21:40,000
 so that the training can tell you what will be the feature which are really useful to make the decision.

207
00:21:40,000 --> 00:21:46,000
 So with that, then these are the record based on your past history

208
00:21:46,000 --> 00:21:49,000
 after working for two or three months.

209
00:21:49,000 --> 00:21:52,000
 So you have deal with a couple of these customers.

210
00:21:52,000 --> 00:21:54,000
 That is your training data, right?

211
00:21:54,000 --> 00:21:59,000
 You have collect this, and you know, actually, this is your target classification output.

212
00:21:59,000 --> 00:22:00,000
 Yes or no?

213
00:22:00,000 --> 00:22:01,000
 Right?

214
00:22:01,000 --> 00:22:02,000
 This is the most important.

215
00:22:02,000 --> 00:22:05,000
 You don't care whether, how old they are, whether they are working or not.

216
00:22:05,000 --> 00:22:10,000
 Ultimately, you care more about whether they buy a car from you.

217
00:22:10,000 --> 00:22:13,000
 So, okay, after three months, you collect this data.

218
00:22:13,000 --> 00:22:17,000
 Okay, you say, okay, since I have taken this course, I'm going to build a model

219
00:22:17,000 --> 00:22:20,000
 to help you to make a better judgment.

220
00:22:20,000 --> 00:22:25,000
 So then you will probably record this, and this is the data you collect.

221
00:22:25,000 --> 00:22:33,000
 You try to fit in as much information as possible that you might think useful.

222
00:22:33,000 --> 00:22:38,000
 And then you pre-process or clean the data.

223
00:22:38,000 --> 00:22:43,000
 So this is a very important process because not all the data are so perfect.

224
00:22:43,000 --> 00:22:45,000
 You could have some missing data.

225
00:22:45,000 --> 00:22:47,000
 You may not know the age of this person.

226
00:22:47,000 --> 00:22:48,000
 Right?

227
00:22:48,000 --> 00:22:51,000
 Then how do you deal with missing data?

228
00:22:51,000 --> 00:22:52,000
 Right?

229
00:22:52,000 --> 00:22:54,000
 And there are a couple of ways to do it.

230
00:22:54,000 --> 00:23:00,000
 For those interested, you can go in Google and either by estimation or based on some

231
00:23:00,000 --> 00:23:07,000
 kind of average of the data set you have, you can, this is called cleaning or pre-processing

232
00:23:07,000 --> 00:23:14,000
 data or could be the, whether it buy a car or not, it's not so clear card.

233
00:23:14,000 --> 00:23:17,000
 The person make an order but haven't really made a payment.

234
00:23:17,000 --> 00:23:19,000
 So it's a yes or no.

235
00:23:19,000 --> 00:23:23,000
 So that becomes something that you have to think carefully.

236
00:23:23,000 --> 00:23:27,000
 How do you clean the data so that it becomes useful?

237
00:23:27,000 --> 00:23:32,000
 So after that, I'll call you to select what will be the attributes or features.

238
00:23:32,000 --> 00:23:37,000
 These are so-called the feature or attribute, the age, the income, the working on status,

239
00:23:37,000 --> 00:23:39,000
 credit rating.

240
00:23:39,000 --> 00:23:43,000
 Then you select, okay, as many as this feature as possible.

241
00:23:43,000 --> 00:23:49,000
 Then you try to divide this data set into two sets.

242
00:23:49,000 --> 00:23:51,000
 One is a training data set.

243
00:23:51,000 --> 00:23:54,000
 The other is a test data set.

244
00:23:54,000 --> 00:23:55,000
 Okay?

245
00:23:55,000 --> 00:24:02,000
 Because you don't want the so-called the model is overfit to your, all your data.

246
00:24:02,000 --> 00:24:11,000
 So you keep a small number of data, so-called samples away when you do the training.

247
00:24:11,000 --> 00:24:12,000
 Okay?

248
00:24:12,000 --> 00:24:17,000
 So you go through some of this training example in the next few lectures.

249
00:24:17,000 --> 00:24:21,000
 So after you go through the training, then you evaluate the performance.

250
00:24:21,000 --> 00:24:22,000
 How good is this?

251
00:24:22,000 --> 00:24:23,000
 And this performance have two types.

252
00:24:23,000 --> 00:24:26,000
 One type is based on your training data.

253
00:24:26,000 --> 00:24:27,000
 Right?

254
00:24:27,000 --> 00:24:30,000
 There's called training error training performance.

255
00:24:30,000 --> 00:24:31,000
 Okay?

256
00:24:31,000 --> 00:24:36,000
 And then once you're the model, you can also use the test data set that you get aside to

257
00:24:36,000 --> 00:24:38,000
 test the model.

258
00:24:38,000 --> 00:24:43,000
 Because you also know the ground truth of the test data, so you can also evaluate the

259
00:24:43,000 --> 00:24:44,000
 performance.

260
00:24:44,000 --> 00:24:45,000
 Right?

261
00:24:45,000 --> 00:24:51,000
 So in general, which error will be, or which so-called performance will be better?

262
00:24:51,000 --> 00:24:56,000
 Is it training data set or test data set?

263
00:24:56,000 --> 00:24:59,000
 Performance will be better.

264
00:24:59,000 --> 00:25:05,000
 So it's generally the training data will give you better performance because your model

265
00:25:05,000 --> 00:25:08,000
 optimizes over the training data.

266
00:25:08,000 --> 00:25:09,000
 Right?

267
00:25:09,000 --> 00:25:15,000
 And your test data normally would have a higher error rate or poorer performance.

268
00:25:15,000 --> 00:25:17,000
 They generally is the case.

269
00:25:17,000 --> 00:25:21,000
 But they are occasionally the other way around, but those are outlier.

270
00:25:21,000 --> 00:25:22,000
 Okay?

271
00:25:22,000 --> 00:25:27,000
 So that's why when you have the data, you'll find that two of them continue to decrease

272
00:25:27,000 --> 00:25:29,000
 based on the iteration that is fine.

273
00:25:29,000 --> 00:25:34,000
 But the test data error will be higher than the training data error.

274
00:25:34,000 --> 00:25:35,000
 Right?

275
00:25:35,000 --> 00:25:37,000
 But you'll find that the error decreases.

276
00:25:37,000 --> 00:25:42,000
 Suddenly, the training data, if your model is correct, will continue to decrease.

277
00:25:42,000 --> 00:25:46,000
 But you find the test data error starts to go up.

278
00:25:46,000 --> 00:25:52,000
 That means the point your model might have been overfit to your training data.

279
00:25:52,000 --> 00:25:53,000
 Right?

280
00:25:53,000 --> 00:25:54,000
 Then you have to stop somehow.

281
00:25:54,000 --> 00:25:59,000
 So these are the processes you will go through in the following example.

282
00:25:59,000 --> 00:26:00,000
 Right?

283
00:26:00,000 --> 00:26:05,000
 So once you are satisfied with the error, then you, okay, you can test them with new

284
00:26:05,000 --> 00:26:08,000
 data, which is beyond this the training or test data.

285
00:26:08,000 --> 00:26:14,000
 You can start to collect some new data to verify whether the model indeed works as well

286
00:26:14,000 --> 00:26:16,000
 as you expected.

287
00:26:16,000 --> 00:26:17,000
 Okay?

288
00:26:17,000 --> 00:26:23,000
 If somehow it doesn't work anymore, then you go back to the first early state to redo.

289
00:26:23,000 --> 00:26:29,000
 Let me select another model or collect more features or look at the data you collect.

290
00:26:29,000 --> 00:26:30,000
 Are they sufficient?

291
00:26:30,000 --> 00:26:33,000
 The cleaning process, do you do it properly?

292
00:26:33,000 --> 00:26:36,000
 The partitioning, is it the right ratio?

293
00:26:36,000 --> 00:26:38,000
 You iterate the process.

294
00:26:38,000 --> 00:26:39,000
 Okay?

295
00:26:39,000 --> 00:26:41,000
 It will not be just once, right?

296
00:26:41,000 --> 00:26:46,000
 After three months or one year, maybe the economic condition changed, right?

297
00:26:46,000 --> 00:26:48,000
 A regression comes.

298
00:26:48,000 --> 00:26:50,000
 The model may not work anymore, right?

299
00:26:50,000 --> 00:26:56,000
 So you have to update or find your model or you move to the other company, right?

300
00:26:56,000 --> 00:27:03,000
 The company may locate in another town the performance or the country, even the customer

301
00:27:03,000 --> 00:27:05,000
 performance behavior may not be the same.

302
00:27:05,000 --> 00:27:09,000
 Then you have to collect the new data set to verify, to update.

303
00:27:09,000 --> 00:27:13,000
 So that is a very typical machine learning process.

304
00:27:13,000 --> 00:27:14,000
 Okay?

305
00:27:14,000 --> 00:27:17,000
 So then let's define the data.

306
00:27:17,000 --> 00:27:20,000
 Normally a data sample acts, right?

307
00:27:20,000 --> 00:27:29,000
 An object, a customer, you can represent them as an n-dimensional attribute vector, right?

308
00:27:29,000 --> 00:27:31,000
 Of course, that's what you want to do.

309
00:27:31,000 --> 00:27:34,000
 Engineers, you have to convert everything into numbers, right?

310
00:27:34,000 --> 00:27:38,000
 And this vector, s could be x1 is the age.

311
00:27:38,000 --> 00:27:40,000
 x2 age group.

312
00:27:40,000 --> 00:27:43,000
 x2 will be whether they are working or not, right?

313
00:27:43,000 --> 00:27:47,000
 Until xn, whether their credit rating is good, right?

314
00:27:47,000 --> 00:27:52,000
 It could be a very high dimensional space, right?

315
00:27:52,000 --> 00:27:58,000
 And they're based on this input and you try to make sense out of it.

316
00:27:58,000 --> 00:28:03,000
 And the data samples, sometimes you read other books.

317
00:28:03,000 --> 00:28:09,000
 They will call it the data points or examples, objects, instance, or tuppers, right?

318
00:28:09,000 --> 00:28:11,000
 All these are possible names, right?

319
00:28:11,000 --> 00:28:15,000
 They are all referring to the same, the data sample, okay?

320
00:28:15,000 --> 00:28:19,000
 Each sample acts belong to a predefined class.

321
00:28:19,000 --> 00:28:24,000
 Like, early on the example I gave is yes or no, right?

322
00:28:24,000 --> 00:28:29,000
 Whether the customer is going to buy or not to buy a car from you.

323
00:28:29,000 --> 00:28:32,000
 These are binary classification output.

324
00:28:32,000 --> 00:28:38,000
 Class labels are generally discrete value, like 1 and 0, buy or not buy.

325
00:28:38,000 --> 00:28:40,000
 And unordered.

326
00:28:40,000 --> 00:28:47,000
 That means they do not really have any so-called meaning in terms of which ones would come first, right?

327
00:28:47,000 --> 00:28:54,000
 So there are four types of attributes and this attribute could refer to these features, right?

328
00:28:54,000 --> 00:28:58,000
 Just now we see the ages, whether they buy car or not, okay?

329
00:28:58,000 --> 00:28:59,000
 So there are four types.

330
00:28:59,000 --> 00:29:03,000
 The first two types are called nominal or ordinal attributes.

331
00:29:03,000 --> 00:29:06,000
 These two types, let's see some example.

332
00:29:06,000 --> 00:29:11,000
 Here, nominers are different names to tell them apart.

333
00:29:11,000 --> 00:29:15,000
 For example, your student ID number, right?

334
00:29:15,000 --> 00:29:18,000
 Your eye color, your hair color, right?

335
00:29:18,000 --> 00:29:24,000
 Or gender, the blood type, or interest group, where you stay, right?

336
00:29:24,000 --> 00:29:29,000
 So they do not necessarily have any so-called order.

337
00:29:29,000 --> 00:29:33,000
 They just tell you the location, the number, the type, okay?

338
00:29:33,000 --> 00:29:37,000
 We call these type nominal attributes, okay?

339
00:29:37,000 --> 00:29:43,000
 The other is the ordinal attribute, which although they are also discrete,

340
00:29:43,000 --> 00:29:46,000
 but you start to, you can make some comparison.

341
00:29:46,000 --> 00:29:48,000
 For example, the rating, right?

342
00:29:48,000 --> 00:29:54,000
 A movie rating, whether this is good, average, poor, or excellent, right?

343
00:29:54,000 --> 00:30:01,000
 Although these belong to four classes, but you probably prefer excellent over good,

344
00:30:01,000 --> 00:30:03,000
 or average over poor.

345
00:30:03,000 --> 00:30:08,000
 So there are some order you can actually make choices.

346
00:30:08,000 --> 00:30:10,000
 These are called ordinal data.

347
00:30:10,000 --> 00:30:18,000
 You can compare them based on some kind of so-called order, so-called preferences,

348
00:30:18,000 --> 00:30:19,000
 like strict number, right?

349
00:30:19,000 --> 00:30:23,000
 If you go to look for your friend at a particular street,

350
00:30:23,000 --> 00:30:25,000
 you look at the number, you know that in one direction,

351
00:30:25,000 --> 00:30:27,000
 the number normally would increase.

352
00:30:27,000 --> 00:30:30,000
 The other direction would be reducing, right?

353
00:30:30,000 --> 00:30:36,000
 If the number are random, then you have a time to find which unit that your friend stays, right?

354
00:30:36,000 --> 00:30:38,000
 So there are some order, okay?

355
00:30:38,000 --> 00:30:44,000
 And the other two types are called interval or ratio attributes.

356
00:30:44,000 --> 00:30:48,000
 So they are numeric and quantitative attributes, okay?

357
00:30:48,000 --> 00:30:55,000
 For example, intervals is you have meaning about the differences between the values.

358
00:30:55,000 --> 00:30:59,000
 You can basically apply addition or subtraction.

359
00:30:59,000 --> 00:31:02,000
 For example, the date of a calendar, right?

360
00:31:02,000 --> 00:31:06,000
 Every month from one to 30 or 31, right?

361
00:31:06,000 --> 00:31:09,000
 So you know that one come before two, right?

362
00:31:09,000 --> 00:31:11,000
 Calendar that you can subtract.

363
00:31:11,000 --> 00:31:14,000
 It's a day seven minus day one.

364
00:31:14,000 --> 00:31:15,000
 There is a six day in between.

365
00:31:15,000 --> 00:31:17,000
 So you have meaning, right?

366
00:31:17,000 --> 00:31:22,000
 So early on, you may not have a meaning of excellent subtract good.

367
00:31:22,000 --> 00:31:27,000
 There is no meaning, no so-called intervals.

368
00:31:27,000 --> 00:31:33,000
 Because the higher will be the ratio which besides addition or subtraction,

369
00:31:33,000 --> 00:31:38,000
 you can also apply multiplication or division, right?

370
00:31:38,000 --> 00:31:43,000
 For example, the length of an object, right?

371
00:31:43,000 --> 00:31:48,000
 If you have an object which is one cm long,

372
00:31:48,000 --> 00:31:51,000
 if you put five of them together, it should be five cm.

373
00:31:51,000 --> 00:31:53,000
 So you can multiply five.

374
00:31:53,000 --> 00:31:56,000
 So they have such a so-called meaning.

375
00:31:56,000 --> 00:32:02,000
 Or if you have some number of objects, total length is 100,

376
00:32:02,000 --> 00:32:05,000
 then you know each of them is about 2.5.

377
00:32:05,000 --> 00:32:09,000
 Then you divide them, you know how many objects are going to be.

378
00:32:09,000 --> 00:32:11,000
 So this division can also apply.

379
00:32:11,000 --> 00:32:14,000
 So these are ratio attributes.

380
00:32:14,000 --> 00:32:15,000
 Okay?

381
00:32:15,000 --> 00:32:17,000
 I think at one time I said a question I was asking.

382
00:32:17,000 --> 00:32:26,000
 So I'll give example of two ratio attributes and two ordinal attributes.

383
00:32:26,000 --> 00:32:27,000
 Okay?

384
00:32:27,000 --> 00:32:33,000
 Then you have to give example and see whether they meet some of these criteria.

385
00:32:33,000 --> 00:32:34,000
 Okay?

386
00:32:34,000 --> 00:32:35,000
 So these are attributes.

387
00:32:35,000 --> 00:32:38,000
 And again, the attributes is happened to the features

388
00:32:38,000 --> 00:32:42,000
 and the class label are normally discrete.

389
00:32:42,000 --> 00:32:43,000
 Okay?

390
00:32:43,000 --> 00:32:45,000
 So that means the class label,

391
00:32:45,000 --> 00:32:49,000
 unlikely will be interval or ratio.

392
00:32:49,000 --> 00:32:52,000
 The class label could be nominal.

393
00:32:52,000 --> 00:32:55,000
 It may be ordinal.

394
00:32:55,000 --> 00:32:58,000
 Unless you are doing regression.

395
00:32:58,000 --> 00:33:00,000
 Remember when we in the first introduction lecture,

396
00:33:00,000 --> 00:33:07,000
 we talked about the housing price rather than you identify whether you buy or not

397
00:33:07,000 --> 00:33:12,000
 or rent or not, you want to estimate what is the rent you should pay.

398
00:33:12,000 --> 00:33:16,000
 There will be some kind of interval or even ratio number that you can estimate.

399
00:33:16,000 --> 00:33:19,000
 That is a regression, not a classification problem.

400
00:33:19,000 --> 00:33:22,000
 Okay?

401
00:33:22,000 --> 00:33:25,000
 So here are some popular classification methods.

402
00:33:25,000 --> 00:33:28,000
 So we will talk about decision trees.

403
00:33:28,000 --> 00:33:33,000
 And we talk about rule-based classification because after you build a decision tree,

404
00:33:33,000 --> 00:33:36,000
 you can come up with some rules to make decision.

405
00:33:36,000 --> 00:33:37,000
 Right?

406
00:33:37,000 --> 00:33:44,000
 Even earlier on when Professor Chen Yi Hui introduced some of the so-called state approach,

407
00:33:44,000 --> 00:33:46,000
 they can also form the rule-based, right?

408
00:33:46,000 --> 00:33:48,000
 Association analysis.

409
00:33:48,000 --> 00:33:50,000
 You can also do that.

410
00:33:50,000 --> 00:33:54,000
 Then we'll talk about K-near-less neighbor classification.

411
00:33:54,000 --> 00:33:57,000
 And these are very basic, but some of the advanced methods,

412
00:33:57,000 --> 00:34:06,000
 they do make use of some of these so-called simple idea to pre-process the data.

413
00:34:06,000 --> 00:34:09,000
 And then we'll talk about support vector machine.

414
00:34:09,000 --> 00:34:11,000
 And remember this was a machine.

415
00:34:11,000 --> 00:34:19,000
 The method somehow create another AI winter, even beat the neural network at that time.

416
00:34:19,000 --> 00:34:22,000
 It was not really well developed.

417
00:34:22,000 --> 00:34:24,000
 They will talk about neural networks.

418
00:34:24,000 --> 00:34:25,000
 Okay?

419
00:34:25,000 --> 00:34:28,000
 So ensemble method is just a collection of them, right?

420
00:34:28,000 --> 00:34:33,000
 And the random forest will be one type of ensemble methods.

421
00:34:33,000 --> 00:34:35,000
 And I will show you some tools.

422
00:34:35,000 --> 00:34:39,000
 I think on the website, you can see there's Orange Tool.

423
00:34:39,000 --> 00:34:47,000
 You can use it, try to play around with some data and then to apply one of these classifier fire

424
00:34:47,000 --> 00:34:49,000
 and see the result.

425
00:34:49,000 --> 00:34:50,000
 Okay?

426
00:34:50,000 --> 00:34:54,000
 So maybe at the time we can go through some of the examples during the class.

427
00:34:54,000 --> 00:34:58,000
 The Bayesian classifier, I think B.Hun is going to talk about it.

428
00:34:58,000 --> 00:35:00,000
 I'll leave him to him.

429
00:35:00,000 --> 00:35:03,000
 So let's start with decision trees.

430
00:35:03,000 --> 00:35:18,000
 It's a very old and very intuitively easy to understand classification method.

431
00:35:18,000 --> 00:35:19,000
 Okay?

432
00:35:19,000 --> 00:35:26,000
 So a decision tree, so far any question, any problem before I go on?

433
00:35:26,000 --> 00:35:33,000
 It's still easy.

434
00:35:33,000 --> 00:35:37,000
 So a decision tree look like this, okay?

435
00:35:37,000 --> 00:35:40,000
 Has a flow chart like three structures, okay?

436
00:35:40,000 --> 00:35:45,000
 Although the tree is kind of hanging down.

437
00:35:45,000 --> 00:35:48,000
 So it has a note.

438
00:35:48,000 --> 00:35:55,000
 This yellow color one, we call this a note, which could be the root note, the top.

439
00:35:55,000 --> 00:35:59,000
 It's upside down, root note.

440
00:35:59,000 --> 00:36:08,000
 All the internal notes like this, which denotes a test on an attribute.

441
00:36:08,000 --> 00:36:10,000
 Earlier I talked about those attributes.

442
00:36:10,000 --> 00:36:19,000
 Basically when you come to each note, you try to ask a question or test whether this attribute fulfills certain requirement.

443
00:36:19,000 --> 00:36:26,000
 And then when this note asks a question, then you are going to have a few branches coming out, right?

444
00:36:26,000 --> 00:36:40,000
 For example, in this case, whether this one is, you want to test whether a particular person is going to conduct a text fraud situation, okay?

445
00:36:40,000 --> 00:36:47,000
 Then you can ask the question like refund whether this person asks for text refund, okay?

446
00:36:47,000 --> 00:36:52,000
 If he asks, then of course you go down this branch.

447
00:36:52,000 --> 00:36:54,000
 If he doesn't ask, go down the other branch.

448
00:36:54,000 --> 00:36:57,000
 So this is the so-called the branch.

449
00:36:57,000 --> 00:37:01,000
 You represent the outcome of the test.

450
00:37:01,000 --> 00:37:08,000
 You perform on one of the root or internal notes.

451
00:37:08,000 --> 00:37:18,000
 Then each leaf note, the one I show in green color, represent the terminal note which holds a class label, right?

452
00:37:18,000 --> 00:37:22,000
 So, and for some, in this case, you ask whether you ask for refund.

453
00:37:22,000 --> 00:37:30,000
 If no, then it's not likely going to conduct a text fraud because he doesn't ask for money return, right?

454
00:37:30,000 --> 00:37:32,000
 So this is a leaf note.

455
00:37:32,000 --> 00:37:36,000
 Then this will be the class label note, okay?

456
00:37:36,000 --> 00:37:44,000
 So if you ask for refund, then you check, ask the next question whether the person is married or not, right?

457
00:37:44,000 --> 00:37:47,000
 If yes, then Lili is not going to conduct a text fraud.

458
00:37:47,000 --> 00:37:51,000
 Okay, this is a very artificial test, right?

459
00:37:51,000 --> 00:38:00,000
 And if you say no, then you ask whether what is the amount of the income, taxable income tax, right?

460
00:38:00,000 --> 00:38:03,000
 If it's less than 100K, then no.

461
00:38:03,000 --> 00:38:08,000
 If it's higher than 100K, then Lili, this person will conduct a text fraud.

462
00:38:08,000 --> 00:38:15,000
 So being those rich people, income, taxable income hire is more likely to conduct a text fraud.

463
00:38:15,000 --> 00:38:17,000
 Again, this is all artificial.

464
00:38:17,000 --> 00:38:19,000
 So this is, again, very logical.

465
00:38:19,000 --> 00:38:23,000
 Basically, it's like human decision, right?

466
00:38:23,000 --> 00:38:28,000
 So when you make judgment, you consider a few factors or features.

467
00:38:28,000 --> 00:38:33,000
 Even without this classifier, this is all for what you do, right?

468
00:38:33,000 --> 00:38:39,000
 You meet someone new, you say, okay, if this person Lili can be your friend, right?

469
00:38:39,000 --> 00:38:47,000
 You say, okay, do they graduate from the same so-called country, study the same subject,

470
00:38:47,000 --> 00:38:50,000
 or have certain common interests?

471
00:38:50,000 --> 00:38:52,000
 You ask all these questions, right?

472
00:38:52,000 --> 00:38:56,000
 And then at the end, you say, okay, whether this person can be your friend or not,

473
00:38:56,000 --> 00:38:58,000
 then you make a decision, right?

474
00:38:58,000 --> 00:39:02,000
 It's a very simple, typical process, okay?

475
00:39:02,000 --> 00:39:04,000
 So there is basically a decision tree.

476
00:39:04,000 --> 00:39:05,000
 That's it.

477
00:39:05,000 --> 00:39:08,000
 End of the story.

478
00:39:08,000 --> 00:39:11,000
 Of course, then it may not be so simple.

479
00:39:11,000 --> 00:39:15,000
 You could have, earlier we talked about two classes, right?

480
00:39:15,000 --> 00:39:22,000
 You could have a decision tree, which you may have multiple outputs, right?

481
00:39:22,000 --> 00:39:24,000
 In here, we have a tree output.

482
00:39:24,000 --> 00:39:28,000
 You try to judge a particular fruit type, right?

483
00:39:28,000 --> 00:39:34,000
 In this case, you look at the size of the fruits, where it's big, the medium, or small, right?

484
00:39:34,000 --> 00:39:39,000
 Then you make decision whether this is durian, apple, or lime, right?

485
00:39:39,000 --> 00:39:46,000
 Based on just the size of the fruits, right?

486
00:39:46,000 --> 00:39:51,000
 So you could have level zero, level one, level two, level three.

487
00:39:51,000 --> 00:39:55,000
 When you have level three, basically, you only need to ask three questions.

488
00:39:55,000 --> 00:40:01,000
 One, two, three, maximum three questions, you will arrive at a decision, right?

489
00:40:01,000 --> 00:40:05,000
 But sometimes, depending on the outcome of the question,

490
00:40:05,000 --> 00:40:09,000
 you could make the decision faster, like this K level two.

491
00:40:09,000 --> 00:40:11,000
 So it depends.

492
00:40:11,000 --> 00:40:13,000
 Okay?

493
00:40:13,000 --> 00:40:16,000
 So now let's look at one example.

494
00:40:16,000 --> 00:40:18,000
 So how do you design?

495
00:40:18,000 --> 00:40:22,000
 Of course, the dataset may not be as simple as you have just seen.

496
00:40:22,000 --> 00:40:25,000
 It may not be all consistent among themselves.

497
00:40:25,000 --> 00:40:35,000
 So today, I give you Excel data, so-called table of 10 million dataset, right?

498
00:40:35,000 --> 00:40:39,000
 And maybe 50 attributes, right?

499
00:40:39,000 --> 00:40:41,000
 Build a tree, okay?

500
00:40:41,000 --> 00:40:44,000
 It's humanly impossible to do that.

501
00:40:44,000 --> 00:40:50,000
 It is actually possible such a data is like, think about those supermarkets, right?

502
00:40:50,000 --> 00:41:01,000
 If they have all these so-called customers, what they buy in each visit to the so-called store of a change,

503
00:41:01,000 --> 00:41:07,000
 then one year record, you could have million of customer visit,

504
00:41:07,000 --> 00:41:14,000
 and each of them can purchase from zero to maybe 20 or 30 items per visit.

505
00:41:14,000 --> 00:41:21,000
 And if your store has, let's say, a thousand or maybe a few thousand different items,

506
00:41:21,000 --> 00:41:26,000
 then you have, according to a few thousand column, right?

507
00:41:26,000 --> 00:41:29,000
 Then you have to see what they buy.

508
00:41:29,000 --> 00:41:37,000
 And that will become something very challenging for you to design the tree, which is somehow optimal, right?

509
00:41:37,000 --> 00:41:40,000
 What will be the attribute when you have so many attributes?

510
00:41:40,000 --> 00:41:42,000
 What will be the attribute that you look at?

511
00:41:42,000 --> 00:41:47,000
 I think in the earlier example, you look at this basket dataset, right?

512
00:41:47,000 --> 00:41:55,000
 You try to see, okay, if the customer buys this few item, what would they buy in the next visit, right?

513
00:41:55,000 --> 00:42:01,000
 So you can build a classification tree to do the project as well, right?

514
00:42:01,000 --> 00:42:11,000
 And then the tree that you build could have many, many levels, many, many so-called branches, many, many levels, right?

515
00:42:11,000 --> 00:42:12,000
 So which would be better?

516
00:42:12,000 --> 00:42:18,000
 Is it the earlier association rule or the classifier, right?

517
00:42:18,000 --> 00:42:24,000
 Depending on what you want to judge, whether the person going to buy the milk in the next visit,

518
00:42:24,000 --> 00:42:27,000
 or how much or the person will spend, right?

519
00:42:27,000 --> 00:42:34,000
 And how frequent the person will come again so that you decide whether you give them the loyalty program,

520
00:42:34,000 --> 00:42:36,000
 customer membership or not, okay?

521
00:42:36,000 --> 00:42:39,000
 So let's look at one example.

522
00:42:39,000 --> 00:42:43,000
 Okay, we only have six data samples, right?

523
00:42:43,000 --> 00:42:46,000
 One, two, three, four, five, six.

524
00:42:46,000 --> 00:42:53,000
 Each of them have three features, fever, cough or tiredness.

525
00:42:53,000 --> 00:43:06,000
 And then your goal is to judge, this is the labor, class labor, whether the person is slightly infected by COVID-19 virus, right?

526
00:43:06,000 --> 00:43:08,000
 Assuming a very simple dataset, okay?

527
00:43:08,000 --> 00:43:15,000
 Because if you recall the time when the COVID just started and no one knows what will be the so-called symptom,

528
00:43:15,000 --> 00:43:19,000
 which will be very decisive, right?

529
00:43:19,000 --> 00:43:27,000
 So then you start to collect some of these so-called input based on some of these examples that you encounter

530
00:43:27,000 --> 00:43:30,000
 and the measure that you make, right?

531
00:43:30,000 --> 00:43:36,000
 For a first person, it doesn't have fever, but if you have a mouth cough, it's not tired,

532
00:43:36,000 --> 00:43:42,000
 then you do the test, test, it comes out, oh, no cough in COVID-19, right?

533
00:43:42,000 --> 00:43:48,000
 So another person come in, your mouth fever have cough, but it doesn't feel tired.

534
00:43:48,000 --> 00:43:52,000
 Then you do the test, oh, it is positive.

535
00:43:52,000 --> 00:43:59,000
 So you collect a few of them, then you decide, okay, what features should you look at to make a decision?

536
00:43:59,000 --> 00:44:02,000
 What would be the most telling features?

537
00:44:02,000 --> 00:44:10,000
 In some kind of scientific or optimal decision you can make, right?

538
00:44:10,000 --> 00:44:13,000
 So, yeah, that is the problem, right?

539
00:44:13,000 --> 00:44:15,000
 So how do you build the trees?

540
00:44:15,000 --> 00:44:17,000
 How do you build the trees?

541
00:44:17,000 --> 00:44:22,000
 And all these problems are not difficult, but it is just tedious.

542
00:44:22,000 --> 00:44:27,000
 If I said an exam question like this in the past, okay?

543
00:44:27,000 --> 00:44:32,000
 It's found that students actually they can do, but often they cannot finish.

544
00:44:32,000 --> 00:44:37,000
 If I ask you to build a tree just with about 16 samples,

545
00:44:37,000 --> 00:44:44,000
 and you will think maybe more than half an hour before you can do all the computation, right?

546
00:44:44,000 --> 00:44:49,000
 Okay, so you have to understand what is your class labor.

547
00:44:49,000 --> 00:44:52,000
 This is the primary focus, okay?

548
00:44:52,000 --> 00:44:56,000
 Always have your eye look at the class outcome, okay?

549
00:44:56,000 --> 00:44:58,000
 So here, okay?

550
00:44:58,000 --> 00:45:04,000
 Then from here, you know that you have three positive, three negative.

551
00:45:04,000 --> 00:45:07,000
 So what does it tell you?

552
00:45:07,000 --> 00:45:14,000
 So if this is the only information you know based on the six page patient that you have examined,

553
00:45:14,000 --> 00:45:20,000
 50% of them have COVID, 50% of them do not have COVID.

554
00:45:20,000 --> 00:45:24,000
 Then there's a new person coming, okay?

555
00:45:24,000 --> 00:45:29,000
 Do you think he is going to have COVID?

556
00:45:29,000 --> 00:45:32,000
 Right, so that is a question, okay?

557
00:45:32,000 --> 00:45:37,000
 Without collecting any feature, you are just a random guess, 50%.

558
00:45:37,000 --> 00:45:43,000
 You cannot make any better decision than a random guess, right?

559
00:45:43,000 --> 00:45:49,000
 So this before you do make all the measures, whether you have fever, a cough, or tired,

560
00:45:49,000 --> 00:45:55,000
 before you ask any question, the person come in, does he or she have COVID or not?

561
00:45:55,000 --> 00:46:01,000
 So that tells you that, tough, 50% chance you get it correct or wrong, okay?

562
00:46:01,000 --> 00:46:06,000
 Then you look at it, okay, if I can now ask the first question,

563
00:46:06,000 --> 00:46:10,000
 does the person have fever, cough, or tired?

564
00:46:10,000 --> 00:46:15,000
 Okay, if you can only ask one question,

565
00:46:15,000 --> 00:46:20,000
 which will be the question you ask to make a better decision?

566
00:46:20,000 --> 00:46:22,000
 So that is the question, right?

567
00:46:22,000 --> 00:46:25,000
 The tree, the roof notes, right?

568
00:46:25,000 --> 00:46:27,000
 That is the first question that you ask.

569
00:46:27,000 --> 00:46:33,000
 You want the question to be as telling, as useful as possible, right?

570
00:46:33,000 --> 00:46:38,000
 Because the patient could be not patient enough.

571
00:46:38,000 --> 00:46:42,000
 After you ask the first question, you or she can just walk out.

572
00:46:42,000 --> 00:46:45,000
 So you have to choose a question to ask.

573
00:46:45,000 --> 00:46:49,000
 Can anyone tell which question you should ask from here?

574
00:46:49,000 --> 00:46:52,000
 Why?

575
00:46:52,000 --> 00:46:55,000
 Tiredness.

576
00:46:55,000 --> 00:46:59,000
 So tiredness, let's see whether this is the attribute you should ask, okay?

577
00:46:59,000 --> 00:47:01,000
 Let's examine all of them, okay?

578
00:47:01,000 --> 00:47:03,000
 Let's try to look at tiredness.

579
00:47:03,000 --> 00:47:05,000
 Oh, you are right.

580
00:47:05,000 --> 00:47:07,000
 Look at this, tiredness.

581
00:47:07,000 --> 00:47:11,000
 If I ask a question whether the person is tired or not,

582
00:47:11,000 --> 00:47:14,000
 and all the answers will be yes and no, right?

583
00:47:14,000 --> 00:47:17,000
 Turn now.

584
00:47:17,000 --> 00:47:22,000
 If the person says yes, she is tired, okay?

585
00:47:22,000 --> 00:47:26,000
 The chance is two of them will have COVID.

586
00:47:26,000 --> 00:47:30,000
 You look at the yes, the one with blue color,

587
00:47:30,000 --> 00:47:34,000
 then two of them have COVID, one of them doesn't have COVID.

588
00:47:34,000 --> 00:47:35,000
 Okay?

589
00:47:35,000 --> 00:47:39,000
 If the person is not tired, then you go to this, another branch.

590
00:47:39,000 --> 00:47:41,000
 Always focus on your class labor.

591
00:47:41,000 --> 00:47:45,000
 Two of them without COVID, one have COVID.

592
00:47:45,000 --> 00:47:46,000
 Okay?

593
00:47:46,000 --> 00:47:49,000
 Does it help?

594
00:47:49,000 --> 00:47:50,000
 Right?

595
00:47:50,000 --> 00:47:52,000
 So just by asking the first question,

596
00:47:52,000 --> 00:47:56,000
 now your error rate dropped to 33%.

597
00:47:56,000 --> 00:47:57,000
 Right?

598
00:47:57,000 --> 00:47:59,000
 Basically, when you tell you I'm tired,

599
00:47:59,000 --> 00:48:01,000
 most likely you will say,

600
00:48:01,000 --> 00:48:03,000
 most likely you have COVID,

601
00:48:03,000 --> 00:48:07,000
 at least 67% chance, right?

602
00:48:07,000 --> 00:48:09,000
 Which is better than 50%, right?

603
00:48:09,000 --> 00:48:11,000
 By asking the first question,

604
00:48:11,000 --> 00:48:16,000
 you make the decision easier, right?

605
00:48:16,000 --> 00:48:17,000
 Okay?

606
00:48:17,000 --> 00:48:22,000
 If it's not tired, then most likely you do not have COVID,

607
00:48:22,000 --> 00:48:26,000
 which you can also make a better decision.

608
00:48:26,000 --> 00:48:28,000
 So earlier on, without asking a question,

609
00:48:28,000 --> 00:48:31,000
 I say 50% chance you get it right or wrong.

610
00:48:31,000 --> 00:48:35,000
 That means your error rate will be three out of six, right?

611
00:48:35,000 --> 00:48:39,000
 Because 50%, if you have six patients,

612
00:48:39,000 --> 00:48:43,000
 most likely 50% you will make an error.

613
00:48:43,000 --> 00:48:44,000
 Okay?

614
00:48:44,000 --> 00:48:47,000
 But now, by asking the first question,

615
00:48:47,000 --> 00:48:52,000
 what is the error rate that you can expect?

616
00:48:52,000 --> 00:48:55,000
 What is the error rate?

617
00:48:55,000 --> 00:48:58,000
 Two out of six, right?

618
00:48:58,000 --> 00:49:00,000
 That is 33%.

619
00:49:00,000 --> 00:49:02,000
 Because this one, you are going to make a mistake.

620
00:49:03,000 --> 00:49:04,000
 Okay?

621
00:49:04,000 --> 00:49:07,000
 So you improve your decision,

622
00:49:07,000 --> 00:49:09,000
 but you still make a mistake.

623
00:49:09,000 --> 00:49:11,000
 So it's not perfect.

624
00:49:11,000 --> 00:49:13,000
 Then let's look at other attributes.

625
00:49:13,000 --> 00:49:15,000
 That's the first question you ask, okay?

626
00:49:15,000 --> 00:49:17,000
 I haven't talked about the second question.

627
00:49:17,000 --> 00:49:22,000
 So tiredness is one, you know that you're going to make 33% error.

628
00:49:22,000 --> 00:49:25,000
 Improved from 55%, 50%.

629
00:49:25,000 --> 00:49:30,000
 So that is the power, the value of asking the right question,

630
00:49:30,000 --> 00:49:32,000
 the first question, okay?

631
00:49:32,000 --> 00:49:35,000
 Now let's examine cough.

632
00:49:35,000 --> 00:49:40,000
 Now look at the, again, always look at the feature together

633
00:49:40,000 --> 00:49:42,000
 with your class labor.

634
00:49:42,000 --> 00:49:46,000
 So for cough, you have three possible outputs.

635
00:49:46,000 --> 00:49:49,000
 Mal, no, or yes.

636
00:49:49,000 --> 00:49:51,000
 No, mal, yes.

637
00:49:51,000 --> 00:49:56,000
 So similarly, you have this sixth sample input.

638
00:49:56,000 --> 00:50:02,000
 Now, based on the answer, you know that no mal, yes, this will be,

639
00:50:02,000 --> 00:50:05,000
 four of them have the mal symptoms,

640
00:50:05,000 --> 00:50:09,000
 but among these four, two have COVID, two without COVID.

641
00:50:09,000 --> 00:50:10,000
 Right?

642
00:50:10,000 --> 00:50:13,000
 So for no cough, no COVID.

643
00:50:13,000 --> 00:50:16,000
 We cough, we cough it.

644
00:50:16,000 --> 00:50:17,000
 Okay.

645
00:50:17,000 --> 00:50:22,000
 So what will be the error rate that you are going to have

646
00:50:22,000 --> 00:50:28,000
 if you ask the first question whether the patient have cough?

647
00:50:28,000 --> 00:50:31,000
 What is the error rate?

648
00:50:31,000 --> 00:50:35,000
 How many samples you will make mistake?

649
00:50:35,000 --> 00:50:39,000
 Two out of six, right?

650
00:50:39,000 --> 00:50:45,000
 So it's the same error rate as tiredness.

651
00:50:45,000 --> 00:50:46,000
 Right?

652
00:50:46,000 --> 00:50:49,000
 So that means whether you ask the patient if tired or not,

653
00:50:49,000 --> 00:50:57,000
 or whether it's cough or not, you make almost similar decision error.

654
00:50:57,000 --> 00:50:58,000
 So it's the same.

655
00:50:58,000 --> 00:51:02,000
 One is not better than the other.

656
00:51:02,000 --> 00:51:03,000
 Okay?

657
00:51:03,000 --> 00:51:08,000
 Then let us look at the third feature, which is fever.

658
00:51:08,000 --> 00:51:09,000
 Okay?

659
00:51:09,000 --> 00:51:12,000
 The fever also had three outputs.

660
00:51:12,000 --> 00:51:16,000
 Again, this may be a simple example, but think about the case

661
00:51:16,000 --> 00:51:21,000
 whether you may have a million of samples, thousand of features,

662
00:51:21,000 --> 00:51:25,000
 then you have to decide which is the best feature to ask,

663
00:51:25,000 --> 00:51:27,000
 which will be the second feature to ask.

664
00:51:27,000 --> 00:51:33,000
 And then you need to have a systematic way to construct the decision tree.

665
00:51:33,000 --> 00:51:35,000
 So this is what we are concerned about.

666
00:51:35,000 --> 00:51:37,000
 Of course, there will be two helping you to do that,

667
00:51:37,000 --> 00:51:41,000
 but at least you need to know the principle, right?

668
00:51:41,000 --> 00:51:43,000
 Behind it, how it works.

669
00:51:43,000 --> 00:51:46,000
 Let's look at the feature, a fever, right?

670
00:51:46,000 --> 00:51:47,000
 The same thing.

671
00:51:47,000 --> 00:51:53,000
 If you have no fever, then based on this, right?

672
00:51:53,000 --> 00:51:54,000
 Okay.

673
00:51:54,000 --> 00:51:56,000
 No fever.

674
00:51:56,000 --> 00:51:57,000
 Look at this one.

675
00:51:57,000 --> 00:52:04,000
 There are two no fever, no so-called COVID.

676
00:52:04,000 --> 00:52:06,000
 And there are two mild fever.

677
00:52:06,000 --> 00:52:08,000
 Both of them have COVID.

678
00:52:08,000 --> 00:52:14,000
 And then two yes, one have COVID, one without COVID.

679
00:52:14,000 --> 00:52:15,000
 Okay?

680
00:52:15,000 --> 00:52:21,000
 So there is outcome by asking whether the person has fever as the first question.

681
00:52:21,000 --> 00:52:27,000
 So why is the error rate?

682
00:52:27,000 --> 00:52:31,000
 One out of six, right?

683
00:52:31,000 --> 00:52:34,000
 Because you can make, based on the output,

684
00:52:34,000 --> 00:52:38,000
 if it's no, certainly 100% you can make correct decision.

685
00:52:38,000 --> 00:52:40,000
 Just based on this training data.

686
00:52:40,000 --> 00:52:42,000
 Mild, you can make correct decision.

687
00:52:42,000 --> 00:52:52,000
 Only when you say it has fever, 50% chance of these two, you will make an error.

688
00:52:52,000 --> 00:52:56,000
 So that means one out of six, you will make a mistake.

689
00:52:56,000 --> 00:53:01,000
 So but this is the best result among these three features.

690
00:53:01,000 --> 00:53:03,000
 So go back to the question again.

691
00:53:03,000 --> 00:53:07,000
 What will be the first attribute you should ask?

692
00:53:07,000 --> 00:53:10,000
 It's a fever, right?

693
00:53:10,000 --> 00:53:12,000
 Okay?

694
00:53:12,000 --> 00:53:15,000
 So now you look at the two with the fever.

695
00:53:15,000 --> 00:53:17,000
 Of course, there are still errors.

696
00:53:17,000 --> 00:53:23,000
 But don't forget that after fever, you can ask whether the person is cough or tired.

697
00:53:23,000 --> 00:53:24,000
 Right?

698
00:53:24,000 --> 00:53:29,000
 So apparently you look at the data, if you look carefully, oh, if this person has fever,

699
00:53:29,000 --> 00:53:35,000
 all I had to do is ask whether the person has cough or whether he's tired.

700
00:53:35,000 --> 00:53:42,000
 Based on the outcome, I can tell them apart, right?

701
00:53:42,000 --> 00:53:48,000
 The green becomes the leaf note when it is pure, when their decisions are consistent.

702
00:53:48,000 --> 00:53:53,000
 So now, by asking the second question, tiredness or cough,

703
00:53:53,000 --> 00:53:58,000
 you'll be able to differentiate these two to yes and no.

704
00:53:58,000 --> 00:54:05,000
 And then you have 100% accuracy by asking just two questions.

705
00:54:05,000 --> 00:54:08,000
 So you may not need to have three features.

706
00:54:08,000 --> 00:54:11,000
 You only need to look at the two.

707
00:54:11,000 --> 00:54:13,000
 Right?

708
00:54:13,000 --> 00:54:16,000
 But it may not be consistent like this.

709
00:54:16,000 --> 00:54:20,000
 Okay, for example, if I now make this to like yes and yes,

710
00:54:20,000 --> 00:54:25,000
 and then tiredness will not be the one helping you to make a decision.

711
00:54:25,000 --> 00:54:26,000
 Right?

712
00:54:26,000 --> 00:54:28,000
 And, right?

713
00:54:28,000 --> 00:54:33,000
 So for example, you can also select the fever and cough to build a tree.

714
00:54:33,000 --> 00:54:38,000
 Then you select another fever and tiredness to build another tree.

715
00:54:38,000 --> 00:54:39,000
 Right?

716
00:54:39,000 --> 00:54:44,000
 Then you have two multiple three because there will be occasion that you may not know

717
00:54:44,000 --> 00:54:47,000
 whether the person is tired or cough, have cough.

718
00:54:47,000 --> 00:54:51,000
 Then you can choose the other three to give you the decision.

719
00:54:51,000 --> 00:54:54,000
 That is the basic idea about the random forest.

720
00:54:54,000 --> 00:55:00,000
 You can choose different combinations of features to build different trees.

721
00:55:00,000 --> 00:55:05,000
 Then one of them will be able to give you definite answer.

722
00:55:05,000 --> 00:55:07,000
 Then you combine all of them.

723
00:55:07,000 --> 00:55:10,000
 Maybe you can get a better decision.

724
00:55:10,000 --> 00:55:13,000
 So that is basically decision three.

725
00:55:13,000 --> 00:55:18,000
 But you have to think about it when you have a lot of samples, a lot of features,

726
00:55:18,000 --> 00:55:21,000
 and not all the features are useful features.

727
00:55:21,000 --> 00:55:23,000
 And not all of them are consistent.

728
00:55:23,000 --> 00:55:25,000
 They will be outliers.

729
00:55:25,000 --> 00:55:33,000
 You could have exactly two sets of persons with yes, no, no, one with COVID, one without COVID.

730
00:55:33,000 --> 00:55:34,000
 Right?

731
00:55:34,000 --> 00:55:40,000
 Then when you build a tree, you are not going to make correct decision to two of them together.

732
00:55:40,000 --> 00:55:42,000
 So one will have error.

733
00:55:42,000 --> 00:55:44,000
 So there will be all such a complication.

734
00:55:44,000 --> 00:55:46,000
 Then you need to have a better way to measure.

735
00:55:46,000 --> 00:55:48,000
 Just now we used the error rate.

736
00:55:48,000 --> 00:55:53,000
 How many samples of the training sample you will make mistake.

737
00:55:53,000 --> 00:56:00,000
 But that may not be the best so-called measure that you can use to select which attribute is the best.

738
00:56:00,000 --> 00:56:04,000
 Another measure that we use is called entropy.

739
00:56:04,000 --> 00:56:07,000
 Or uncertainty.

740
00:56:07,000 --> 00:56:09,000
 Some of you may have heard about this.

741
00:56:09,000 --> 00:56:11,000
 It's a Shannon entropy.

742
00:56:11,000 --> 00:56:14,000
 Basically, for cases, you have two classes in here.

743
00:56:14,000 --> 00:56:17,000
 PI, I from one in here.

744
00:56:17,000 --> 00:56:20,000
 Of course, I can go up to M.

745
00:56:20,000 --> 00:56:22,000
 But this example, I give you only two classes.

746
00:56:22,000 --> 00:56:23,000
 Right?

747
00:56:23,000 --> 00:56:30,000
 Because 0.5, you plug in PI equal to 0.5, you get entropy equal to one, which is one bit.

748
00:56:30,000 --> 00:56:32,000
 So which is another measure.

749
00:56:32,000 --> 00:56:39,000
 Entropy is a way to measure how random the so-called data is.

750
00:56:40,000 --> 00:56:43,000
 The smaller, the better.

751
00:56:43,000 --> 00:56:45,000
 That means you can make a definite decision.

752
00:56:45,000 --> 00:56:47,000
 The highest, the worst.

753
00:56:47,000 --> 00:56:49,000
 For example, earlier we talked about this.

754
00:56:49,000 --> 00:56:51,000
 50, 50 percent.

755
00:56:51,000 --> 00:56:53,000
 PI equal to one divided by two divided by two.

756
00:56:53,000 --> 00:56:55,000
 H equal to one.

757
00:56:55,000 --> 00:56:57,000
 That is the maximum randomness.

758
00:56:57,000 --> 00:57:01,000
 You want to have one equal to zero, one equal to one.

759
00:57:01,000 --> 00:57:03,000
 Then you have zero entropy.

760
00:57:04,000 --> 00:57:10,000
 So the probability based on, for example, how many percent of the data is positive.

761
00:57:10,000 --> 00:57:12,000
 Here is one over three.

762
00:57:12,000 --> 00:57:15,000
 The other is two over three.

763
00:57:15,000 --> 00:57:18,000
 So that will be the PI.

764
00:57:18,000 --> 00:57:20,000
 So there is one possible measure.

765
00:57:20,000 --> 00:57:22,000
 We'll talk about it.

766
00:57:22,000 --> 00:57:24,000
 So there is a decision tree.

767
00:57:24,000 --> 00:57:30,000
 Again, just to summarize again, you basically construct a tree based on a training set,

768
00:57:30,000 --> 00:57:34,000
 which you know the class label, the one that we saw, six of them.

769
00:57:34,000 --> 00:57:37,000
 And then it's a top-down approach.

770
00:57:37,000 --> 00:57:43,000
 By first getting all the data on the group nodes,

771
00:57:43,000 --> 00:57:50,000
 then you start to recursively partition them into smaller set by asking questions.

772
00:57:50,000 --> 00:57:57,000
 And if all the samples in the subset like this belong to the same class,

773
00:57:57,000 --> 00:58:00,000
 then the node becomes a leaf node here.

774
00:58:00,000 --> 00:58:07,000
 Then you label the class label with the label of this all the sample, minus and plus, the green color.

775
00:58:07,000 --> 00:58:12,000
 So if not, you continue to divide the data based on attributes.

776
00:58:12,000 --> 00:58:17,000
 And the measure you can use is you can use the error rate that we thought about it,

777
00:58:17,000 --> 00:58:21,000
 or you can use the entropy or some other we are going to introduce later.

778
00:58:21,000 --> 00:58:27,000
 So select what will be the feature which give you the best result.

779
00:58:27,000 --> 00:58:38,000
 And then you continue to the process until at the end you all result at the so-called one pure class label.

780
00:58:38,000 --> 00:58:41,000
 Then you can make final decision.

781
00:58:41,000 --> 00:58:44,000
 But there will be cases that regardless of what you do,

782
00:58:44,000 --> 00:58:49,000
 you may not be able to tell them apart due to perhaps noise in the data set

783
00:58:49,000 --> 00:58:52,000
 or missing so-called data attribute.

784
00:58:52,000 --> 00:58:55,000
 You will not be able to make all the decisions.

785
00:58:55,000 --> 00:58:59,000
 So there is a decision tree construction process.

786
00:58:59,000 --> 00:59:06,000
 So then let's look at what are the attributes that you can choose.

787
00:59:06,000 --> 00:59:10,000
 The one that we thought about error rate, then we thought about entropy,

788
00:59:10,000 --> 00:59:13,000
 which is the information gain.

789
00:59:13,000 --> 00:59:17,000
 And then there are the gain ratio or Gini index.

790
00:59:17,000 --> 00:59:24,000
 So these are the some feature selection measure you can use when you build your tree.

791
00:59:24,000 --> 00:59:29,000
 So consider D is a set of all the training sample.

792
00:59:29,000 --> 00:59:36,000
 In this case, you have six of them whose class label have M different values,

793
00:59:36,000 --> 00:59:39,000
 representing the M distinct classes.

794
00:59:39,000 --> 00:59:42,000
 In this case, how many classes you have?

795
00:59:42,000 --> 00:59:49,000
 M equal to classes 2.

796
00:59:49,000 --> 00:59:58,000
 So that is the part that unfortunately human is not so-called as good as computer to tell what is what.

797
00:59:58,000 --> 01:00:00,000
 So this is actually CI.

798
01:00:00,000 --> 01:00:02,000
 You only have two classes.

799
01:00:02,000 --> 01:00:05,000
 Earlier on it's COVID or no COVID.

800
01:00:05,000 --> 01:00:08,000
 So I equal to 2 for this particular case.

801
01:00:08,000 --> 01:00:11,000
 Not the number or sample.

802
01:00:11,000 --> 01:00:17,000
 Let CID be the set of sample of class CI in D.

803
01:00:17,000 --> 01:00:20,000
 For example, in this case, the D is the root node.

804
01:00:20,000 --> 01:00:23,000
 Be the set of sample of class.

805
01:00:23,000 --> 01:00:25,000
 You have class positive and negative.

806
01:00:25,000 --> 01:00:29,000
 So for CID positive, you have three of them.

807
01:00:29,000 --> 01:00:33,000
 CID negative, you have another three of them.

808
01:00:33,000 --> 01:00:40,000
 So this so-called number, the cardinal value, D,

809
01:00:40,000 --> 01:00:42,000
 is six.

810
01:00:42,000 --> 01:00:46,000
 And this one depends on positive or negative class.

811
01:00:46,000 --> 01:00:49,000
 You have three and three respectively.

812
01:00:49,000 --> 01:00:54,000
 We present the number or sample in D and CID.

813
01:00:54,000 --> 01:01:03,000
 CID is the number or sample which have a particular class type.

814
01:01:03,000 --> 01:01:09,000
 So once you define this, then you can compute the information gain,

815
01:01:09,000 --> 01:01:13,000
 which is one attribute selection measure.

816
01:01:13,000 --> 01:01:21,000
 So information gain is used by this so-called ID3 algorithm.

817
01:01:21,000 --> 01:01:26,000
 ID3 is a very popular decision tree,

818
01:01:26,000 --> 01:01:31,000
 which was so-called proposed by Quinland in 1986.

819
01:01:31,000 --> 01:01:35,000
 I think he is a professor in New Zealand.

820
01:01:35,000 --> 01:01:40,000
 He was very famous and even started a company, consultant company,

821
01:01:40,000 --> 01:01:47,000
 actually helped a lot of companies to decide decision tree to solve their problem.

822
01:01:47,000 --> 01:01:53,000
 So basically ID3 algorithm minimizes the information,

823
01:01:53,000 --> 01:01:56,000
 which is entropy or uncertainty,

824
01:01:56,000 --> 01:02:01,000
 in the resulting partition to achieve the best randomness or,

825
01:02:01,000 --> 01:02:04,000
 sorry, the least randomness or impurity.

826
01:02:04,000 --> 01:02:07,000
 So basically you look at an earlier example.

827
01:02:07,000 --> 01:02:12,000
 We want the resulting class as pure as possible in this case.

828
01:02:12,000 --> 01:02:15,000
 They are all belonging to the same class,

829
01:02:15,000 --> 01:02:19,000
 which is all very pure, least random.

830
01:02:19,000 --> 01:02:23,000
 And these are the cases you want to avoid as much as possible

831
01:02:23,000 --> 01:02:27,000
 because they are the most random for a two-class problem,

832
01:02:27,000 --> 01:02:31,000
 50% chance you get it right or wrong.

833
01:02:31,000 --> 01:02:35,000
 Similarly, these are the cases you try to avoid if possible.

834
01:02:35,000 --> 01:02:41,000
 This is a better, look at this, this 50% is here,

835
01:02:41,000 --> 01:02:45,000
 it's the highest so-called randomness or entropy.

836
01:02:45,000 --> 01:02:51,000
 If you have this 1, 3rd and 2, 3rd, basically you are here.

837
01:02:51,000 --> 01:03:00,000
 33% here, so your entropy drop, which is better, less random.

838
01:03:00,000 --> 01:03:05,000
 So this is the information gain proposed by Quinglin

839
01:03:05,000 --> 01:03:09,000
 in the ID3 algorithm.

840
01:03:09,000 --> 01:03:13,000
 So basically after you partition, first is you base on all the data set

841
01:03:13,000 --> 01:03:16,000
 in 4D, which is 3 positive, 3 negative,

842
01:03:16,000 --> 01:03:19,000
 in the example we showed earlier.

843
01:03:19,000 --> 01:03:25,000
 Then PI, you have two classes, I1 to 2, for P1 you have 1 divided by 2,

844
01:03:25,000 --> 01:03:27,000
 50%, 3 divided by 6, right?

845
01:03:27,000 --> 01:03:29,000
 For P2 you have the same thing.

846
01:03:29,000 --> 01:03:35,000
 So, and then this is how you compute the information in all the training data

847
01:03:35,000 --> 01:03:39,000
 before you partition them by asking questions.

848
01:03:39,000 --> 01:03:45,000
 So first you compute that, then you test all the attributes,

849
01:03:45,000 --> 01:03:48,000
 you first start with tionist.

850
01:03:48,000 --> 01:03:55,000
 So when you ask the tionist, then the D will split into two subset in here,

851
01:03:55,000 --> 01:03:58,000
 so it could be multiple D1 to Db.

852
01:03:58,000 --> 01:04:01,000
 Early on we had two subset.

853
01:04:01,000 --> 01:04:05,000
 I think I have this here, yeah, two subset.

854
01:04:05,000 --> 01:04:14,000
 So D1 is the subset we know, D2 is a subset we yes.

855
01:04:14,000 --> 01:04:22,000
 Then now rather than in 4D, now you have in 4D1 or DS and Dno.

856
01:04:22,000 --> 01:04:31,000
 So DS is actually one third with no COVID, two third with COVID.

857
01:04:31,000 --> 01:04:38,000
 So you plug into the formula again, but now your P1, P2 is one third and two third.

858
01:04:38,000 --> 01:04:45,000
 Okay, that is for the case of D2 with the answer or DS with the answer equal to yes.

859
01:04:45,000 --> 01:04:50,000
 Okay, so you compute the info and you know that there are three,

860
01:04:50,000 --> 01:04:53,000
 two percent or sample come here, so three there by six.

861
01:04:53,000 --> 01:04:59,000
 So this is how you compute, because each of them you can compute in 4DJ.

862
01:04:59,000 --> 01:05:04,000
 And two of them you just do the weighted sum, sum them up,

863
01:05:04,000 --> 01:05:10,000
 then there will be your information after you ask the question

864
01:05:10,000 --> 01:05:13,000
 about whether the person is tired or not.

865
01:05:13,000 --> 01:05:16,000
 In general, this number will reduce.

866
01:05:16,000 --> 01:05:19,000
 Okay, it's the same or reduced.

867
01:05:19,000 --> 01:05:23,000
 So you want the reduction as high as possible.

868
01:05:23,000 --> 01:05:29,000
 Basically you want ultimately this info AD, hopefully at the end is zero.

869
01:05:29,000 --> 01:05:34,000
 That means no randomness, very pure.

870
01:05:34,000 --> 01:05:41,000
 Okay, so then the gain is, the gain, the internal information gain is in 4D,

871
01:05:41,000 --> 01:05:48,000
 the original information in all the data minus info AD after you ask attribute A.

872
01:05:49,000 --> 01:05:58,000
 Okay, so then the ID tree algorithm you apply it recursively on each of the resulting partition.

873
01:05:58,000 --> 01:06:04,000
 Like early on, after you do one, you do the second until at the end.

874
01:06:04,000 --> 01:06:12,000
 Now fever, tired until at the end, all the sample belong to a same class

875
01:06:12,000 --> 01:06:17,000
 or there is no further gain possible around the feature

876
01:06:17,000 --> 01:06:23,000
 or the feature they use would not reduce the information further.

877
01:06:23,000 --> 01:06:28,000
 And that is the end of the classification.

878
01:06:28,000 --> 01:06:32,000
 So beside two, you could also have three classes here.

879
01:06:32,000 --> 01:06:36,000
 If you ask a fever, then V will become three.

880
01:06:36,000 --> 01:06:39,000
 So just make sure you know how to look at this number.

881
01:06:39,000 --> 01:06:40,000
 Now V is three.

882
01:06:40,000 --> 01:06:43,000
 So D1 now is a no class.

883
01:06:43,000 --> 01:06:45,000
 You have two out of six.

884
01:06:45,000 --> 01:06:50,000
 Then you compute the information D0, which is zero because it's very pure.

885
01:06:50,000 --> 01:06:53,000
 Then Dm is also zero.

886
01:06:53,000 --> 01:06:57,000
 So at the end, you also have two out of three for this fever.

887
01:06:57,000 --> 01:07:04,000
 With yes answer, info D is actually one.

888
01:07:05,000 --> 01:07:14,000
 So ultimately this information AD for fever will be two divided by six times one.

889
01:07:14,000 --> 01:07:16,000
 So it's one third.

890
01:07:16,000 --> 01:07:19,000
 Same as the error rate that we mentioned earlier.

891
01:07:19,000 --> 01:07:22,000
 So this is how you compute.

892
01:07:22,000 --> 01:07:25,000
 Let's look at one example.

893
01:07:25,000 --> 01:07:33,000
 So you see just with a 14 number, now if I ask you with four features,

894
01:07:33,000 --> 01:07:39,000
 now you want to decide whether the patient is infected with the flu virus.

895
01:07:39,000 --> 01:07:42,000
 Now you have a more simple, right?

896
01:07:42,000 --> 01:07:46,000
 Additional sort through feature.

897
01:07:46,000 --> 01:07:51,000
 Now you need to compute.

898
01:07:51,000 --> 01:07:52,000
 So you will try out.

899
01:07:52,000 --> 01:08:02,000
 You find that it's not as simple as we just saw.

900
01:08:02,000 --> 01:08:10,000
 First you compute the information D, which is the case that you look at the classes of flu.

901
01:08:10,000 --> 01:08:20,000
 You find that they are one, two, three, four, five negative and nine positive.

902
01:08:20,000 --> 01:08:23,000
 Therefore the information D, so they tell you what.

903
01:08:23,000 --> 01:08:28,000
 Without looking at any feature, if you had to do a random guess,

904
01:08:28,000 --> 01:08:32,000
 they are most likely the next patient would have flu, right?

905
01:08:32,000 --> 01:08:34,000
 Maybe a flu season.

906
01:08:34,000 --> 01:08:39,000
 So then the info D is nine divided by 14.

907
01:08:39,000 --> 01:08:43,000
 This is the P positive, the probability.

908
01:08:43,000 --> 01:08:46,000
 Locked two, nine divided by 15.

909
01:08:46,000 --> 01:08:50,000
 There's entropy minus summation PI lock, PI.

910
01:08:51,000 --> 01:08:59,000
 Then five with negative class, then you find that this information D had 0.94.

911
01:08:59,000 --> 01:09:03,000
 Entropy, the unit is bits.

912
01:09:03,000 --> 01:09:12,000
 They tell you how many bits in general you need to tell the result apart.

913
01:09:12,000 --> 01:09:16,000
 Of course, if you do not write the unit bits, it's also okay to write the number.

914
01:09:16,000 --> 01:09:19,000
 You understood that this is a bits.

915
01:09:19,000 --> 01:09:22,000
 Then look at the fever.

916
01:09:22,000 --> 01:09:27,000
 The fever now is whether this will further reduce the info D.

917
01:09:27,000 --> 01:09:31,000
 This is the worst scenario without asking any question.

918
01:09:31,000 --> 01:09:36,000
 This is so-called the randomness of 0.94, very close to one.

919
01:09:36,000 --> 01:09:40,000
 One is the highest for a two class problem.

920
01:09:40,000 --> 01:09:44,000
 Now you have this fever.

921
01:09:44,000 --> 01:09:47,000
 With a fever, you have three outcomes.

922
01:09:47,000 --> 01:09:55,000
 No, which you have five of them, then you have a mild, which you have four of them, then yes, you have five of them.

923
01:09:55,000 --> 01:10:02,000
 So five, four, five, which is the DJ, the other by Lee, this number.

924
01:10:02,000 --> 01:10:09,000
 You break it into three subsets based on the outcome of no, mild, and yes.

925
01:10:09,000 --> 01:10:13,000
 So five, four, five, okay?

926
01:10:13,000 --> 01:10:16,000
 Five, four, five, total 14 of them.

927
01:10:16,000 --> 01:10:20,000
 So when you have five of them, you look at the no classes.

928
01:10:20,000 --> 01:10:26,000
 You look at the flu, you have three negative and two positive.

929
01:10:26,000 --> 01:10:33,000
 So this is entropy of three negative, two positive, two divided by five, three divided by five.

930
01:10:33,000 --> 01:10:37,000
 Plus, when you have mild, all of them are positive.

931
01:10:37,000 --> 01:10:44,000
 So this entropy will be four, four, which will give you a zero because log two, one is zero.

932
01:10:44,000 --> 01:10:45,000
 So this is zero.

933
01:10:45,000 --> 01:10:48,000
 So the mild class immediately disappear.

934
01:10:48,000 --> 01:10:54,000
 You can consider this is very close to the error rate that we were discussing earlier.

935
01:10:54,000 --> 01:10:56,000
 Then we have the yes class.

936
01:10:56,000 --> 01:11:01,000
 You have again three positive and two negative.

937
01:11:01,000 --> 01:11:02,000
 Then you have this entropy.

938
01:11:02,000 --> 01:11:09,000
 So you do the weighted sum based on the number of per patient with fever, no, mild, yes.

939
01:11:09,000 --> 01:11:13,000
 Then you get 0.694 bits.

940
01:11:13,000 --> 01:11:14,000
 So which is better, right?

941
01:11:14,000 --> 01:11:23,000
 So asking the first question, you have reduced the randomness from 0.94 down to 0.694.

942
01:11:23,000 --> 01:11:25,000
 Improved, right?

943
01:11:25,000 --> 01:11:29,000
 If you ask a question, turn out the number remain the same.

944
01:11:29,000 --> 01:11:32,000
 What does it tell you?

945
01:11:32,000 --> 01:11:34,000
 What does it tell you?

946
01:11:34,000 --> 01:11:42,000
 If you ask a feature X, turn out your compute based on this in 4XD, turn out the number equal to 0.94.

947
01:11:43,000 --> 01:11:49,000
 What does it mean?

948
01:11:49,000 --> 01:11:53,000
 What does it mean?

949
01:11:53,000 --> 01:11:57,000
 No, no, no.

950
01:11:57,000 --> 01:11:59,000
 It doesn't change the entropy.

951
01:11:59,000 --> 01:12:02,000
 The so-called information doesn't change.

952
01:12:02,000 --> 01:12:09,000
 Assume I have one feature X, then I compute the in 4XD, right?

953
01:12:09,000 --> 01:12:13,000
 And the in X may come to me three or even four outcome.

954
01:12:13,000 --> 01:12:19,000
 I compute this, I find that output the same as 0.940 bits.

955
01:12:19,000 --> 01:12:24,000
 So what does it mean physically?

956
01:12:24,000 --> 01:12:31,000
 The feature is useless to make you make a better decision.

957
01:12:31,000 --> 01:12:35,000
 It doesn't reduce the randomness at all.

958
01:12:35,000 --> 01:12:38,000
 You do not really need the feature at all.

959
01:12:38,000 --> 01:12:41,000
 It does not help you to make a better decision.

960
01:12:41,000 --> 01:12:44,000
 The information is still as random.

961
01:12:44,000 --> 01:12:47,000
 Basically, the gain is zero, right?

962
01:12:47,000 --> 01:12:55,000
 Early on the fever, if I compute the gain, it's 0.94 minus 0.694, I have the gain.

963
01:12:55,000 --> 01:12:59,000
 The gain means that the amount of randomness I can reduce.

964
01:12:59,000 --> 01:13:00,000
 Okay?

965
01:13:00,000 --> 01:13:04,000
 If this one turns out to be the same, the gain will be zero.

966
01:13:04,000 --> 01:13:09,000
 That means I do not reduce any randomness at all.

967
01:13:09,000 --> 01:13:16,000
 So that means the feature is very much useless for me to make a better decision.

968
01:13:16,000 --> 01:13:20,000
 Okay?

969
01:13:20,000 --> 01:13:28,000
 And similarly, you can compute the gain by going through the same process for cough,

970
01:13:28,000 --> 01:13:32,000
 salt truth, and tiredness, right?

971
01:13:32,000 --> 01:13:42,000
 So the final result of the gain is 0.029 for cough, 0.151 for salt truth,

972
01:13:42,000 --> 01:13:45,000
 and 0.048 for tiredness.

973
01:13:45,000 --> 01:13:46,000
 Right?

974
01:13:46,000 --> 01:13:50,000
 Look at which one gives you the highest gain.

975
01:13:50,000 --> 01:13:54,000
 It's the fever feature, right?

976
01:13:54,000 --> 01:14:01,000
 And with that, the fever will be select as the first attribute you are going to use

977
01:14:01,000 --> 01:14:05,000
 to split the trees into subsets.

978
01:14:05,000 --> 01:14:09,000
 It will be the root note question.

979
01:14:09,000 --> 01:14:10,000
 Okay?

980
01:14:10,000 --> 01:14:15,000
 So that is a very systematic and determined way.

981
01:14:15,000 --> 01:14:19,000
 Once you have the data, you just go through the programming.

982
01:14:19,000 --> 01:14:22,000
 You compute the one, give you the list.

983
01:14:22,000 --> 01:14:27,000
 Select the one as the feature to ask the first question.

984
01:14:27,000 --> 01:14:28,000
 Okay?

985
01:14:28,000 --> 01:14:30,000
 Now, the second question.

986
01:14:30,000 --> 01:14:37,000
 If among all the features, I have two features, give me exactly the same gain, not zero,

987
01:14:37,000 --> 01:14:41,000
 but both of them give me 0.246.

988
01:14:41,000 --> 01:14:46,000
 In fact, I said one question like that before.

989
01:14:46,000 --> 01:14:48,000
 Then they say, okay, which is the choose?

990
01:14:48,000 --> 01:14:49,000
 Now, I have two.

991
01:14:49,000 --> 01:14:54,000
 Give me exactly the same gain.

992
01:14:54,000 --> 01:14:56,000
 Which one?

993
01:14:56,000 --> 01:15:04,000
 Which one would you choose as the first question to ask?

994
01:15:04,000 --> 01:15:10,000
 If two different features among maybe 100 of the features, two of them give you exactly

995
01:15:10,000 --> 01:15:16,000
 the same information gain, not zero, but the highest.

996
01:15:16,000 --> 01:15:26,000
 Which feature of these two that you will select to ask the first question?

997
01:15:26,000 --> 01:15:27,000
 Both are okay, yes.

998
01:15:27,000 --> 01:15:37,000
 For example, basically, each of them, of course, give you the same so-called information to

999
01:15:37,000 --> 01:15:39,000
 make better decision.

1000
01:15:39,000 --> 01:15:45,000
 But if I try to be critical, I say, no, you must select one for me.

1001
01:15:45,000 --> 01:15:48,000
 Two of them, right?

1002
01:15:48,000 --> 01:15:49,000
 Yeah.

1003
01:15:49,000 --> 01:15:55,000
 In the first question, they may give you the same information, but it all depends on the

1004
01:15:55,000 --> 01:15:57,000
 foreign feature.

1005
01:15:57,000 --> 01:16:03,000
 Ultimately, you want to build the whole three to see at the end which give you the least

1006
01:16:03,000 --> 01:16:05,000
 randomness at the end.

1007
01:16:05,000 --> 01:16:11,000
 They may appear the same in the first question, but if you start to build two, three, you

1008
01:16:11,000 --> 01:16:15,000
 will find that one may eventually give you a better outcome.

1009
01:16:15,000 --> 01:16:20,000
 If you look at the first step, you may not be able to tell them apart, then you can continue

1010
01:16:20,000 --> 01:16:28,000
 to build two different trees, but they are not exactly the same after the first question.

1011
01:16:28,000 --> 01:16:33,000
 So these are things that, of course, will make the question more interesting.

1012
01:16:33,000 --> 01:16:35,000
 Yes.

1013
01:16:35,000 --> 01:16:40,000
 You don't have to combine.

1014
01:16:40,000 --> 01:16:42,000
 Combine subset?

1015
01:16:42,000 --> 01:16:49,000
 You don't have to combine.

1016
01:16:49,000 --> 01:16:52,000
 Just choose the one, give you the least random.

1017
01:16:52,000 --> 01:16:54,000
 If you're only allowed to use one, right?

1018
01:16:54,000 --> 01:16:59,000
 If you can use multiple of them, that is the concept of random forest.

1019
01:16:59,000 --> 01:17:01,000
 You can build a few of them.

1020
01:17:01,000 --> 01:17:03,000
 The first one is the least.

1021
01:17:03,000 --> 01:17:08,000
 The idea of random forest is you may not be able to tell, although the first one gives

1022
01:17:08,000 --> 01:17:14,000
 you the least, the most gain, but it may not be always the case when you go further down.

1023
01:17:14,000 --> 01:17:17,000
 Then you can build based on this.

1024
01:17:17,000 --> 01:17:22,000
 You can actually start with all so-called different features, build different trees.

1025
01:17:22,000 --> 01:17:26,000
 You can have four of them, four different trees.

1026
01:17:26,000 --> 01:17:28,000
 Each of them you ask a question.

1027
01:17:28,000 --> 01:17:33,000
 At the end, you'll see the classification, maybe three, one, and two give you class no.

1028
01:17:33,000 --> 01:17:38,000
 Three give you class yes, and then five, four give you class no.

1029
01:17:38,000 --> 01:17:39,000
 The majority of them is no.

1030
01:17:39,000 --> 01:17:41,000
 Then you say no.

1031
01:17:41,000 --> 01:17:44,000
 You don't have to combine them somehow.

1032
01:17:44,000 --> 01:17:47,000
 Each of them can make their independent decision.

1033
01:17:47,000 --> 01:17:57,000
 So then, of course, then you can try to see which at the end give you the most robust answer.

1034
01:17:57,000 --> 01:18:03,000
 Because after you select the fever, and then, of course, the job is not done yet, you split

1035
01:18:03,000 --> 01:18:10,000
 them now, the original data 14 of them, now you split into three groups.

1036
01:18:10,000 --> 01:18:18,000
 This is one with no fever, one with mild fever, and then the with yes fever.

1037
01:18:18,000 --> 01:18:24,000
 Then, of course, this one, because the information is zero, you can make this already become a

1038
01:18:24,000 --> 01:18:27,000
 no, because all of them is positive.

1039
01:18:27,000 --> 01:18:33,000
 This one, the job not done yet, you can further divide them by, if you look carefully, the

1040
01:18:33,000 --> 01:18:41,000
 through three minus two positive actually can be differentiated by sort through.

1041
01:18:41,000 --> 01:18:47,000
 So you should ask a question in the second level for this branch is sort through.

1042
01:18:47,000 --> 01:18:54,000
 In the attribute, they will give you maximum information gain.

1043
01:18:54,000 --> 01:19:01,000
 And this one, you have two, three positive, two no, you can ask a tiredness, which allow

1044
01:19:01,000 --> 01:19:07,000
 you to further break it into positive and negative class.

1045
01:19:07,000 --> 01:19:17,000
 So after the two level, then you are going to arrive at zero randomness.

1046
01:19:17,000 --> 01:19:20,000
 I've got that information gain.

1047
01:19:20,000 --> 01:19:31,000
 But then, and the same guy, Quinglin found that, actually, this information gain will favor

1048
01:19:31,000 --> 01:19:34,000
 if you have more and more classes.

1049
01:19:34,000 --> 01:19:42,000
 The attribute have more outcome, like you will give you a lowest information, give you

1050
01:19:42,000 --> 01:19:44,000
 a higher gain.

1051
01:19:44,000 --> 01:19:46,000
 So that is not fair.

1052
01:19:46,000 --> 01:19:52,000
 Should I favor the feature with fewer outcome or more outcome?

1053
01:19:52,000 --> 01:20:02,000
 Then in 1993, after seven years that he proposed the ID tree, he come out with this C4.5.

1054
01:20:02,000 --> 01:20:08,000
 So I asked why so the number looks so strange, three, 4.5.

1055
01:20:08,000 --> 01:20:14,000
 Because these are the different version of the tree that he have tried to change the attribute

1056
01:20:14,000 --> 01:20:16,000
 here, the gain here and there.

1057
01:20:16,000 --> 01:20:18,000
 So come out with different version.

1058
01:20:18,000 --> 01:20:24,000
 And then they find this 4.5 is the best so-called compromise result.

1059
01:20:24,000 --> 01:20:32,000
 A successor of ID tree do the gain ratio, the information gain.

1060
01:20:32,000 --> 01:20:36,000
 If further, gain A was the one we compute early.

1061
01:20:36,000 --> 01:20:43,000
 But then they penalize, he penalized the gain A by the split in four, which is based on the

1062
01:20:43,000 --> 01:20:48,000
 number of so-called classes that you have.

1063
01:20:48,000 --> 01:20:53,000
 So let's say, early on you have, we have two examples, two class and three class, right?

1064
01:20:53,000 --> 01:21:00,000
 Here, the information gain A, if you apply this diner, you only have two classes.

1065
01:21:00,000 --> 01:21:06,000
 You have to divide this gain A, you compute early by this split in four, which now you

1066
01:21:06,000 --> 01:21:12,000
 have only two, one to two, which each of them is three divided by six, three divided by

1067
01:21:12,000 --> 01:21:14,000
 six, so divided by one.

1068
01:21:14,000 --> 01:21:20,000
 But when you have three classes, then you have now two, two, two.

1069
01:21:20,000 --> 01:21:28,000
 Then you compute the information gain, you divide this by a larger number, more than one.

1070
01:21:28,000 --> 01:21:35,000
 So that means he penalized the output with more classes.

1071
01:21:35,000 --> 01:21:44,000
 The same way to compute the gain A earlier, but then you normalize it by the number of

1072
01:21:45,000 --> 01:21:49,000
 classes, the number of sample belong to each classes that you have.

1073
01:21:49,000 --> 01:21:53,000
 And that is called gain ratio.

1074
01:21:53,000 --> 01:22:02,000
 You find that for this case, it can normalize the output, not so that they do not just favor

1075
01:22:02,000 --> 01:22:06,000
 the decision to attribute with more classes.

1076
01:22:06,000 --> 01:22:10,000
 So here is the continuing example that you have earlier.

1077
01:22:10,000 --> 01:22:25,000
 You look at the cough, the cough give you four years, then five, six miles, and then four

1078
01:22:25,000 --> 01:22:26,000
 no.

1079
01:22:26,000 --> 01:22:33,000
 Then the split info with the cough is four, six, four divided by 14.

1080
01:22:33,000 --> 01:22:35,000
 So PI log two, PI a three.

1081
01:22:35,000 --> 01:22:39,000
 So the gain, the split info is 1.557.

1082
01:22:39,000 --> 01:22:42,000
 So early on, you have 0.029.

1083
01:22:42,000 --> 01:22:47,000
 Now you divide it by a higher number, you further reduce the gain.

1084
01:22:47,000 --> 01:22:52,000
 So when you have more classes, when you only have two classes, the maximum number of the

1085
01:22:52,000 --> 01:22:55,000
 split info is one.

1086
01:22:55,000 --> 01:23:00,000
 If you have more than two classes, this number will continue to increase unless the class

1087
01:23:00,000 --> 01:23:06,000
 number is really skewed towards a particular class type.

1088
01:23:06,000 --> 01:23:13,000
 Otherwise, when you have more classes, this split info will likely going to increase.

1089
01:23:13,000 --> 01:23:21,000
 Then you penalize the gain and least favored feature with more classes.

1090
01:23:21,000 --> 01:23:25,000
 So there is gain ratio.

1091
01:23:25,000 --> 01:23:35,000
 Then there is another Gini index, which is another type of attribute measure.

1092
01:23:35,000 --> 01:23:38,000
 So this one is used in CUT.

1093
01:23:38,000 --> 01:23:43,000
 Sometimes you see this decision tree based on algorithm CUT.

1094
01:23:43,000 --> 01:23:58,000
 You know that attribute selection feature measure it uses Gini index.

1095
01:23:58,000 --> 01:24:06,000
 Gini index is easier because PI is the probability that in D belong to class CI.

1096
01:24:06,000 --> 01:24:08,000
 We thought about this, right?

1097
01:24:08,000 --> 01:24:16,000
 If you have two classes, the number three divided by six, then you have P1 equal to 0.5, P2 equal to 0.5.

1098
01:24:16,000 --> 01:24:25,000
 Then the Gini index, for example, we showed earlier with six patients, will be one minus one divided by four minus one divided by four.

1099
01:24:25,000 --> 01:24:36,000
 So it's going to be equal to one divided by two, which is the maximum number that you have for Gini index for two classes.

1100
01:24:36,000 --> 01:24:41,000
 So one good thing is in here, you do not need to use a log two.

1101
01:24:41,000 --> 01:24:48,000
 It's just a power so-called calculation.

1102
01:24:48,000 --> 01:24:56,000
 But for Gini index, it's only considered binary split because you only have PI one to two.

1103
01:24:56,000 --> 01:25:02,000
 So that means your outcome can only allow you to have two classes.

1104
01:25:02,000 --> 01:25:12,000
 I mean, the feature can only have two possible answers, yes and no, or positive or negative, or one or two.

1105
01:25:12,000 --> 01:25:21,000
 So then the Gini index after the feature A with two output, then assuming you partition the result into two classes,

1106
01:25:21,000 --> 01:25:31,000
 two subset with D1, D2 number, then you compute the Gini of D1, which is based on this formula.

1107
01:25:31,000 --> 01:25:38,000
 How many percent of the data belong to the positive class negative?

1108
01:25:38,000 --> 01:25:44,000
 Then weighted by how many number of sample go to this D1 class, D2 class?

1109
01:25:44,000 --> 01:25:50,000
 So the same thing as how we compute the information gain weighted, right, here.

1110
01:25:50,000 --> 01:25:54,000
 The same thing that you compute here.

1111
01:25:54,000 --> 01:25:57,000
 This is a dj divided by d.

1112
01:25:57,000 --> 01:26:05,000
 You compute the info di, but now you use a Gini di.

1113
01:26:05,000 --> 01:26:18,000
 So then you compute for all the attributes, then the one give you the highest gain will be selected as a feature to ask the question.

1114
01:26:18,000 --> 01:26:29,000
 And for attribute which you have more than two classes, then you can try to separate them into two groups.

1115
01:26:29,000 --> 01:26:31,000
 There's a different way to do it.

1116
01:26:31,000 --> 01:26:39,000
 One, you select a split point, which is all the attribute less than or equal to the split point belong to D1.

1117
01:26:39,000 --> 01:26:42,000
 And larger than that belong to D2, right.

1118
01:26:42,000 --> 01:26:53,000
 That is how you convert from a so-called ratio, so-called attribute into binary attributes.

1119
01:26:53,000 --> 01:26:55,000
 Or you can also regroup.

1120
01:26:55,000 --> 01:26:59,000
 So no, mal, and yes.

1121
01:26:59,000 --> 01:27:04,000
 Then you can say no and mal belong to D1, yes belong to D2.

1122
01:27:04,000 --> 01:27:07,000
 That's how you also convert to two classes.

1123
01:27:07,000 --> 01:27:12,000
 Or then you convert the no belong to D1, mal and yes belong to D2.

1124
01:27:12,000 --> 01:27:16,000
 There's another possible way to break it into two groups.

1125
01:27:16,000 --> 01:27:22,000
 Then with that you can use apply the Gini index.

1126
01:27:22,000 --> 01:27:30,000
 So you have to split based on one example based on the earlier patient through example.

1127
01:27:30,000 --> 01:27:36,000
 So you have 9 and 5 with the through.

1128
01:27:36,000 --> 01:27:41,000
 Then the Gini D is 0.459.

1129
01:27:41,000 --> 01:27:50,000
 Then if you now split based on the cough feature, let the cough no and mal belong to D1, yes belong to D2.

1130
01:27:51,000 --> 01:27:56,000
 Further split after asking whether the patient have cough or not.

1131
01:27:56,000 --> 01:28:01,000
 You break it into two groups, one with 10 patients with no and mal.

1132
01:28:01,000 --> 01:28:04,000
 The other four patients with yes.

1133
01:28:04,000 --> 01:28:10,000
 Then each of them you compute the Gini D1, which is the how many of them with the through or no.

1134
01:28:10,000 --> 01:28:14,000
 And G2 you get the 0.443.

1135
01:28:14,000 --> 01:28:17,000
 So that reduce the Gini index a bit.

1136
01:28:17,000 --> 01:28:23,000
 So that means this cough is a useful feature to use.

1137
01:28:23,000 --> 01:28:26,000
 But is it the best one you don't know yet?

1138
01:28:26,000 --> 01:28:30,000
 So to examine other feature go to the same calculation again.

1139
01:28:30,000 --> 01:28:38,000
 And the one give you the best so-called reduction, the highest reduction will be selected as the first feature.

1140
01:28:38,000 --> 01:28:41,000
 Then you can start to see how T-ere is it.

1141
01:28:41,000 --> 01:28:45,000
 After you select the first feature then you still have another three feature.

1142
01:28:45,000 --> 01:28:49,000
 So you can start to see which will be the next question to ask.

1143
01:28:49,000 --> 01:28:52,000
 Go to the same calculation again.

1144
01:28:52,000 --> 01:28:56,000
 At the end is very tedious.

1145
01:28:59,000 --> 01:29:04,000
 So why so many different so-called measure?

1146
01:29:04,000 --> 01:29:06,000
 Which one is better?

1147
01:29:06,000 --> 01:29:10,000
 In fact if you look closely they are not very much different.

1148
01:29:10,000 --> 01:29:14,000
 So this is the graph illustration to tell you.

1149
01:29:14,000 --> 01:29:17,000
 I'll show you this so-called entropy.

1150
01:29:17,000 --> 01:29:20,000
 Entropy the light gray color.

1151
01:29:20,000 --> 01:29:26,000
 But because this one the highest value is one different from those for the two class problem.

1152
01:29:26,000 --> 01:29:30,000
 Different from the Gini and the classification error.

1153
01:29:30,000 --> 01:29:36,000
 The one we when we went to the example I asked how many sample cannot make correct decision.

1154
01:29:36,000 --> 01:29:41,000
 There is a percentage of error like 33%, 50%.

1155
01:29:41,000 --> 01:29:48,000
 So you can see that in terms of the index they are just different kind of curve.

1156
01:29:48,000 --> 01:29:51,000
 For error is this, the triangle.

1157
01:29:51,000 --> 01:29:55,000
 And for Gini is the green color.

1158
01:29:55,000 --> 01:30:02,000
 For entropy original is this gray color but you scale them so that the maximum all the same as 0.5.

1159
01:30:02,000 --> 01:30:08,000
 You just divided this by two you get the red color.

1160
01:30:08,000 --> 01:30:13,000
 So what it tell you is they are very much performing the same way.

1161
01:30:13,000 --> 01:30:18,000
 Basically the least random they all come down to 0.

1162
01:30:18,000 --> 01:30:21,000
 The most random they give you the highest value.

1163
01:30:21,000 --> 01:30:29,000
 Just the way they reduce when you are more and more certain and this one linearly reduce.

1164
01:30:29,000 --> 01:30:35,000
 This one start slowly then it favor the one with least random.

1165
01:30:35,000 --> 01:30:40,000
 So it drop faster the rate is faster than this.

1166
01:30:40,000 --> 01:30:44,000
 And this Gini is somehow in between.

1167
01:30:44,000 --> 01:30:51,000
 So therefore for many many example that you try if your test data is not a huge data set.

1168
01:30:51,000 --> 01:30:57,000
 You find that three of them basically give you almost the same result.

1169
01:30:57,000 --> 01:31:04,000
 And but there are also cases that you'll find one slightly better than the other.

1170
01:31:04,000 --> 01:31:07,000
 But to me they are more or less the same.

1171
01:31:07,000 --> 01:31:13,000
 And if the three in the exam I ask a question I say build a decision tree.

1172
01:31:13,000 --> 01:31:19,000
 I don't restrict you on which measure to use.

1173
01:31:19,000 --> 01:31:26,000
 So which one will you select in your three building so-called exercise.

1174
01:31:27,000 --> 01:31:30,000
 Which one.

1175
01:31:30,000 --> 01:31:35,000
 Go by the error classification error which will be easier to count.

1176
01:31:35,000 --> 01:31:38,000
 You don't have to further do the log or square.

1177
01:31:38,000 --> 01:31:42,000
 Just count how many sample you still cannot make correct decision.

1178
01:31:42,000 --> 01:31:48,000
 So the blue line will be the one you don't even need to write any formula just count.

1179
01:31:48,000 --> 01:31:52,000
 Then you can decide which give you the best so-called feature.

1180
01:31:52,000 --> 01:31:55,000
 Okay.

1181
01:31:56,000 --> 01:32:00,000
 So that is a decision tree.

1182
01:32:00,000 --> 01:32:04,000
 So shall we.

1183
01:32:04,000 --> 01:32:08,000
 Okay maybe two more slides then we take a break they will come back.

1184
01:32:08,000 --> 01:32:14,000
 So three pruning of course you can build the tree of course as I say when the data set is huge.

1185
01:32:14,000 --> 01:32:20,000
 And you may create many many levels of tree and many many branches.

1186
01:32:20,000 --> 01:32:29,000
 And this could be caused by noise right when you collect the data you cannot be certain that all the data are accurate.

1187
01:32:29,000 --> 01:32:32,000
 There are no outliers right.

1188
01:32:32,000 --> 01:32:41,000
 The person that you ask a question may not provide the so-called valid information to you right.

1189
01:32:41,000 --> 01:32:45,000
 They create noise right then you make error right.

1190
01:32:45,000 --> 01:32:59,000
 So then you need to so-called prune the tree to make sure that your tree doesn't overfit those noise or those error or those outliers right.

1191
01:32:59,000 --> 01:33:04,000
 And sometimes you also want to prune the tree so that you can make a faster decision right.

1192
01:33:04,000 --> 01:33:11,000
 If you certain branch you go down to many level you only make one more correct.

1193
01:33:11,000 --> 01:33:14,000
 But you have to ask another 10 questions right.

1194
01:33:14,000 --> 01:33:22,000
 So you find it okay they may not be worthwhile to ask so many questions at the end you only get a 10% gain whatever.

1195
01:33:22,000 --> 01:33:33,000
 So you have to prune the tree that's how you make simpler decision and sometimes you actually make better decision because you might overfit noise to the tree.

1196
01:33:33,000 --> 01:33:46,000
 And sometimes you want to make a simple decision which often may give you a better clarity in terms of the decision making process.

1197
01:33:46,000 --> 01:33:50,000
 So in you have two ways to prune the tree right.

1198
01:33:50,000 --> 01:33:52,000
 One is the pre pruning the approach.

1199
01:33:52,000 --> 01:33:58,000
 Basically when you stop building the tree you find there a particular note.

1200
01:33:58,000 --> 01:34:05,000
 Note that now majority belong to one class only very small number of sample belong to another class.

1201
01:34:05,000 --> 01:34:08,000
 You will say okay I will just stop here.

1202
01:34:08,000 --> 01:34:11,000
 95% accuracy is good enough for me right.

1203
01:34:11,000 --> 01:34:16,000
 Some of the decision may not be so critical you need to have 100 right.

1204
01:34:16,000 --> 01:34:19,000
 You say okay this is I can accept right.

1205
01:34:19,000 --> 01:34:22,000
 So you just stop there you don't have to ask more questions.

1206
01:34:22,000 --> 01:34:24,000
 So it's called pre pruning.

1207
01:34:24,000 --> 01:34:31,000
 The other is you build the whole tree first ask all the questions you can ask and you look at the tree right huge.

1208
01:34:31,000 --> 01:34:41,000
 You will say okay there are certain branches here if I cut here I may not need to ask a lot of questions which just give me another 2% gain right.

1209
01:34:41,000 --> 01:34:44,000
 I think here is the one right.

1210
01:34:44,000 --> 01:34:49,000
 So you can build a tree on the left after you build everything.

1211
01:34:49,000 --> 01:34:59,000
 You look carefully you say okay I think that because there could be 95% or the sample or come here only 5% come here.

1212
01:34:59,000 --> 01:35:07,000
 But with the 5% of them come here only one sample is yes most of them are no right.

1213
01:35:07,000 --> 01:35:10,000
 So I will just stop here.

1214
01:35:10,000 --> 01:35:12,000
 I don't want to ask two more questions.

1215
01:35:12,000 --> 01:35:16,000
 2 and 3 5 I will just stop here right.

1216
01:35:16,000 --> 01:35:21,000
 So that means for all the questions come here now I can only ask two questions.

1217
01:35:21,000 --> 01:35:35,000
 I will get the result right and early on I had to ask 5 questions maximum 5 questions or possible 3 questions but you need to have the answer for all the 5 questions which path that you go right.

1218
01:35:35,000 --> 01:35:38,000
 So this is called pruning right.

1219
01:35:38,000 --> 01:35:46,000
 So you will not necessarily always give you poorer result because you will have the structure you train the tree build the tree using your training data.

1220
01:35:46,000 --> 01:35:51,000
 But ultimately you want to apply this tree to your unseen data right.

1221
01:35:51,000 --> 01:36:00,000
 So you might try both of them to the unseen data they may both give you the same result maybe even the smaller tree give you a better result.

1222
01:36:00,000 --> 01:36:04,000
 So that is the testing error right.

1223
01:36:04,000 --> 01:36:12,000
 So okay can we go through two more slides.

1224
01:36:12,000 --> 01:36:23,000
 So advantages of decision tree compared to other method we are going to look at later and they are quite inexpensive to build right.

1225
01:36:23,000 --> 01:36:44,000
 So manually is tedious but if you have a computer program you find that compared to those the neural network running which you have to go through many so called iteration to arrive at the so called minimum error the gradient descent calculation.

1226
01:36:44,000 --> 01:36:47,000
 And this is relatively easy to build right.

1227
01:36:47,000 --> 01:36:57,000
 So you try out some program you find the total moment you hit enter the tree is there right compared to the network you have to wait for half an hour sometime before you see the result.

1228
01:36:57,000 --> 01:37:10,000
 So once a decision tree is built you only need to ask the order of how maximum level of question the order of W which is the maximum level this is the worst case scenario.

1229
01:37:10,000 --> 01:37:19,000
 So you know how many level you have that is the maximum number of question you need to ask to arrive at the decision.

1230
01:37:19,000 --> 01:37:39,000
 It's also easy to interpret right it's so called the expandable when you make the decision you can go back to the tree why you make the decision or because the patient have cough because the patient also have mouth fever and you also have salt throat therefore it likely to have the flu symptom.

1231
01:37:39,000 --> 01:37:49,000
 So there are reasons that you can provide to the patient why he is likely have cough so easy to interpret.

1232
01:37:49,000 --> 01:37:58,000
 So in machine learning this call explainable right a lot of the new network you don't know why it give you the output like that.

1233
01:37:58,000 --> 01:38:08,000
 Can you explain sorry I don't know it's a black box so you don't want that just like a lot of so called medical decision right.

1234
01:38:08,000 --> 01:38:26,000
 When you tell the patient you have certain disease cancer the patient ask you why I don't know so you don't have to trust you what why why he says the new number tell me so but it's not going to buy any so called confident from them.

1235
01:38:26,000 --> 01:38:37,000
 It has to it's quite robust to noise and redundant attributes right if you have a lot of data is certain attributes are not useful to arrive at the decision depending on the branches you go down.

1236
01:38:37,000 --> 01:39:02,000
 You may not ask a question number five you may not ask this attribute those noise those redundant may not so called misled you to make a wrong decision right in the center is quite robust based on all the data if they are somehow homogeneous or consistent and you most of you get to the result even occasionally you have some noise or missing data.

1237
01:39:02,000 --> 01:39:31,000
 Can avoid overfitting the problems if you do the pruning try not to build too many levels where you build a tree can decide I want to stop at level five right then it may avoid overfitting the problem because the more you go down the tree and then the number of sample reduced and then you most likely will overfit to the training data which is not always good because the test data may not have those kind of irregularity.

1238
01:39:33,000 --> 01:39:48,000
 And you do not need to have the distribution of the attribute follow like Gaussian or Laplacian you do not need to make all the assumption you just based on the sample you have you can just build a tree regardless what kind of distribution they have.

1239
01:39:49,000 --> 01:40:00,000
 And more importantly and often if you compare the result with some new name what other approach right the result is not bad and sometimes even better if you apply random for us.

1240
01:40:00,000 --> 01:40:01,000
 Okay.

1241
01:40:01,000 --> 01:40:10,000
 Random for us is basically an ensemble classifier which leveraged the power of multiple decision tree for making better decision.

1242
01:40:12,000 --> 01:40:13,000
 Right.

1243
01:40:13,000 --> 01:40:26,000
 Early on I say that you could have a thousand of features right if we use all the thousand feature to build one tree you can think of the tree is going to be very huge and very complex.

1244
01:40:26,000 --> 01:40:31,000
 So rather than that you are not guaranteed that all the thousand features are useful.

1245
01:40:32,000 --> 01:40:33,000
 Right.

1246
01:40:33,000 --> 01:40:36,000
 So then you can use this so-called random for us.

1247
01:40:37,000 --> 01:40:39,000
 One is there are different way to build it.

1248
01:40:39,000 --> 01:40:41,000
 You build multiple trees.

1249
01:40:41,000 --> 01:40:55,000
 But one type is each tree you only build the tree using a subset of the features random subset you just select right and thousand you select 100 of them to build the tree.

1250
01:40:56,000 --> 01:41:03,000
 Then in our tree you then select another hundred of them or another 50 of them they can build 100 trees.

1251
01:41:04,000 --> 01:41:05,000
 Right.

1252
01:41:05,000 --> 01:41:14,000
 Each of them will give you a decision and then based on the majority of the decision made by the tree then you decide.

1253
01:41:14,000 --> 01:41:15,000
 Okay.

1254
01:41:15,000 --> 01:41:18,000
 Most likely this patient have this and this and this.

1255
01:41:19,000 --> 01:41:20,000
 Okay.

1256
01:41:20,000 --> 01:41:21,000
 All right.

1257
01:41:21,000 --> 01:41:27,000
 In the future you can use different sample because you could have millions of data set right.

1258
01:41:27,000 --> 01:41:38,000
 You could use all the million data set to build a tree or you could divide all the data set each time you select 1000 of them sample build one tree.

1259
01:41:38,000 --> 01:41:40,000
 Next time select randomly and a thousand of them.

1260
01:41:40,000 --> 01:41:41,000
 Build another tree.

1261
01:41:41,000 --> 01:41:42,000
 Right.

1262
01:41:42,000 --> 01:41:49,000
 You can replicate some of the sample called the replacement you select by allowed to put back the next tree you can select the sample again.

1263
01:41:49,000 --> 01:41:55,000
 Then you build three they call backing or bootstrap aggregating.

1264
01:41:56,000 --> 01:41:59,000
 Then each tree you can make a decision.

1265
01:42:00,000 --> 01:42:08,000
 Then you have multiple tree you have multiple decision then you just select out of this multiple and three that's called wisdom or crowd.

1266
01:42:08,000 --> 01:42:09,000
 Right.

1267
01:42:09,000 --> 01:42:16,000
 Sometimes you see those show they cannot make decision they decide to ask their friends or ask the audience.

1268
01:42:16,000 --> 01:42:23,000
 Tell me the answer then the one with the most choices like the will be a good decision to make.

1269
01:42:23,000 --> 01:42:25,000
 It's called wisdom or crowd.

1270
01:42:25,000 --> 01:42:37,000
 So random forest can be more accurate than decision trees and it can handle missing data because if you have a sample that which you know have all the attribute.

1271
01:42:37,000 --> 01:42:47,000
 But some tree would have some of the feature you have they can make decision some tree cannot make decision no output but you still have partial three can make decision.

1272
01:42:47,000 --> 01:42:50,000
 So that means you do not need all the feature it can be.

1273
01:42:50,000 --> 01:43:03,000
 It can handle missing data well can handle outlier well so that often that random forest is one of the popular method even today if many questions you can do.

1274
01:43:03,000 --> 01:43:05,000
 Okay let's take a break here.

1275
01:43:05,000 --> 01:43:10,000
 15 minutes break come back at 2.30 they will continue the rest.

1276
01:43:12,000 --> 01:43:14,000
 Come back at 2.30.

1277
01:43:33,000 --> 01:43:34,000
 Okay.

1278
01:44:03,000 --> 01:44:04,000
 Okay.

1279
01:44:33,000 --> 01:44:34,000
 Okay.

1280
01:45:03,000 --> 01:45:04,000
 Okay.

1281
01:45:33,000 --> 01:45:34,000
 Okay.

1282
01:46:03,000 --> 01:46:04,000
 Okay.

1283
01:46:33,000 --> 01:46:34,000
 Okay.

1284
01:47:03,000 --> 01:47:05,000
 Okay.

1285
01:47:33,000 --> 01:47:34,000
 Okay.

1286
01:48:03,000 --> 01:48:05,000
 Okay.

1287
01:48:33,000 --> 01:48:34,000
 Okay.

1288
01:49:03,000 --> 01:49:05,000
 Okay.

1289
01:49:33,000 --> 01:49:35,000
 Okay.

1290
01:50:03,000 --> 01:50:05,000
 Okay.

1291
01:50:33,000 --> 01:50:35,000
 Okay.

1292
01:51:03,000 --> 01:51:05,000
 Okay.

1293
01:51:33,000 --> 01:51:35,000
 Okay.

1294
01:52:03,000 --> 01:52:05,000
 Okay.

1295
01:52:33,000 --> 01:52:35,000
 Okay.

1296
01:53:03,000 --> 01:53:05,000
 Okay.

1297
01:53:33,000 --> 01:53:35,000
 Okay.

1298
01:54:03,000 --> 01:54:05,000
 Okay.

1299
01:54:33,000 --> 01:54:35,000
 Okay.

1300
01:55:03,000 --> 01:55:05,000
 Okay.

1301
01:55:33,000 --> 01:55:35,000
 Okay.

1302
01:56:03,000 --> 01:56:05,000
 Okay.

1303
01:56:33,000 --> 01:56:35,000
 Okay.

1304
01:57:03,000 --> 01:57:05,000
 Okay.

1305
01:57:33,000 --> 01:57:35,000
 Okay.

1306
01:58:03,000 --> 01:58:05,000
 Okay.

1307
01:58:33,000 --> 01:58:35,000
 Okay.

1308
01:59:03,000 --> 01:59:05,000
 Okay.

1309
01:59:33,000 --> 01:59:35,000
 Okay.

1310
02:00:03,000 --> 02:00:05,000
 Okay.

1311
02:00:05,000 --> 02:00:07,000
 Okay.

1312
02:00:07,000 --> 02:00:09,000
 Okay.

1313
02:00:09,000 --> 02:00:11,000
 Okay.

1314
02:00:11,000 --> 02:00:13,000
 Okay.

1315
02:00:13,000 --> 02:00:15,000
 Okay.

1316
02:00:15,000 --> 02:00:17,000
 Okay.

1317
02:00:17,000 --> 02:00:19,000
 Okay.

1318
02:00:19,000 --> 02:00:21,000
 Okay.

1319
02:00:21,000 --> 02:00:23,000
 Okay.

1320
02:00:23,000 --> 02:00:25,000
 Okay.

1321
02:00:25,000 --> 02:00:27,000
 Okay.

1322
02:00:27,000 --> 02:00:29,000
 Okay.

1323
02:00:29,000 --> 02:00:31,000
 Okay.

1324
02:00:32,000 --> 02:00:35,000
 Okay.

1325
02:00:35,000 --> 02:00:36,000
 Okay.

1326
02:00:36,000 --> 02:00:37,000
 Okay.

1327
02:00:37,000 --> 02:00:38,000
 Okay.

1328
02:00:38,000 --> 02:00:39,000
 Okay.

1329
02:00:39,000 --> 02:00:40,000
 Okay.

1330
02:00:40,000 --> 02:00:41,000
 Okay.

1331
02:00:41,000 --> 02:00:42,000
 Okay.

1332
02:00:42,000 --> 02:00:51,440
 I

1333
02:00:51,440 --> 02:00:51,740
 Das k Heil

1334
02:00:51,740 --> 02:00:53,000
 Das k Heil

1335
02:00:53,000 --> 02:00:53,340
 Das k Heil

1336
02:00:53,420 --> 02:00:54,520
 Yes,

1337
02:00:54,520 --> 02:01:00,000
 D

1338
02:01:00,000 --> 02:01:04,600
 the nearest neighbor, SVM, Newnight work.

1339
02:01:04,600 --> 02:01:09,100
 It's just how you can evaluate classification method.

1340
02:01:13,280 --> 02:01:17,680
 So, of course, very often that we measure

1341
02:01:17,680 --> 02:01:21,000
 the outcome accuracy, right?

1342
02:01:21,000 --> 02:01:24,280
 Which is how many percent of the data

1343
02:01:24,280 --> 02:01:28,000
 you can make correct decision, okay?

1344
02:01:28,000 --> 02:01:33,000
 And of course, you can also just focus on the test set

1345
02:01:33,840 --> 02:01:36,160
 or unseen data set.

1346
02:01:36,160 --> 02:01:40,440
 And then which allow you to be more objective.

1347
02:01:40,440 --> 02:01:44,360
 You do not really train so-called,

1348
02:01:45,480 --> 02:01:48,360
 so-called optimize the classifier

1349
02:01:48,360 --> 02:01:49,760
 based on your trained data.

1350
02:01:49,760 --> 02:01:52,960
 So you want to keep the test set aside

1351
02:01:52,960 --> 02:01:56,200
 so you can have more independent evaluation.

1352
02:01:58,120 --> 02:02:01,560
 So let's consider two class of sample.

1353
02:02:01,560 --> 02:02:05,120
 You certainly can extend this to multiple class,

1354
02:02:05,120 --> 02:02:06,400
 but to make it simple,

1355
02:02:06,400 --> 02:02:08,400
 we just look at the two class problem.

1356
02:02:08,400 --> 02:02:12,080
 Let's say even a multiple class, 0, 1, 2, 3, 4,

1357
02:02:12,080 --> 02:02:14,400
 you can always break this multiple class

1358
02:02:14,400 --> 02:02:17,440
 into two group of classes, right?

1359
02:02:17,440 --> 02:02:21,080
 Yes or no or the rest of them, okay?

1360
02:02:21,080 --> 02:02:22,760
 So there's a two class problem.

1361
02:02:22,760 --> 02:02:25,280
 Basically, you are going to, let's say,

1362
02:02:25,280 --> 02:02:29,559
 decide whether a patient got COVID or not, right?

1363
02:02:29,559 --> 02:02:33,559
 Positive or negative sample, okay?

1364
02:02:33,559 --> 02:02:36,800
 So these are some of the so-called measure

1365
02:02:37,759 --> 02:02:38,920
 that you can apply.

1366
02:02:38,920 --> 02:02:40,920
 One is a true positive.

1367
02:02:40,920 --> 02:02:42,840
 First, you look at the number first.

1368
02:02:42,840 --> 02:02:45,360
 So let's say the number of them,

1369
02:02:45,360 --> 02:02:47,880
 the sample have a positive sample

1370
02:02:47,880 --> 02:02:52,440
 and you, it is correctly labeled by the classifier.

1371
02:02:52,440 --> 02:02:55,360
 We call this true positive.

1372
02:02:55,360 --> 02:02:58,280
 If you have a sample which has COVID

1373
02:02:58,280 --> 02:03:01,559
 and then your classifier also says so,

1374
02:03:01,559 --> 02:03:04,799
 then this is a true positive, right?

1375
02:03:04,799 --> 02:03:06,080
 But if you have a sample,

1376
02:03:06,080 --> 02:03:09,120
 it doesn't have the COVID

1377
02:03:09,120 --> 02:03:11,759
 and then your classifier also labels so,

1378
02:03:11,759 --> 02:03:13,400
 there is a true negative.

1379
02:03:13,400 --> 02:03:15,919
 I mean, these true positive and true negatives

1380
02:03:15,919 --> 02:03:19,160
 are correct classification result.

1381
02:03:20,120 --> 02:03:23,400
 However, you could have this a false positive.

1382
02:03:23,400 --> 02:03:25,840
 That means a sample without COVID

1383
02:03:25,840 --> 02:03:29,880
 or without a true, but your classifier say yes,

1384
02:03:29,880 --> 02:03:32,519
 it has a true, okay?

1385
02:03:32,519 --> 02:03:34,760
 And you could have false negative.

1386
02:03:34,760 --> 02:03:38,960
 That means a sample with a positive COVID,

1387
02:03:38,960 --> 02:03:43,200
 then your classifier say it doesn't.

1388
02:03:43,200 --> 02:03:46,280
 So it becomes a false negative.

1389
02:03:46,280 --> 02:03:50,160
 And sometimes look at the possible cause or consequence.

1390
02:03:50,160 --> 02:03:54,280
 Let's say if you are looking at a so-called classifier,

1391
02:03:54,280 --> 02:03:58,480
 which decide whether a patient has cancer or not.

1392
02:03:59,960 --> 02:04:04,960
 Apparently you want the true positive as high as possible

1393
02:04:06,840 --> 02:04:11,280
 and because if a patient had cancer,

1394
02:04:11,280 --> 02:04:14,719
 you better detect it so that you can so-called apply

1395
02:04:14,720 --> 02:04:17,880
 the treatment as soon as possible, okay?

1396
02:04:17,880 --> 02:04:21,880
 You also want the true negatives as high as possible,

1397
02:04:21,880 --> 02:04:23,800
 but it is not as critical

1398
02:04:23,800 --> 02:04:28,800
 because even your first diagnosed without cancer, okay?

1399
02:04:30,320 --> 02:04:34,680
 And then you actually did not make a correction,

1400
02:04:34,680 --> 02:04:37,040
 diagnose it as a so-called false positive.

1401
02:04:37,040 --> 02:04:39,760
 Then later on you can verify it,

1402
02:04:39,760 --> 02:04:41,880
 it is actually a true negative, right?

1403
02:04:42,880 --> 02:04:46,000
 But this false negative will be serious, right?

1404
02:04:46,000 --> 02:04:48,240
 If someone actually had cancer,

1405
02:04:48,240 --> 02:04:51,720
 but your classifier say you do not have cancer, right?

1406
02:04:51,720 --> 02:04:53,040
 And after a few months,

1407
02:04:53,040 --> 02:04:55,120
 again this patient will come back to tell you

1408
02:04:55,120 --> 02:04:56,680
 that you make a wrong decision,

1409
02:04:56,680 --> 02:04:59,400
 you'll cause him the life, right?

1410
02:04:59,400 --> 02:05:02,400
 And therefore the consequence may not be the same, right?

1411
02:05:02,400 --> 02:05:04,680
 So you have to look at how you wait

1412
02:05:04,680 --> 02:05:09,160
 the so-called cause of all these decisions, right?

1413
02:05:09,240 --> 02:05:13,160
 So then you can kind of define a number

1414
02:05:13,160 --> 02:05:17,240
 of these so-called measure, performance measure, right?

1415
02:05:17,240 --> 02:05:18,880
 For example, these are the,

1416
02:05:18,880 --> 02:05:21,400
 based on the earlier two class example,

1417
02:05:21,400 --> 02:05:23,880
 these are the actual classes, right?

1418
02:05:23,880 --> 02:05:28,800
 The one with certain so-called symptom

1419
02:05:28,800 --> 02:05:31,680
 or without symptom, cancer, no cancer.

1420
02:05:31,680 --> 02:05:34,000
 And this is what you predict, right?

1421
02:05:34,000 --> 02:05:36,480
 If there's cancer you predict cancer,

1422
02:05:36,480 --> 02:05:38,360
 then it's a true positive.

1423
02:05:38,400 --> 02:05:42,679
 Cancer you predict, no cancer is a false negative.

1424
02:05:42,679 --> 02:05:45,599
 With no cancer you predict as a cancer,

1425
02:05:45,599 --> 02:05:47,440
 then it's a false positive

1426
02:05:47,440 --> 02:05:51,559
 and then true negative for no and no, right?

1427
02:05:51,559 --> 02:05:55,480
 So you can see that this is a total number of

1428
02:05:57,480 --> 02:06:01,040
 sample that you actually,

1429
02:06:01,040 --> 02:06:06,040
 the original sample have this so-called yes class, right?

1430
02:06:06,320 --> 02:06:10,280
 The number of sample with no actual classes.

1431
02:06:10,280 --> 02:06:13,760
 But this P-pram will be the number of sample

1432
02:06:13,760 --> 02:06:18,519
 you predicted to have this a yes class, okay?

1433
02:06:18,519 --> 02:06:21,280
 No, M-pram is no, okay?

1434
02:06:21,280 --> 02:06:24,320
 Assuming you can see that this P plus N

1435
02:06:24,320 --> 02:06:27,560
 equal to P-prams plus M-prams is same thing, okay?

1436
02:06:28,760 --> 02:06:32,160
 Then the accuracy of the classifier is defined as

1437
02:06:32,160 --> 02:06:35,160
 TP plus TN divided by P plus N.

1438
02:06:35,160 --> 02:06:38,920
 This is total number of samples.

1439
02:06:38,920 --> 02:06:42,280
 These are the number of sample you make correct decision.

1440
02:06:42,280 --> 02:06:46,200
 Yes is yes, no is no, okay?

1441
02:06:46,200 --> 02:06:48,680
 And then the error rate of the classifier

1442
02:06:48,680 --> 02:06:51,240
 become one minus this accuracy.

1443
02:06:51,240 --> 02:06:54,240
 So this is actually the false positive

1444
02:06:54,240 --> 02:06:59,120
 plus false negative divided by total number of sample, right?

1445
02:06:59,120 --> 02:07:01,200
 So that is error rate.

1446
02:07:01,200 --> 02:07:03,280
 Okay, certainly you want the accuracy

1447
02:07:03,280 --> 02:07:05,000
 as high as possible.

1448
02:07:05,000 --> 02:07:09,080
 So that means the error rate is also as low as possible.

1449
02:07:10,320 --> 02:07:12,480
 But these are not all the measure you have

1450
02:07:12,480 --> 02:07:16,320
 and you can also look at the sensitivity, right?

1451
02:07:16,320 --> 02:07:20,080
 Which is a true positive divided by the total number

1452
02:07:20,080 --> 02:07:21,760
 of positive sample.

1453
02:07:21,760 --> 02:07:22,640
 So what does that mean?

1454
02:07:22,640 --> 02:07:27,240
 That means for all those sample with the symptom

1455
02:07:27,240 --> 02:07:30,360
 with the cancer, how many of them

1456
02:07:30,360 --> 02:07:34,040
 you can correctly label them so, right?

1457
02:07:34,080 --> 02:07:35,600
 That is called sensitivity.

1458
02:07:35,600 --> 02:07:39,240
 Whether your classifier is sensitive enough

1459
02:07:39,240 --> 02:07:43,560
 to pick up those all those positive cases, right?

1460
02:07:43,560 --> 02:07:45,440
 Let's say cancer example.

1461
02:07:45,440 --> 02:07:50,080
 You certainly want your classifier to be very sensitive.

1462
02:07:50,080 --> 02:07:53,320
 Whoever have the symptom you want to detect,

1463
02:07:53,320 --> 02:07:57,000
 COVID-19, you don't want them to actually go back

1464
02:07:57,000 --> 02:08:00,440
 and then continue to spread the virus, right?

1465
02:08:01,160 --> 02:08:04,759
 You also want to have the specificity,

1466
02:08:04,759 --> 02:08:07,919
 this you mean that true negative divided by negative.

1467
02:08:07,919 --> 02:08:09,679
 For those without the symptom,

1468
02:08:09,679 --> 02:08:14,440
 you also want the classifier to say so

1469
02:08:14,440 --> 02:08:16,839
 as highly as possible.

1470
02:08:16,839 --> 02:08:21,320
 So for cancer example, and certainly sensitivity

1471
02:08:21,320 --> 02:08:24,919
 is more important than specificity, right?

1472
02:08:24,919 --> 02:08:27,480
 Because for specificity, if you wrongly diagnose it,

1473
02:08:27,480 --> 02:08:30,719
 then you can have further examination.

1474
02:08:30,719 --> 02:08:35,719
 At the end, you may verify whether it's correct or not.

1475
02:08:37,000 --> 02:08:38,919
 Then accuracy, if you define so,

1476
02:08:38,919 --> 02:08:42,400
 become the weighted average of the sensitivity

1477
02:08:42,400 --> 02:08:46,120
 plus the specificity based on total number

1478
02:08:46,120 --> 02:08:50,000
 of true positive and negative sample.

1479
02:08:52,280 --> 02:08:56,120
 And precision is another measure you might come across.

1480
02:08:56,160 --> 02:08:58,519
 This happened more in the retriever.

1481
02:08:58,519 --> 02:09:02,040
 Let's say you want to search some information

1482
02:09:02,040 --> 02:09:04,640
 and you want the return, right?

1483
02:09:04,640 --> 02:09:07,120
 Think about like you want to go to Google,

1484
02:09:07,120 --> 02:09:11,120
 go to website, you want to search for some information, right?

1485
02:09:11,120 --> 02:09:15,320
 Certainly you want the return to be as accurate as possible.

1486
02:09:15,320 --> 02:09:18,640
 You don't want to have a lot of those website or link

1487
02:09:18,640 --> 02:09:22,480
 which is not relevant to what you are looking for, right?

1488
02:09:22,480 --> 02:09:26,000
 But if you are looking for, for example,

1489
02:09:26,000 --> 02:09:27,559
 pass your exam question,

1490
02:09:28,719 --> 02:09:33,719
 you may also want the recall to be as high as possible, right?

1491
02:09:33,719 --> 02:09:36,679
 Whatever likely to be a pass your exam question

1492
02:09:36,679 --> 02:09:38,400
 or even better this year exam question,

1493
02:09:38,400 --> 02:09:39,919
 you want to be returned to you,

1494
02:09:39,919 --> 02:09:42,160
 then you do all the practice.

1495
02:09:42,160 --> 02:09:44,679
 You don't want to leave anyone out.

1496
02:09:44,679 --> 02:09:46,879
 So for the cases, you may want to have

1497
02:09:46,879 --> 02:09:49,120
 as high recall as possible.

1498
02:09:49,120 --> 02:09:51,519
 And precision may not be so important, right?

1499
02:09:51,560 --> 02:09:54,080
 It all depends on your application, right?

1500
02:09:55,280 --> 02:09:58,000
 And therefore these two is a trade off.

1501
02:09:58,000 --> 02:10:00,680
 Often that when the precision is high,

1502
02:10:00,680 --> 02:10:05,080
 the recall is low, okay?

1503
02:10:05,080 --> 02:10:10,080
 You likely cannot have both of them is 100%.

1504
02:10:10,520 --> 02:10:12,960
 For example, give me an example

1505
02:10:12,960 --> 02:10:17,880
 is I want to search for this year exam question.

1506
02:10:17,880 --> 02:10:22,880
 I go to the internet or you want to ask a chat, GPD.

1507
02:10:23,800 --> 02:10:25,760
 Can you find me?

1508
02:10:25,760 --> 02:10:27,960
 What are the question that are you going to appear

1509
02:10:27,960 --> 02:10:31,840
 in this year, six, four, eight, three final exam?

1510
02:10:33,320 --> 02:10:36,160
 Okay, so can I have

1511
02:10:40,840 --> 02:10:43,360
 high recall or low precision

1512
02:10:43,360 --> 02:10:46,700
 or you want low high precision or low recall, right?

1513
02:10:47,700 --> 02:10:49,380
 So these are the trade off, right?

1514
02:10:50,860 --> 02:10:52,420
 Whatever you want to search from,

1515
02:10:52,420 --> 02:10:55,059
 let's say, assuming the answer is out there

1516
02:10:55,059 --> 02:11:00,059
 in the internet, I can always give you 100% recall

1517
02:11:00,780 --> 02:11:02,620
 but 0% precision.

1518
02:11:03,500 --> 02:11:06,740
 You ask me, where can I find this information?

1519
02:11:06,740 --> 02:11:10,059
 I say, it is out there in all the internet, right?

1520
02:11:10,059 --> 02:11:14,220
 Recall is 100%, but precision is almost zero.

1521
02:11:14,300 --> 02:11:18,140
 That means you have to go and exam all the link, right?

1522
02:11:18,140 --> 02:11:22,060
 Finally, you may find it, but just take a longer time.

1523
02:11:22,060 --> 02:11:26,620
 And I can also have high precision as high as possible

1524
02:11:26,620 --> 02:11:30,020
 but with zero recall or very low recall.

1525
02:11:30,020 --> 02:11:32,380
 I only find the one, you give me an example,

1526
02:11:32,380 --> 02:11:36,700
 can you find me a website or object similar to this?

1527
02:11:36,700 --> 02:11:40,260
 I say, yeah, that is the one, that's the only one I have.

1528
02:11:40,260 --> 02:11:44,780
 So there's 100% precision but almost zero recall

1529
02:11:44,780 --> 02:11:47,980
 because there are many out there, I will not return to you.

1530
02:11:47,980 --> 02:11:52,100
 So these are the trade off that you sometimes have to consider.

1531
02:11:52,100 --> 02:11:54,780
 And for convenience, of course, sometimes two-figure,

1532
02:11:54,780 --> 02:11:58,020
 you have to evaluate this classifier is better than not.

1533
02:11:58,020 --> 02:12:00,580
 If you have two number, then it's hard to compare.

1534
02:12:00,580 --> 02:12:02,580
 One is high, the other could be low, right?

1535
02:12:02,580 --> 02:12:04,260
 Which ones do you choose?

1536
02:12:04,260 --> 02:12:08,260
 So you can combine them by into a so-called F measure,

1537
02:12:09,260 --> 02:12:12,260
 which is two type precision and recall,

1538
02:12:12,260 --> 02:12:16,140
 divided by precision plus recall, right?

1539
02:12:16,140 --> 02:12:18,300
 This F measure, you look at it carefully,

1540
02:12:18,300 --> 02:12:23,300
 will only be a high value when both precision and recalls

1541
02:12:23,300 --> 02:12:25,900
 are high, okay?

1542
02:12:25,900 --> 02:12:30,900
 When any of them is low, then this F will be low, okay?

1543
02:12:31,660 --> 02:12:34,180
 So if one is very high, the other is a meter,

1544
02:12:34,180 --> 02:12:35,260
 then it's somewhere in between.

1545
02:12:35,260 --> 02:12:37,700
 So but at least now they give you a one number, right?

1546
02:12:37,700 --> 02:12:40,420
 Then two number you have to trade off.

1547
02:12:40,420 --> 02:12:42,500
 And you can find many other F measure

1548
02:12:42,500 --> 02:12:44,620
 with different so-called weightage

1549
02:12:44,620 --> 02:12:48,220
 or different so-called weight to combine them together.

1550
02:12:49,420 --> 02:12:54,059
 So here is another evaluation curve.

1551
02:12:54,059 --> 02:12:56,019
 So early on when we talk about this,

1552
02:12:56,019 --> 02:12:59,860
 this assume that you already have one classifier,

1553
02:12:59,860 --> 02:13:04,620
 which always give you a fixed result.

1554
02:13:04,620 --> 02:13:07,740
 But you could have a set of classifier,

1555
02:13:07,740 --> 02:13:11,420
 which depend on the threshold that you set, right?

1556
02:13:11,420 --> 02:13:13,700
 Whether you want to have weighty deliver,

1557
02:13:13,700 --> 02:13:16,700
 I'll return or all the return,

1558
02:13:16,700 --> 02:13:18,580
 then you can adjust the threshold.

1559
02:13:18,580 --> 02:13:20,820
 What is the confident that you have?

1560
02:13:20,820 --> 02:13:22,900
 Then for that, you have a method,

1561
02:13:22,900 --> 02:13:25,780
 but there's also parameter you can adjust.

1562
02:13:25,780 --> 02:13:27,620
 Then you want to evaluate the performer,

1563
02:13:27,620 --> 02:13:30,380
 all the possible parameter you can set.

1564
02:13:30,380 --> 02:13:34,340
 And that you come up with this so-called AUC.

1565
02:13:34,420 --> 02:13:36,220
 So it's called area under curve.

1566
02:13:37,340 --> 02:13:39,420
 So what you measure is a curve, right?

1567
02:13:39,420 --> 02:13:41,100
 Then a number.

1568
02:13:41,100 --> 02:13:46,100
 And this curve will operate at different force positive rate

1569
02:13:46,380 --> 02:13:50,420
 and over this true positive rate, okay?

1570
02:13:50,420 --> 02:13:54,180
 So true positive rate is again TP divided by P.

1571
02:13:54,180 --> 02:13:57,980
 In general, you want this to be as high as possible.

1572
02:13:57,980 --> 02:13:59,940
 And force positive rate,

1573
02:13:59,940 --> 02:14:03,180
 you want this to be as low as possible.

1574
02:14:04,540 --> 02:14:06,540
 So let me ideally you want this curve,

1575
02:14:06,540 --> 02:14:08,740
 the area here like this.

1576
02:14:08,740 --> 02:14:09,940
 This is the line that,

1577
02:14:09,940 --> 02:14:14,940
 this is the perhaps the ideal line that you are looking for.

1578
02:14:14,940 --> 02:14:19,740
 One here, then drop to zero, right?

1579
02:14:19,740 --> 02:14:23,020
 So this area will be 100%.

1580
02:14:23,020 --> 02:14:25,820
 You want to have the true positive rate

1581
02:14:25,820 --> 02:14:27,220
 as high as possible,

1582
02:14:27,220 --> 02:14:31,620
 while the force negative rate is very low.

1583
02:14:31,620 --> 02:14:34,540
 So you can work on this region, right?

1584
02:14:35,740 --> 02:14:40,220
 This is the area that you want your classifier to operate on.

1585
02:14:40,220 --> 02:14:43,019
 So therefore the area will be the best.

1586
02:14:43,019 --> 02:14:43,860
 You select the treasure.

1587
02:14:43,860 --> 02:14:44,700
 When you change basically,

1588
02:14:44,700 --> 02:14:48,500
 you select different so-called treasure, different computer.

1589
02:14:48,500 --> 02:14:50,780
 So you can certainly select the one here,

1590
02:14:50,780 --> 02:14:54,740
 which give you the highest true positive rate

1591
02:14:54,740 --> 02:14:57,620
 and lowest false positive rate, okay?

1592
02:14:57,620 --> 02:15:00,380
 So that means this is the best classifier.

1593
02:15:00,380 --> 02:15:01,860
 And this is probably the worst,

1594
02:15:01,860 --> 02:15:05,580
 meaning that 50%, 50% error, right?

1595
02:15:05,580 --> 02:15:08,740
 You don't do anything, it's a give you 50%.

1596
02:15:08,740 --> 02:15:13,420
 And most classifier with some parameters,

1597
02:15:13,420 --> 02:15:17,060
 it just fall between these two extreme line here.

1598
02:15:17,060 --> 02:15:19,860
 Therefore you measure the area under the curve.

1599
02:15:19,860 --> 02:15:22,540
 The higher the area,

1600
02:15:22,540 --> 02:15:26,020
 the better the performance of the classifier.

1601
02:15:26,940 --> 02:15:31,940
 And this was early on used for so-called detecting

1602
02:15:32,980 --> 02:15:36,140
 whether there is a aircraft coming or enemy coming.

1603
02:15:36,140 --> 02:15:41,140
 They call receiver operating characteristic measure, right?

1604
02:15:41,340 --> 02:15:43,620
 They try to have radar to check

1605
02:15:43,620 --> 02:15:47,220
 whether the enemy aircraft is coming in or not.

1606
02:15:47,220 --> 02:15:50,020
 They adjust different kind of sensitivity.

1607
02:15:50,020 --> 02:15:52,300
 Then test the performance.

1608
02:15:52,300 --> 02:15:54,660
 So we come with the name ROC curve.

1609
02:15:56,620 --> 02:15:58,620
 Then here are some example

1610
02:15:58,620 --> 02:16:02,660
 to make sure that you know how to calculate them.

1611
02:16:06,720 --> 02:16:08,580
 So for example, in here,

1612
02:16:08,580 --> 02:16:12,060
 we consider, okay, when I give you

1613
02:16:12,060 --> 02:16:14,260
 so-called actual sample number,

1614
02:16:15,100 --> 02:16:17,060
 actually for example, you can go back to

1615
02:16:18,060 --> 02:16:20,860
 the decision tree we built earlier, right?

1616
02:16:20,860 --> 02:16:22,580
 We have built some decision tree.

1617
02:16:22,580 --> 02:16:24,440
 You can apply the training data

1618
02:16:24,440 --> 02:16:29,440
 or a few test samples to apply to the decision tree

1619
02:16:31,080 --> 02:16:32,359
 to get outcome.

1620
02:16:32,359 --> 02:16:35,359
 So you know the actual class, right?

1621
02:16:35,359 --> 02:16:38,799
 For your test sample, then the decision tree predict

1622
02:16:38,799 --> 02:16:41,160
 what is a yes number or yes and no,

1623
02:16:41,160 --> 02:16:42,799
 then you can fill in this number

1624
02:16:42,799 --> 02:16:45,139
 based on the number or sample you have.

1625
02:16:45,139 --> 02:16:47,279
 Then for different decision tree,

1626
02:16:47,279 --> 02:16:49,760
 you can decide which one is better.

1627
02:16:50,760 --> 02:16:55,760
 So in this case, we have 10,000 patient information

1628
02:16:58,280 --> 02:17:03,280
 which try to predict whether the output

1629
02:17:03,680 --> 02:17:08,360
 of the class is yes or no for the flu virus, right?

1630
02:17:08,360 --> 02:17:11,680
 So basically you have 10,000 patients.

1631
02:17:11,680 --> 02:17:16,160
 Among them, 300 has a flu virus.

1632
02:17:16,280 --> 02:17:21,080
 Majority 9,700 do not have a flu so-called symptom.

1633
02:17:22,520 --> 02:17:24,879
 But when you predict them, you use a tree,

1634
02:17:24,879 --> 02:17:29,879
 you build, you predict 250 out of 300 correctly

1635
02:17:30,160 --> 02:17:33,400
 with so-called flu virus.

1636
02:17:33,400 --> 02:17:37,719
 And then, I mean, with virus yes,

1637
02:17:37,719 --> 02:17:39,959
 and then those without virus.

1638
02:17:41,240 --> 02:17:45,600
 So you predicted 250 of them have the flu virus,

1639
02:17:45,600 --> 02:17:49,720
 but among them only 100 of them are true positive.

1640
02:17:49,720 --> 02:17:53,240
 And another 150 of them are false positive.

1641
02:17:53,240 --> 02:17:56,200
 That means those patients without virus,

1642
02:17:56,200 --> 02:18:01,200
 but you incorrectly label them have virus, right?

1643
02:18:01,240 --> 02:18:05,320
 And similarly for the majority you predict no,

1644
02:18:05,320 --> 02:18:07,640
 but among these no, in fact,

1645
02:18:07,640 --> 02:18:11,280
 200 of them actually have virus, okay?

1646
02:18:11,280 --> 02:18:14,520
 Then you can compute the sensitivity

1647
02:18:14,520 --> 02:18:18,200
 and like this sensitivity is a TP divided by P, right?

1648
02:18:18,200 --> 02:18:23,200
 TP is 100 divided by 300, so 33% of them, right?

1649
02:18:24,800 --> 02:18:28,640
 You actually are sensitive enough to detect

1650
02:18:28,640 --> 02:18:33,640
 among the 300 patient with true virus,

1651
02:18:36,080 --> 02:18:38,720
 you only pick up 33% of them, which is 100.

1652
02:18:39,679 --> 02:18:44,679
 And this specificity is the,

1653
02:18:44,679 --> 02:18:48,320
 this so-called TN divided by N, right?

1654
02:18:48,320 --> 02:18:52,519
 So among the 9,700 without virus,

1655
02:18:52,519 --> 02:18:56,840
 you can pinpoint 9,550 of them,

1656
02:18:56,840 --> 02:19:00,439
 so there is a 98.45%.

1657
02:19:00,439 --> 02:19:01,279
 Okay?

1658
02:19:01,840 --> 02:19:06,840
 So let's say if you apply this for the cancer diagnosis,

1659
02:19:09,200 --> 02:19:10,960
 it is really not very good

1660
02:19:10,960 --> 02:19:14,120
 because you actually missed the patient

1661
02:19:14,120 --> 02:19:18,840
 which actually have cancer, majority of them, right?

1662
02:19:18,840 --> 02:19:21,000
 Of course, this one, 98 of them,

1663
02:19:21,000 --> 02:19:25,600
 there will be another 12.55% of them

1664
02:19:25,600 --> 02:19:27,120
 will be labeled as yes,

1665
02:19:27,120 --> 02:19:30,000
 and then they will to suffer further test.

1666
02:19:31,000 --> 02:19:36,000
 Then accuracy will be this so-called 100 plus 9,550, right?

1667
02:19:39,719 --> 02:19:44,719
 TP plus TN divided by total number, which is 96.50.

1668
02:19:45,799 --> 02:19:48,720
 So although you have high, rather high accuracy,

1669
02:19:48,720 --> 02:19:53,440
 but bear in mind that your sensitivity is rather low, right?

1670
02:19:54,520 --> 02:19:58,320
 So if you go to a hospital, the doctor tell you that,

1671
02:19:58,440 --> 02:20:03,440
 my accuracy 96.5% or seem reliable,

1672
02:20:03,840 --> 02:20:07,199
 you say, can I know your sensitivity?

1673
02:20:07,199 --> 02:20:08,920
 33% okay, better not.

1674
02:20:10,119 --> 02:20:13,279
 I don't know whether you trust your life with this person,

1675
02:20:13,279 --> 02:20:15,760
 say you only identify one out of three

1676
02:20:17,080 --> 02:20:18,360
 with the so-called symptoms,

1677
02:20:18,360 --> 02:20:20,119
 because the base is huge, right?

1678
02:20:20,119 --> 02:20:23,039
 Most people would not have that kind of symptom.

1679
02:20:23,039 --> 02:20:25,080
 Therefore you could have very high accuracy,

1680
02:20:25,080 --> 02:20:30,080
 but not really sensitive enough to identify your symptom.

1681
02:20:32,840 --> 02:20:37,840
 So precision is 100 divided by 250,

1682
02:20:38,600 --> 02:20:43,280
 so TP divided by P-prum here, okay?

1683
02:20:43,280 --> 02:20:45,960
 Recall is 33%, right?

1684
02:20:45,960 --> 02:20:49,240
 So F is, because one of them is low, right?

1685
02:20:49,240 --> 02:20:53,240
 So therefore F become quite low, 36.36%.

1686
02:20:54,240 --> 02:20:59,240
 Even, actually two of them precision and the recall are both low.

1687
02:21:02,680 --> 02:21:04,480
 So you have to understand the meaning

1688
02:21:04,480 --> 02:21:07,560
 of all these so-called measures and see how they apply.

1689
02:21:08,760 --> 02:21:12,240
 Of course, beside the accuracy, sensitivity,

1690
02:21:13,560 --> 02:21:15,640
 this so-called specificity,

1691
02:21:15,640 --> 02:21:17,640
 you can also evaluate a classifier

1692
02:21:17,640 --> 02:21:20,440
 based on other measures like speed, right?

1693
02:21:20,480 --> 02:21:25,480
 You certainly want it to so-called decide as soon as possible,

1694
02:21:25,680 --> 02:21:28,520
 just like the Chatchapiti.

1695
02:21:28,520 --> 02:21:31,040
 At first it came out, you ask a question,

1696
02:21:31,040 --> 02:21:33,680
 it takes a while before it can answer you, right?

1697
02:21:33,680 --> 02:21:37,480
 Because its speed is not fast enough,

1698
02:21:37,480 --> 02:21:41,080
 but some of the smaller so-called large location model

1699
02:21:41,080 --> 02:21:43,640
 can give you the answer almost immediately.

1700
02:21:45,120 --> 02:21:46,480
 Robustness, right?

1701
02:21:46,480 --> 02:21:50,960
 You want the classifier to make correct decision,

1702
02:21:50,960 --> 02:21:54,520
 even you have some noisy data or redundant data

1703
02:21:54,520 --> 02:21:56,480
 or missing data, right?

1704
02:21:57,480 --> 02:22:01,640
 If it doesn't tell you, let's say, what is the income level,

1705
02:22:01,640 --> 02:22:02,840
 right?

1706
02:22:02,840 --> 02:22:04,800
 It can still make correct decision.

1707
02:22:04,800 --> 02:22:07,600
 Actually, it's not the important whether this patient

1708
02:22:07,600 --> 02:22:09,640
 is rich or not.

1709
02:22:09,640 --> 02:22:12,960
 It's more important whether he has a symptom or not, right?

1710
02:22:13,000 --> 02:22:19,000
 So, scalability, you want it to be applied to a lot of data,

1711
02:22:19,119 --> 02:22:22,839
 you may build your network based on this hospital,

1712
02:22:22,839 --> 02:22:25,640
 but this classifier hopefully can be applied

1713
02:22:25,640 --> 02:22:28,759
 to another hospital or another country.

1714
02:22:28,759 --> 02:22:30,160
 You don't want it to fail.

1715
02:22:30,160 --> 02:22:32,599
 So you want it to be able to scale,

1716
02:22:32,599 --> 02:22:36,560
 not just to a different number of user

1717
02:22:36,560 --> 02:22:40,000
 and also different area, different patient.

1718
02:22:40,000 --> 02:22:43,360
 You need to be able to interpret the data, right?

1719
02:22:43,360 --> 02:22:45,880
 Just like classifier, you can tell the patient

1720
02:22:45,880 --> 02:22:50,880
 why I make this decision because this, this, this, right?

1721
02:22:50,880 --> 02:22:53,880
 You have to be able to provide the insight,

1722
02:22:53,880 --> 02:22:55,520
 provide the reason.

1723
02:22:55,520 --> 02:22:59,680
 So here, this is another way to also evaluate

1724
02:22:59,680 --> 02:23:01,640
 the classifier performance.

1725
02:23:03,680 --> 02:23:05,680
 Of course, how do you test them?

1726
02:23:05,680 --> 02:23:08,040
 Let's say if you have trained data, you have test data,

1727
02:23:08,040 --> 02:23:11,680
 but there are many different ways you can do it.

1728
02:23:11,680 --> 02:23:14,160
 The most common is the handout method,

1729
02:23:14,160 --> 02:23:17,840
 which you divide your, this one we mentioned earlier,

1730
02:23:17,840 --> 02:23:20,880
 you divide your data into two parts.

1731
02:23:20,880 --> 02:23:25,160
 And one part, majority of them you use for training.

1732
02:23:25,160 --> 02:23:29,800
 The rest of them, minority, you use for testing, right?

1733
02:23:29,800 --> 02:23:33,120
 And the training set is used to learn the classifier,

1734
02:23:33,120 --> 02:23:38,120
 and accuracy is evaluated by the test set, okay?

1735
02:23:38,760 --> 02:23:41,520
 Hand out method, this is also known as this.

1736
02:23:41,520 --> 02:23:44,160
 But whether it's any two-third one-third is,

1737
02:23:44,160 --> 02:23:47,120
 depend on the number of data you have, right?

1738
02:23:47,120 --> 02:23:49,800
 If you only have six data, right?

1739
02:23:51,840 --> 02:23:54,960
 One-third is already two out of six.

1740
02:23:56,000 --> 02:23:57,680
 Then the four may not be able to build

1741
02:23:57,680 --> 02:23:59,840
 any delivery, right?

1742
02:23:59,960 --> 02:24:03,920
 If you have a billion of so-called patients,

1743
02:24:03,920 --> 02:24:06,360
 then you may not need one-third.

1744
02:24:06,360 --> 02:24:08,240
 There's a lot of data, right?

1745
02:24:08,240 --> 02:24:13,240
 You may only need one percent, which could be enough, right?

1746
02:24:13,920 --> 02:24:17,560
 So it all depends on how many data sample you have.

1747
02:24:19,300 --> 02:24:22,040
 And the other is called cross-varyration,

1748
02:24:22,040 --> 02:24:24,960
 and which, assuming you have a lot of data,

1749
02:24:24,960 --> 02:24:29,960
 rather than you test it only for one particular test set,

1750
02:24:30,640 --> 02:24:34,560
 you can divide your data into 10 different subsets,

1751
02:24:34,560 --> 02:24:37,599
 or V different subsets.

1752
02:24:37,599 --> 02:24:40,679
 Each time you use the remaining,

1753
02:24:40,679 --> 02:24:42,919
 you pick out one as a test set,

1754
02:24:42,919 --> 02:24:45,160
 the remaining use it to do the training.

1755
02:24:45,160 --> 02:24:46,880
 Then you build one tree, right?

1756
02:24:46,880 --> 02:24:48,640
 To evaluate performance.

1757
02:24:48,640 --> 02:24:52,679
 Then you can select the next subset as a test set,

1758
02:24:52,720 --> 02:24:57,720
 and then the other remaining one as a so-called training set.

1759
02:25:00,200 --> 02:25:02,240
 So at the end, if you're a K of them,

1760
02:25:02,240 --> 02:25:05,920
 then you're going to have a K different so-called

1761
02:25:05,920 --> 02:25:09,040
 evaluation, then you can take the average of them.

1762
02:25:09,040 --> 02:25:13,640
 So that evaluates the performance capability

1763
02:25:13,640 --> 02:25:17,640
 of the architecture of the scheme that you have

1764
02:25:17,640 --> 02:25:20,240
 by going through different combination

1765
02:25:20,240 --> 02:25:22,160
 of training and test data.

1766
02:25:22,160 --> 02:25:24,560
 This is called cross-varyration.

1767
02:25:24,560 --> 02:25:26,200
 You don't use it just once.

1768
02:25:26,200 --> 02:25:29,119
 You can select different set of a test set,

1769
02:25:29,119 --> 02:25:31,720
 and then apply multiple times.

1770
02:25:33,039 --> 02:25:36,280
 In the extreme, you only have a small data set.

1771
02:25:36,280 --> 02:25:38,039
 You can say, leave one out.

1772
02:25:38,039 --> 02:25:39,960
 You're going to reserve one for testing.

1773
02:25:39,960 --> 02:25:43,960
 The rest, for just now the sixth example, right?

1774
02:25:43,960 --> 02:25:46,640
 And you just keep one,

1775
02:25:46,640 --> 02:25:48,960
 and then use the other five to build a tree,

1776
02:25:48,960 --> 02:25:50,800
 and then use the last one to test.

1777
02:25:50,800 --> 02:25:53,880
 And you can repeat this six times

1778
02:25:53,880 --> 02:25:56,279
 then to see whether the results are good enough.

1779
02:25:56,279 --> 02:25:57,640
 This is called leave one out.

1780
02:25:57,640 --> 02:25:59,920
 You may come across such term.

1781
02:26:01,480 --> 02:26:02,320
 Okay?

1782
02:26:02,320 --> 02:26:03,759
 So that is decision three.

1783
02:26:03,759 --> 02:26:05,039
 Let's summarize it.

1784
02:26:07,920 --> 02:26:10,960
 And we thought about the classification step,

1785
02:26:10,960 --> 02:26:13,480
 which normally involve a training,

1786
02:26:13,480 --> 02:26:18,480
 and then test set to predict the classes of data.

1787
02:26:19,480 --> 02:26:23,160
 And then in terms of attributes, the feature,

1788
02:26:23,160 --> 02:26:25,680
 there are four types of attributes,

1789
02:26:25,680 --> 02:26:29,560
 nominal, on-in, interval, and ratio.

1790
02:26:31,199 --> 02:26:34,880
 And not all the classifier can use

1791
02:26:34,880 --> 02:26:37,600
 all these kind of features.

1792
02:26:37,600 --> 02:26:41,600
 Some of them depend on the so-called

1793
02:26:41,600 --> 02:26:43,039
 calculation involved,

1794
02:26:43,039 --> 02:26:46,720
 and you may only use certain type of attribute

1795
02:26:46,720 --> 02:26:49,400
 rather than all these attributes.

1796
02:26:49,400 --> 02:26:52,039
 But certainly you want to have a classifier

1797
02:26:52,039 --> 02:26:56,119
 that can take input like nominal, ordinal,

1798
02:26:57,160 --> 02:27:00,560
 so-called input together with interval.

1799
02:27:00,560 --> 02:27:02,439
 It could be text, could be image,

1800
02:27:02,439 --> 02:27:05,080
 could be audio, could be speech, all together.

1801
02:27:05,080 --> 02:27:08,140
 They come out in model classifier.

1802
02:27:09,840 --> 02:27:14,359
 So decision three basically try to ask a sequence of question

1803
02:27:14,359 --> 02:27:16,279
 and based on the answer,

1804
02:27:16,280 --> 02:27:19,320
 if asked further question,

1805
02:27:19,320 --> 02:27:22,600
 then derive at the decision.

1806
02:27:24,320 --> 02:27:27,480
 So, and then when you build the decision tree,

1807
02:27:27,480 --> 02:27:31,440
 you have to select one attribute selection measure

1808
02:27:31,440 --> 02:27:34,680
 to select which attribute is the best

1809
02:27:34,680 --> 02:27:37,880
 to ask a question at the level.

1810
02:27:37,880 --> 02:27:41,960
 So this attribute measure including the information gain,

1811
02:27:41,960 --> 02:27:44,480
 Gini gain index,

1812
02:27:46,080 --> 02:27:50,800
 and these are the gain ratio, Gini index,

1813
02:27:50,800 --> 02:27:52,960
 these are the measure that you can use.

1814
02:27:57,599 --> 02:27:59,759
 Yeah, and then perform measure,

1815
02:27:59,759 --> 02:28:01,519
 there are many more measures you can use

1816
02:28:01,519 --> 02:28:04,039
 to assess the classifier, not just accuracy,

1817
02:28:04,039 --> 02:28:06,839
 many other things that you have to consider,

1818
02:28:06,839 --> 02:28:08,919
 like the speed, the cost,

1819
02:28:08,920 --> 02:28:12,800
 and whether you allow missing data

1820
02:28:12,800 --> 02:28:16,160
 and allow some of these so-called outlier,

1821
02:28:16,160 --> 02:28:18,920
 and these are things that you have to be aware of.

1822
02:28:39,560 --> 02:28:44,560
 Okay, let's go through the nearest neighbor classifier

1823
02:28:46,440 --> 02:28:49,080
 before we end the lecture today.

1824
02:28:49,080 --> 02:28:50,880
 And this nearest neighbor classifier,

1825
02:28:50,880 --> 02:28:53,440
 these are the two different classifiers

1826
02:28:53,440 --> 02:28:57,160
 that you may come across.

1827
02:28:57,160 --> 02:29:02,160
 The nearest neighbor classifier is perhaps the simplest

1828
02:29:02,800 --> 02:29:05,560
 that classifier can think of.

1829
02:29:06,560 --> 02:29:10,560
 Again, this is just a recap.

1830
02:29:10,560 --> 02:29:15,560
 These are the classification so-called definition.

1831
02:29:19,439 --> 02:29:21,720
 For nearest neighbor classifier,

1832
02:29:21,720 --> 02:29:24,279
 basically you try to assign each sample

1833
02:29:24,279 --> 02:29:28,920
 the class label of the nearest training sample.

1834
02:29:28,920 --> 02:29:29,920
 So what does it mean?

1835
02:29:31,080 --> 02:29:33,240
 Here, if you have a training sample,

1836
02:29:33,240 --> 02:29:35,240
 the red and the blue, right?

1837
02:29:35,240 --> 02:29:39,199
 You know the class label of all these training samples,

1838
02:29:39,199 --> 02:29:42,560
 the red classes and the blue classes.

1839
02:29:42,560 --> 02:29:47,080
 So now you are given a new sample X.

1840
02:29:47,080 --> 02:29:50,400
 You want to ask what is the label of this X?

1841
02:29:52,080 --> 02:29:55,320
 So intuitively, you just say that,

1842
02:29:55,320 --> 02:29:58,720
 okay, this X is most likely similar to

1843
02:29:58,720 --> 02:30:01,480
 based on whatever feature or attribute.

1844
02:30:01,480 --> 02:30:03,119
 You look at it, right?

1845
02:30:03,320 --> 02:30:05,800
 Okay, it's similar to this object

1846
02:30:05,800 --> 02:30:08,240
 and this object belong to blue class.

1847
02:30:08,240 --> 02:30:12,200
 So therefore, this new object should belong to the blue class.

1848
02:30:12,200 --> 02:30:14,480
 So this is called nearest neighbor, right?

1849
02:30:15,360 --> 02:30:17,640
 So basically you check this one.

1850
02:30:22,840 --> 02:30:25,800
 And this one, if it's closer to the blue,

1851
02:30:25,800 --> 02:30:29,400
 you have to find a way to measure so-called the distance,

1852
02:30:29,400 --> 02:30:32,080
 the similarity between this sample

1853
02:30:32,080 --> 02:30:35,280
 with all the rest of the training sample, right?

1854
02:30:35,280 --> 02:30:37,480
 Before you know the nearest neighbor,

1855
02:30:37,480 --> 02:30:38,960
 of course you have to know to ask,

1856
02:30:38,960 --> 02:30:40,480
 is this closer to this?

1857
02:30:40,480 --> 02:30:41,960
 Is it closer to that?

1858
02:30:41,960 --> 02:30:43,200
 To this, to this?

1859
02:30:43,200 --> 02:30:45,080
 So you do a lot of checking

1860
02:30:45,080 --> 02:30:48,480
 before you know who is your nearest neighbor.

1861
02:30:48,480 --> 02:30:52,520
 So you can see that you do not need any training, right?

1862
02:30:52,520 --> 02:30:54,720
 Because you just need to have all the sample,

1863
02:30:54,720 --> 02:30:56,800
 you know the label, right?

1864
02:30:56,800 --> 02:31:00,440
 But you need to store all these samples with the label

1865
02:31:00,440 --> 02:31:02,640
 and then you need a lot of computation

1866
02:31:02,640 --> 02:31:04,800
 when you come to the inference.

1867
02:31:04,800 --> 02:31:06,800
 Given any sample you know to check,

1868
02:31:06,800 --> 02:31:10,400
 who is your nearest neighbor, right?

1869
02:31:10,400 --> 02:31:12,920
 It's very time consuming, right?

1870
02:31:12,920 --> 02:31:15,120
 Assuming that you have to compare this,

1871
02:31:15,120 --> 02:31:18,520
 let's say phase recognition, I give a phase, okay?

1872
02:31:18,520 --> 02:31:20,240
 Who is this person?

1873
02:31:20,240 --> 02:31:23,240
 They look, okay, he's similar to no, doesn't see, okay?

1874
02:31:23,240 --> 02:31:25,960
 You have to compare with all the other person

1875
02:31:25,960 --> 02:31:27,760
 in your data batch, right?

1876
02:31:27,760 --> 02:31:30,880
 Before you can draw who this person is.

1877
02:31:33,480 --> 02:31:36,600
 And to be robust to the noise seed sample,

1878
02:31:36,600 --> 02:31:41,600
 sometimes your sample actually may not be noise free, right?

1879
02:31:43,640 --> 02:31:45,360
 So for example, in this case,

1880
02:31:45,360 --> 02:31:49,080
 you could have some training sample red and blue.

1881
02:31:49,080 --> 02:31:51,080
 Now you have this, okay?

1882
02:31:51,080 --> 02:31:55,480
 Now the question is who is your nearest neighbor, right?

1883
02:31:55,520 --> 02:32:00,279
 It seems like it's equal distance to red and blue.

1884
02:32:00,279 --> 02:32:02,359
 They cannot make decision.

1885
02:32:02,359 --> 02:32:06,359
 Therefore you come with this K-nearest neighbor, right?

1886
02:32:06,359 --> 02:32:09,279
 So rather than one, I say, okay,

1887
02:32:09,279 --> 02:32:13,439
 who are my three nearest neighbor, right?

1888
02:32:13,439 --> 02:32:18,000
 Among these three, I have two blue, one red.

1889
02:32:18,000 --> 02:32:21,320
 So I say, okay, most likely this is a blue, right?

1890
02:32:21,320 --> 02:32:24,080
 You seem somehow maturity vote idea

1891
02:32:24,880 --> 02:32:29,880
 that allow you to be more robust to some outlier noise

1892
02:32:30,160 --> 02:32:34,240
 or imperfect so-called data collection, right?

1893
02:32:35,320 --> 02:32:39,400
 So as you can see that you need to have a way to measure

1894
02:32:39,400 --> 02:32:44,400
 who are my K-nearest neighbor, right?

1895
02:32:44,600 --> 02:32:47,520
 So you can use some kind of distance

1896
02:32:47,520 --> 02:32:51,940
 which are often used to measure the dissimilarity

1897
02:32:51,940 --> 02:32:55,060
 between two data sample or attribute.

1898
02:32:55,060 --> 02:32:58,140
 Earlier we say the attribute could be a vector, right?

1899
02:32:58,140 --> 02:32:59,420
 Then when you have vector,

1900
02:32:59,420 --> 02:33:02,540
 you have those so-called number

1901
02:33:02,540 --> 02:33:06,020
 that you can do the difference, you can do the division.

1902
02:33:06,020 --> 02:33:10,060
 Then you can apply some of these so-called measure.

1903
02:33:10,060 --> 02:33:13,380
 One of them is called Minkowski distance,

1904
02:33:13,380 --> 02:33:17,580
 Minkowski distance, and between two data point

1905
02:33:17,580 --> 02:33:21,180
 which x, y could be vector of n dimensional

1906
02:33:22,140 --> 02:33:26,380
 vector, you can define them as summation.

1907
02:33:26,380 --> 02:33:31,020
 This component wise, the subtract xk minus yk,

1908
02:33:31,020 --> 02:33:34,500
 you raise the power r, you sum them up,

1909
02:33:34,500 --> 02:33:39,500
 then you take the root of one divided by r, right?

1910
02:33:41,220 --> 02:33:46,220
 So if this is equal to r is some kind of parameter

1911
02:33:46,820 --> 02:33:48,180
 that you can choose, right?

1912
02:33:48,180 --> 02:33:51,740
 It could be one, could be two, could be three,

1913
02:33:52,340 --> 02:33:54,940
 could be even a very large number, right?

1914
02:33:54,940 --> 02:33:56,860
 So when r equal to one,

1915
02:33:56,860 --> 02:34:01,140
 and this Minkowski distance,

1916
02:34:01,140 --> 02:34:05,780
 actually turning into this so-called component y distance,

1917
02:34:05,780 --> 02:34:09,920
 which is called L1 norm, right?

1918
02:34:09,920 --> 02:34:12,380
 Basically x1 minus y1,

1919
02:34:12,380 --> 02:34:16,580
 take the absolute value plus x2 minus y2,

1920
02:34:16,580 --> 02:34:21,580
 absolute value plus x-ectron under xn minus yn, okay?

1921
02:34:21,700 --> 02:34:24,140
 And this is also known as a city block

1922
02:34:24,140 --> 02:34:27,700
 because like New York, actually the street is very square,

1923
02:34:27,700 --> 02:34:28,539
 right?

1924
02:34:28,539 --> 02:34:31,220
 From here to there, you basically go through how many blocks?

1925
02:34:31,220 --> 02:34:33,700
 Regardless, whether you go to horizontal or vertical,

1926
02:34:33,700 --> 02:34:37,300
 you still have to go through the same number of blocks distance.

1927
02:34:38,460 --> 02:34:39,300
 Okay?

1928
02:34:39,300 --> 02:34:41,380
 This is called L1 norm.

1929
02:34:41,380 --> 02:34:44,980
 And of course, something that you probably very familiar with

1930
02:34:44,980 --> 02:34:49,980
 is called L2 norm, which is also known as equidian distance,

1931
02:34:50,900 --> 02:34:53,619
 which is the direct measure of a two point

1932
02:34:53,619 --> 02:34:58,119
 in the 3D Cartesian coordinate, right?

1933
02:34:59,020 --> 02:35:01,060
 And when r become infinity,

1934
02:35:01,060 --> 02:35:04,740
 which we call it the supremum or L infinity

1935
02:35:04,740 --> 02:35:07,060
 or L maximum norm,

1936
02:35:07,060 --> 02:35:10,220
 which basically just measure the maximum difference

1937
02:35:10,220 --> 02:35:13,779
 between any corresponding attribute.

1938
02:35:13,779 --> 02:35:18,380
 Basically, sk minus yk, if r approach infinity,

1939
02:35:18,380 --> 02:35:22,900
 only the component with the largest xk minus yk

1940
02:35:22,900 --> 02:35:24,779
 will be the deciding factor.

1941
02:35:24,779 --> 02:35:26,580
 The value will be the largest difference,

1942
02:35:26,580 --> 02:35:28,259
 regardless which direction, right?

1943
02:35:28,259 --> 02:35:29,220
 Which dimension?

1944
02:35:29,220 --> 02:35:31,339
 The one which is the largest at the end

1945
02:35:31,339 --> 02:35:35,300
 will dominate all the distance, okay?

1946
02:35:35,300 --> 02:35:38,099
 So there is a L sub-penial norm.

1947
02:35:38,099 --> 02:35:40,660
 So let's look at the graphically, what does it mean?

1948
02:35:40,660 --> 02:35:43,939
 It's just like the genie, the grain ratio

1949
02:35:43,939 --> 02:35:48,140
 or the information gain, right?

1950
02:35:48,140 --> 02:35:51,660
 They are more or less a distant measure,

1951
02:35:51,660 --> 02:35:56,060
 just how you keep the weightage of each dimension, right?

1952
02:35:56,060 --> 02:35:59,099
 So in this case, assuming xk,

1953
02:35:59,099 --> 02:36:01,699
 we have two dimensional data, x1 minus one,

1954
02:36:01,699 --> 02:36:05,880
 I call it e1, the difference, e2 is the difference.

1955
02:36:05,880 --> 02:36:09,260
 So another x2 minus y2,

1956
02:36:09,260 --> 02:36:14,180
 and this is the curve of the point on this,

1957
02:36:14,180 --> 02:36:16,180
 when r equal to one,

1958
02:36:16,220 --> 02:36:19,700
 these are all the point, e1 and e2 distant,

1959
02:36:19,700 --> 02:36:22,460
 they have an equal distance equal to one.

1960
02:36:22,460 --> 02:36:24,660
 They are all here, right?

1961
02:36:24,660 --> 02:36:28,660
 e1 is here, e2 is here, then the distant r,

1962
02:36:29,700 --> 02:36:31,700
 this is dxy equal to one.

1963
02:36:32,800 --> 02:36:34,980
 And when the r increase,

1964
02:36:34,980 --> 02:36:38,420
 you can see that the equal distant one,

1965
02:36:38,420 --> 02:36:42,660
 it becomes so-called further away from the center

1966
02:36:43,660 --> 02:36:46,619
 until r equal to infinity, right?

1967
02:36:46,619 --> 02:36:49,020
 This one actually measure the maximum,

1968
02:36:49,020 --> 02:36:51,539
 you have e1 and e2, right?

1969
02:36:51,539 --> 02:36:55,740
 For this point, all the e1 equal to one,

1970
02:36:55,740 --> 02:36:57,380
 here all the e2 equal to one,

1971
02:36:57,380 --> 02:37:00,300
 regardless what is the other e,

1972
02:37:00,300 --> 02:37:02,560
 then you have this one equal to maximum.

1973
02:37:02,560 --> 02:37:05,940
 So they are all some kind of distant measure,

1974
02:37:05,940 --> 02:37:08,820
 just how you want to give the weight

1975
02:37:09,660 --> 02:37:14,020
 in the different so-called component differences.

1976
02:37:14,980 --> 02:37:16,220
 You can just choose anyone,

1977
02:37:16,220 --> 02:37:20,580
 of course, normally we will use the L2 norm, right?

1978
02:37:20,580 --> 02:37:24,060
 Which allow you to do a lot of so-called

1979
02:37:25,619 --> 02:37:28,500
 gradient descent, back propagation.

1980
02:37:28,500 --> 02:37:30,180
 We also want in the Neural Network,

1981
02:37:30,180 --> 02:37:33,900
 using L2 norm, then how do you devise

1982
02:37:33,900 --> 02:37:37,400
 a back propagation measure to optimize the parameter?

1983
02:37:39,779 --> 02:37:42,460
 So when r is larger equal to one,

1984
02:37:42,460 --> 02:37:46,500
 the Minkowski distant has this useful measure.

1985
02:37:46,500 --> 02:37:49,440
 Okay, when you measure the difference between two points,

1986
02:37:49,440 --> 02:37:53,500
 you want that the distant from x to y

1987
02:37:53,500 --> 02:37:56,220
 should be the same of a so-called distant

1988
02:37:56,220 --> 02:37:57,980
 from y to x, right?

1989
02:37:57,980 --> 02:38:00,900
 Because all the, they're called symmetric, right?

1990
02:38:00,900 --> 02:38:02,820
 You want this property to hold.

1991
02:38:03,740 --> 02:38:05,260
 And also because it's a distant,

1992
02:38:05,260 --> 02:38:07,900
 you want the distance is always larger than,

1993
02:38:07,900 --> 02:38:09,080
 or equal to zero.

1994
02:38:10,060 --> 02:38:14,660
 They are equal to zero only if x equal to y.

1995
02:38:14,660 --> 02:38:16,420
 This is called positivity.

1996
02:38:17,340 --> 02:38:20,740
 And the third one is called triangle inequality.

1997
02:38:20,740 --> 02:38:24,060
 Here, let's say you have a distant of xy,

1998
02:38:24,060 --> 02:38:26,619
 xz and yz.

1999
02:38:26,619 --> 02:38:31,619
 You want the xz should not be larger than yz plus xz.

2000
02:38:32,940 --> 02:38:35,699
 You want to walk, right, in a straight way,

2001
02:38:35,700 --> 02:38:37,260
 and you can walk two distant,

2002
02:38:37,260 --> 02:38:41,540
 but certainly you expect that the straight direction

2003
02:38:41,540 --> 02:38:44,180
 will be shorter than the other two, right?

2004
02:38:45,820 --> 02:38:48,420
 If you measure distant measures satisfy

2005
02:38:48,420 --> 02:38:50,500
 all these three properties,

2006
02:38:50,500 --> 02:38:53,180
 then they are known as a matrix.

2007
02:38:54,260 --> 02:38:58,060
 Matrix, there is a how, the Euclidean,

2008
02:38:58,060 --> 02:38:59,900
 of course, when I go to two,

2009
02:38:59,900 --> 02:39:03,140
 the Euclidean distant will satisfy the matrix.

2010
02:39:04,140 --> 02:39:05,340
 Okay?

2011
02:39:05,340 --> 02:39:07,580
 But there are some measures that do not satisfy

2012
02:39:07,580 --> 02:39:11,820
 one or more of these matrix properties.

2013
02:39:11,820 --> 02:39:14,699
 You can think about when r is less than one.

2014
02:39:15,980 --> 02:39:16,779
 Okay?

2015
02:39:16,779 --> 02:39:21,779
 Find out when, what measure is not satisfied.

2016
02:39:21,779 --> 02:39:23,019
 Think about it.

2017
02:39:23,019 --> 02:39:26,260
 If r is less than one for the Minkowski,

2018
02:39:26,260 --> 02:39:27,500
 one of the measures,

2019
02:39:27,500 --> 02:39:28,939
 one of these, the so-called property,

2020
02:39:28,939 --> 02:39:31,060
 would not really satisfy.

2021
02:39:32,060 --> 02:39:36,420
 And of course, beside the Minkowski distant,

2022
02:39:36,420 --> 02:39:41,420
 you can also use a so-called cosine similarity, right?

2023
02:39:42,019 --> 02:39:44,380
 And which is often used to measure

2024
02:39:44,380 --> 02:39:46,140
 the document similarity.

2025
02:39:46,140 --> 02:39:49,699
 Basically, let's say you want to compare the two documents,

2026
02:39:49,699 --> 02:39:52,779
 whether they are similar or not, how do you do that?

2027
02:39:52,779 --> 02:39:55,180
 You cannot compare word by word,

2028
02:39:55,180 --> 02:39:57,939
 and you can rewrite the whole document

2029
02:39:57,939 --> 02:40:00,380
 and turn out, they may look very different,

2030
02:40:00,380 --> 02:40:03,939
 or you miss one line, the rest may be still the same,

2031
02:40:03,939 --> 02:40:05,460
 but if you compare word by word,

2032
02:40:05,460 --> 02:40:07,279
 everything will be different, right?

2033
02:40:07,279 --> 02:40:11,220
 So it's not a good measure, just compare word by word,

2034
02:40:11,220 --> 02:40:13,359
 but you can use a cosine similarity,

2035
02:40:14,580 --> 02:40:17,099
 which you can have two different vectors,

2036
02:40:17,099 --> 02:40:21,900
 x could be the count of those keywords in document one.

2037
02:40:21,900 --> 02:40:26,900
 You could have, let's say, 1000 commonly used keywords.

2038
02:40:27,019 --> 02:40:29,019
 Then in document one, you find, okay,

2039
02:40:29,060 --> 02:40:31,060
 keyword number one appear how many times,

2040
02:40:31,060 --> 02:40:33,340
 keyword number two appear how many times, right?

2041
02:40:33,340 --> 02:40:36,100
 Then you can build a vector or attribute x,

2042
02:40:36,100 --> 02:40:37,660
 then document two y.

2043
02:40:38,620 --> 02:40:41,260
 Then you can use this cosine similarity,

2044
02:40:41,260 --> 02:40:46,260
 which is nothing but a dot product, x dot product with y.

2045
02:40:46,540 --> 02:40:49,100
 I assume you know how to compute a dot product

2046
02:40:49,100 --> 02:40:50,820
 for two vector, right?

2047
02:40:50,820 --> 02:40:54,980
 And which actually, if you heard about the transformer,

2048
02:40:54,980 --> 02:40:57,140
 the attention model, right?

2049
02:40:57,140 --> 02:41:01,020
 The first input is actually a dot product.

2050
02:41:01,020 --> 02:41:03,340
 They try to decide the input sentence,

2051
02:41:03,340 --> 02:41:06,180
 the similarity of this to itself,

2052
02:41:06,180 --> 02:41:07,500
 self similarity, right?

2053
02:41:07,500 --> 02:41:09,100
 By applying dot product.

2054
02:41:11,380 --> 02:41:14,699
 So again, you should be familiar with this one.

2055
02:41:14,699 --> 02:41:19,020
 If you cosine x and y is similar to x dot product y,

2056
02:41:19,020 --> 02:41:24,020
 which is this, normalized by this L2 norm of x

2057
02:41:24,300 --> 02:41:26,780
 and L2 norm of y, for example,

2058
02:41:26,780 --> 02:41:28,820
 which is also the same as a measure

2059
02:41:28,820 --> 02:41:31,540
 the angle between these two vector,

2060
02:41:31,540 --> 02:41:34,380
 the cosine theta between these two vector.

2061
02:41:34,380 --> 02:41:37,740
 When this theta equal to zero,

2062
02:41:37,740 --> 02:41:40,540
 then it is two x and y overlap.

2063
02:41:40,540 --> 02:41:44,700
 So cosine zero will be equal to one.

2064
02:41:44,700 --> 02:41:48,700
 It is the highest similar, okay, possible.

2065
02:41:48,700 --> 02:41:52,180
 When this theta is 90 degree,

2066
02:41:52,180 --> 02:41:53,980
 then this will be equal to zero.

2067
02:41:53,980 --> 02:41:57,900
 They mean these two vectors are perpendicular to each other.

2068
02:41:57,900 --> 02:41:59,340
 Okay?

2069
02:41:59,340 --> 02:42:03,740
 So question, if I distant,

2070
02:42:03,740 --> 02:42:07,939
 if I one minus cosine xy, cosine xy similarity,

2071
02:42:07,939 --> 02:42:10,100
 meaning when the value is high,

2072
02:42:10,100 --> 02:42:12,380
 then the distance is small.

2073
02:42:12,380 --> 02:42:17,380
 So one minus cosine xy will become the distant, right?

2074
02:42:17,900 --> 02:42:22,820
 When the cosine xy equal to zero,

2075
02:42:22,820 --> 02:42:26,060
 then the distance is maximum equal to one.

2076
02:42:26,060 --> 02:42:27,260
 Okay?

2077
02:42:27,260 --> 02:42:29,340
 Assuming I only look at the cosine xy,

2078
02:42:29,340 --> 02:42:30,779
 which is a positive value.

2079
02:42:32,500 --> 02:42:36,619
 So is this matrix,

2080
02:42:36,619 --> 02:42:39,340
 is the cosine symmetric for few,

2081
02:42:39,340 --> 02:42:42,060
 all the, this requirement,

2082
02:42:47,460 --> 02:42:52,460
 is a distant positivity symmetric triangle inequality.

2083
02:42:53,100 --> 02:42:53,940
 Satisfied?

2084
02:42:55,980 --> 02:42:56,820
 Go and check.

2085
02:42:58,300 --> 02:43:02,580
 And I can tell you that it doesn't meet this, right?

2086
02:43:02,580 --> 02:43:06,740
 You could have x and y, they are on the same direction,

2087
02:43:06,740 --> 02:43:08,900
 but x equal to two y.

2088
02:43:10,300 --> 02:43:11,140
 Okay?

2089
02:43:11,140 --> 02:43:14,020
 And then the distant base on cosine similarity

2090
02:43:14,020 --> 02:43:15,820
 will be equal to zero.

2091
02:43:15,820 --> 02:43:18,060
 Even one is longer, the other is shorter,

2092
02:43:18,060 --> 02:43:21,140
 but because the angle between them is zero.

2093
02:43:21,140 --> 02:43:25,060
 So cosine zero is one, one minus one is zero.

2094
02:43:25,060 --> 02:43:28,500
 So that means you could have x not equal to y,

2095
02:43:28,500 --> 02:43:30,539
 but the xy equal to zero.

2096
02:43:31,619 --> 02:43:32,960
 Okay?

2097
02:43:32,960 --> 02:43:34,019
 Yeah.

2098
02:43:34,019 --> 02:43:38,340
 So nearest neighbor certainly is very useful,

2099
02:43:38,340 --> 02:43:42,900
 but when you try to apply it to complex data like this,

2100
02:43:42,900 --> 02:43:45,060
 let's say you want to use one image

2101
02:43:45,060 --> 02:43:47,279
 to find similar images.

2102
02:43:47,279 --> 02:43:49,699
 So this is a C-far 10 data set

2103
02:43:49,740 --> 02:43:52,580
 that we introduced before,

2104
02:43:52,580 --> 02:43:57,580
 which has 60,000 images, 32 by 32 pixel color images,

2105
02:43:58,660 --> 02:43:59,660
 10 classes.

2106
02:43:59,660 --> 02:44:01,060
 These are the 10 classes,

2107
02:44:01,060 --> 02:44:06,060
 A-plane, car, bird, cat, deer, dog, right?

2108
02:44:06,180 --> 02:44:07,820
 X, X, X, X, right?

2109
02:44:07,820 --> 02:44:12,740
 So if I apply the k nearest neighbor,

2110
02:44:12,740 --> 02:44:17,620
 I combine all the 32 by 32 pixel times three color

2111
02:44:17,660 --> 02:44:20,020
 into a long vector.

2112
02:44:20,020 --> 02:44:22,180
 Then I do the direct subtraction

2113
02:44:23,100 --> 02:44:26,460
 to find which are the nearest neighbor.

2114
02:44:26,460 --> 02:44:28,940
 So for example, you can find this one,

2115
02:44:28,940 --> 02:44:32,500
 a truck, the nearest neighbor are this,

2116
02:44:32,500 --> 02:44:34,980
 the 10 nearest neighbor.

2117
02:44:34,980 --> 02:44:38,500
 You can see that even the horse becomes the most similar.

2118
02:44:38,500 --> 02:44:41,140
 It had the same so-called structure,

2119
02:44:41,140 --> 02:44:44,180
 but the object are different, right?

2120
02:44:44,180 --> 02:44:46,660
 So this one is slightly better,

2121
02:44:46,700 --> 02:44:49,980
 and you can see that some of them

2122
02:44:49,980 --> 02:44:52,220
 is really not at the same object.

2123
02:44:52,220 --> 02:44:55,260
 If you want to use this to retrieve the object

2124
02:44:55,260 --> 02:44:57,180
 of the same class,

2125
02:44:57,180 --> 02:45:01,140
 k nearest neighbor classifier would not work well.

2126
02:45:01,980 --> 02:45:03,980
 Therefore you need a higher line

2127
02:45:05,860 --> 02:45:10,860
 neural network to help you to build a better classifier.

2128
02:45:11,119 --> 02:45:14,340
 You can go there to this website to try out yourself.

2129
02:45:15,260 --> 02:45:20,260
 The k nearest neighbor classifier is not a good measure

2130
02:45:20,860 --> 02:45:23,980
 when you apply it to such an example

2131
02:45:23,980 --> 02:45:28,040
 to find similar images of similar content.

2132
02:45:29,140 --> 02:45:29,980
 Okay?

2133
02:45:30,980 --> 02:45:34,220
 So we will come back with the support vector machine

2134
02:45:34,220 --> 02:45:36,020
 next lecture, and then after that,

2135
02:45:36,020 --> 02:45:37,540
 I will start the neural network.

2136
02:45:37,540 --> 02:45:39,660
 I will spend more time on neural network.

2137
02:45:40,500 --> 02:45:44,500
 And okay, this is the orange data set.

2138
02:45:44,500 --> 02:45:49,500
 You can go to try out yourself, right?

2139
02:45:49,539 --> 02:45:50,539
 And go to this website.

2140
02:45:50,539 --> 02:45:55,539
 Yeah, this is a tool that you can use to do some kind

2141
02:45:58,539 --> 02:46:02,700
 of basic data mining or model like this.

2142
02:46:02,700 --> 02:46:07,700
 You can do k nearest neighbor, decision tree, random.

2143
02:46:08,700 --> 02:46:11,180
 You don't have to write any program.

2144
02:46:11,180 --> 02:46:16,180
 You can load in a lot of data and then play around with it.

2145
02:46:16,580 --> 02:46:20,140
 Just watch some of the YouTube video, how to do that.

2146
02:46:20,140 --> 02:46:23,420
 For example, data, okay, you could have data here.

2147
02:46:23,420 --> 02:46:28,420
 You can select the data and here is a Titanic data, right?

2148
02:46:28,860 --> 02:46:33,620
 So it tell you some of this category, the feature, right?

2149
02:46:33,620 --> 02:46:36,940
 You heard about the Titanic data or other data set, like,

2150
02:46:38,140 --> 02:46:39,660
 iris data, right?

2151
02:46:39,660 --> 02:46:43,220
 This one to give you a different type of flower.

2152
02:46:43,220 --> 02:46:46,220
 So you can do in this a numeric data.

2153
02:46:46,220 --> 02:46:51,020
 These are the feature and these are the target category, right?

2154
02:46:51,020 --> 02:46:56,020
 So you can go in, do this.

2155
02:46:56,620 --> 02:46:59,740
 Then you can visualize the data, for example,

2156
02:47:01,340 --> 02:47:04,780
 it's my visualization, okay?

2157
02:47:04,820 --> 02:47:08,020
 You can plot this, it's quite easy to link them together.

2158
02:47:09,060 --> 02:47:10,660
 No need to write program, right?

2159
02:47:10,660 --> 02:47:11,700
 It's free.

2160
02:47:12,940 --> 02:47:15,140
 I think you can look at the data, you see?

2161
02:47:15,140 --> 02:47:16,260
 You have three data type.

2162
02:47:16,260 --> 02:47:21,260
 This is iris centosa, iris versicolor virginica,

2163
02:47:22,220 --> 02:47:26,420
 and this is the separate length,

2164
02:47:26,420 --> 02:47:29,580
 the length of the flower, the leaf, right?

2165
02:47:29,580 --> 02:47:31,180
 This is the width, right?

2166
02:47:31,180 --> 02:47:33,700
 This is a distribution of the data

2167
02:47:33,700 --> 02:47:36,300
 and then the petal length, or this is very good

2168
02:47:36,300 --> 02:47:38,180
 in the sense that you can differentiate this

2169
02:47:38,180 --> 02:47:41,180
 from the other two and width, okay?

2170
02:47:41,180 --> 02:47:43,980
 You can inspect the data.

2171
02:47:43,980 --> 02:47:47,220
 Then you can build a classifier, let's see,

2172
02:47:47,220 --> 02:47:49,100
 let's show you how easy it can be.

2173
02:47:50,180 --> 02:47:51,700
 Where is my decision tree?

2174
02:47:53,740 --> 02:47:57,300
 Model, okay, this is a decision tree, okay?

2175
02:47:59,140 --> 02:48:01,380
 Here, I build the data here.

2176
02:48:01,380 --> 02:48:06,019
 I perform decision tree, but I have to view the tree,

2177
02:48:06,019 --> 02:48:08,420
 tree viewer, check the result.

2178
02:48:09,660 --> 02:48:13,019
 Okay, this is a decision tree, now done, right?

2179
02:48:13,019 --> 02:48:15,779
 Based on the petal length, it is cited,

2180
02:48:15,779 --> 02:48:20,140
 you can go and examine the number of the feature.

2181
02:48:20,140 --> 02:48:24,140
 So I tell you that you should use the petal length.

2182
02:48:24,140 --> 02:48:26,539
 Then these are the question you ask,

2183
02:48:26,539 --> 02:48:29,099
 and this is a decision, how many they make correctly,

2184
02:48:29,099 --> 02:48:31,019
 how many make it wrongly, right?

2185
02:48:31,060 --> 02:48:34,940
 You can also, this undimitted, I can set it to two level,

2186
02:48:34,940 --> 02:48:36,980
 three level, right?

2187
02:48:36,980 --> 02:48:41,980
 And of course you can do other more complex canes.

2188
02:48:42,100 --> 02:48:46,740
 You have the other projector, like canes and neighbor, right?

2189
02:48:46,740 --> 02:48:47,900
 You can also do that.

2190
02:48:50,260 --> 02:48:51,820
 Move this canes and neighbor,

2191
02:48:51,820 --> 02:48:54,260
 and then I can do the performer measure.

2192
02:48:55,700 --> 02:48:57,300
 Where's my performer measure?

2193
02:48:58,300 --> 02:49:01,580
 Okay, evaluation, okay?

2194
02:49:01,580 --> 02:49:03,500
 You can do the confusion matrix.

2195
02:49:05,500 --> 02:49:09,259
 Go, you have to do some kind of test and scoring, yeah.

2196
02:49:12,900 --> 02:49:13,900
 Here, okay.

2197
02:49:15,019 --> 02:49:18,980
 Test and scoring, and then performer measure, yeah.

2198
02:49:18,980 --> 02:49:20,980
 Okay, confusion, call it.

2199
02:49:20,980 --> 02:49:21,820
 Okay.

2200
02:49:27,260 --> 02:49:28,859
 I think something is not correct,

2201
02:49:28,859 --> 02:49:32,420
 evaluation, the result, performance, curve prediction.

2202
02:49:32,420 --> 02:49:33,820
 Maybe prediction first.

2203
02:49:40,260 --> 02:49:41,740
 Go and play around this data.

2204
02:49:41,740 --> 02:49:45,460
 You can also have other NUNA network,

2205
02:49:46,740 --> 02:49:50,939
 many other classifier, random forest, SVM,

2206
02:49:50,940 --> 02:49:55,940
 and linear regression NUNA network.

2207
02:49:56,740 --> 02:49:58,460
 And yeah, you can also put different

2208
02:49:58,460 --> 02:50:02,140
 kind of performance curve, ROC, and yeah.

2209
02:50:02,140 --> 02:50:04,340
 So quite easy to use, okay?

2210
02:50:04,340 --> 02:50:05,260
 Download and play around.

2211
02:50:05,260 --> 02:50:09,500
 You can load in a lot of data and to play around, okay?

2212
02:50:09,500 --> 02:50:12,580
 So this is one of the two that hopefully you can,

2213
02:50:12,580 --> 02:50:14,060
 I think you have a project, right?

2214
02:50:14,060 --> 02:50:16,100
 I'm not sure whether you can use it,

2215
02:50:16,100 --> 02:50:19,420
 at least verify the result, okay?

2216
02:50:19,420 --> 02:50:22,540
 So I will see you next Thursday again.

2217
02:50:22,540 --> 02:50:23,540
 Yeah, next Thursday.

2218
02:50:26,020 --> 02:50:28,100
 Next week, I will probably send out the assignment,

2219
02:50:28,100 --> 02:50:30,860
 homework assignment, so you have two assignments to do,

2220
02:50:30,860 --> 02:50:31,700
 right?

2221
02:50:44,020 --> 02:50:48,620
 Class is recess week, and after the recess week, okay?

2222
02:50:49,700 --> 02:50:51,140
 I may send out the homework earlier,

2223
02:50:51,140 --> 02:50:52,500
 so we can take a look at it.

2224
02:52:19,420 --> 02:52:20,260
 Thank you.

2225
02:52:49,420 --> 02:52:50,260
 Thank you.

2226
02:53:19,420 --> 02:53:20,260
 Thank you.

2227
02:53:49,420 --> 02:53:50,260
 Thank you.

2228
02:54:19,420 --> 02:54:20,260
 Thank you.

2229
02:54:49,420 --> 02:54:50,260
 Thank you.

2230
02:55:19,420 --> 02:55:20,260
 Thank you.

2231
02:55:49,420 --> 02:55:50,260
 Thank you.

2232
02:56:19,420 --> 02:56:20,260
 Thank you.

2233
02:56:49,420 --> 02:56:50,260
 Thank you.

2234
02:57:19,420 --> 02:57:20,260
 Thank you.

2235
02:57:49,420 --> 02:57:50,260
 Thank you.

2236
02:58:19,420 --> 02:58:20,260
 Thank you.

2237
02:58:49,420 --> 02:58:50,260
 Thank you.

2238
02:59:19,420 --> 02:59:20,260
 Thank you.

2239
02:59:49,420 --> 02:59:50,260
 Thank you.

