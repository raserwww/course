1
00:05:30,000 --> 00:05:55,100
 Good evening, welcome back.

2
00:05:55,100 --> 00:05:56,100
 Welcome back.

3
00:05:56,100 --> 00:05:59,940
 So just before we start the lecture, there's a number of announcements.

4
00:05:59,940 --> 00:06:05,620
 So number one is that today you'll notice that the lecture, there's lots of flowers.

5
00:06:05,620 --> 00:06:10,180
 Not for today's lecture, it's actually tomorrow there's some VIP who's going to use this

6
00:06:10,180 --> 00:06:11,180
 banning.

7
00:06:11,180 --> 00:06:18,220
 So I was asked to inform you all not to touch the flowers and the decoration in front.

8
00:06:18,220 --> 00:06:19,220
 Okay.

9
00:06:19,220 --> 00:06:20,220
 All right.

10
00:06:20,220 --> 00:06:21,220
 Okay.

11
00:06:21,220 --> 00:06:22,220
 So that's our first announcement.

12
00:06:22,220 --> 00:06:25,220
 Number one is regarding the makeup lecture.

13
00:06:25,220 --> 00:06:30,900
 So I think in week seven, actually I informed you that next week I'll be attending a conference

14
00:06:30,900 --> 00:06:32,000
 overseas.

15
00:06:32,000 --> 00:06:35,420
 So therefore we need to arrange a makeup.

16
00:06:35,420 --> 00:06:41,300
 So therefore for the makeup lecture is going to be on this coming Saturday.

17
00:06:41,300 --> 00:06:43,820
 So 12th of October.

18
00:06:43,820 --> 00:06:46,980
 The time will be from 9am to 12 noon.

19
00:06:46,980 --> 00:06:49,300
 So it's actually an online Zoom lecture.

20
00:06:49,300 --> 00:06:53,020
 So I'll try to record the lecture and afterwards I'll make it available online.

21
00:06:53,020 --> 00:06:57,180
 But of course you should try to attend it if you're available.

22
00:06:57,180 --> 00:07:03,660
 And then because there's this makeup lecture, so on week nine, okay, 17th of October, which

23
00:07:03,660 --> 00:07:08,240
 is the regular Thursday lecture, there will be no lecture on week nine.

24
00:07:08,240 --> 00:07:12,180
 So it'll be replaced by this coming Saturday makeup lecture.

25
00:07:12,180 --> 00:07:13,180
 Right.

26
00:07:13,180 --> 00:07:14,380
 So that's for the makeup lecture one.

27
00:07:14,380 --> 00:07:18,580
 So for makeup lecture two, once it's a bit closer, I'll remind you again.

28
00:07:18,740 --> 00:07:19,740
 Okay.

29
00:07:22,260 --> 00:07:28,180
 So okay, I have also sent out this particular announcement.

30
00:07:28,180 --> 00:07:35,460
 So all of you should have received an announcement regarding this makeup lecture for week nine.

31
00:07:35,460 --> 00:07:37,219
 So there's this announcement.

32
00:07:37,219 --> 00:07:41,180
 So what you need to do is just go to your email.

33
00:07:41,180 --> 00:07:43,539
 You'll see that there's actually a link.

34
00:07:43,539 --> 00:07:47,820
 You just click on the link and you'll be able to join the Zoom lecture.

35
00:07:47,820 --> 00:07:51,820
 So there's arrangement for this coming Saturday makeup lecture.

36
00:07:51,820 --> 00:07:53,219
 So that's the first thing.

37
00:07:53,219 --> 00:07:58,940
 So the second thing I want to remind you is also regarding the crease.

38
00:08:01,620 --> 00:08:02,620
 Right.

39
00:08:02,620 --> 00:08:06,180
 So because, okay, so this week eight, right, we have today's lecture.

40
00:08:06,180 --> 00:08:08,340
 Saturday is makeup of week nine.

41
00:08:08,340 --> 00:08:14,140
 So that means after Saturday, I won't be seeing you in week nine, but week 10 itself is already

42
00:08:14,140 --> 00:08:15,140
 the crease.

43
00:08:15,140 --> 00:08:18,900
 So I also want to take the opportunity to remind you regarding the crease.

44
00:08:18,900 --> 00:08:19,900
 Right.

45
00:08:19,900 --> 00:08:22,460
 So the crease just to remind you.

46
00:08:22,460 --> 00:08:23,460
 Okay.

47
00:08:23,460 --> 00:08:28,940
 So it'll be on week 10, 24th of October, nine, sorry, eight to eight, 40 PM.

48
00:08:28,940 --> 00:08:34,260
 So in the first one hour or so, we'll still conduct a short kind of lecture.

49
00:08:34,260 --> 00:08:39,059
 And afterwards I'll give you a break and then we'll start the crease at 8 PM.

50
00:08:39,059 --> 00:08:40,059
 Right.

51
00:08:40,059 --> 00:08:42,620
 So the venue is the same venue as this LT.

52
00:08:42,620 --> 00:08:44,420
 So it's a Likong Shin LT.

53
00:08:44,420 --> 00:08:47,380
 So topic cover is part three and part four.

54
00:08:47,380 --> 00:08:48,380
 Right.

55
00:08:48,380 --> 00:08:49,380
 Okay.

56
00:08:49,380 --> 00:08:51,540
 Once on the day, try to sit alternately.

57
00:08:51,540 --> 00:08:52,540
 Okay.

58
00:08:52,540 --> 00:08:53,740
 In the LT.

59
00:08:53,740 --> 00:08:58,579
 So arrive 10 minutes early, bring along your matriculation card for attendance taking.

60
00:08:58,579 --> 00:08:59,579
 Right.

61
00:08:59,579 --> 00:09:04,140
 And for whatever reason, if you are absent, please send me an email with some reason,

62
00:09:04,140 --> 00:09:05,939
 for example, MC and so on.

63
00:09:05,939 --> 00:09:08,260
 So that I can arrange a makeup for you.

64
00:09:08,260 --> 00:09:09,260
 Right.

65
00:09:09,260 --> 00:09:12,380
 So that's the announcement that I shared with you last time.

66
00:09:12,380 --> 00:09:13,380
 Okay.

67
00:09:13,380 --> 00:09:18,380
 So then the next thing is regarding the class list.

68
00:09:18,380 --> 00:09:19,380
 Yeah.

69
00:09:19,380 --> 00:09:20,380
 Okay.

70
00:09:20,380 --> 00:09:22,300
 So as I mentioned before, please check your serial number.

71
00:09:22,300 --> 00:09:23,300
 Okay.

72
00:09:23,300 --> 00:09:27,220
 So especially for those students with the same name, please make sure that which check your

73
00:09:27,220 --> 00:09:34,180
 serial number carefully because later on on the question booklet, the question booklet,

74
00:09:34,180 --> 00:09:36,500
 you're supposed to write down this serial number.

75
00:09:36,500 --> 00:09:37,500
 Okay.

76
00:09:37,500 --> 00:09:42,700
 So this is to make it a lot more easier for us to check who, who is present and who is

77
00:09:42,700 --> 00:09:46,060
 absent and also for our mark entering purpose.

78
00:09:46,060 --> 00:09:47,300
 Okay.

79
00:09:47,300 --> 00:09:52,180
 So if you have not done that, please try to scan through the name list and found what

80
00:09:52,180 --> 00:09:56,100
 is your sit number or serial number.

81
00:09:56,100 --> 00:09:57,600
 Okay.

82
00:09:57,600 --> 00:10:02,240
 All these documents are actually available in the course site under the crease.

83
00:10:02,240 --> 00:10:04,260
 So you can also read it up.

84
00:10:04,260 --> 00:10:05,260
 Okay.

85
00:10:05,260 --> 00:10:08,260
 And then for the crease, this is the question booklet.

86
00:10:08,260 --> 00:10:09,420
 This how it's going to be.

87
00:10:09,420 --> 00:10:10,420
 Right.

88
00:10:10,420 --> 00:10:13,099
 So on the top left corner, there's a serial number.

89
00:10:13,099 --> 00:10:17,020
 So this is the one that just now you should found what's the number and then you should

90
00:10:17,020 --> 00:10:18,020
 fill in here.

91
00:10:18,020 --> 00:10:19,020
 Okay.

92
00:10:19,020 --> 00:10:21,140
 And the creases 40 minutes, right?

93
00:10:21,140 --> 00:10:24,699
 Make sure you write down your full name and your methylation number.

94
00:10:24,699 --> 00:10:25,699
 Right.

95
00:10:25,699 --> 00:10:29,020
 So, uh, you, the question, there'll be a few questions, right?

96
00:10:29,020 --> 00:10:33,140
 There'll be some space for you to write your answer or calculation.

97
00:10:33,140 --> 00:10:37,020
 And at the bottom, usually there are some, uh, designated place for you to write down

98
00:10:37,020 --> 00:10:38,140
 your answer.

99
00:10:38,140 --> 00:10:44,780
 So please try to write down your final answer in this, uh, designated, uh, no places.

100
00:10:44,780 --> 00:10:48,660
 Usually when I do the marking, if there are some calculation marking, I'll just, uh,

101
00:10:48,660 --> 00:10:52,620
 if there's some calculation problem, I'll just check your answer at the bottom.

102
00:10:52,620 --> 00:10:57,140
 If your answer agree with what I have, usually I'll think, no, you, you answer it correctly.

103
00:10:57,140 --> 00:10:59,380
 So I wouldn't check too much detail.

104
00:10:59,380 --> 00:11:02,819
 But on the other hand, if you, for example, your numerical answer is not correct, then

105
00:11:02,819 --> 00:11:06,220
 I'll probably go back to check where other steps they make mistake.

106
00:11:06,220 --> 00:11:07,220
 Okay.

107
00:11:07,220 --> 00:11:11,740
 Uh, anyway, make sure that you write down your answer at the, uh, no, the bottom.

108
00:11:11,740 --> 00:11:14,260
 So there's going to be a few, uh, pages.

109
00:11:14,260 --> 00:11:15,340
 Yeah.

110
00:11:15,340 --> 00:11:16,580
 And then, uh, yeah.

111
00:11:16,580 --> 00:11:18,340
 So this is just the cover page.

112
00:11:18,340 --> 00:11:19,700
 So that's for the quiz.

113
00:11:19,700 --> 00:11:20,700
 Right.

114
00:11:20,700 --> 00:11:21,700
 One more thing.

115
00:11:21,700 --> 00:11:22,700
 Um, okay.

116
00:11:22,700 --> 00:11:30,140
 There's one more thing which is regarding this, uh, sitting arrangement because the

117
00:11:30,140 --> 00:11:31,580
 class is really big.

118
00:11:31,580 --> 00:11:32,580
 Yeah.

119
00:11:32,580 --> 00:11:37,020
 On that day, right, we need to get some student helper to help me to take attendance.

120
00:11:37,020 --> 00:11:42,660
 And also when we collect it, we need to tell you, right, whether, uh, no, attendance.

121
00:11:42,660 --> 00:11:46,620
 So therefore, instead of letting your sit randomly, which will make it very hard for

122
00:11:46,620 --> 00:11:49,579
 us to, you know, uh, to tell you, right.

123
00:11:49,579 --> 00:11:54,900
 So we think that the, okay, probably one of the most efficient way is try to sit according

124
00:11:54,900 --> 00:11:56,660
 to this sitting arrangement.

125
00:11:56,660 --> 00:12:01,180
 So just now for those students, you check your serial number here.

126
00:12:01,180 --> 00:12:05,780
 So for those students whose serial number is between one to 100, please sit at this,

127
00:12:05,780 --> 00:12:06,900
 uh, section here.

128
00:12:06,900 --> 00:12:08,780
 So which is the top, right?

129
00:12:08,780 --> 00:12:10,260
 Uh, a bit.

130
00:12:10,260 --> 00:12:11,260
 Okay.

131
00:12:11,260 --> 00:12:13,900
 So this three sections is one to 100.

132
00:12:13,900 --> 00:12:19,300
 And then the central three sections is, uh, 100 to one to 200.

133
00:12:19,300 --> 00:12:23,380
 And then this section here is, uh, 200 and one to 300.

134
00:12:23,380 --> 00:12:24,380
 Okay.

135
00:12:24,380 --> 00:12:25,380
 And then vice versa.

136
00:12:25,380 --> 00:12:27,500
 So you can see, uh, this sitting arrangement.

137
00:12:27,500 --> 00:12:31,740
 Please try your best to sit according to this arrangement so that when we conduct, uh, your

138
00:12:31,740 --> 00:12:35,540
 quiz paper, we can file the attendance very quickly.

139
00:12:35,540 --> 00:12:36,740
 All right.

140
00:12:36,740 --> 00:12:39,100
 Any questions so far regarding the crease?

141
00:12:39,100 --> 00:12:40,100
 Okay.

142
00:12:40,100 --> 00:12:41,100
 So I hope it's clear.

143
00:12:41,100 --> 00:12:42,100
 Huh?

144
00:12:42,100 --> 00:12:43,100
 Right.

145
00:12:43,100 --> 00:12:45,500
 So this is the announcement regarding the crease, right?

146
00:12:45,500 --> 00:12:52,660
 Then, uh, next we should be ready to, uh, continue with the lecture.

147
00:12:52,660 --> 00:12:54,660
 Right.

148
00:12:54,660 --> 00:12:57,140
 Okay.

149
00:12:57,140 --> 00:13:01,940
 So, so we are going to continue on with the CNN as I mentioned, right?

150
00:13:01,940 --> 00:13:05,100
 Some of you probably have already studied this before, but they are some students have

151
00:13:05,100 --> 00:13:06,980
 not, uh, studied this right.

152
00:13:06,980 --> 00:13:10,860
 So if you have, uh, no studied this before, just bear with me, right?

153
00:13:10,860 --> 00:13:15,740
 Probably you can also get some, uh, new insight, but I'll probably try to, you know, increase

154
00:13:15,740 --> 00:13:18,700
 the pace a little bit when we cover this section.

155
00:13:18,700 --> 00:13:24,100
 So in, uh, the lecture before the break, we look at this, uh, CNN architecture.

156
00:13:24,100 --> 00:13:28,220
 So we mainly mentioned that this CNN actually has a number of different layer.

157
00:13:28,220 --> 00:13:29,220
 Okay.

158
00:13:29,220 --> 00:13:32,100
 You have the input, you have the convolution and ReLU layer.

159
00:13:32,300 --> 00:13:33,300
 Okay.

160
00:13:33,300 --> 00:13:38,660
 Which is used to extract some, um, high level cover extract some features in a hierarchical

161
00:13:38,660 --> 00:13:39,660
 manner.

162
00:13:39,660 --> 00:13:40,660
 Okay.

163
00:13:40,660 --> 00:13:44,500
 But pretty much the first few layers mainly is for feature learning, right?

164
00:13:44,500 --> 00:13:49,260
 So which is the one to extract some feature that can represent the image.

165
00:13:49,260 --> 00:13:52,420
 And then the next few layers use the classification, right?

166
00:13:52,420 --> 00:13:57,700
 Which consists of the FC layer as the, as well as the soft max layer is to classify this,

167
00:13:58,100 --> 00:13:58,700
 right?

168
00:13:58,700 --> 00:14:02,300
 Feature or this embedding or this factor into one of the classes.

169
00:14:02,300 --> 00:14:02,900
 Right.

170
00:14:02,900 --> 00:14:04,600
 So this is the high level overview.

171
00:14:04,600 --> 00:14:08,500
 So next we're going to look at each of the layer in CNN a bit more in details.

172
00:14:08,500 --> 00:14:09,500
 Right.

173
00:14:09,500 --> 00:14:10,500
 Okay.

174
00:14:10,500 --> 00:14:12,500
 So the basic architecture of CNN is as follow.

175
00:14:12,500 --> 00:14:14,300
 So we have the convolutional layer.

176
00:14:14,300 --> 00:14:15,300
 Okay.

177
00:14:15,300 --> 00:14:17,700
 Activation function layer, pulling layer.

178
00:14:17,700 --> 00:14:18,700
 Okay.

179
00:14:18,700 --> 00:14:19,700
 FC layer.

180
00:14:19,700 --> 00:14:22,820
 FC layer is also the linear layer that we have studied in previous lecture.

181
00:14:22,820 --> 00:14:25,100
 And then finally the soft max layer.

182
00:14:25,100 --> 00:14:26,100
 Right.

183
00:14:26,100 --> 00:14:28,660
 So what's a convolutional layer?

184
00:14:28,660 --> 00:14:33,700
 So the name CNN actually takes after this convolution process here.

185
00:14:33,700 --> 00:14:38,860
 So convolution layer, the goal is that is try to extract some features in a hierarchical

186
00:14:38,860 --> 00:14:39,860
 manner.

187
00:14:39,860 --> 00:14:42,300
 So early layers extract simple features.

188
00:14:42,300 --> 00:14:46,020
 Later layers extract more complicated features.

189
00:14:46,020 --> 00:14:51,020
 So we can see early layer extract simple feature like low level feature like point edge and

190
00:14:51,020 --> 00:14:52,140
 so on.

191
00:14:52,140 --> 00:14:54,300
 And later layers extract high level feature.

192
00:14:54,300 --> 00:14:55,300
 Right.

193
00:14:55,300 --> 00:14:58,579
 So one of the important properties of this convolutional layer is that it reduce the

194
00:14:58,579 --> 00:15:04,859
 number of parameter by weight sharing through this filter or also known as a kernel.

195
00:15:04,859 --> 00:15:05,859
 Right.

196
00:15:05,859 --> 00:15:12,740
 So you're going to apply the train filter or kernel across all the positions in the image.

197
00:15:12,740 --> 00:15:16,439
 So the same filter is going to be applied across all the positions.

198
00:15:16,439 --> 00:15:19,620
 So therefore you reduce the number of parameter quite significantly.

199
00:15:19,620 --> 00:15:20,620
 Okay.

200
00:15:20,620 --> 00:15:26,380
 So this is known as the, you know, this which sharing through the same kernel or filter

201
00:15:26,380 --> 00:15:29,900
 across all different spatial position in the image.

202
00:15:29,900 --> 00:15:30,900
 Right.

203
00:15:30,900 --> 00:15:31,900
 Okay.

204
00:15:31,900 --> 00:15:33,540
 So this slide shows the visualizations.

205
00:15:33,540 --> 00:15:34,540
 Right.

206
00:15:34,540 --> 00:15:35,540
 Suppose you have an image.

207
00:15:35,540 --> 00:15:36,540
 Right.

208
00:15:36,540 --> 00:15:37,540
 You have a few different layers.

209
00:15:37,540 --> 00:15:41,540
 So you can see early layer tends to extract some low level feature, very simple feature.

210
00:15:41,540 --> 00:15:42,540
 Okay.

211
00:15:42,540 --> 00:15:48,100
 Higher level tends to extract a lot more complicated features ready because it's progressively

212
00:15:48,100 --> 00:15:54,460
 trying to extract combinations of, no, from low level to become combinations of mid level

213
00:15:54,460 --> 00:15:59,780
 feature combinations of mid level feature that will obtain some more complex high level

214
00:15:59,780 --> 00:16:00,780
 feature.

215
00:16:00,780 --> 00:16:01,780
 Okay.

216
00:16:01,780 --> 00:16:05,700
 So, and then finally you have the classifier that will perform the classification, which

217
00:16:05,700 --> 00:16:10,460
 usually consists of the FC as well as the softmax layers.

218
00:16:10,460 --> 00:16:11,460
 Right.

219
00:16:11,460 --> 00:16:12,460
 Okay.

220
00:16:12,460 --> 00:16:16,180
 So next we are going to go through how do we perform the convolution so you can take

221
00:16:16,180 --> 00:16:18,900
 this as a revision if you have studied before.

222
00:16:18,900 --> 00:16:22,339
 So suppose you have an image, which is 32 by 32 by three.

223
00:16:22,339 --> 00:16:27,540
 So 32 by 32 and is three is three channel RGB image here.

224
00:16:27,540 --> 00:16:31,660
 So when you want to apply a filter to perform the convolution, right.

225
00:16:31,660 --> 00:16:34,939
 So this particular filter, the spatial dimension is five by five.

226
00:16:34,939 --> 00:16:35,939
 Okay.

227
00:16:35,939 --> 00:16:37,979
 So you can see it's five by five here.

228
00:16:37,979 --> 00:16:41,760
 And then the last dimension three is actually the depth.

229
00:16:41,760 --> 00:16:49,880
 So this three here, this number here three need to be consistent with the input, no volume,

230
00:16:49,880 --> 00:16:52,520
 the depth or the number of channel for the input volume.

231
00:16:52,520 --> 00:16:53,520
 Right.

232
00:16:53,520 --> 00:16:54,520
 Okay.

233
00:16:54,520 --> 00:16:59,439
 So that when you superimpose on each other, the filter that dimension and the input volume

234
00:16:59,439 --> 00:17:01,040
 channel is consistent.

235
00:17:01,040 --> 00:17:02,040
 Right.

236
00:17:02,040 --> 00:17:03,040
 Okay.

237
00:17:03,040 --> 00:17:06,240
 So this thing is, this that dimension is kind of understood.

238
00:17:06,240 --> 00:17:11,040
 Sometimes when we talk about a convolution, very often sometimes when we talk about filter,

239
00:17:11,040 --> 00:17:15,440
 we ignore this number, we just simply say it's five by five filter, for example, but

240
00:17:15,440 --> 00:17:21,800
 it's understood that, okay, there's still a depth channel and this num, the depth channel

241
00:17:21,800 --> 00:17:25,760
 right need to be consistent with the number of channel for your input volume.

242
00:17:25,760 --> 00:17:26,760
 Right.

243
00:17:26,760 --> 00:17:27,760
 Okay.

244
00:17:27,760 --> 00:17:28,760
 So that's just for your information.

245
00:17:28,760 --> 00:17:29,760
 Right.

246
00:17:29,760 --> 00:17:30,760
 So how do we perform convolution?

247
00:17:30,760 --> 00:17:34,480
 So when perform convolution, what we do is that we take one of these particular filter,

248
00:17:34,480 --> 00:17:38,720
 for example, five by five by three filter, we put it at the top left corner.

249
00:17:38,720 --> 00:17:39,720
 Okay.

250
00:17:39,720 --> 00:17:43,160
 So you superimpose and then you perform a sum of product operation.

251
00:17:43,160 --> 00:17:46,600
 Some of product operation means that you multiply the corresponding value.

252
00:17:46,600 --> 00:17:47,600
 Okay.

253
00:17:47,600 --> 00:17:48,880
 And then you sum it up.

254
00:17:48,880 --> 00:17:52,120
 So when you do that, you generate a single output value.

255
00:17:52,120 --> 00:17:55,280
 So this single output value would then be written as here.

256
00:17:55,280 --> 00:17:59,680
 And then next you will take this particular filter, you shift to the, no, to the right

257
00:17:59,680 --> 00:18:01,920
 by the amount shift.

258
00:18:01,920 --> 00:18:07,040
 For example, if the amount shift is one position, okay, so this filter now is located at this

259
00:18:07,040 --> 00:18:08,360
 position here.

260
00:18:08,360 --> 00:18:13,360
 You perform this sum of product operation or the filtering operation or roughly speaking

261
00:18:13,360 --> 00:18:18,360
 the convolution operation, then you'll be able to obtain the second value, which is

262
00:18:18,360 --> 00:18:19,679
 located here.

263
00:18:19,679 --> 00:18:25,120
 So therefore you slides this particular corner or filter across all the position.

264
00:18:25,120 --> 00:18:29,240
 Your output will be what is known as the activation map or feature map.

265
00:18:29,240 --> 00:18:30,240
 Okay.

266
00:18:30,240 --> 00:18:31,479
 Activation map or feature map.

267
00:18:31,479 --> 00:18:32,479
 Right.

268
00:18:32,479 --> 00:18:37,820
 So you can see that in this case here, the dimension is 28 by 28.

269
00:18:37,820 --> 00:18:45,500
 So if you pause one moment and think about why is the output dimension 28 by 28?

270
00:18:45,500 --> 00:18:48,580
 So if you think about it, right, you have this five by five filter.

271
00:18:48,580 --> 00:18:54,340
 When this five by five filter is put at the top left corner here, the center of this particular

272
00:18:54,340 --> 00:19:00,620
 filter here with respect to the coordinate for this particular input volume here is actually

273
00:19:00,620 --> 00:19:01,620
 at three.

274
00:19:01,620 --> 00:19:02,620
 Yeah.

275
00:19:02,620 --> 00:19:07,419
 And as you shift it to the extreme right direction, okay, the center of this particular

276
00:19:08,020 --> 00:19:11,220
 filter now will be located at 30.

277
00:19:11,220 --> 00:19:18,340
 So it's known from position three all the way to 30 is 30 minus three, which is 27,

278
00:19:18,340 --> 00:19:21,540
 but you also need to add in a boundary condition.

279
00:19:21,540 --> 00:19:25,580
 So add up with one that will give you the dimensions of 28.

280
00:19:25,580 --> 00:19:26,580
 All right.

281
00:19:26,580 --> 00:19:32,300
 So therefore if you slice it, every time you perform this convolution or the filtering or

282
00:19:32,300 --> 00:19:35,780
 the sum of product, you're all referred to the same thing, then you are going to get

283
00:19:35,780 --> 00:19:44,100
 one slice or one channel of your activation map, which looks like this 28 by 28 by one.

284
00:19:44,100 --> 00:19:45,100
 Okay.

285
00:19:45,100 --> 00:19:51,060
 So therefore by using one particular filter, you have extract certain information.

286
00:19:51,060 --> 00:19:56,740
 Maybe this filter is good at extracting, for example, vertical edge information.

287
00:19:56,740 --> 00:19:57,740
 Yeah.

288
00:19:57,740 --> 00:20:02,780
 But in order for you to be able to extract different types of information from your input

289
00:20:02,780 --> 00:20:05,180
 volume, you need different filter.

290
00:20:05,180 --> 00:20:10,500
 So that's why you need additional filter.

291
00:20:10,500 --> 00:20:14,820
 So now, okay, so we need an addition, we need other filters.

292
00:20:14,820 --> 00:20:18,080
 So suppose now let's consider a second filter here now.

293
00:20:18,080 --> 00:20:22,100
 So this is the second filter is also five by five by three.

294
00:20:22,100 --> 00:20:28,180
 So now if you repeat the same process of moving through all this position, performing the convolution

295
00:20:28,180 --> 00:20:33,340
 or filtering operation, then the output is you are going to get another channel or another

296
00:20:33,340 --> 00:20:38,780
 slice of the activation map here, which is 28 by 28 by one.

297
00:20:38,780 --> 00:20:45,860
 So when you have two feature map or two activation map now, so we call activation maps now.

298
00:20:45,860 --> 00:20:47,699
 So now it becomes a prurer.

299
00:20:47,699 --> 00:20:53,260
 So therefore you can actually use as many filter as you want.

300
00:20:53,260 --> 00:20:59,139
 And then you'll be actually generating different layers of this particular activation map.

301
00:20:59,140 --> 00:21:03,140
 So this activation map sometimes is also known as the output volume, okay, because

302
00:21:03,140 --> 00:21:07,140
 now it has that information.

303
00:21:07,140 --> 00:21:08,140
 Right.

304
00:21:08,140 --> 00:21:09,140
 Okay.

305
00:21:09,140 --> 00:21:12,700
 So now if you do that, if you put everything together, let's look at this example here.

306
00:21:12,700 --> 00:21:16,580
 Suppose your input image is 32 by 32 by three, right?

307
00:21:16,580 --> 00:21:21,220
 And then if you use a six, five by five by three filter.

308
00:21:21,220 --> 00:21:26,940
 So for each of the five by five by three filter, you can see just now from the previous example,

309
00:21:26,940 --> 00:21:30,740
 you're going to get one slice of 28 by 28 by one.

310
00:21:30,740 --> 00:21:31,740
 Okay.

311
00:21:31,740 --> 00:21:32,940
 Activation map.

312
00:21:32,940 --> 00:21:39,220
 But because you're using six filter, so now your output volume is going to have six channel.

313
00:21:39,220 --> 00:21:40,220
 Right.

314
00:21:40,220 --> 00:21:43,880
 So your output volume now you can see because they're using six filter now.

315
00:21:43,880 --> 00:21:46,380
 So your output volume will have six channel.

316
00:21:46,380 --> 00:21:47,380
 Right.

317
00:21:47,380 --> 00:21:48,380
 Okay.

318
00:21:48,380 --> 00:21:53,980
 So next suppose now in the next layer, we use 10, five by five by six filter here.

319
00:21:53,980 --> 00:21:58,420
 So now the filter is five by five, but the depth has to be six now.

320
00:21:58,420 --> 00:22:03,620
 This number six need to be consistent with the input volume as we have mentioned.

321
00:22:03,620 --> 00:22:07,820
 So each of these five by five by six is going to generate a channel, right?

322
00:22:07,820 --> 00:22:11,980
 A slice of 24 by 24 by one activation map.

323
00:22:11,980 --> 00:22:17,900
 But since they're using 10 filter, so your output volume now is 24 by 24 by 10.

324
00:22:17,900 --> 00:22:18,900
 Right.

325
00:22:18,900 --> 00:22:26,060
 So now you're able to extract features, no layer by layer by using no different filters.

326
00:22:26,060 --> 00:22:29,060
 So that's the basic operations of the convolution.

327
00:22:29,060 --> 00:22:30,060
 Right.

328
00:22:30,060 --> 00:22:31,060
 Okay.

329
00:22:31,060 --> 00:22:35,660
 So when we perform convolution, very often we also combine with this padding process.

330
00:22:35,660 --> 00:22:36,740
 Okay.

331
00:22:36,740 --> 00:22:38,860
 So the idea of padding is quite simple.

332
00:22:38,860 --> 00:22:45,700
 Suppose if you have a particular input image just now, which is a 32 by 32 by three, 32

333
00:22:45,700 --> 00:22:47,940
 by 32, right by three.

334
00:22:47,980 --> 00:22:53,180
 It's the depth, but for this visualization, we do not show the depth information.

335
00:22:53,180 --> 00:22:56,140
 We only show the spatial information here.

336
00:22:56,140 --> 00:23:01,500
 So from the earlier example, you can see that if we use a five by five by three filter,

337
00:23:01,500 --> 00:23:06,340
 the size of your output volume is going to be reduced to 28 by 28.

338
00:23:06,340 --> 00:23:07,340
 Right.

339
00:23:07,340 --> 00:23:11,860
 But at times you may want to retain the size of this output volume.

340
00:23:11,860 --> 00:23:17,660
 You want your output volume spatial dimension to be consistent with your input volume, no

341
00:23:17,700 --> 00:23:19,020
 spatial dimension.

342
00:23:19,020 --> 00:23:20,020
 Right.

343
00:23:20,020 --> 00:23:24,500
 No, sometimes you just want to do it for consistency purpose or for other consideration.

344
00:23:24,500 --> 00:23:30,380
 So if you want to do that, then one easy trick that you can do is to try to pack some layers

345
00:23:30,380 --> 00:23:32,980
 of zero next to your input volume.

346
00:23:32,980 --> 00:23:33,980
 Okay.

347
00:23:33,980 --> 00:23:35,620
 So we add some zeros next to it.

348
00:23:35,620 --> 00:23:40,500
 For example, in this case, we add two layers of zero at each side here.

349
00:23:40,500 --> 00:23:45,940
 So if you add two layer now, this particular input volume now will have the have the dimension

350
00:23:45,980 --> 00:23:48,820
 of 36 by 36 by three.

351
00:23:48,820 --> 00:23:49,820
 Right.

352
00:23:49,820 --> 00:23:50,820
 You have that.

353
00:23:50,820 --> 00:23:54,380
 No, the depth is still three, but we ignore the depth information for this visualization.

354
00:23:54,380 --> 00:23:57,020
 We just focus on the spatial information.

355
00:23:57,020 --> 00:24:03,100
 So now after padding the dimension 36 by 36, suppose now if we use a five by five by three

356
00:24:03,100 --> 00:24:10,340
 filter, you put it at this top positions here, you can see respect to the index image index

357
00:24:10,340 --> 00:24:11,980
 of this enlarged image.

358
00:24:11,980 --> 00:24:12,980
 Okay.

359
00:24:12,980 --> 00:24:16,300
 So if you coordinate here is actually, we recall this one, two, three.

360
00:24:16,300 --> 00:24:17,300
 This is three.

361
00:24:17,300 --> 00:24:18,300
 Okay.

362
00:24:18,300 --> 00:24:23,300
 And then afterwards, all the way when you come to here, right, you'll be 34.

363
00:24:23,300 --> 00:24:24,300
 Right.

364
00:24:24,300 --> 00:24:30,900
 So it's 34 minus three, which give you 31, but you need to add up with one for the boundary

365
00:24:30,900 --> 00:24:31,900
 condition.

366
00:24:31,900 --> 00:24:35,660
 So therefore your output is actually 32 by 32 again.

367
00:24:35,660 --> 00:24:41,860
 So then you can see that you can preserve the dimensions of your input volume spatial

368
00:24:42,020 --> 00:24:46,340
 dimension and your output volume spatial dimension is the same.

369
00:24:46,340 --> 00:24:49,060
 So this is one simple trick to do that.

370
00:24:49,060 --> 00:24:50,060
 Right.

371
00:24:50,060 --> 00:24:53,340
 So you can see this is the basic explanation here.

372
00:24:53,340 --> 00:24:54,340
 Right.

373
00:24:54,340 --> 00:25:00,379
 So if we have this initially, this 32 by 32 by three volume, if we pat two borders of

374
00:25:00,379 --> 00:25:05,219
 zero, it becomes 36 by 36 by three, 36 by 36 by three.

375
00:25:05,219 --> 00:25:06,219
 Okay.

376
00:25:06,219 --> 00:25:10,500
 And if we apply just now the same filter of five by five by three, okay, we can see that

377
00:25:10,500 --> 00:25:14,180
 our output volume now is 32 by 32 by three.

378
00:25:14,180 --> 00:25:19,220
 So we retain the dimension spatial dimension of your input volume and output volume is

379
00:25:19,220 --> 00:25:22,940
 the same now by simply doing some padding operation.

380
00:25:22,940 --> 00:25:23,940
 Right.

381
00:25:23,940 --> 00:25:24,940
 Okay.

382
00:25:24,940 --> 00:25:30,340
 So this simple animation just show how do we perform the, you know, the convolution

383
00:25:30,340 --> 00:25:31,340
 process.

384
00:25:31,340 --> 00:25:32,580
 So you have some input.

385
00:25:32,580 --> 00:25:33,580
 Okay.

386
00:25:33,580 --> 00:25:34,580
 Volume.

387
00:25:34,580 --> 00:25:35,580
 Right.

388
00:25:35,580 --> 00:25:36,740
 So we ignore the debt information.

389
00:25:36,740 --> 00:25:39,180
 We just focus on the spatial information.

390
00:25:39,180 --> 00:25:41,180
 So we have the input volume.

391
00:25:41,180 --> 00:25:42,980
 You pet up with some zero.

392
00:25:42,980 --> 00:25:43,980
 Okay.

393
00:25:43,980 --> 00:25:47,500
 Suppose if your filter in this case happened to be a four by four filter.

394
00:25:47,500 --> 00:25:52,380
 So you just slide through every time at a certain position, you will generate an output

395
00:25:52,380 --> 00:25:54,140
 and then you just slice it through.

396
00:25:54,140 --> 00:25:55,140
 Okay.

397
00:25:55,140 --> 00:25:59,220
 And then you perform this filtering or some of product operation.

398
00:25:59,220 --> 00:26:03,100
 Then you'll be generating this particular output.

399
00:26:03,100 --> 00:26:04,100
 Okay.

400
00:26:04,980 --> 00:26:08,980
 Or this activation map or activation feature.

401
00:26:08,980 --> 00:26:11,379
 Right.

402
00:26:11,379 --> 00:26:12,540
 So next, right.

403
00:26:12,540 --> 00:26:18,659
 So this particular example, just try to show suppose now, okay, you have a particular input

404
00:26:18,659 --> 00:26:19,659
 image now.

405
00:26:19,659 --> 00:26:20,659
 Okay.

406
00:26:20,659 --> 00:26:21,659
 So this is the RGB image here.

407
00:26:21,659 --> 00:26:22,659
 Right.

408
00:26:22,659 --> 00:26:23,659
 So you can see it.

409
00:26:23,659 --> 00:26:26,260
 You have a RGB image that has three channel.

410
00:26:26,260 --> 00:26:27,780
 This is the red channel.

411
00:26:27,780 --> 00:26:28,780
 Okay.

412
00:26:28,780 --> 00:26:31,179
 This corresponds to the green channel of image.

413
00:26:31,179 --> 00:26:33,379
 This corresponds to the blue channel of image.

414
00:26:33,580 --> 00:26:34,580
 Right.

415
00:26:34,580 --> 00:26:36,940
 Suppose we are only looking at the top left corner.

416
00:26:36,940 --> 00:26:38,940
 So we have one layers of zero padding.

417
00:26:38,940 --> 00:26:39,940
 Okay.

418
00:26:39,940 --> 00:26:42,340
 We introduced one layer of zero padding.

419
00:26:42,340 --> 00:26:45,860
 Suppose our current filter now is, you can see it's three by three.

420
00:26:45,860 --> 00:26:46,860
 Right.

421
00:26:46,860 --> 00:26:51,260
 So the spatial dimension is three by three, but the depth is actually three as well because

422
00:26:51,260 --> 00:26:54,340
 you have one for red, one for green and one for blue channel.

423
00:26:54,340 --> 00:26:57,420
 So the filter that we have is three by three by three.

424
00:26:57,420 --> 00:26:58,420
 Okay.

425
00:26:58,420 --> 00:27:02,660
 So if you want to perform the filtering operation, yeah, it says that, you know, you're just

426
00:27:02,660 --> 00:27:08,780
 superimposed this particular filter into, you know, current positions of the image.

427
00:27:08,780 --> 00:27:09,780
 Yeah.

428
00:27:09,780 --> 00:27:14,500
 For example, I'm not sure whether I can stop it, but anyway, suppose it's at the top left

429
00:27:14,500 --> 00:27:19,780
 corner, you just know, superimposed, you multiply their corresponding value.

430
00:27:19,780 --> 00:27:20,780
 You sum it up.

431
00:27:20,780 --> 00:27:21,780
 Okay.

432
00:27:21,780 --> 00:27:23,620
 That will give, for example, this value.

433
00:27:23,620 --> 00:27:25,820
 So you repeat that for the green channel.

434
00:27:25,820 --> 00:27:30,900
 Suppose at this position, you multiply their corresponding value and you add them up.

435
00:27:31,020 --> 00:27:32,980
 That will give you this value.

436
00:27:32,980 --> 00:27:40,140
 So anyway, you add up the output due to the green, red, red, green and blue channel.

437
00:27:40,140 --> 00:27:43,100
 And if you also use bias, this bias is optional.

438
00:27:43,100 --> 00:27:46,100
 If you choose to use the bias, then you add it up.

439
00:27:46,100 --> 00:27:47,300
 This will be output.

440
00:27:47,300 --> 00:27:49,900
 So at each position, right?

441
00:27:49,900 --> 00:27:54,900
 So you generate output and then as you slide through this filter, then you'll be filling

442
00:27:54,900 --> 00:27:57,100
 all those entry.

443
00:27:57,100 --> 00:28:04,340
 So pretty much this is just an animation to again explain what we have mentioned, right?

444
00:28:04,340 --> 00:28:07,100
 There's nothing too new about this.

445
00:28:07,100 --> 00:28:08,179
 Okay.

446
00:28:08,179 --> 00:28:12,659
 So the next concept that we need to introduce very quickly is a strike.

447
00:28:12,659 --> 00:28:14,820
 So as the name suggests, strike is a mouth.

448
00:28:14,820 --> 00:28:20,100
 You know, when you move your filter from one position to the next position, how much you

449
00:28:20,100 --> 00:28:21,179
 have to move.

450
00:28:21,179 --> 00:28:23,500
 So strike is like, you know, the human, right?

451
00:28:23,500 --> 00:28:26,659
 Strike means that when you walk, how big is your step size?

452
00:28:26,700 --> 00:28:28,060
 So that's a meaning of strike.

453
00:28:28,060 --> 00:28:32,980
 So in the context of CNN, that means how much as you move from the filter from one position

454
00:28:32,980 --> 00:28:36,380
 to the other position, how much you move.

455
00:28:36,380 --> 00:28:40,420
 So suppose if you take this example here, you have input volume, which is like this.

456
00:28:40,420 --> 00:28:41,420
 Okay.

457
00:28:41,420 --> 00:28:43,780
 Again, we ignore the depth dimension.

458
00:28:43,780 --> 00:28:45,780
 If this is the input volume, right?

459
00:28:45,780 --> 00:28:49,020
 Suppose we use a three by three filter, right?

460
00:28:49,020 --> 00:28:54,940
 So at this top left position, after you perform the convolution operation, you generate this

461
00:28:54,940 --> 00:28:56,300
 output.

462
00:28:56,300 --> 00:29:01,860
 So the next position you move, if it's a strike of one, that means you only move to the next

463
00:29:01,860 --> 00:29:04,820
 position with a step size of one away.

464
00:29:04,820 --> 00:29:05,820
 Okay.

465
00:29:05,820 --> 00:29:10,820
 So now with this filter and the underlying image, you perform the filtering process or

466
00:29:10,820 --> 00:29:13,340
 you multiply their corresponding value.

467
00:29:13,340 --> 00:29:14,580
 You sum it up.

468
00:29:14,580 --> 00:29:17,820
 That would give you the green value, right?

469
00:29:17,820 --> 00:29:22,980
 And afterwards, if you repeat, when you come to here, next, if you move down, right, if

470
00:29:22,980 --> 00:29:27,540
 you do not indicate otherwise, then you also move down with a strike of one.

471
00:29:27,540 --> 00:29:28,540
 Okay.

472
00:29:28,540 --> 00:29:31,100
 Then you'll be generating the output at the next row.

473
00:29:31,100 --> 00:29:32,100
 Okay.

474
00:29:32,100 --> 00:29:34,700
 And then afterwards, you repeat the whole process.

475
00:29:34,700 --> 00:29:37,940
 This is how you're going to obtain your output volume.

476
00:29:37,940 --> 00:29:39,060
 Okay.

477
00:29:39,060 --> 00:29:44,380
 So the next example is if you again use a three by three filter, but now with a strike

478
00:29:44,380 --> 00:29:49,460
 of two, so what I mean is initially it's at this position, performing the convolution

479
00:29:49,460 --> 00:29:52,860
 of filtering operation, you'll get generate this rate output.

480
00:29:52,860 --> 00:29:59,300
 Next, when you shift, you shift by a position of two, you move right by a position of two

481
00:29:59,300 --> 00:30:03,860
 to this green position, performing filtering or convolution, you'll get this value.

482
00:30:03,860 --> 00:30:04,979
 And the next one is here.

483
00:30:04,979 --> 00:30:05,979
 You get this value.

484
00:30:05,979 --> 00:30:09,459
 Next, when you move it down, you also move down by a position of two.

485
00:30:09,459 --> 00:30:10,459
 Right.

486
00:30:10,459 --> 00:30:12,860
 So if you perform filtering, this is your blue output.

487
00:30:12,860 --> 00:30:13,860
 Okay.

488
00:30:13,860 --> 00:30:16,780
 So that's pretty much what strike is about.

489
00:30:16,780 --> 00:30:17,780
 Right.

490
00:30:17,780 --> 00:30:22,659
 So this animation shows the very simple examples of the strike of two.

491
00:30:22,660 --> 00:30:24,460
 We can see initially it's here.

492
00:30:24,460 --> 00:30:25,460
 Okay.

493
00:30:25,460 --> 00:30:28,900
 You move to, when you move to a side is by a strike of two, when you move down is also

494
00:30:28,900 --> 00:30:32,860
 with a strike of two.

495
00:30:32,860 --> 00:30:36,340
 Okay.

496
00:30:36,340 --> 00:30:44,940
 So the next thing after the convolution, right, the next layer that we have is the activation

497
00:30:44,940 --> 00:30:45,940
 layer.

498
00:30:45,940 --> 00:30:46,940
 Right.

499
00:30:46,940 --> 00:30:50,900
 So for this activation layer, what it does is that once you get the output of this convolution,

500
00:30:50,900 --> 00:30:53,820
 you immediately let it pass through an activation.

501
00:30:53,820 --> 00:30:58,100
 So activation is an element by element nonlinear activation function.

502
00:30:58,100 --> 00:30:59,620
 You simply get the output.

503
00:30:59,620 --> 00:31:03,800
 You let it pass through a nonlinear function to generate the output.

504
00:31:03,800 --> 00:31:07,660
 So why do you want to let it go through a nonlinear function?

505
00:31:07,660 --> 00:31:13,580
 It's because nonlinear function provide a more dynamic modeling capability.

506
00:31:13,580 --> 00:31:17,220
 It can model the value in a more powerful manner.

507
00:31:17,220 --> 00:31:18,220
 Right.

508
00:31:18,220 --> 00:31:19,540
 So therefore, yeah.

509
00:31:19,540 --> 00:31:26,020
 So we are going to let the output of the convolution to let it through a nonlinear activation function.

510
00:31:26,020 --> 00:31:30,020
 So among them, value is one of the most popular activation functions.

511
00:31:30,020 --> 00:31:31,379
 Some of you probably have seen it.

512
00:31:31,379 --> 00:31:32,379
 Right.

513
00:31:32,379 --> 00:31:36,340
 So this activation layer is commonly closely coupled with the convolution together.

514
00:31:36,340 --> 00:31:37,340
 Yeah.

515
00:31:37,340 --> 00:31:41,260
 Usually as soon as you get the convolution output, you just let it go through this activation

516
00:31:41,260 --> 00:31:43,500
 layer immediately.

517
00:31:43,500 --> 00:31:48,820
 So therefore they are very often written as con plus, no, for example, activation or con

518
00:31:48,820 --> 00:31:52,060
 plus rellu right away.

519
00:31:52,060 --> 00:31:57,060
 So this slide shows some of the sample activation function in the activation layer.

520
00:31:57,060 --> 00:32:00,939
 So some of the well known one, for example, is risk rellu function.

521
00:32:00,939 --> 00:32:05,740
 So this rellu function horizontal axis is the input, vertical axis is the output.

522
00:32:05,740 --> 00:32:11,540
 So you can see when the input is positive or zero, your output is the same.

523
00:32:11,540 --> 00:32:12,899
 No distortion.

524
00:32:12,899 --> 00:32:18,300
 But when your input is actually a negative value, your output will be kept at zero.

525
00:32:18,300 --> 00:32:22,419
 So this particular modeling can be shown by this equations here, the maximum between

526
00:32:22,419 --> 00:32:23,980
 zero or X.

527
00:32:23,980 --> 00:32:27,139
 So when X is positive, right, you just take the X value.

528
00:32:27,139 --> 00:32:30,740
 But when your X is negative, you'll be taking the zero value.

529
00:32:30,740 --> 00:32:33,980
 So this is a very popular activation function.

530
00:32:33,980 --> 00:32:38,820
 There are also some other popular functions such as, for example, this sigmoid function

531
00:32:38,820 --> 00:32:45,260
 here, which map your input value to an output value between zero and one and 10 H, which

532
00:32:45,260 --> 00:32:48,660
 map your input value to output value between minus one and one.

533
00:32:48,660 --> 00:32:54,100
 So these are some popular activation function.

534
00:32:54,100 --> 00:32:59,940
 So if we now put convolution and rellu together, so pretty much is that assuming that we have

535
00:32:59,940 --> 00:33:05,980
 an input volume, which is 2 to 4 by 2 to 4 by 3, for example, in VGG network, this is

536
00:33:05,980 --> 00:33:08,020
 the dimensions of an input image.

537
00:33:08,020 --> 00:33:10,540
 2 to 4 by 2 to 4 by 3.

538
00:33:10,540 --> 00:33:16,500
 We use a filter of 3 by 3 by 3 filter.

539
00:33:16,500 --> 00:33:20,700
 So we know that we are going to go through each of these particular positions to perform

540
00:33:20,700 --> 00:33:21,700
 the convolution.

541
00:33:21,700 --> 00:33:25,540
 Suppose this is the current position it's operating on.

542
00:33:25,540 --> 00:33:31,420
 So you take this particular filter at this particular position, you superimpose, you

543
00:33:31,420 --> 00:33:37,020
 multiply their corresponding term, and you sum it up.

544
00:33:37,020 --> 00:33:39,220
 So that will give you the output.

545
00:33:39,220 --> 00:33:42,540
 So this output would then immediately go through the activation function.

546
00:33:42,540 --> 00:33:46,700
 For example, in this case, it's a sigmoid activation function, and then you generate

547
00:33:46,700 --> 00:33:49,260
 this output directly.

548
00:33:49,260 --> 00:33:52,660
 So afterwards, you actually repeat it for all the positions.

549
00:33:52,660 --> 00:33:55,780
 That means you'll be generating the output directly.

550
00:33:55,780 --> 00:34:03,340
 So we combine, in this case, convolution and activation together.

551
00:34:03,340 --> 00:34:05,780
 So we have covered two of the layers already.

552
00:34:05,780 --> 00:34:08,580
 So the next layer that we have is the pooling layer.

553
00:34:08,580 --> 00:34:14,100
 So the objective of the pooling layer is that you want to reduce the size of your activation

554
00:34:14,100 --> 00:34:15,299
 map.

555
00:34:15,299 --> 00:34:19,460
 Because if you have a very large activation map, there's a lot of computation you need

556
00:34:19,460 --> 00:34:20,460
 to perform.

557
00:34:20,460 --> 00:34:22,020
 There's also a lot of storage.

558
00:34:22,020 --> 00:34:28,340
 So therefore, sometimes you want to reduce the number of the size of the activation map.

559
00:34:28,340 --> 00:34:32,299
 So therefore, the goals of this pooling layer is to reduce the activation map dimension or

560
00:34:32,299 --> 00:34:33,460
 size.

561
00:34:33,460 --> 00:34:38,860
 By reducing it, you can reduce a number of downstream or subsequent computation, as

562
00:34:38,860 --> 00:34:40,820
 well as the storage.

563
00:34:40,820 --> 00:34:44,860
 So there's a few common pooling operations, such as max pooling, average pooling, that

564
00:34:44,860 --> 00:34:46,780
 we'll show later.

565
00:34:46,780 --> 00:34:52,260
 So for the pooling operation, each of the channel, each channel in the feature map or

566
00:34:52,260 --> 00:34:56,780
 the activation map, we perform it independently.

567
00:34:56,780 --> 00:35:00,740
 So let's look at this diagram to show how do we perform the pooling.

568
00:35:00,740 --> 00:35:05,819
 Suppose this is the current channel or the current slice that you want to perform the

569
00:35:05,819 --> 00:35:07,180
 pooling.

570
00:35:07,180 --> 00:35:16,020
 And if you are performing as max pooling with a 2 by 2 filter and with a strut of 2, so

571
00:35:16,020 --> 00:35:21,140
 2 by 2 filter means that we partition this particular, first of all, we examine this

572
00:35:21,140 --> 00:35:24,740
 2 by 2 block here.

573
00:35:24,740 --> 00:35:28,540
 And then because it's max pooling, means that you choose the largest value in this 2 by

574
00:35:28,540 --> 00:35:30,439
 2 block here.

575
00:35:30,440 --> 00:35:35,000
 So in this example, the largest value is 6, so you write down the 6.

576
00:35:35,000 --> 00:35:36,240
 Next what's the strut?

577
00:35:36,240 --> 00:35:37,920
 You have to examine what strut you're using.

578
00:35:37,920 --> 00:35:42,480
 If you're using a strut of 2, that means now you are going to move this particular filter

579
00:35:42,480 --> 00:35:44,400
 to this new position now.

580
00:35:44,400 --> 00:35:48,600
 So within this 2 by 2 block, you are trying to find what's the largest value.

581
00:35:48,600 --> 00:35:50,120
 In this example, it's 8.

582
00:35:50,120 --> 00:35:52,120
 So we just write it down as 8.

583
00:35:52,120 --> 00:35:55,480
 And afterwards, when you move it down, you move by a strut of 2.

584
00:35:55,480 --> 00:35:56,920
 Again, you repeat the exercise.

585
00:35:56,920 --> 00:35:59,720
 You see this is 3 and this block is 4.

586
00:35:59,720 --> 00:36:03,480
 So this is how you perform the max pooling.

587
00:36:03,480 --> 00:36:08,759
 So you can see in this example here, after you perform the max pooling, the dimensions

588
00:36:08,759 --> 00:36:17,680
 of the vertical and the horizontal dimension has been half from 4 to 4 to 2 by 2 dimension

589
00:36:17,680 --> 00:36:18,680
 now.

590
00:36:18,680 --> 00:36:24,560
 So therefore, you reduce the size of your output volume and hence, for your subsequent

591
00:36:24,560 --> 00:36:30,880
 layer, you need less computation and storage.

592
00:36:30,880 --> 00:36:34,360
 So early on, we also mentioned that for this pooling layer, for example, this is our current

593
00:36:34,360 --> 00:36:39,560
 volume, which is 224 by 224 by 64 channel here.

594
00:36:39,560 --> 00:36:45,560
 So when you perform pooling, we consider each of this particular channel or slice independently.

595
00:36:45,560 --> 00:36:47,880
 Suppose this is our current channel.

596
00:36:47,880 --> 00:36:53,840
 When we perform just now the 2 by 2 max pooling, you can see the size have you reduced to

597
00:36:54,600 --> 00:36:58,320
 112 by 112 and then you put it back here.

598
00:36:58,320 --> 00:37:01,120
 And each of the channels you operate independently.

599
00:37:01,120 --> 00:37:09,920
 So now the size of your output volume now will be 112 by 112 by 64.

600
00:37:09,920 --> 00:37:13,440
 So now we have actually introduced quite a number of layers already.

601
00:37:13,440 --> 00:37:15,240
 So let's see where we are now.

602
00:37:15,240 --> 00:37:17,320
 So we have the input.

603
00:37:17,320 --> 00:37:22,080
 We say that first we have a convolutional layer.

604
00:37:22,080 --> 00:37:25,960
 And then, the coloristic couple with it is the activation layer.

605
00:37:25,960 --> 00:37:28,400
 So it's not shown here, but there's an activation layer.

606
00:37:28,400 --> 00:37:32,480
 And afterwards, we'll do the pooling layer to reduce the dimension.

607
00:37:32,480 --> 00:37:37,640
 So this convolution, activation, and pooling, typically we repeat a few times.

608
00:37:37,640 --> 00:37:42,319
 So for example, in this illustration, you show it, you just do it once and you do it

609
00:37:42,319 --> 00:37:44,120
 twice.

610
00:37:44,120 --> 00:37:49,120
 So once you reach this particular junctures here, this is the output volume at this juncture

611
00:37:49,120 --> 00:37:49,960
 here.

612
00:37:49,960 --> 00:37:55,280
 So what we are going to do is that we are going to convert this particular output into

613
00:37:55,280 --> 00:37:57,960
 a vector.

614
00:37:57,960 --> 00:38:04,240
 So we are going to convert this into a particular vector here.

615
00:38:04,240 --> 00:38:07,960
 So this particular process is known as a flattening.

616
00:38:07,960 --> 00:38:10,560
 So you flatten it into a vector.

617
00:38:10,560 --> 00:38:13,440
 So how do we perform the flattening is actually quite simple.

618
00:38:13,440 --> 00:38:16,240
 We just scan channel by channel.

619
00:38:16,240 --> 00:38:20,359
 For example, first row, second row, third row of the first channel.

620
00:38:20,359 --> 00:38:23,839
 Each row you scan you write as part of the vector.

621
00:38:23,839 --> 00:38:27,080
 So you have vector 1, vector 2, vector 3, vector 4.

622
00:38:27,080 --> 00:38:31,200
 So after you scan the first channel, then you move on to the next channel.

623
00:38:31,200 --> 00:38:35,879
 So therefore, by performing this scanning operation or the flattening operation, you

624
00:38:35,879 --> 00:38:38,919
 can convert it into a vector now.

625
00:38:38,919 --> 00:38:44,279
 So this particular vector now, then subsequently will be passed through the FC layer, fully

626
00:38:44,279 --> 00:38:45,959
 connected layer.

627
00:38:45,960 --> 00:38:51,000
 So if you recall this fully connected layer, it's just like the linear layer that you

628
00:38:51,000 --> 00:38:53,560
 have studied before, the linear classifier.

629
00:38:53,560 --> 00:38:59,680
 And then finally, if the particular, this application is for the task of classification,

630
00:38:59,680 --> 00:39:08,120
 then often you will be following by a softmax layer to convert the output into the probability.

631
00:39:08,120 --> 00:39:11,040
 So next, let's look at the FC layer now.

632
00:39:11,040 --> 00:39:15,759
 So for FC layer, all the nodes in one layer are connected to all the nodes in the next

633
00:39:15,759 --> 00:39:16,759
 layer.

634
00:39:16,759 --> 00:39:23,320
 So for example, if you have this is one FC layer here, you have 1000 nodes here.

635
00:39:23,320 --> 00:39:26,040
 Suppose you have the next FC layer, also 1000.

636
00:39:26,040 --> 00:39:33,840
 So all the nodes in one FC layer is fully connected to all the nodes in the next layer.

637
00:39:33,840 --> 00:39:40,400
 So because they are fully connected, so therefore, for FC layer is usually very expensive.

638
00:39:40,400 --> 00:39:43,720
 It's very expensive because all the nodes are fully connected.

639
00:39:43,720 --> 00:39:47,160
 So you need to store lots of parameters.

640
00:39:47,160 --> 00:39:52,360
 So for example, if this particular FC layer, if this dimension is 1000, and the next layer

641
00:39:52,360 --> 00:39:58,840
 dimension is 1000, then all together you need 1000 times 1000, which is one million parameters,

642
00:39:58,840 --> 00:40:00,840
 which is a lot.

643
00:40:00,840 --> 00:40:08,160
 You can see all the nodes in one layer are connected to all the nodes in the next layer.

644
00:40:08,160 --> 00:40:15,200
 So this particular FC layer feature vector can also be interpreted as a feature or embedding

645
00:40:15,200 --> 00:40:17,000
 to represent the image.

646
00:40:17,000 --> 00:40:23,520
 So one very useful way to interpret this FC layer is that at any of this particular FC

647
00:40:23,520 --> 00:40:30,120
 layer, you can interpret this is actually a vector, or sometimes you call it a feature,

648
00:40:30,120 --> 00:40:34,839
 or sometimes you call it an embedding that is used to represent this image here.

649
00:40:34,840 --> 00:40:42,560
 So each of these vector representations here at different layer can be interpreted as extracting

650
00:40:42,560 --> 00:40:46,600
 some useful information from your input image.

651
00:40:46,600 --> 00:40:54,920
 So we either call embedding, we call it a vector, or we call it feature.

652
00:40:54,920 --> 00:41:00,520
 So FC layer just behaves like the linear layer for classification, like what I've explained

653
00:41:00,520 --> 00:41:02,640
 to you.

654
00:41:02,640 --> 00:41:09,279
 So this is the diagram that we have shown before when we study about the linear classifier.

655
00:41:09,279 --> 00:41:15,440
 So suppose input, right, initially your input image is 32 by 32 by 3.

656
00:41:15,440 --> 00:41:20,440
 We use a lexical graphical ordering, we can convert it into a long vector.

657
00:41:20,440 --> 00:41:23,600
 So this is our long vector here, 3072.

658
00:41:23,600 --> 00:41:26,720
 So this is a vector 3072.

659
00:41:26,720 --> 00:41:33,200
 So if we use this FC layer, so FC layer is just like the linear layer that you can model

660
00:41:33,200 --> 00:41:36,600
 using this linear operation.

661
00:41:36,600 --> 00:41:39,919
 Wx plus b, the b is optional.

662
00:41:39,919 --> 00:41:43,480
 So in this illustration here, it doesn't consider the bias.

663
00:41:43,480 --> 00:41:48,480
 So pretty much you have this long input vector, and you have this particular weight matrix

664
00:41:48,480 --> 00:41:52,000
 W that need to be trained or estimated.

665
00:41:52,960 --> 00:41:57,800
 So you can see that when you want to calculate each of the output layer, each of the output

666
00:41:57,800 --> 00:42:04,480
 nodes here, to calculate each of the output nodes involved taking a row vector in W, multiply

667
00:42:04,480 --> 00:42:07,840
 with this column vector x.

668
00:42:07,840 --> 00:42:12,840
 So therefore each of this particular output here will be examining or will be taking input

669
00:42:12,840 --> 00:42:17,440
 from each of the points in this vector x here.

670
00:42:17,440 --> 00:42:20,080
 So anyway this is what we have studied before.

671
00:42:21,040 --> 00:42:23,279
 We'll talk about FC layer.

672
00:42:23,279 --> 00:42:32,480
 Suppose if your input vector is 3072 by 1, if you want to classify into 10 category,

673
00:42:32,480 --> 00:42:39,240
 then your size of your matrix W will be 3072 times 10.

674
00:42:39,240 --> 00:42:41,799
 Sorry, it's 10 times 3072.

675
00:42:41,799 --> 00:42:46,400
 So you can see which is lots of parameter.

676
00:42:46,400 --> 00:42:49,799
 So now we have explained about the FC layer.

677
00:42:49,800 --> 00:42:55,880
 So the FC layer you can interpret is as like a general purpose transformation.

678
00:42:55,880 --> 00:43:02,600
 It's a general purpose transformation to transform one vector to the next vector.

679
00:43:02,600 --> 00:43:04,720
 It's a general purpose transformation.

680
00:43:04,720 --> 00:43:10,440
 So which generally speaking is good, except there's one price that you have to pay.

681
00:43:10,440 --> 00:43:12,920
 What's the price you have to pay?

682
00:43:13,920 --> 00:43:22,560
 Now if you want to learn any, you can actually always use many FC layer because each FC layer

683
00:43:22,560 --> 00:43:24,840
 is just like a general purpose transformation.

684
00:43:24,840 --> 00:43:27,120
 It can even sufficient training data.

685
00:43:27,120 --> 00:43:33,320
 You can always map your input factor into another output vector which is more, which

686
00:43:33,320 --> 00:43:36,600
 further extract the feature and so on and so forth.

687
00:43:36,600 --> 00:43:40,320
 But in practice it's very difficult to do that.

688
00:43:40,320 --> 00:43:41,320
 What's the reason?

689
00:43:42,920 --> 00:43:56,760
 So the reason why in practice we do not use many FC layer or we only use it sparingly

690
00:43:56,760 --> 00:44:00,160
 is because of the reason that we mentioned before.

691
00:44:00,160 --> 00:44:02,720
 Because each node is connected to other nodes.

692
00:44:02,720 --> 00:44:05,760
 So you require a large number of parameter.

693
00:44:05,760 --> 00:44:07,080
 That means you need to store.

694
00:44:07,080 --> 00:44:08,760
 Your storage becomes very large.

695
00:44:08,760 --> 00:44:12,440
 The second thing is when you have lots of weight, that means you need a lot of training

696
00:44:12,440 --> 00:44:13,720
 data to train it.

697
00:44:13,720 --> 00:44:16,000
 So birth unknown ideal.

698
00:44:16,000 --> 00:44:20,320
 So that's why we only use FC layer quite sparingly.

699
00:44:20,320 --> 00:44:24,200
 Usually at the end of the layer, once you have already extract most of the feature, you

700
00:44:24,200 --> 00:44:31,560
 just want to do some final car mapping to boost up the performance a bit.

701
00:44:31,560 --> 00:44:36,880
 So next anyway, suppose now you have the last FC layer already.

702
00:44:37,040 --> 00:44:44,840
 Assuming that if our application is to perform classification, then your output, you need

703
00:44:44,840 --> 00:44:46,440
 to map it into the probability.

704
00:44:46,440 --> 00:44:52,200
 If for the classification problem, then the last layer that we have typically will be

705
00:44:52,200 --> 00:44:54,080
 the softmax layer.

706
00:44:54,080 --> 00:45:00,400
 So if you remember in our previous lecture, the objective of softmax is to convert output

707
00:45:00,400 --> 00:45:04,240
 into probability so that you can perform classification.

708
00:45:04,240 --> 00:45:10,640
 So therefore the softmax layer what it does is you take the output from the last FC layer.

709
00:45:10,640 --> 00:45:16,240
 So the output from the last FC layer has a unique name we call logit.

710
00:45:16,240 --> 00:45:22,399
 So we take this logit value and we convert it into probability for the purpose of training

711
00:45:22,399 --> 00:45:24,319
 and classification later.

712
00:45:24,319 --> 00:45:29,120
 So how do we take the output from the last FC layer and convert it into a probability?

713
00:45:29,120 --> 00:45:34,600
 We use the softmax normalization that we have studied in the last lecture.

714
00:45:34,600 --> 00:45:43,400
 So if you take these particular examples here, suppose this is the input to the last layer.

715
00:45:43,400 --> 00:45:49,200
 So for your last layer, so you can kind of think about it as input multiplied with W

716
00:45:49,200 --> 00:45:51,200
 matrix.

717
00:45:51,200 --> 00:45:55,279
 Then you generate this particular output for the last FC layer.

718
00:45:55,279 --> 00:45:57,960
 So this is called the logit.

719
00:45:57,960 --> 00:46:03,240
 So this logit next we need to let it go through the softmax normalization to convert it into

720
00:46:03,240 --> 00:46:04,800
 the probability.

721
00:46:04,800 --> 00:46:09,560
 So if you recall, the process to convert this output into the probability is using this

722
00:46:09,560 --> 00:46:10,560
 equation.

723
00:46:10,560 --> 00:46:17,800
 Either a power of the logit, the current logit divided by the denominator.

724
00:46:17,800 --> 00:46:24,080
 This denominator is either a power of the output is summed up over all the possible

725
00:46:24,080 --> 00:46:29,640
 nodes here, all the possible classes.

726
00:46:29,640 --> 00:46:35,799
 So by doing that, then you will convert this into the probability now.

727
00:46:35,799 --> 00:46:39,960
 So in this illustration, it's trying to show, for example, if you want to classify into

728
00:46:39,960 --> 00:46:43,400
 different types of probability.

729
00:46:43,400 --> 00:46:48,920
 So now we have almost studied all the basic structures of a vanilla CNN.

730
00:46:48,920 --> 00:46:55,080
 So let's just take one particular code to see whether you can put everything together.

731
00:46:55,080 --> 00:46:59,520
 So we are going to look at a simple PyTorch CNN implementation.

732
00:46:59,520 --> 00:47:06,280
 So in order to implement this simple vanilla CNN, so first of all, we'll define a particular

733
00:47:06,280 --> 00:47:08,160
 new class called net.

734
00:47:08,160 --> 00:47:12,240
 And this net is extending its parent class call module.

735
00:47:12,240 --> 00:47:17,880
 So it has a lot of the properties from the parent class.

736
00:47:17,880 --> 00:47:21,320
 And then next, when you want to define this network, you need to have these constructor

737
00:47:21,320 --> 00:47:22,840
 methods here.

738
00:47:22,840 --> 00:47:25,500
 This particular method is a constructor method here.

739
00:47:25,500 --> 00:47:29,720
 So we need to define all the layers for this current CNN.

740
00:47:29,720 --> 00:47:36,960
 So in order to do that, we call this superclass, initialization of this superclass.

741
00:47:36,960 --> 00:47:41,320
 And then afterwards, the next thing is that this is the first layer here.

742
00:47:41,320 --> 00:47:46,240
 So the first layer that we are going to have is the 2D convolutional layer.

743
00:47:46,240 --> 00:47:52,600
 So it's implemented using this particular command nn.con2d.

744
00:47:52,600 --> 00:47:56,200
 And then there have some input argument.

745
00:47:56,200 --> 00:48:01,839
 The first one is one here corresponding to the input channel.

746
00:48:01,839 --> 00:48:05,959
 So the second parameter corresponds to the output channel, which is 6.

747
00:48:05,959 --> 00:48:11,319
 And then finally, this last parameter is actually the size of a filter.

748
00:48:11,319 --> 00:48:15,600
 So therefore, if you look at the first line of the command, what it says is that you are

749
00:48:15,600 --> 00:48:19,600
 going to construct your first convolutional layer.

750
00:48:19,600 --> 00:48:22,080
 Input is actually one channel.

751
00:48:22,080 --> 00:48:24,319
 Output is six channel.

752
00:48:24,319 --> 00:48:29,640
 And then the filter you're using is 3 by 3 filter.

753
00:48:29,640 --> 00:48:35,360
 So if you look at the first line of this command here, what kind of image are we dealing with?

754
00:48:35,360 --> 00:48:38,520
 Are we dealing with a grayscale image?

755
00:48:38,520 --> 00:48:44,000
 Or are we dealing with a colored image?

756
00:48:45,000 --> 00:48:53,440
 We are dealing with a grayscale image because it says that the input volume is one.

757
00:48:53,440 --> 00:48:55,960
 That means input is only one channel.

758
00:48:55,960 --> 00:49:00,400
 So if it's a colored image, what do you think this number would be?

759
00:49:00,400 --> 00:49:02,400
 It would be 3.

760
00:49:02,400 --> 00:49:06,440
 So therefore, from this particular structure, you have some sense about what kind of data

761
00:49:06,440 --> 00:49:08,680
 you'll be handling.

762
00:49:08,680 --> 00:49:13,520
 So the next convolutional layer you can see is nn.con2d.

763
00:49:13,520 --> 00:49:14,640
 Output volume is 6.

764
00:49:14,640 --> 00:49:15,800
 Output volume is 16.

765
00:49:15,800 --> 00:49:18,240
 And then again, we're using 3 by 3 filter.

766
00:49:18,240 --> 00:49:23,800
 So now you can see this six here need to agree with this six here.

767
00:49:23,800 --> 00:49:26,560
 Because after the first layer, you have six channel.

768
00:49:26,560 --> 00:49:29,759
 So this will then serve as an input to the next conv layer.

769
00:49:29,759 --> 00:49:32,160
 So this is also six here.

770
00:49:32,160 --> 00:49:34,280
 So you have defined two convolutional layer.

771
00:49:34,280 --> 00:49:39,440
 And then afterwards, you continue to define the FC layer now, fully connected layer.

772
00:49:39,440 --> 00:49:45,400
 So the FC layer is defined using nn.linear, using this PyTorch.

773
00:49:45,400 --> 00:49:48,280
 It's defined using this function nn.linear.

774
00:49:48,280 --> 00:49:54,640
 So this nn.linear, the first parameter here is actually the number, the input dimensions

775
00:49:54,640 --> 00:49:56,480
 of your input vector.

776
00:49:56,480 --> 00:49:58,080
 Dimensions of your input vector.

777
00:49:58,080 --> 00:50:03,000
 The second parameter is the output dimension vector.

778
00:50:03,000 --> 00:50:05,840
 So you can see that in this particular case here.

779
00:50:05,840 --> 00:50:10,640
 So input dimension vector is 16 multiplied by 6 by 6.

780
00:50:10,640 --> 00:50:17,520
 This is because for this particular example, people already know the input image is actually

781
00:50:17,520 --> 00:50:19,720
 32 by 32 by 3.

782
00:50:19,720 --> 00:50:22,000
 32 by 32.

783
00:50:22,000 --> 00:50:25,120
 So they already know the input image.

784
00:50:25,120 --> 00:50:27,200
 Therefore they also know the structure.

785
00:50:27,200 --> 00:50:32,720
 So after some going through a few different layers, they know what is the dimensions of

786
00:50:32,720 --> 00:50:35,480
 your output volume.

787
00:50:36,120 --> 00:50:38,600
 Therefore they do a bit of a hard coding.

788
00:50:38,600 --> 00:50:41,640
 It's probably not the best way, but they do a bit of hard coding.

789
00:50:41,640 --> 00:50:45,800
 If they already know the input dimension, they do some calculation and found that the

790
00:50:45,800 --> 00:50:49,400
 output volume is 16 by 6 by 6.

791
00:50:49,400 --> 00:50:53,280
 So you can visualise this a little bit like this.

792
00:50:53,280 --> 00:50:58,040
 They already know the size of the input image after and they also know the structure.

793
00:50:58,040 --> 00:51:01,880
 So therefore they can pre-compute and know that the output now is...

794
00:51:01,880 --> 00:51:02,880
 What's that?

795
00:51:03,800 --> 00:51:11,080
 Anyway they know the spatial dimension as well as the number of this output channel.

796
00:51:11,080 --> 00:51:14,720
 So that is assuming that they already know the structure.

797
00:51:14,720 --> 00:51:20,000
 They do some pre-computation and they know that this is the dimensions of your output

798
00:51:20,000 --> 00:51:24,200
 volume after the last layer before the FC layer.

799
00:51:24,200 --> 00:51:26,720
 So that's why they hard-coded it here.

800
00:51:26,720 --> 00:51:30,520
 So the FC layer is this number by this number here.

801
00:51:30,520 --> 00:51:35,160
 And afterwards the second FC layer now is mapping from 120 to 84.

802
00:51:35,160 --> 00:51:38,840
 The last FC layer is mapping from 84 to 10.

803
00:51:38,840 --> 00:51:43,880
 So for this particular problem, suppose it's for the purpose of classification, how many

804
00:51:43,880 --> 00:51:46,640
 classes do you think you will classify into?

805
00:51:52,080 --> 00:51:56,120
 How many classes do you think you will try to classify this particular...

806
00:51:56,120 --> 00:52:00,640
 I mean this code is for classification of how many categories of objects.

807
00:52:08,560 --> 00:52:09,720
 10 classes.

808
00:52:09,720 --> 00:52:17,080
 So you can see at the last layer it's going to map from a vector dimensions of 84 to 10.

809
00:52:17,080 --> 00:52:22,200
 So these 10 usually correspond to 10 different categories because these 10 classes later

810
00:52:22,200 --> 00:52:24,960
 on you're going to perform the four softmax.

811
00:52:24,960 --> 00:52:29,120
 You're going to let it pass through the softmax layer to convert it into probability.

812
00:52:29,120 --> 00:52:32,560
 So therefore from this code you can more or less tell that you're trying to classify

813
00:52:32,560 --> 00:52:34,960
 into 10 categories here.

814
00:52:34,960 --> 00:52:36,720
 So now we have defined the structure.

815
00:52:36,720 --> 00:52:39,760
 Next we are going to define how the signal flow looks like.

816
00:52:39,760 --> 00:52:42,880
 So the signal flow is actually based on our understanding.

817
00:52:42,880 --> 00:52:45,200
 So we have our input image.

818
00:52:45,200 --> 00:52:50,160
 You go through a first convolutional layer and afterwards go through the ReLU layer,

819
00:52:50,160 --> 00:52:54,360
 which is our activation layer, and then afterwards go through a max pooling layer.

820
00:52:54,360 --> 00:52:56,600
 So this max pooling layer is 2x2.

821
00:52:56,600 --> 00:52:59,480
 2x2 max pooling.

822
00:52:59,480 --> 00:53:00,720
 You get your output.

823
00:53:00,720 --> 00:53:06,960
 So this output now will then go through your second convolutional layer, activation layer,

824
00:53:06,960 --> 00:53:09,640
 max pooling layer to get the output.

825
00:53:09,640 --> 00:53:14,680
 So this output now, afterwards we are going to use this particular function to calculate

826
00:53:14,680 --> 00:53:19,520
 what is the dimensions of this final output volume.

827
00:53:19,520 --> 00:53:23,040
 So we use this particular function to calculate what's the dimension.

828
00:53:23,040 --> 00:53:30,440
 Then we use this X view to do the flattening to convert this volume into a vector.

829
00:53:30,440 --> 00:53:33,520
 So this line will do the flattening process.

830
00:53:33,520 --> 00:53:39,240
 So after this line here, your volume, feature volume will become a vector, just like what

831
00:53:39,240 --> 00:53:40,480
 we described.

832
00:53:40,480 --> 00:53:48,000
 So this vector now will then go through your first FC layer followed by ReLU to get your

833
00:53:48,000 --> 00:53:49,000
 output.

834
00:53:49,640 --> 00:53:55,120
 Your output from the first FC layer go through your second FC layer, ReLU and the output.

835
00:53:55,120 --> 00:54:03,240
 So the output from the last FC layer will go through your last FC layer to get the output.

836
00:54:03,240 --> 00:54:05,640
 And then finally this output is returned.

837
00:54:05,640 --> 00:54:10,320
 So for this particular illustration here, it does not perform the normalization, the

838
00:54:10,320 --> 00:54:18,320
 softmax normalization, because the softmax normalization will be handled elsewhere later.

839
00:54:18,320 --> 00:54:21,000
 So all right, I actually have one question for you.

840
00:54:21,000 --> 00:54:26,920
 You can see that for this FC layer here, after each FC layer it will go through activation

841
00:54:26,920 --> 00:54:29,520
 function, ReLU and ReLU here.

842
00:54:29,520 --> 00:54:32,720
 Is it okay if I remove this two ReLU layer?

843
00:54:37,720 --> 00:54:42,160
 No, someone may say, okay, no, can I just remove this two ReLU layer and just simply

844
00:54:42,160 --> 00:54:46,880
 put X go through the first FC layer to obtain the output.

845
00:54:46,880 --> 00:54:49,920
 Go through the second FC layer to obtain the output.

846
00:54:49,920 --> 00:54:54,560
 And then this output then go through the last FC layer to obtain your final output.

847
00:54:54,560 --> 00:54:57,720
 Can you remove the activation layer?

848
00:54:57,720 --> 00:55:02,200
 Okay, if you cannot remove the activation layer, why?

849
00:55:02,200 --> 00:55:07,080
 Yeah, okay, yeah, I think I heard someone mention.

850
00:55:07,080 --> 00:55:11,400
 Because if you remove this activation layer, then this particular mathematical operation

851
00:55:11,400 --> 00:55:15,920
 is more or less modeled by a matrix, for example, W1.

852
00:55:15,960 --> 00:55:20,040
 Afterwards, this one is modeled by W2, this one is modeled by W3.

853
00:55:20,040 --> 00:55:25,920
 So the operation is actually X multiplied with W1, later on multiplied with W2, later

854
00:55:25,920 --> 00:55:27,560
 on multiplied with W3.

855
00:55:27,560 --> 00:55:30,880
 So multiplications of three matrices is still a matrix.

856
00:55:30,880 --> 00:55:35,680
 Then this has been reduced to simply one FC layer.

857
00:55:35,680 --> 00:55:37,920
 So therefore you cannot remove this.

858
00:55:37,920 --> 00:55:44,920
 We need to have an activation in between to enrich its dynamic modeling capability.

859
00:55:45,920 --> 00:55:52,920
 Right, okay, so now we have covered most of the basic concepts about the CNN already.

860
00:55:52,920 --> 00:55:56,800
 So I think it's time that we try up your understanding now.

861
00:55:56,800 --> 00:55:58,440
 So we have one exercise.

862
00:55:58,440 --> 00:56:01,080
 Actually this is one pass here exam question.

863
00:56:01,080 --> 00:56:03,920
 So you can give a try to see your understanding.

864
00:56:03,920 --> 00:56:10,600
 Okay, so in this question it says that a simple CNN, in a simple CNN, an input image A pass

865
00:56:10,600 --> 00:56:13,920
 through a convolution layer, activation layer and max pooling.

866
00:56:14,920 --> 00:56:19,920
 And then the output from the max pooling is then used for further subsequent processing.

867
00:56:19,920 --> 00:56:23,920
 So first of all, you are given an input image A that looks like this.

868
00:56:24,920 --> 00:56:27,920
 And the convolution layer has the following setting.

869
00:56:27,920 --> 00:56:30,920
 So the current filter that you have is given here.

870
00:56:30,920 --> 00:56:32,920
 So this is our current filter.

871
00:56:32,920 --> 00:56:35,920
 And the amount of zero padding at each side is one.

872
00:56:35,920 --> 00:56:39,920
 The strike both horizontally and vertically is two.

873
00:56:39,920 --> 00:56:46,920
 And the activation layer that we, activation function that we use in the activation layer is given by this sigmoid function.

874
00:56:46,920 --> 00:56:51,920
 Right, okay, the pooling that we use is the two by two max pooling with a strike of two.

875
00:56:51,920 --> 00:56:55,920
 Right, so afterwards there's a series of questions.

876
00:56:55,920 --> 00:56:59,920
 So I'll give you some time to go through this question and then we'll go through the answer together.

877
00:56:59,920 --> 00:57:02,920
 But one thing at a time, let's look at question one.

878
00:57:02,920 --> 00:57:07,920
 So question one is it asks you to find the output after the convolution layer.

879
00:57:07,920 --> 00:57:11,920
 So I'll give you a bit of time to try it out and then we'll go through the answer together.

880
00:57:11,920 --> 00:57:22,920
 At the end of this exercise, if you can get all the steps correct, pretty much that means you have a pretty good understanding of all the operations in this CNN process already.

881
00:57:37,920 --> 00:57:39,920
 Okay.

882
00:58:07,920 --> 00:58:09,920
 Okay.

883
00:58:37,920 --> 00:58:39,920
 Okay.

884
00:59:07,920 --> 00:59:09,920
 Okay.

885
00:59:37,920 --> 00:59:39,920
 Okay.

886
01:00:07,920 --> 01:00:09,920
 Okay.

887
01:00:09,920 --> 01:00:11,920
 Okay.

888
01:00:11,920 --> 01:00:32,920
 So at least we have some time to think about it.

889
01:00:32,920 --> 01:00:34,920
, so any suggestions.

890
01:00:34,920 --> 01:00:36,920
 How do we solve this problem?

891
01:00:36,920 --> 01:00:39,720
 Any suggestion?

892
01:00:42,720 --> 01:00:46,000
 So first of all, we take our input image

893
01:00:46,000 --> 01:00:49,240
 and then what's the layers of padding that you have to do?

894
01:00:49,240 --> 01:00:50,120
 One layer, right?

895
01:00:50,120 --> 01:00:51,520
 So you pad up with one layer,

896
01:00:51,520 --> 01:00:53,600
 so zero surrounding it.

897
01:00:53,600 --> 01:00:57,960
 And afterwards, you simply take this particular filter,

898
01:00:57,960 --> 01:00:59,920
 put on top, right?

899
01:00:59,920 --> 01:01:02,040
 You perform the filtering

900
01:01:02,040 --> 01:01:05,600
 or multiply the corresponding entry and sum it up.

901
01:01:05,600 --> 01:01:07,440
 That will generate the first value.

902
01:01:07,440 --> 01:01:12,200
 And afterwards, you do the shifting.

903
01:01:12,200 --> 01:01:14,640
 The strike here is two, right?

904
01:01:14,640 --> 01:01:16,120
 So just be careful.

905
01:01:16,120 --> 01:01:18,319
 You need to move by a step size of two.

906
01:01:18,319 --> 01:01:20,600
 So then afterwards, you repeat the operation.

907
01:01:20,600 --> 01:01:23,680
 You should be able to generate your output.

908
01:01:23,680 --> 01:01:27,560
 Okay, so let's show, go through the answer together.

909
01:01:36,160 --> 01:01:39,160
 Right, okay, so...

910
01:01:39,160 --> 01:01:40,560
 Right, okay.

911
01:01:40,560 --> 01:01:42,960
 I think this one we can just show it.

912
01:01:42,960 --> 01:01:46,960
 Right, okay, so this is actually our given image.

913
01:01:46,960 --> 01:01:51,960
 So we use one padding of zero on both sides of the image.

914
01:01:51,960 --> 01:01:54,319
 And now this is a given filter.

915
01:01:54,319 --> 01:01:58,400
 So you put your filter initially at this top left corner.

916
01:01:58,400 --> 01:02:00,360
 You perform the filtering operation.

917
01:02:00,360 --> 01:02:02,560
 That means you multiply their corresponding value.

918
01:02:02,560 --> 01:02:03,880
 You sum it up, right?

919
01:02:03,880 --> 01:02:05,759
 So for example, I'll just go through one.

920
01:02:05,759 --> 01:02:07,520
 So the first one, when you put it here,

921
01:02:07,520 --> 01:02:10,320
 so it turned out we multiply together, everything is zero.

922
01:02:10,320 --> 01:02:14,080
 So let's just look at, okay, so you position here,

923
01:02:14,080 --> 01:02:16,160
 you perform the filtering, you get zero.

924
01:02:16,160 --> 01:02:18,480
 Next, because you're told the strike is two,

925
01:02:18,480 --> 01:02:21,480
 so you move, okay, to this position,

926
01:02:21,480 --> 01:02:24,960
 you perform the filtering, your C is zero.

927
01:02:24,960 --> 01:02:28,279
 So next, you move down to this position here.

928
01:02:28,279 --> 01:02:30,279
 If you move down to this position here,

929
01:02:31,280 --> 01:02:32,760
 if you move down to this position,

930
01:02:32,760 --> 01:02:35,320
 you can see this multiply with this will be zero.

931
01:02:35,320 --> 01:02:36,560
 Four multiply is zero, okay,

932
01:02:36,560 --> 01:02:38,520
 so it's zero multiply with one,

933
01:02:38,520 --> 01:02:41,400
 zero multiply with two, zero multiply with one,

934
01:02:41,400 --> 01:02:44,040
 all is zero, okay, and then this one multiply also zero.

935
01:02:44,040 --> 01:02:47,520
 So the only non-zero value is when you have two

936
01:02:47,520 --> 01:02:50,840
 multiply with minus two, that will give you minus four.

937
01:02:50,840 --> 01:02:53,480
 So therefore the value here is minus four, right?

938
01:02:53,480 --> 01:02:56,080
 So anyway, this is a very simple calculation

939
01:02:56,080 --> 01:02:58,880
 because I don't want it to be too complicated,

940
01:02:58,920 --> 01:03:00,440
 especially at early stage.

941
01:03:00,440 --> 01:03:02,680
 So if you go through this process,

942
01:03:02,680 --> 01:03:05,920
 your output after the convolution will be this value, okay?

943
01:03:05,920 --> 01:03:08,600
 So this is the output after the,

944
01:03:12,440 --> 01:03:14,060
 after the convolution.

945
01:03:20,400 --> 01:03:22,160
 Right, so let's look at the,

946
01:03:24,680 --> 01:03:27,720
 okay, so we managed to obtain the first one already.

947
01:03:27,720 --> 01:03:30,720
 So part two, it asks you to briefly describe

948
01:03:30,720 --> 01:03:32,919
 the effect of this particular filter F,

949
01:03:32,919 --> 01:03:36,160
 where it's applied to the input image, okay?

950
01:03:36,160 --> 01:03:38,879
 So you look at this particular filter F here.

951
01:03:38,879 --> 01:03:41,919
 What can you observe from this filter F?

952
01:03:42,919 --> 01:03:45,480
 What is the objective of this particular filter F?

953
01:03:50,959 --> 01:03:51,799
 Is trying to,

954
01:03:58,279 --> 01:03:59,799
 sorry, quite stuff, I can't.

955
01:04:07,359 --> 01:04:09,319
 Okay, actually there's a specific goal

956
01:04:09,319 --> 01:04:11,240
 is trying to achieve here, right?

957
01:04:11,240 --> 01:04:15,040
 Actually, I can't really fully hear you very clearly,

958
01:04:15,040 --> 01:04:16,759
 but in this particular case here,

959
01:04:16,759 --> 01:04:18,279
 for those of you who have studied

960
01:04:18,279 --> 01:04:20,279
 some image processing before,

961
01:04:20,279 --> 01:04:23,279
 you know that there's some basic filter operation

962
01:04:23,279 --> 01:04:24,120
 that you can do.

963
01:04:24,120 --> 01:04:27,439
 For example, this one is a well-known filter

964
01:04:27,440 --> 01:04:31,360
 that will extract gradient in the horizontal direction.

965
01:04:31,360 --> 01:04:33,240
 Right, so you try to extract gradient

966
01:04:33,240 --> 01:04:34,920
 in the horizontal direction,

967
01:04:34,920 --> 01:04:38,520
 or some edge information in the vertical direction.

968
01:04:38,520 --> 01:04:41,320
 So this is actually, you can see, right?

969
01:04:41,320 --> 01:04:42,880
 This is two, this minus two,

970
01:04:42,880 --> 01:04:44,840
 this is one, this minus one,

971
01:04:44,840 --> 01:04:46,040
 this is one, this minus one.

972
01:04:46,040 --> 01:04:48,680
 So you are trying to find the difference, okay,

973
01:04:48,680 --> 01:04:52,240
 the gradient in the horizontal direction, okay?

974
01:04:52,240 --> 01:04:54,600
 So therefore, this particular filter is objective,

975
01:04:54,600 --> 01:04:57,880
 is to extract the gradient information

976
01:04:57,880 --> 01:04:59,920
 in the horizontal direction.

977
01:04:59,920 --> 01:05:03,680
 Okay, so this objective of this particular,

978
01:05:03,680 --> 01:05:05,319
 the effect of it, right?

979
01:05:05,319 --> 01:05:08,080
 The effect of this particular filter here.

980
01:05:08,080 --> 01:05:09,520
 Okay, right, so next,

981
01:05:09,520 --> 01:05:11,240
 later on I'll show you the answer for this.

982
01:05:11,240 --> 01:05:13,480
 The next one is that I ask you to find

983
01:05:13,480 --> 01:05:16,279
 the output after the activation layer.

984
01:05:16,279 --> 01:05:18,360
 Yeah, so I give you a bit of time, right?

985
01:05:18,360 --> 01:05:21,640
 To try to find the output after the activation layer.

986
01:05:22,640 --> 01:05:24,560
 Okay, at this particular point,

987
01:05:24,560 --> 01:05:26,879
 I also probably want to highlight to you.

988
01:05:26,879 --> 01:05:30,279
 So in this particular course,

989
01:05:30,279 --> 01:05:33,080
 there'll be, sometimes you may need to use,

990
01:05:33,080 --> 01:05:37,279
 for example, sigmoid function, or 10-H function.

991
01:05:37,279 --> 01:05:39,680
 So make sure that you familiarize yourself, right?

992
01:05:39,680 --> 01:05:41,839
 Or lone function, okay?

993
01:05:41,839 --> 01:05:45,319
 Lock, and, right, this is a lone function.

994
01:05:45,319 --> 01:05:47,240
 So make sure that you familiarize yourself

995
01:05:47,240 --> 01:05:48,960
 with your calculator, right?

996
01:05:48,960 --> 01:05:52,160
 So in the event you come out in the exam,

997
01:05:52,160 --> 01:05:55,560
 you can just simply, you know how to calculate, yeah?

998
01:05:55,560 --> 01:05:59,080
 Okay, so anyway, I'll give you some time to quickly calculate

999
01:05:59,080 --> 01:06:01,280
 the output after the activation layer.

1000
01:06:05,640 --> 01:06:09,640
 Right, and also the output after the max pooling layer.

1001
01:06:09,640 --> 01:06:12,480
 These are relatively calculation-wise,

1002
01:06:12,480 --> 01:06:13,480
 relatively straightforward,

1003
01:06:13,480 --> 01:06:15,480
 so we'll go through the answers together.

1004
01:06:18,960 --> 01:06:19,800
 Okay.

1005
01:06:28,960 --> 01:06:29,800
 Right.

1006
01:06:34,160 --> 01:06:36,840
 In case some of you have not got the answer before,

1007
01:06:36,840 --> 01:06:38,280
 right, so early on,

1008
01:06:38,280 --> 01:06:41,240
 the output after the convolution is this one, okay?

1009
01:06:43,520 --> 01:06:45,120
 So we know after convolution,

1010
01:06:45,120 --> 01:06:47,640
 you need to let it go through the activation, right?

1011
01:06:47,640 --> 01:06:50,640
 So you step to try to find...

1012
01:07:17,640 --> 01:07:18,480
 Okay.

1013
01:07:28,040 --> 01:07:31,040
 Okay, so I think this part should be quite straightforward

1014
01:07:31,040 --> 01:07:33,040
 because I actually provide you

1015
01:07:33,040 --> 01:07:35,000
 with the activation functionality.

1016
01:07:35,000 --> 01:07:36,920
 And activation function is the element

1017
01:07:36,920 --> 01:07:39,359
 by element operation.

1018
01:07:39,359 --> 01:07:42,160
 So that means you put zero into this,

1019
01:07:42,160 --> 01:07:43,680
 you'll be able to get your output,

1020
01:07:43,680 --> 01:07:46,200
 and afterwards you put each of the element

1021
01:07:46,200 --> 01:07:48,040
 right into this function.

1022
01:07:48,040 --> 01:07:51,799
 You'll be able to get the output of the activation function.

1023
01:07:51,799 --> 01:07:54,600
 So that part is quite straightforward.

1024
01:07:54,600 --> 01:07:58,560
 And then the next layer is actually a two by two max pooling.

1025
01:07:58,560 --> 01:08:00,879
 So in this example, you only have a two by two

1026
01:08:00,879 --> 01:08:01,720
 because of block, right?

1027
01:08:01,720 --> 01:08:04,560
 So you just write down the largest value, okay?

1028
01:08:04,560 --> 01:08:07,600
 So the reason is because in the context of exam,

1029
01:08:07,600 --> 01:08:10,240
 sometimes I don't want to make it too complicated,

1030
01:08:10,240 --> 01:08:11,799
 otherwise you'll be spending a lot of time

1031
01:08:11,799 --> 01:08:13,799
 doing all the calculation, yeah?

1032
01:08:16,200 --> 01:08:17,880
 So anyway, right?

1033
01:08:17,880 --> 01:08:19,279
 So if you look at the...

1034
01:08:29,920 --> 01:08:32,439
 Yeah, so just now we say the if our filter F

1035
01:08:32,439 --> 01:08:35,120
 is to compute the horizontal gradient, okay?

1036
01:08:35,120 --> 01:08:37,679
 Which in a sense that means you will capture

1037
01:08:37,679 --> 01:08:40,439
 or reflect some of the edge information.

1038
01:08:40,439 --> 01:08:41,960
 Okay, and then, right?

1039
01:08:41,960 --> 01:08:44,559
 So for part three, if you just now apply

1040
01:08:44,560 --> 01:08:48,320
 the activation function element by element,

1041
01:08:48,320 --> 01:08:50,520
 you'll see that this is the output, okay?

1042
01:08:50,520 --> 01:08:52,160
 You can go and check later.

1043
01:08:52,160 --> 01:08:54,320
 And then the next thing is actually doing

1044
01:08:54,320 --> 01:08:55,640
 the two by two max pooling.

1045
01:08:55,640 --> 01:08:58,040
 That means within this two by two pixel block,

1046
01:08:58,040 --> 01:09:02,240
 you choose the largest value, which is this 0.982.

1047
01:09:02,240 --> 01:09:05,080
 So therefore, this is the output after your max pooling.

1048
01:09:07,240 --> 01:09:09,640
 So let's continue to the last part of the question.

1049
01:09:11,440 --> 01:09:13,880
 So last part of the question is says that

1050
01:09:14,040 --> 01:09:16,000
 a student would like to make the following changes

1051
01:09:16,000 --> 01:09:19,080
 to the input image and the convolutional layer, right?

1052
01:09:19,080 --> 01:09:21,640
 So he changed this grayscale image A to now

1053
01:09:21,640 --> 01:09:23,680
 and RGB image B.

1054
01:09:23,680 --> 01:09:27,640
 And the dimensions of this RGB image is 100 by 100, okay?

1055
01:09:27,640 --> 01:09:30,880
 And then he changed the channel number

1056
01:09:30,880 --> 01:09:33,960
 of the output feature map to six now, okay?

1057
01:09:33,960 --> 01:09:34,800
 All right?

1058
01:09:34,800 --> 01:09:37,760
 And then afterwards, no, assume that the spatial dimensions

1059
01:09:37,760 --> 01:09:39,359
 of the filter remain the same, right?

1060
01:09:39,359 --> 01:09:42,920
 Spatial dimension in this case is three by three, okay?

1061
01:09:42,960 --> 01:09:44,600
 So it asks you to find what's the number

1062
01:09:44,600 --> 01:09:46,720
 of trainable parameter, okay?

1063
01:09:46,720 --> 01:09:50,600
 For the new convolutional layer after these changes,

1064
01:09:50,600 --> 01:09:53,600
 assuming no biases use, yeah?

1065
01:09:53,600 --> 01:09:56,440
 Okay, I'll give you a bit of time to think about it

1066
01:09:56,440 --> 01:09:58,600
 and then we'll go through the answer together.

1067
01:10:12,920 --> 01:10:13,760
 Okay.

1068
01:10:36,280 --> 01:10:38,640
 Right, okay, so I think this question is quite,

1069
01:10:39,480 --> 01:10:41,320
 yeah, it's either you get it or you don't get it.

1070
01:10:41,320 --> 01:10:43,360
 There's no point thinking too much about it.

1071
01:10:43,360 --> 01:10:45,480
 So what do you think is the answer?

1072
01:10:47,400 --> 01:10:48,639
 The calculation is,

1073
01:10:55,240 --> 01:10:56,759
 so what is the number of parameter?

1074
01:10:56,759 --> 01:10:59,320
 The number of parameter is the number of coefficient

1075
01:10:59,320 --> 01:11:02,960
 or weight that you have for the filter, right?

1076
01:11:03,799 --> 01:11:07,679
 So first of all, what's the spatial dimensions of,

1077
01:11:07,679 --> 01:11:10,160
 suppose if you just assume one filter first.

1078
01:11:10,160 --> 01:11:12,200
 So if you have one filter, now,

1079
01:11:12,200 --> 01:11:15,680
 so now your new image now, you can see it's RGB image,

1080
01:11:15,680 --> 01:11:18,639
 that means the input volume is three channel, right?

1081
01:11:18,639 --> 01:11:21,160
 Okay, so if you want to apply this filter here,

1082
01:11:21,160 --> 01:11:23,639
 what is the, suppose if it's just one filter,

1083
01:11:23,639 --> 01:11:25,080
 you want to apply on it,

1084
01:11:25,080 --> 01:11:29,200
 what is the dimensions of this one filter is?

1085
01:11:32,880 --> 01:11:36,240
 We want to modify initially, in this example here,

1086
01:11:36,240 --> 01:11:39,480
 initially this filter is three by three, right?

1087
01:11:39,480 --> 01:11:42,440
 And the number of channel is one

1088
01:11:42,440 --> 01:11:46,040
 because this is the grayscale image.

1089
01:11:46,040 --> 01:11:48,480
 Now we say we want to change this grayscale image

1090
01:11:48,480 --> 01:11:50,280
 to color image now.

1091
01:11:50,280 --> 01:11:54,519
 That means now your input volume now has three channel, right?

1092
01:11:54,519 --> 01:11:57,080
 So this filter now initially is three by three

1093
01:11:57,080 --> 01:12:00,240
 for grayscale, three by three by one for grayscale.

1094
01:12:00,240 --> 01:12:03,879
 Now your input now is from one channel,

1095
01:12:03,879 --> 01:12:06,759
 grayscale image to three channel RGB image.

1096
01:12:06,760 --> 01:12:10,160
 So your filter now need to be three by three by,

1097
01:12:12,720 --> 01:12:15,200
 three by three by three, right?

1098
01:12:15,200 --> 01:12:17,000
 Because you need to make sure now

1099
01:12:17,000 --> 01:12:18,840
 this particular filter dimension

1100
01:12:18,840 --> 01:12:20,920
 need to be consistent with your input volume.

1101
01:12:20,920 --> 01:12:23,480
 That means if you are using only one filter,

1102
01:12:23,480 --> 01:12:26,000
 it'll be three by three by three.

1103
01:12:26,000 --> 01:12:29,160
 But the question also tell you now,

1104
01:12:29,160 --> 01:12:31,120
 your output now, your output,

1105
01:12:31,120 --> 01:12:35,040
 you want to increase it to the number of channel now is six.

1106
01:12:35,040 --> 01:12:36,960
 That means your output volume dimension

1107
01:12:36,960 --> 01:12:39,680
 in terms of the number of channel is six.

1108
01:12:39,680 --> 01:12:42,080
 How do you generate six channel?

1109
01:12:42,920 --> 01:12:46,160
 You need to use six different filter, right?

1110
01:12:46,160 --> 01:12:48,040
 So therefore the total number of parameter now

1111
01:12:48,040 --> 01:12:51,080
 for one filter is three by three by three.

1112
01:12:51,080 --> 01:12:52,400
 So that's one filter.

1113
01:12:52,400 --> 01:12:54,920
 And because now you're going to generate six channel,

1114
01:12:54,920 --> 01:12:58,960
 so it's three by three by three times a six.

1115
01:12:58,960 --> 01:13:00,560
 That would be the answer.

1116
01:13:05,600 --> 01:13:06,440
 So now,

1117
01:13:10,440 --> 01:13:12,680
 okay, so therefore the number of channel parameter

1118
01:13:12,680 --> 01:13:15,760
 for this new convolutional layer is three by three by three

1119
01:13:15,760 --> 01:13:16,920
 by six, yeah?

1120
01:13:16,920 --> 01:13:18,519
 So just now going through the explanation.

1121
01:13:18,519 --> 01:13:20,080
 So it's one, six, two.

1122
01:13:20,080 --> 01:13:23,440
 So that is the answer to that particular question.

1123
01:13:25,080 --> 01:13:27,480
 Right, okay, so perhaps let's try to go through one more

1124
01:13:27,480 --> 01:13:29,920
 exercise before I give you a break.

1125
01:13:36,040 --> 01:13:40,040
 Okay, so the next exercise is this, yeah?

1126
01:13:42,200 --> 01:13:46,480
 Okay, the FC, we're going to focus on FC and softmax layer.

1127
01:13:46,480 --> 01:13:48,840
 It says that the input feature X will pass through

1128
01:13:48,840 --> 01:13:51,160
 a FC layer of a CNN, right?

1129
01:13:51,160 --> 01:13:54,680
 Okay, so this is an input feature.

1130
01:13:54,680 --> 01:13:58,880
 This is actually your weight matrix and this is the bias.

1131
01:13:58,880 --> 01:14:01,920
 So it says that this softmax function is then applied

1132
01:14:01,920 --> 01:14:04,360
 to the output of this FC layer.

1133
01:14:04,360 --> 01:14:06,360
 Right, number one, you're asked to find what's the output

1134
01:14:06,360 --> 01:14:07,799
 of the FC layer.

1135
01:14:07,799 --> 01:14:11,839
 Number two is try to find the output of this softmax layer.

1136
01:14:11,839 --> 01:14:13,759
 Right, okay, so it's not that difficult,

1137
01:14:13,759 --> 01:14:16,759
 but it's a process that you need to finalize yourself

1138
01:14:16,759 --> 01:14:21,000
 on how to convert this softmax normalization, yeah?

1139
01:14:21,000 --> 01:14:23,280
 So I'll give you a bit of time to try it out

1140
01:14:23,280 --> 01:14:25,480
 and then we'll go through the answers together.

1141
01:14:34,360 --> 01:14:36,200
 Okay.

1142
01:15:04,360 --> 01:15:05,200
 Okay.

1143
01:15:34,360 --> 01:15:35,200
 Okay.

1144
01:16:04,360 --> 01:16:05,200
 Okay.

1145
01:16:34,360 --> 01:16:35,200
 Okay.

1146
01:17:04,440 --> 01:17:05,280
 Okay.

1147
01:17:21,120 --> 01:17:24,719
 Okay, so for step one, how do we calculate the output

1148
01:17:24,719 --> 01:17:26,320
 after the FC layer?

1149
01:17:27,320 --> 01:17:29,759
 Right, so if you remember, the equation for FC layer

1150
01:17:29,759 --> 01:17:32,240
 is nothing but WX plus B, right?

1151
01:17:32,240 --> 01:17:34,200
 So you just go through the motion, yeah?

1152
01:17:34,200 --> 01:17:36,240
 You should be able to find the output.

1153
01:17:38,000 --> 01:17:41,320
 Right, okay, so the output after FC layer is WX plus B,

1154
01:17:41,320 --> 01:17:43,840
 so you plug in the value, you do some calculation,

1155
01:17:43,840 --> 01:17:47,160
 you'll see that this is your output.

1156
01:17:47,160 --> 01:17:51,960
 Okay, so next, this output will go through a softmax layer.

1157
01:17:51,960 --> 01:17:55,240
 So the softmax layers involve the softmax normalization,

1158
01:17:55,240 --> 01:17:57,940
 which is given by this equations here.

1159
01:17:57,940 --> 01:18:01,000
 So this equation, Pj, refer to the probability

1160
01:18:01,000 --> 01:18:05,040
 for the category, for node J or category J, right?

1161
01:18:05,040 --> 01:18:07,760
 So it's either a power of the output

1162
01:18:07,760 --> 01:18:10,240
 for the particular category J, right?

1163
01:18:10,240 --> 01:18:15,240
 Over either a power of, no, either a power of ZK,

1164
01:18:16,320 --> 01:18:19,680
 you sum up over all the category, okay?

1165
01:18:19,680 --> 01:18:21,360
 So in this case here, what it means is that

1166
01:18:21,360 --> 01:18:25,200
 for this denominator, right, is either a power of 1.2

1167
01:18:25,200 --> 01:18:28,800
 plus either a power of 1.3 plus either a power of 2.2, right?

1168
01:18:28,840 --> 01:18:31,240
 If you calculate, you see this is the value.

1169
01:18:31,240 --> 01:18:34,960
 So now, the first probability after normalization

1170
01:18:34,960 --> 01:18:39,000
 is either a power of 1.2 divided by this value

1171
01:18:39,000 --> 01:18:43,600
 that we have evaluated earlier, your C is 0.207.

1172
01:18:43,600 --> 01:18:48,480
 Okay, and then for P2, it's either a power of 1.3

1173
01:18:48,480 --> 01:18:50,960
 divided by the common denominator.

1174
01:18:50,960 --> 01:18:54,760
 But so if you do that, your C is 0.229, right?

1175
01:18:54,760 --> 01:18:57,720
 You repeat that for the third category,

1176
01:18:57,760 --> 01:19:01,960
 the C is 0.56636.

1177
01:19:01,960 --> 01:19:04,880
 So therefore, the output after the softmax layer

1178
01:19:04,880 --> 01:19:06,160
 would be this value here.

1179
01:19:07,160 --> 01:19:09,840
 So you can see that this particular value

1180
01:19:09,840 --> 01:19:12,160
 after the softmax is actually a probability.

1181
01:19:12,160 --> 01:19:14,720
 When you sum it up, it should be equal to one.

1182
01:19:14,720 --> 01:19:17,440
 So therefore, this particular output

1183
01:19:17,440 --> 01:19:19,920
 after softmax normalization,

1184
01:19:19,920 --> 01:19:21,800
 you can compare to the ground truth.

1185
01:19:21,800 --> 01:19:24,920
 The ground truth is usually in the form of one hot modeling.

1186
01:19:24,920 --> 01:19:27,960
 But for the category, the ground truth category is one,

1187
01:19:27,960 --> 01:19:28,920
 the rest is zero.

1188
01:19:28,920 --> 01:19:31,640
 That's why now, these two factor,

1189
01:19:31,640 --> 01:19:33,800
 their distance is comparable.

1190
01:19:33,800 --> 01:19:36,120
 And then you can use it to calculate the loss.

1191
01:19:36,120 --> 01:19:38,760
 And this loss is the cross entropy

1192
01:19:38,760 --> 01:19:41,360
 of the softmax loss that we have studied before.

1193
01:19:41,360 --> 01:19:42,260
 Okay, right?

1194
01:19:42,260 --> 01:19:45,960
 So that's pretty much for this exercise.

1195
01:19:50,560 --> 01:19:53,360
 Right, okay, so before, I think we can take a short break

1196
01:19:53,360 --> 01:19:54,800
 before we go to the next part.

1197
01:19:54,800 --> 01:19:59,000
 So now it's 750, let's come back, 805, okay?

1198
01:35:54,800 --> 01:35:55,640
 Okay.

1199
01:36:24,800 --> 01:36:29,800
 Okay, so let's continue with the training of the CNN.

1200
01:36:45,040 --> 01:36:47,080
 So how do we train a CNN?

1201
01:36:47,080 --> 01:36:49,200
 So training a CNN essentially means

1202
01:36:49,200 --> 01:36:54,040
 that you want to estimate the parameters in your,

1203
01:36:54,080 --> 01:36:59,000
 just in the convolutional layer, you have the filter, right?

1204
01:36:59,000 --> 01:37:02,720
 And also in the FC layer, you have the big W matrix.

1205
01:37:02,720 --> 01:37:04,440
 So when we say train a CNN,

1206
01:37:04,440 --> 01:37:07,280
 that means we want to train those parameters in the network,

1207
01:37:07,280 --> 01:37:10,400
 okay, including the weight matrix in the FC layer,

1208
01:37:10,400 --> 01:37:14,880
 as well as a filter or kernel in the convolutional layer.

1209
01:37:14,880 --> 01:37:16,840
 Right, okay, so what's the high level objective

1210
01:37:16,840 --> 01:37:19,120
 of this training is that we want to minimize

1211
01:37:19,120 --> 01:37:20,500
 the loss function.

1212
01:37:20,500 --> 01:37:23,400
 So for example, in our case of classification,

1213
01:37:23,400 --> 01:37:25,519
 the loss function would be the softmax loss

1214
01:37:25,519 --> 01:37:27,639
 or the cross-entrophy loss.

1215
01:37:27,639 --> 01:37:29,360
 So the idea is based on what is known

1216
01:37:29,360 --> 01:37:32,240
 as a stochastic gradient descent, okay?

1217
01:37:32,240 --> 01:37:35,320
 So the idea of stochastic gradient descent is like this.

1218
01:37:35,320 --> 01:37:37,440
 Right, suppose if you look at this particular graph,

1219
01:37:37,440 --> 01:37:40,440
 for Wang-Men-Wen, let's assume that for this network,

1220
01:37:40,440 --> 01:37:43,400
 we only have two parameters we need to optimize.

1221
01:37:43,400 --> 01:37:46,519
 Right, let's call this thing X1 and X2, right?

1222
01:37:46,519 --> 01:37:50,200
 For action network, you may have millions of parameters,

1223
01:37:50,200 --> 01:37:51,839
 okay, that you need to optimize.

1224
01:37:51,840 --> 01:37:53,400
 But let's start with something simple

1225
01:37:53,400 --> 01:37:55,320
 that we can visualize and then afterwards,

1226
01:37:55,320 --> 01:37:58,380
 we can extend to something more complicated.

1227
01:37:58,380 --> 01:37:59,800
 Suppose for this simple network,

1228
01:37:59,800 --> 01:38:02,360
 we only have two parameters we need to optimize.

1229
01:38:02,360 --> 01:38:05,080
 We call X1 and X2, okay?

1230
01:38:05,080 --> 01:38:08,880
 So with respect to different values of X1 and X2,

1231
01:38:08,880 --> 01:38:11,280
 you have different value of loss.

1232
01:38:11,280 --> 01:38:15,320
 So this loss value now will look like actually,

1233
01:38:15,320 --> 01:38:18,360
 the landscape, the mountain and valley, okay?

1234
01:38:18,360 --> 01:38:20,320
 So our objective is that initially,

1235
01:38:20,320 --> 01:38:24,719
 because we do not know what is the best parameter of weight,

1236
01:38:24,719 --> 01:38:28,400
 so initially we may start off with some position here,

1237
01:38:28,400 --> 01:38:30,559
 okay, somewhere that correspond to below

1238
01:38:31,559 --> 01:38:34,639
 a non-ideal X1 and X2 value.

1239
01:38:34,639 --> 01:38:37,280
 Our goal is that we want to reach the minima,

1240
01:38:37,280 --> 01:38:38,799
 okay, this point here.

1241
01:38:38,799 --> 01:38:40,639
 So what do we do?

1242
01:38:40,639 --> 01:38:42,360
 What do we do is that at this position,

1243
01:38:42,360 --> 01:38:46,440
 we look around, we find the direction of the steepest descent

1244
01:38:46,440 --> 01:38:49,920
 and then we walk a certain steps in the direction.

1245
01:38:49,920 --> 01:38:53,160
 So once you reach the direction, you look around,

1246
01:38:53,160 --> 01:38:55,000
 you find the direction of steepest descent

1247
01:38:55,000 --> 01:38:57,760
 and you continue to walk in the direction.

1248
01:38:57,760 --> 01:39:00,400
 So you repeat the process a number of times

1249
01:39:00,400 --> 01:39:04,040
 until hopefully you reach a good local minima.

1250
01:39:04,040 --> 01:39:06,480
 So that means at this point, you look around,

1251
01:39:06,480 --> 01:39:08,400
 you find this direction is the direction

1252
01:39:08,400 --> 01:39:09,480
 of the steepest descent.

1253
01:39:09,480 --> 01:39:12,880
 So you walk in a certain percentage until you reach here.

1254
01:39:12,880 --> 01:39:15,440
 So next, at this point, you look around, okay?

1255
01:39:15,440 --> 01:39:17,640
 You see this is the direction of the steepest descent,

1256
01:39:17,680 --> 01:39:20,400
 so you walk in this direction here

1257
01:39:20,400 --> 01:39:22,200
 and afterwards you repeatedly doing that

1258
01:39:22,200 --> 01:39:24,520
 until you reach a local minima.

1259
01:39:24,520 --> 01:39:26,760
 How much you need to walk is controlled

1260
01:39:26,760 --> 01:39:30,560
 by a parameter known as the learning rate, okay?

1261
01:39:30,560 --> 01:39:34,240
 So that's the basic ideas of stochastic gradient descent.

1262
01:39:34,240 --> 01:39:36,520
 So in order to do that, you typically need

1263
01:39:36,520 --> 01:39:39,760
 to construct a computational graph

1264
01:39:39,760 --> 01:39:42,720
 to help you to find out what is the gradient, okay?

1265
01:39:42,720 --> 01:39:44,320
 And how to update the parameter.

1266
01:39:45,320 --> 01:39:48,799
 So now if you look at this particular training here,

1267
01:39:48,799 --> 01:39:51,000
 so in order to look at this particular,

1268
01:39:51,000 --> 01:39:53,240
 just now we talked about the learning rate here.

1269
01:39:53,240 --> 01:39:54,599
 So let's look at this equation.

1270
01:39:54,599 --> 01:39:58,120
 So this equation is actually the loss function

1271
01:39:58,120 --> 01:40:01,160
 or the error function for I training data.

1272
01:40:01,160 --> 01:40:04,960
 So suppose our current training data is called XI,

1273
01:40:04,960 --> 01:40:08,559
 our current label is called YI, the ground truth is YI,

1274
01:40:08,559 --> 01:40:12,360
 and the view is your current parameter

1275
01:40:12,799 --> 01:40:14,400
 or weight that you need to learn,

1276
01:40:14,400 --> 01:40:18,519
 which can include all the convolutional layer kernel

1277
01:40:18,519 --> 01:40:22,960
 plus all the FC weight matrix.

1278
01:40:22,960 --> 01:40:26,160
 So it's modeled by this big W here, right?

1279
01:40:26,160 --> 01:40:31,160
 So even input, even this W matrix,

1280
01:40:31,280 --> 01:40:34,480
 this weight parameter W, you can get your predicted output.

1281
01:40:34,480 --> 01:40:35,900
 Once you get your predicted output,

1282
01:40:35,900 --> 01:40:38,719
 you can compare with your ground truth, right?

1283
01:40:38,719 --> 01:40:42,320
 Then you'll get your current loss for I training sample.

1284
01:40:42,719 --> 01:40:45,400
 So when we want to find the total loss,

1285
01:40:45,400 --> 01:40:48,599
 we need to add up all the training sample,

1286
01:40:48,599 --> 01:40:50,080
 and then we divide it by N.

1287
01:40:50,080 --> 01:40:52,799
 So this, therefore, is our loss function now.

1288
01:40:52,799 --> 01:40:55,320
 Our loss function is the functions of W.

1289
01:40:56,280 --> 01:41:00,280
 So this W is a parameter that we want to optimize or learn.

1290
01:41:01,320 --> 01:41:03,200
 So now we have this loss function.

1291
01:41:03,200 --> 01:41:05,000
 So when we want to learn this parameter,

1292
01:41:05,000 --> 01:41:07,000
 that means we need to take this loss function

1293
01:41:07,000 --> 01:41:11,360
 and differentiate with respect to this W parameter.

1294
01:41:11,360 --> 01:41:14,320
 So we differentiate with respect to this W parameter.

1295
01:41:14,320 --> 01:41:16,200
 So mathematically, what it means is that

1296
01:41:16,200 --> 01:41:18,920
 you also take this particular loss function

1297
01:41:18,920 --> 01:41:21,280
 and you perform partial, no,

1298
01:41:21,280 --> 01:41:23,120
 kind of a differentiation with respect

1299
01:41:23,120 --> 01:41:25,799
 to this W parameter here, okay?

1300
01:41:25,799 --> 01:41:29,519
 Right, so this particular term here can be interpreted

1301
01:41:29,519 --> 01:41:33,000
 as trying to find the steepest gradient, okay?

1302
01:41:33,000 --> 01:41:34,920
 Steepest gradient up works.

1303
01:41:34,920 --> 01:41:38,679
 So now when you want to do the training of the network,

1304
01:41:38,679 --> 01:41:40,000
 essentially what it means is that

1305
01:41:40,000 --> 01:41:43,680
 you have a current set of the parameter already, okay?

1306
01:41:43,680 --> 01:41:46,840
 So you have the current set of the parameter here, okay?

1307
01:41:46,840 --> 01:41:50,920
 You look at the directions of the steepest ascent, okay?

1308
01:41:50,920 --> 01:41:53,600
 And then after works, you have a negative sign in front,

1309
01:41:53,600 --> 01:41:56,920
 which means that you are now trying to find the direction

1310
01:41:56,920 --> 01:41:59,360
 of the steepest ascent now, okay?

1311
01:41:59,360 --> 01:42:03,440
 And then this parameter alpha is the learning rate.

1312
01:42:03,440 --> 01:42:06,080
 So this alpha means how much you want to walk

1313
01:42:06,080 --> 01:42:07,400
 in the direction.

1314
01:42:07,400 --> 01:42:09,719
 So therefore, when you want to learn it,

1315
01:42:09,719 --> 01:42:11,160
 it's actually quite easy.

1316
01:42:11,160 --> 01:42:13,559
 That means our next step learning,

1317
01:42:13,559 --> 01:42:17,240
 our next step parameter, next iteration parameter,

1318
01:42:17,240 --> 01:42:20,200
 is your current parameter, okay?

1319
01:42:20,200 --> 01:42:23,360
 You walk in the directions of the steepest ascent

1320
01:42:23,360 --> 01:42:26,120
 and how much you want to walk is controlled

1321
01:42:26,120 --> 01:42:28,559
 by this learning rate alpha, okay?

1322
01:42:28,559 --> 01:42:30,759
 So that's the high level interpretation.

1323
01:42:30,759 --> 01:42:32,519
 So now if you look at this particular graph,

1324
01:42:32,519 --> 01:42:35,839
 if you try to plot the loss function versus the epoch,

1325
01:42:35,880 --> 01:42:39,280
 so epoch is the number of training,

1326
01:42:39,280 --> 01:42:41,440
 okay, all the training data that you have,

1327
01:42:41,440 --> 01:42:44,240
 if you use all your training data to train it once,

1328
01:42:44,240 --> 01:42:46,000
 we call it one epoch, right?

1329
01:42:46,000 --> 01:42:49,240
 So horizontal axis is the number of epochs that you have,

1330
01:42:49,240 --> 01:42:50,840
 vertical axis is the loss.

1331
01:42:50,840 --> 01:42:54,200
 So suppose initially you start off with some parameter W,

1332
01:42:54,200 --> 01:42:56,080
 so the loss is quite high.

1333
01:42:56,080 --> 01:42:57,760
 If you choose a good learning rate,

1334
01:42:57,760 --> 01:43:01,480
 then your loss will decrease nicely.

1335
01:43:01,480 --> 01:43:03,920
 That means your W will become better and better

1336
01:43:03,920 --> 01:43:06,360
 and your loss will start to decrease.

1337
01:43:06,360 --> 01:43:10,560
 So on the other hand, if you choose a low learning rate,

1338
01:43:10,560 --> 01:43:15,560
 then your loss is going to reduce slowly, okay?

1339
01:43:15,760 --> 01:43:18,320
 If you choose a large learning rate, okay,

1340
01:43:18,320 --> 01:43:19,600
 then the ideal one,

1341
01:43:19,600 --> 01:43:22,240
 then your loss is going to decrease quickly,

1342
01:43:22,240 --> 01:43:25,280
 but you'll reach an optimal or steady state,

1343
01:43:25,280 --> 01:43:28,120
 which is not as good as a good learning rate.

1344
01:43:28,120 --> 01:43:30,520
 And finally, if you choose a very large learning rate,

1345
01:43:30,520 --> 01:43:32,880
 sometimes it may not converge, yeah?

1346
01:43:32,920 --> 01:43:34,640
 Because it may overshoot.

1347
01:43:34,640 --> 01:43:36,960
 So therefore, a lot of the time,

1348
01:43:37,960 --> 01:43:41,680
 this choosing a good learning rate involves trying error.

1349
01:43:41,680 --> 01:43:44,120
 In your data set, you have to try different range

1350
01:43:44,120 --> 01:43:45,680
 of the learning rate, right?

1351
01:43:45,680 --> 01:43:47,800
 There's some strategy to do that, okay?

1352
01:43:49,320 --> 01:43:51,800
 So next, we are going to look at some well-known

1353
01:43:51,800 --> 01:43:53,640
 CNN architecture, right?

1354
01:43:53,640 --> 01:43:55,320
 So this particular timeline shows some

1355
01:43:55,320 --> 01:43:57,240
 of the CNN architecture.

1356
01:43:57,240 --> 01:44:02,040
 So what we have studied so far is a vanilla CNN, yeah?

1357
01:44:02,080 --> 01:44:04,400
 But over time, actually people start to improve

1358
01:44:04,400 --> 01:44:08,080
 and propose better and better CNN network here, right?

1359
01:44:08,080 --> 01:44:10,160
 Okay, so this diagram just shows you

1360
01:44:10,160 --> 01:44:12,000
 some of the well-known network.

1361
01:44:12,000 --> 01:44:14,960
 So the conf net, just now we see a number of layers

1362
01:44:14,960 --> 01:44:16,480
 of convolutional layer.

1363
01:44:16,480 --> 01:44:18,400
 The net is by Yang Li-kun,

1364
01:44:18,400 --> 01:44:21,720
 early layers to recognize character.

1365
01:44:21,720 --> 01:44:25,920
 Alex net, okay, some of them we'll explain later, right?

1366
01:44:25,920 --> 01:44:27,240
 I will not go through some of them,

1367
01:44:27,240 --> 01:44:29,800
 all of them are just highlighted, okay, Alex net, right?

1368
01:44:29,800 --> 01:44:31,440
 This is the network that indicate,

1369
01:44:31,719 --> 01:44:35,480
 really first time demonstrate if you train using a CNN,

1370
01:44:35,480 --> 01:44:38,719
 using a large data set, is really going to outperform

1371
01:44:38,719 --> 01:44:40,759
 traditional handcrafted methods.

1372
01:44:40,759 --> 01:44:43,400
 So this is considered as a turning point.

1373
01:44:43,400 --> 01:44:45,240
 Okay, VGG is an important network

1374
01:44:45,240 --> 01:44:48,240
 that shows the elegance of the network.

1375
01:44:48,240 --> 01:44:50,519
 Resnet is also a very important network.

1376
01:44:50,519 --> 01:44:53,480
 There's lots of a network which is currently

1377
01:44:53,480 --> 01:44:54,559
 still using Resnet.

1378
01:44:54,559 --> 01:44:56,480
 For example, the transformer that probably some

1379
01:44:56,480 --> 01:44:58,599
 of you heard about is still leveraging

1380
01:44:58,599 --> 01:45:01,400
 some of the ideas in Resnet, okay?

1381
01:45:02,440 --> 01:45:06,320
 Right, so let's try to very quickly go through

1382
01:45:06,320 --> 01:45:07,960
 some of this network here.

1383
01:45:07,960 --> 01:45:10,799
 So the first one that we have is Alex net, right?

1384
01:45:10,799 --> 01:45:13,000
 So it was the first neural network that is applied

1385
01:45:13,000 --> 01:45:15,240
 on large scale image data set, right?

1386
01:45:15,240 --> 01:45:18,839
 Which is actually image net by their group,

1387
01:45:18,839 --> 01:45:21,000
 is Feifei Li's group, right?

1388
01:45:21,000 --> 01:45:25,040
 Okay, and then it had make use of a classical structure

1389
01:45:25,040 --> 01:45:28,559
 for image classification, consisting of convolutional layer

1390
01:45:28,559 --> 01:45:29,679
 and FC layer.

1391
01:45:29,679 --> 01:45:30,919
 So this is the Alex net.

1392
01:45:30,920 --> 01:45:33,240
 At first glance, it looks a little bit different,

1393
01:45:33,240 --> 01:45:35,200
 but actually essentially, right?

1394
01:45:35,200 --> 01:45:39,080
 The basic layers are very similar to the vanilla CNN

1395
01:45:39,080 --> 01:45:40,360
 that you have studied, okay?

1396
01:45:40,360 --> 01:45:44,040
 So it make use of convolutional layer and FC layer.

1397
01:45:44,040 --> 01:45:47,480
 So it's actually the champion in 2012

1398
01:45:47,480 --> 01:45:51,600
 in this image net large scale visual recognition

1399
01:45:51,600 --> 01:45:53,600
 championship, right?

1400
01:45:53,600 --> 01:45:54,440
 Or competition.

1401
01:45:54,440 --> 01:45:56,480
 So this one, the objective is,

1402
01:45:56,480 --> 01:46:00,440
 given the image you want to classify into 1000 category.

1403
01:46:00,480 --> 01:46:05,480
 Okay, so this is also known as an image net challenge in 2012.

1404
01:46:05,919 --> 01:46:07,960
 So this particular year 2012,

1405
01:46:07,960 --> 01:46:11,040
 commonly is interpreted as a turning point, yeah?

1406
01:46:11,040 --> 01:46:15,000
 Right, so this is the first time that people actually

1407
01:46:15,000 --> 01:46:17,679
 more conveniently show that deep learning

1408
01:46:17,679 --> 01:46:20,440
 or this kind of network can outperform many classical

1409
01:46:20,440 --> 01:46:21,280
 approach.

1410
01:46:21,280 --> 01:46:24,960
 So if you count today, yeah, I mean now it's 2014.

1411
01:46:24,960 --> 01:46:28,240
 So pretty much we only have like 12 years of AI,

1412
01:46:28,280 --> 01:46:31,160
 which really demonstrates just within 12 years

1413
01:46:31,160 --> 01:46:33,719
 how things has changed from this year.

1414
01:46:35,559 --> 01:46:38,440
 Right, okay, so the next network is called VGG network.

1415
01:46:38,440 --> 01:46:43,440
 So some basic information about VGG is a runner up

1416
01:46:43,440 --> 01:46:47,519
 to this particular image net challenge in 2014.

1417
01:46:47,519 --> 01:46:49,639
 Right, so why VGG is very important

1418
01:46:49,639 --> 01:46:52,920
 is because it has a very elegant network architecture.

1419
01:46:52,920 --> 01:46:55,559
 The network architecture it uses is very elegant,

1420
01:46:55,560 --> 01:46:59,480
 is very similar to the vanilla CNN that we have studied.

1421
01:46:59,480 --> 01:47:03,320
 Okay, so it make use a deep network

1422
01:47:03,320 --> 01:47:04,960
 with three by three kernel.

1423
01:47:04,960 --> 01:47:08,000
 So the filter size is actually three by three kernel,

1424
01:47:08,000 --> 01:47:12,240
 filter size, and at the time is, right,

1425
01:47:12,240 --> 01:47:14,160
 usually the number of layers that you have,

1426
01:47:14,160 --> 01:47:15,480
 there's a few different variants,

1427
01:47:15,480 --> 01:47:19,160
 like you have 11 layers, 16 layer, 19 layers.

1428
01:47:19,160 --> 01:47:23,280
 So at this, at our current time we consider 16 to 19 layer,

1429
01:47:23,320 --> 01:47:26,599
 it's not a lot, but you have to remember during that time,

1430
01:47:26,599 --> 01:47:30,320
 right, like 19 layer is considered as quite deep already.

1431
01:47:30,320 --> 01:47:33,960
 So at that time it's considered as a deep network, right.

1432
01:47:33,960 --> 01:47:36,519
 But one of the problem of VGG is that it requires

1433
01:47:36,519 --> 01:47:38,840
 a large number of parameters, okay.

1434
01:47:38,840 --> 01:47:42,200
 So shortly we'll see why that's the case, yeah.

1435
01:47:42,200 --> 01:47:45,320
 Okay, so there are a few variants for this VGG among them

1436
01:47:45,320 --> 01:47:47,240
 is the VGG 16 and 19,

1437
01:47:47,240 --> 01:47:50,800
 which indicate the number of layers, yeah.

1438
01:47:50,800 --> 01:47:53,800
 So, right, so this is actually the network structures

1439
01:47:53,800 --> 01:47:55,640
 of a VGG network, right.

1440
01:47:55,640 --> 01:48:00,400
 So it takes the input image of 224 by 224 by three RGB image,

1441
01:48:00,400 --> 01:48:03,320
 right, so you can see this early channels here

1442
01:48:03,320 --> 01:48:06,520
 are actually the convolutional plus value layer, okay.

1443
01:48:06,520 --> 01:48:09,400
 For example, this two layer, the dimension is 224

1444
01:48:09,400 --> 01:48:12,720
 by 224 by 64 channel, okay.

1445
01:48:12,720 --> 01:48:17,120
 And afterwards it let go through the max pooling, okay.

1446
01:48:17,120 --> 01:48:19,400
 And then two convolutional, max pooling,

1447
01:48:19,440 --> 01:48:20,799
 convolutional, max pooling,

1448
01:48:20,799 --> 01:48:23,440
 convolutional, max pooling, convolutional, max pooling.

1449
01:48:23,440 --> 01:48:25,920
 And then finally you have this FC layer.

1450
01:48:25,920 --> 01:48:28,639
 So the FC layer that we've seen in the previous example

1451
01:48:28,639 --> 01:48:31,160
 is indicated as a vertical vector,

1452
01:48:31,160 --> 01:48:33,240
 but in this again they redraw it,

1453
01:48:33,240 --> 01:48:35,360
 no as if it's like a no lying down,

1454
01:48:35,360 --> 01:48:37,519
 but it's actually the same as a vertical

1455
01:48:37,519 --> 01:48:39,240
 kind of a vectors here.

1456
01:48:39,240 --> 01:48:41,240
 So you have this FC layers here, right.

1457
01:48:41,240 --> 01:48:44,759
 You can see it's 4096, 4096 dimension, right.

1458
01:48:44,759 --> 01:48:48,719
 And then this one is 1000 and this one is 1000, okay.

1459
01:48:48,760 --> 01:48:53,760
 So this is the structures of the VCC VGG networks here.

1460
01:48:55,880 --> 01:48:58,200
 So early on we mentioned that, right,

1461
01:48:58,200 --> 01:49:02,120
 one of the shortcomings of VGG is that it requires a large,

1462
01:49:02,120 --> 01:49:05,280
 no, it requires a large number of parameters.

1463
01:49:05,280 --> 01:49:07,400
 Where do you think this large number of parameters

1464
01:49:07,400 --> 01:49:08,240
 is coming from?

1465
01:49:08,240 --> 01:49:13,240
 VGG requires a large number of parameters.

1466
01:49:20,599 --> 01:49:23,639
 And where does this large number of parameters comes from?

1467
01:49:24,719 --> 01:49:28,400
 It comes from, from our earlier discussion,

1468
01:49:28,400 --> 01:49:31,240
 we say that which part of the network,

1469
01:49:31,240 --> 01:49:34,200
 which part of CNN requires the largest number of parameters?

1470
01:49:35,200 --> 01:49:38,720
 FC layer, huh.

1471
01:49:38,720 --> 01:49:43,280
 So actually the reason why this particular VGG network

1472
01:49:43,280 --> 01:49:44,639
 requires such a number of parameters

1473
01:49:44,639 --> 01:49:47,800
 is because you can see the dimensions of the FC layer

1474
01:49:47,800 --> 01:49:48,639
 is very large.

1475
01:49:48,639 --> 01:49:52,760
 For example, 4096, 4096, 1000, 1000.

1476
01:49:52,760 --> 01:49:55,120
 That means there's lots of connection in between

1477
01:49:55,120 --> 01:49:58,800
 and hence it's very, it requires lots of parameters.

1478
01:49:58,800 --> 01:50:01,760
 So therefore one of the shortcomings of VGG is,

1479
01:50:01,760 --> 01:50:03,880
 no, it requires a large number of parameters.

1480
01:50:03,880 --> 01:50:06,880
 So it's not so commonly used in some network

1481
01:50:06,880 --> 01:50:09,000
 that cannot afford so many parameters.

1482
01:50:10,280 --> 01:50:12,440
 Right, so the next network that we have,

1483
01:50:12,440 --> 01:50:14,480
 very well known one is called ResNet.

1484
01:50:14,480 --> 01:50:18,720
 So ResNet is the winner of this ImageNet challenge in 2015.

1485
01:50:18,720 --> 01:50:20,280
 It has a very deep structure.

1486
01:50:20,280 --> 01:50:23,880
 So this deep structure at the time correspond to about,

1487
01:50:23,880 --> 01:50:28,640
 like, 50 layers, 101 layers, 15 something layers.

1488
01:50:28,640 --> 01:50:30,680
 So it's about 100 plus layer.

1489
01:50:30,680 --> 01:50:32,200
 So at the time, even now, yeah,

1490
01:50:32,200 --> 01:50:34,560
 it's considered reasonably deep already.

1491
01:50:34,560 --> 01:50:36,400
 So it's a very deep structure.

1492
01:50:36,400 --> 01:50:39,240
 And then it makes use of what is known as a residue block

1493
01:50:39,240 --> 01:50:41,200
 and a highway or skip connection

1494
01:50:41,200 --> 01:50:44,120
 for better gradient back propagation.

1495
01:50:44,120 --> 01:50:46,920
 So what is the meaning of this residue block

1496
01:50:46,920 --> 01:50:50,800
 and, no, skip and highway connection

1497
01:50:50,800 --> 01:50:54,120
 and how does it facilitate gradient back propagation?

1498
01:50:54,120 --> 01:50:56,440
 So let's look at this particular diagram here.

1499
01:50:56,440 --> 01:50:59,240
 So this is the one of the smallest versions

1500
01:50:59,240 --> 01:51:00,880
 of the ResNet here.

1501
01:51:00,880 --> 01:51:02,880
 So you can see in this ResNet here,

1502
01:51:02,880 --> 01:51:05,880
 some of the key building block is like these structures here.

1503
01:51:07,080 --> 01:51:09,639
 So it consists of many of this kind of block

1504
01:51:09,639 --> 01:51:12,000
 which is known as a residue block here.

1505
01:51:12,000 --> 01:51:14,719
 So if you take out one of these residue blocks,

1506
01:51:14,719 --> 01:51:16,559
 this is how it looks like.

1507
01:51:16,559 --> 01:51:18,000
 Okay, so this residue block,

1508
01:51:18,000 --> 01:51:20,120
 you can see you have the X input

1509
01:51:20,120 --> 01:51:22,480
 and then you have these particular connections here.

1510
01:51:22,480 --> 01:51:25,120
 So this connection here is known as a highway

1511
01:51:25,120 --> 01:51:26,360
 or skip connection.

1512
01:51:26,679 --> 01:51:30,240
 Okay, and then afterwards you have a small network here.

1513
01:51:30,240 --> 01:51:31,559
 So this small network here,

1514
01:51:31,559 --> 01:51:34,559
 we can interpret it as like a function FX.

1515
01:51:34,559 --> 01:51:36,120
 You take this input X

1516
01:51:36,120 --> 01:51:38,440
 and it's going to learn something FX.

1517
01:51:38,440 --> 01:51:40,839
 Okay, and then the output at this particular junction here,

1518
01:51:40,839 --> 01:51:43,200
 you can see it's nothing but FX.

1519
01:51:43,200 --> 01:51:46,200
 Okay, what this network is learning FX plus

1520
01:51:46,200 --> 01:51:49,519
 because this is just a cable or highway connection,

1521
01:51:49,519 --> 01:51:50,519
 it's always X.

1522
01:51:50,519 --> 01:51:52,280
 So the output at this particular junction

1523
01:51:52,280 --> 01:51:55,000
 is actually FX plus X.

1524
01:51:55,000 --> 01:51:57,040
 Okay, so output at this junction.

1525
01:51:57,040 --> 01:52:00,080
 So now, okay, so in early years,

1526
01:52:00,080 --> 01:52:02,640
 when people start to design CNN,

1527
01:52:02,640 --> 01:52:03,960
 they find that, okay,

1528
01:52:03,960 --> 01:52:07,920
 if I start to increase the number of layers of CNN,

1529
01:52:07,920 --> 01:52:09,800
 the performance start to increase.

1530
01:52:10,800 --> 01:52:15,040
 But now, if you continue to add in more and more layers

1531
01:52:15,040 --> 01:52:17,400
 of CNN, do you think that the performance

1532
01:52:17,400 --> 01:52:19,880
 will continue to increase all the time?

1533
01:52:21,840 --> 01:52:23,800
 The answer is no.

1534
01:52:23,800 --> 01:52:26,240
 So if you keep adding many different layers

1535
01:52:26,240 --> 01:52:27,680
 to the vanilla CNN,

1536
01:52:27,680 --> 01:52:30,320
 actually the performance after it will increase a while

1537
01:52:30,320 --> 01:52:32,080
 and then it will start to come down.

1538
01:52:32,080 --> 01:52:35,440
 So what is the problem with this?

1539
01:52:37,760 --> 01:52:40,440
 What is the issue if you keep adding

1540
01:52:40,440 --> 01:52:44,720
 many different layers of this to your CNN?

1541
01:52:46,480 --> 01:52:49,720
 So this particular problem is a well-known problem

1542
01:52:49,720 --> 01:52:51,720
 known as a vanishing gradient problem.

1543
01:52:51,760 --> 01:52:53,840
 So vanishing gradient problem is that,

1544
01:52:53,840 --> 01:52:56,720
 but suppose if you're a network with many different layers,

1545
01:52:56,720 --> 01:52:59,560
 so when you want to compute,

1546
01:52:59,560 --> 01:53:01,760
 when you want to perform, train your network,

1547
01:53:01,760 --> 01:53:04,960
 first of all, you have an input image or input.

1548
01:53:04,960 --> 01:53:07,760
 You'll go through many different layers,

1549
01:53:07,760 --> 01:53:09,680
 to get your predicted output.

1550
01:53:09,680 --> 01:53:12,400
 You compare your predicted output with your ground truth

1551
01:53:12,400 --> 01:53:13,520
 to get the error.

1552
01:53:13,520 --> 01:53:16,400
 And then this error will then start to back propagate.

1553
01:53:18,360 --> 01:53:20,120
 And this error, back propagate,

1554
01:53:20,120 --> 01:53:24,640
 we then be used to adjust the parameter for the network.

1555
01:53:24,640 --> 01:53:27,880
 Some of them is FC layer, some of them is convolutional layer.

1556
01:53:27,880 --> 01:53:30,680
 But one of the problems when you have many different layers

1557
01:53:30,680 --> 01:53:32,559
 is that this gradient, right,

1558
01:53:32,559 --> 01:53:34,320
 initially this gradient is large,

1559
01:53:34,320 --> 01:53:36,400
 but after you propagate more and more,

1560
01:53:36,400 --> 01:53:39,559
 the gradient becomes smaller and smaller and smaller.

1561
01:53:39,559 --> 01:53:43,240
 Until you reach the earlier layers,

1562
01:53:43,240 --> 01:53:45,599
 the gradient is almost zero.

1563
01:53:45,599 --> 01:53:47,599
 So when the gradient is almost zero,

1564
01:53:47,600 --> 01:53:51,160
 then you have no way of updating your parameter

1565
01:53:51,160 --> 01:53:52,520
 in the earlier layer.

1566
01:53:52,520 --> 01:53:54,360
 So therefore, this is a well-known problem

1567
01:53:54,360 --> 01:53:56,360
 known as a vanishing gradient problem.

1568
01:53:56,360 --> 01:54:00,120
 So if you stack your gradient, many, many different layers,

1569
01:54:00,120 --> 01:54:04,480
 your earlier layers cannot be updated effectively.

1570
01:54:04,480 --> 01:54:05,720
 So that's the problem.

1571
01:54:05,720 --> 01:54:08,560
 So how does RESTED address this problem?

1572
01:54:08,560 --> 01:54:10,440
 So RESTED address this problem

1573
01:54:10,440 --> 01:54:14,040
 by having this particular connection known as the skip

1574
01:54:14,040 --> 01:54:15,640
 or highway connection.

1575
01:54:15,640 --> 01:54:17,480
 So the skip and highway connection is that

1576
01:54:17,480 --> 01:54:20,440
 suppose you have the predicted output

1577
01:54:20,440 --> 01:54:23,000
 and your ground truth, you get some error already.

1578
01:54:23,000 --> 01:54:26,920
 So this error now can go through this highway connection.

1579
01:54:26,920 --> 01:54:30,720
 Yeah, this gradient can then use this particular path

1580
01:54:30,720 --> 01:54:33,920
 as a highway connection to back propagate

1581
01:54:33,920 --> 01:54:36,480
 this gradient more effectively.

1582
01:54:36,480 --> 01:54:39,680
 So that's the high level reasoning.

1583
01:54:39,680 --> 01:54:43,920
 Why for RESTED, even when you have many different layer,

1584
01:54:43,920 --> 01:54:47,360
 the gradient can still be back propagated effectively

1585
01:54:47,360 --> 01:54:49,800
 and hence, even earlier layer,

1586
01:54:49,800 --> 01:54:51,880
 you can still train it effectively.

1587
01:54:51,880 --> 01:54:56,400
 So that's one of the important reasoning.

1588
01:54:56,400 --> 01:54:59,360
 Okay, another kind of a high level hypothesis

1589
01:54:59,360 --> 01:55:01,840
 that people think about it is like this.

1590
01:55:01,840 --> 01:55:04,640
 Okay, why RESTED is performing better is that,

1591
01:55:04,640 --> 01:55:08,400
 okay, so RESTED consists of many of this small network here.

1592
01:55:08,400 --> 01:55:10,280
 Each of this network here is,

1593
01:55:10,280 --> 01:55:12,240
 it can be interpreted as,

1594
01:55:12,320 --> 01:55:15,599
 your input is X, your output is X plus FX here.

1595
01:55:15,599 --> 01:55:19,760
 So this FX is actually only learning some small tasks,

1596
01:55:19,760 --> 01:55:21,320
 a very simple task.

1597
01:55:21,320 --> 01:55:23,880
 So the example is like, for example,

1598
01:55:23,880 --> 01:55:28,800
 if you want to take this course here,

1599
01:55:28,800 --> 01:55:32,719
 so you learn every week, you learn some basic concepts.

1600
01:55:32,719 --> 01:55:34,639
 So week one, you learn a basic concept,

1601
01:55:34,639 --> 01:55:36,280
 week two, you learn a basic concept.

1602
01:55:36,280 --> 01:55:38,559
 So every week, you learn a basic concept

1603
01:55:38,559 --> 01:55:41,160
 so that when you put all these weeks together,

1604
01:55:41,200 --> 01:55:43,960
 you turn out that you can learn the final outcome

1605
01:55:43,960 --> 01:55:46,120
 for this goal more effectively.

1606
01:55:46,120 --> 01:55:50,440
 So each week can then be interpreted like this.

1607
01:55:50,440 --> 01:55:54,160
 So each week, you are learning a simple task, FX.

1608
01:55:54,160 --> 01:55:56,120
 So this is as compared to,

1609
01:55:56,120 --> 01:55:59,320
 if I just give you a very complicated networks,

1610
01:55:59,320 --> 01:56:01,720
 I say, this is the input,

1611
01:56:01,720 --> 01:56:04,720
 this is the output you need to learn at the end.

1612
01:56:04,720 --> 01:56:05,960
 So if you just simply say,

1613
01:56:05,960 --> 01:56:09,440
 input, output, this is a big network you want,

1614
01:56:09,440 --> 01:56:11,400
 you need to learn a lot of the time,

1615
01:56:11,400 --> 01:56:13,799
 the network are not able to learn effectively.

1616
01:56:13,799 --> 01:56:18,320
 So you need to learn a little bit followed by a little bit.

1617
01:56:18,320 --> 01:56:21,080
 So this is another common analogy

1618
01:56:21,080 --> 01:56:24,559
 to explain why RESTnet usually perform well.

1619
01:56:26,320 --> 01:56:29,440
 So this slide shows some of the performance comparison.

1620
01:56:29,440 --> 01:56:31,360
 So for this of different networks,

1621
01:56:31,360 --> 01:56:34,360
 so the vertical axis is the arrow rate,

1622
01:56:34,360 --> 01:56:38,120
 top five arrow rate, so for this ImageNet challenge.

1623
01:56:38,120 --> 01:56:40,680
 So that means so long as your predictor output

1624
01:56:40,680 --> 01:56:42,640
 is in any of the top five category,

1625
01:56:42,640 --> 01:56:44,800
 you consider it to be correct.

1626
01:56:44,800 --> 01:56:47,440
 So horizontal axis is actually the number of years

1627
01:56:47,440 --> 01:56:49,800
 this network was produced.

1628
01:56:49,800 --> 01:56:51,720
 So you can see all these are the winner.

1629
01:56:52,599 --> 01:56:54,840
 So what you notice is that number one

1630
01:56:54,840 --> 01:56:57,559
 is the arrow rate start to decrease.

1631
01:56:57,559 --> 01:57:01,280
 Over here, the arrow start to decrease as you expect.

1632
01:57:01,280 --> 01:57:02,680
 Number two is that you notice

1633
01:57:02,680 --> 01:57:05,400
 that the number of layers start to increase.

1634
01:57:05,400 --> 01:57:09,280
 So over the time, the number of networks start to increase

1635
01:57:09,280 --> 01:57:10,960
 and the arrow start to decrease.

1636
01:57:12,240 --> 01:57:15,759
 Okay, so this particular diagram shows the performance

1637
01:57:15,759 --> 01:57:19,960
 comparison for different network for the ImageNet challenge.

1638
01:57:19,960 --> 01:57:22,440
 So ImageNet challenge as you recall,

1639
01:57:22,440 --> 01:57:26,920
 is given an image you want to classify into 1000 category.

1640
01:57:26,920 --> 01:57:28,360
 So if we look at this diagram here,

1641
01:57:28,360 --> 01:57:31,000
 vertical axis is the top one accuracy.

1642
01:57:31,520 --> 01:57:34,280
 Okay, so horizontal axis is,

1643
01:57:34,280 --> 01:57:37,320
 flop means this number of calculation,

1644
01:57:37,320 --> 01:57:40,240
 floating point operation like addition,

1645
01:57:40,240 --> 01:57:41,640
 subtraction and so on.

1646
01:57:41,640 --> 01:57:45,640
 So for a good network, we want the accuracy to be high.

1647
01:57:46,760 --> 01:57:49,080
 We want the number of calculation,

1648
01:57:49,080 --> 01:57:52,720
 number of addition, subtraction required

1649
01:57:52,720 --> 01:57:55,000
 to perform a full pass to be small.

1650
01:57:55,000 --> 01:57:59,680
 So therefore a good network should be in this region here.

1651
01:57:59,680 --> 01:58:02,680
 Large accuracy, low number of parameters.

1652
01:58:02,680 --> 01:58:04,160
 So it's in this region here.

1653
01:58:05,120 --> 01:58:07,600
 And then what's the size of this bubble here?

1654
01:58:07,600 --> 01:58:09,920
 So the size of this particular bubble here

1655
01:58:09,920 --> 01:58:14,480
 means the number of parameter that is required

1656
01:58:14,480 --> 01:58:15,320
 for this network.

1657
01:58:15,320 --> 01:58:18,240
 So therefore a good network, CNN network,

1658
01:58:18,240 --> 01:58:20,360
 should be in this corner

1659
01:58:20,360 --> 01:58:22,920
 and the size of bubble should be small.

1660
01:58:22,920 --> 01:58:26,640
 So you can see these are some of the top choices.

1661
01:58:27,480 --> 01:58:30,840
 Among them, right, you can see these are the VGG network here.

1662
01:58:30,840 --> 01:58:33,400
 One thing you recognize right away is VGG,

1663
01:58:33,400 --> 01:58:34,920
 the bubble is very big.

1664
01:58:34,920 --> 01:58:35,760
 Why?

1665
01:58:44,280 --> 01:58:46,040
 Why the VGG bubble is so big?

1666
01:58:47,560 --> 01:58:49,880
 I think we just explained a few minutes ago

1667
01:58:51,120 --> 01:58:55,880
 because, yeah, FC layer,

1668
01:58:55,880 --> 01:58:57,600
 it's got a lot of parameters.

1669
01:58:57,600 --> 01:58:59,520
 So therefore VGG is so big.

1670
01:58:59,520 --> 01:59:03,520
 So therefore for applications such as edge computation,

1671
01:59:03,520 --> 01:59:06,840
 no rest, no number of storage is limited,

1672
01:59:06,840 --> 01:59:09,800
 and so on, you usually don't want to go for VGG.

1673
01:59:09,800 --> 01:59:11,560
 Yeah, VGG is elegant,

1674
01:59:11,560 --> 01:59:14,480
 but actually nowadays is a little bit obsolete already.

1675
01:59:17,920 --> 01:59:20,720
 Right, so, okay, when we talk about network,

1676
01:59:20,720 --> 01:59:23,200
 what are some key performance matrix here?

1677
01:59:23,360 --> 01:59:26,320
 So when we measure how good a network is,

1678
01:59:26,320 --> 01:59:29,519
 one of the important one is accuracy,

1679
01:59:29,519 --> 01:59:32,679
 whether you can achieve high accuracy,

1680
01:59:32,679 --> 01:59:35,160
 whether you can classify it correctly.

1681
01:59:35,160 --> 01:59:38,519
 The second thing to consider is the memory footprint.

1682
01:59:38,519 --> 01:59:41,920
 So how much parameter you need to store

1683
01:59:41,920 --> 01:59:43,880
 to train the network?

1684
01:59:43,880 --> 01:59:46,040
 What is the activation map

1685
01:59:46,040 --> 01:59:48,320
 or the feature map that you need to store?

1686
01:59:48,320 --> 01:59:51,200
 So this is also an important consideration,

1687
01:59:51,320 --> 01:59:53,519
 how much memory you'll occupy.

1688
01:59:53,519 --> 01:59:56,480
 The next one is a speed or computational complexity

1689
01:59:56,480 --> 01:59:58,240
 or flock, right?

1690
01:59:58,240 --> 01:59:59,320
 When you have a forward,

1691
01:59:59,320 --> 02:00:00,679
 when you have doing the testing,

1692
02:00:00,679 --> 02:00:01,880
 when you have an image,

1693
02:00:01,880 --> 02:00:04,440
 how many operation do you need to compute

1694
02:00:04,440 --> 02:00:06,320
 before you can get the output, right?

1695
02:00:06,320 --> 02:00:07,480
 So this is a flock.

1696
02:00:07,480 --> 02:00:09,920
 So generally speaking, the flock,

1697
02:00:09,920 --> 02:00:11,200
 the smaller, the better,

1698
02:00:11,200 --> 02:00:14,120
 or the speed, the higher, the better, right?

1699
02:00:14,120 --> 02:00:16,400
 So often, why is the most important matrix

1700
02:00:16,400 --> 02:00:19,080
 need not necessarily be accuracy or this?

1701
02:00:19,080 --> 02:00:21,040
 It depends on application, right?

1702
02:00:21,040 --> 02:00:23,640
 Some application, some factors is more important

1703
02:00:23,640 --> 02:00:24,519
 than the others.

1704
02:00:24,519 --> 02:00:25,960
 For other application,

1705
02:00:25,960 --> 02:00:28,560
 other factors would be more important.

1706
02:00:28,560 --> 02:00:32,920
 For example, if you have an edge computing devices,

1707
02:00:32,920 --> 02:00:36,360
 so some device that you put somewhere else,

1708
02:00:36,360 --> 02:00:39,800
 and then it doesn't have the cable to power,

1709
02:00:39,800 --> 02:00:41,400
 it power using the battery.

1710
02:00:41,400 --> 02:00:44,200
 So for those cases, then you know that,

1711
02:00:44,200 --> 02:00:45,960
 it need to be very efficient.

1712
02:00:45,960 --> 02:00:49,440
 So this flock computational complexity

1713
02:00:49,440 --> 02:00:52,080
 then could be very important consideration

1714
02:00:52,080 --> 02:00:54,759
 because otherwise your model will run out,

1715
02:00:54,759 --> 02:00:56,799
 the battery will run flat very quickly.

1716
02:00:56,799 --> 02:00:58,240
 So in those cases,

1717
02:00:58,240 --> 02:01:00,320
 probably the computational complexity

1718
02:01:00,320 --> 02:01:02,599
 is one of the key consideration.

1719
02:01:02,599 --> 02:01:05,440
 The accuracy probably is not the most important one.

1720
02:01:05,440 --> 02:01:07,400
 Often it's a compromise, yeah?

1721
02:01:07,400 --> 02:01:09,320
 But it would depends on what application

1722
02:01:09,320 --> 02:01:10,559
 you are referring to,

1723
02:01:10,559 --> 02:01:12,160
 see which is a good choice.

1724
02:01:12,160 --> 02:01:15,240
 Not necessarily that accuracy is the most important thing.

1725
02:01:17,160 --> 02:01:19,080
 Right, so next, let's very quickly look at

1726
02:01:19,120 --> 02:01:20,800
 some of the potential application.

1727
02:01:20,800 --> 02:01:24,000
 So for CNN, there's many, many, many different applications.

1728
02:01:24,000 --> 02:01:25,800
 So here I'm just going to share with you

1729
02:01:25,800 --> 02:01:28,760
 some of the potential application.

1730
02:01:28,760 --> 02:01:30,120
 So one of the potential applications

1731
02:01:30,120 --> 02:01:32,000
 is person re-identification.

1732
02:01:32,000 --> 02:01:34,320
 So the idea of person re-identification

1733
02:01:34,320 --> 02:01:37,280
 is that suppose in, for example, in university,

1734
02:01:37,280 --> 02:01:40,840
 where many different camera networks, yeah?

1735
02:01:40,840 --> 02:01:45,720
 So even but the image that appear in one camera,

1736
02:01:45,760 --> 02:01:48,760
 we want to find where this person appear

1737
02:01:48,760 --> 02:01:50,960
 across other camera, okay?

1738
02:01:50,960 --> 02:01:55,160
 So this is known as the person re-identification.

1739
02:01:55,160 --> 02:01:56,800
 Given a query image,

1740
02:01:56,800 --> 02:01:58,480
 you want to find, okay,

1741
02:01:58,480 --> 02:02:02,480
 where this person appear in different cameras, yeah?

1742
02:02:02,480 --> 02:02:05,600
 So I remember, I think last year or yeah,

1743
02:02:05,600 --> 02:02:08,000
 in China there was this news that

1744
02:02:08,000 --> 02:02:11,880
 an old lady actually went missing in China

1745
02:02:11,880 --> 02:02:13,640
 and then they managed to find out

1746
02:02:13,640 --> 02:02:15,560
 where this person has walked, right?

1747
02:02:15,600 --> 02:02:18,400
 So they're using this kind of technology

1748
02:02:18,400 --> 02:02:21,720
 because in, I think just like in Singapore, in China,

1749
02:02:21,720 --> 02:02:23,280
 there's lots of surveillance camera.

1750
02:02:23,280 --> 02:02:25,560
 So maybe much when you walk, then they will know.

1751
02:02:25,560 --> 02:02:27,560
 They can find out where you have walked, yeah?

1752
02:02:27,560 --> 02:02:31,640
 And then they can identify the missing person very quickly.

1753
02:02:32,720 --> 02:02:35,240
 Right, okay, so for this particular method here,

1754
02:02:35,240 --> 02:02:36,440
 the idea is using,

1755
02:02:36,440 --> 02:02:38,800
 I'm not going to go into the detail by stat.

1756
02:02:38,800 --> 02:02:41,520
 Given an image, you use a rest net, right?

1757
02:02:41,520 --> 02:02:43,600
 To extract some feature.

1758
02:02:43,600 --> 02:02:46,680
 So this feature then will be divided into three parts.

1759
02:02:46,680 --> 02:02:48,960
 One is the global feature.

1760
02:02:48,960 --> 02:02:50,280
 The other is the park feature.

1761
02:02:50,280 --> 02:02:52,360
 And then finally is attribute feature.

1762
02:02:52,360 --> 02:02:54,720
 So the global feature means that you look at a feature

1763
02:02:54,720 --> 02:02:57,520
 for your whole image, a whole person.

1764
02:02:57,520 --> 02:03:00,200
 Park feature means that you probably divide your body

1765
02:03:00,200 --> 02:03:02,160
 into different parts.

1766
02:03:02,160 --> 02:03:04,480
 And then the attribute feature is you look at things

1767
02:03:04,480 --> 02:03:06,720
 like whether the person is wearing some backpack,

1768
02:03:06,720 --> 02:03:08,720
 wearing some hats and so on.

1769
02:03:08,720 --> 02:03:10,400
 And then you use some attention map

1770
02:03:10,400 --> 02:03:12,640
 to highlight those attributes.

1771
02:03:12,640 --> 02:03:14,640
 And afterwards you combine these three laws

1772
02:03:14,640 --> 02:03:16,720
 to train your network, okay?

1773
02:03:16,720 --> 02:03:20,720
 So this is an example so you can extend your basic CNN

1774
02:03:21,600 --> 02:03:25,880
 to something which can fit into this kind of application.

1775
02:03:25,880 --> 02:03:29,320
 So anyway, this is one of the work of my earlier student,

1776
02:03:29,320 --> 02:03:31,040
 a couple of years back already.

1777
02:03:32,280 --> 02:03:35,080
 Right, okay, so another potential application

1778
02:03:35,080 --> 02:03:36,440
 is a visual food recognition.

1779
02:03:36,440 --> 02:03:39,280
 So the idea is quite similar, simple.

1780
02:03:39,280 --> 02:03:41,600
 So we want to develop a mobile app

1781
02:03:41,640 --> 02:03:45,680
 such that you can take pictures of the food images

1782
02:03:45,680 --> 02:03:47,240
 and recognize what it is.

1783
02:03:47,240 --> 02:03:49,440
 So once you can recognize it, right,

1784
02:03:49,440 --> 02:03:51,840
 for individual person, you can understand

1785
02:03:51,840 --> 02:03:54,160
 his dietary habits, right?

1786
02:03:54,160 --> 02:03:57,480
 But if you look at the community, then you will know, right?

1787
02:03:57,480 --> 02:04:01,760
 What is the dietary habit of a community or a certain country?

1788
02:04:01,760 --> 02:04:03,360
 So this particular project, actually,

1789
02:04:03,360 --> 02:04:06,880
 we used to work with a beta, right?

1790
02:04:06,880 --> 02:04:09,080
 So this is one of the projects that we worked

1791
02:04:09,080 --> 02:04:10,320
 with beta before.

1792
02:04:11,320 --> 02:04:15,040
 Okay, so the approach that we propose here is that, right,

1793
02:04:15,040 --> 02:04:19,280
 we use a technique known as a knowledge distillation, yep?

1794
02:04:19,280 --> 02:04:24,280
 So given an image, we train a large teacher network,

1795
02:04:24,360 --> 02:04:26,559
 but because this large teacher network

1796
02:04:26,559 --> 02:04:30,480
 requires a lot of the parameters and so on.

1797
02:04:30,480 --> 02:04:33,559
 So because this network need to be later on deployed

1798
02:04:33,559 --> 02:04:37,240
 on your mobile devices, so it need to be lightweight.

1799
02:04:37,240 --> 02:04:39,320
 But at the same time, we want it to be able

1800
02:04:39,320 --> 02:04:43,840
 to extract the information that is learned

1801
02:04:43,840 --> 02:04:46,280
 by a large teacher network.

1802
02:04:46,280 --> 02:04:48,120
 So therefore, the technique that was used

1803
02:04:48,120 --> 02:04:51,200
 is actually knowledge distillation.

1804
02:04:51,200 --> 02:04:55,360
 So this image here, right, we use it to train

1805
02:04:55,360 --> 02:04:58,519
 a large teacher network, and this large teacher network

1806
02:04:58,519 --> 02:05:00,440
 would then distill the knowledge

1807
02:05:00,440 --> 02:05:02,980
 to a smaller student network.

1808
02:05:02,980 --> 02:05:06,000
 Then later on, once this student network is fully trained,

1809
02:05:06,000 --> 02:05:08,360
 we use this student network, we pluck it out,

1810
02:05:08,400 --> 02:05:12,400
 and then we deploy it on some devices such as mobile phone

1811
02:05:12,400 --> 02:05:16,000
 so that it require less storage and computation.

1812
02:05:16,000 --> 02:05:20,599
 So there's a high level principle of this particular work.

1813
02:05:21,559 --> 02:05:24,880
 Okay, so in short, in section one, right,

1814
02:05:24,880 --> 02:05:26,480
 we cover the following.

1815
02:05:26,480 --> 02:05:28,480
 So introduction, linear classifier.

1816
02:05:28,480 --> 02:05:31,839
 So today we cover CNN training, right,

1817
02:05:31,839 --> 02:05:33,799
 and then just now we just very briefly explain

1818
02:05:33,799 --> 02:05:36,320
 some well-known CNN architecture,

1819
02:05:36,320 --> 02:05:39,040
 and also we just take one or two applications

1820
02:05:39,040 --> 02:05:40,639
 to very quickly highlight it.

1821
02:05:53,160 --> 02:05:56,559
 Right, okay, so because for the Cree's on week 10,

1822
02:05:56,559 --> 02:05:59,200
 it includes this part four, that's why I need to go,

1823
02:05:59,200 --> 02:06:01,639
 just touch faster to make sure that we can finish

1824
02:06:01,639 --> 02:06:05,040
 this part four of the lecture.

1825
02:06:05,040 --> 02:06:07,000
 So the second section we're going to look at it

1826
02:06:07,000 --> 02:06:10,040
 is the recurrent neural network and LSTM, right?

1827
02:06:11,360 --> 02:06:12,440
 Right, so under this section,

1828
02:06:12,440 --> 02:06:13,600
 we'll be looking at the following.

1829
02:06:13,600 --> 02:06:16,280
 First we'll be looking at the introduction,

1830
02:06:16,280 --> 02:06:19,200
 and afterwards we'll look at recurrent neural network,

1831
02:06:19,200 --> 02:06:22,320
 RNN, what this, how do we train it, okay,

1832
02:06:22,320 --> 02:06:25,240
 and afterwards we'll be looking at variants of RNN,

1833
02:06:25,240 --> 02:06:28,120
 which we call LSTM, long shutter memory,

1834
02:06:28,120 --> 02:06:29,440
 and then finally we'll be looking

1835
02:06:29,440 --> 02:06:31,140
 at some selected application.

1836
02:06:32,140 --> 02:06:34,940
 Okay, so first of all, before we look at,

1837
02:06:34,940 --> 02:06:36,540
 why do we want to study RNN,

1838
02:06:36,540 --> 02:06:40,780
 let's look at the AI model that we've studied before.

1839
02:06:40,780 --> 02:06:45,780
 So far we've studied linear classifier, MLP, CNN, right?

1840
02:06:45,860 --> 02:06:49,900
 So this type of network is actually a fit forward network,

1841
02:06:49,900 --> 02:06:51,140
 a simple fit forward network,

1842
02:06:51,140 --> 02:06:52,660
 whereas you have some input,

1843
02:06:52,660 --> 02:06:53,860
 it goes through some network

1844
02:06:53,860 --> 02:06:56,420
 and you generate some output here, right?

1845
02:06:56,420 --> 02:06:58,740
 But those networks that we have studied before,

1846
02:06:58,780 --> 02:07:01,940
 like CNN, linear classifier and so on,

1847
02:07:01,940 --> 02:07:05,340
 one of the problem for this type of network is that

1848
02:07:05,340 --> 02:07:07,700
 when it comes to sequential data,

1849
02:07:07,700 --> 02:07:11,740
 a data that can be modeled by a sequence of values,

1850
02:07:11,740 --> 02:07:16,580
 such as time sequence data, share price,

1851
02:07:16,580 --> 02:07:20,139
 or state series data, such as languages,

1852
02:07:20,139 --> 02:07:21,460
 they have different works.

1853
02:07:21,460 --> 02:07:23,860
 So models that we have studied before,

1854
02:07:23,860 --> 02:07:28,219
 like CNN, cannot handle this sequential model very well, okay?

1855
02:07:28,220 --> 02:07:31,140
 So these are the limitations of earlier model

1856
02:07:31,140 --> 02:07:32,820
 that we have studied when they tried

1857
02:07:32,820 --> 02:07:34,820
 to handle sequential data.

1858
02:07:34,820 --> 02:07:36,780
 So therefore for sequential data,

1859
02:07:36,780 --> 02:07:39,580
 the model that we've studied before for CNN,

1860
02:07:39,580 --> 02:07:41,580
 it cannot handle sequential data

1861
02:07:41,580 --> 02:07:43,820
 because it only consider the current input,

1862
02:07:43,820 --> 02:07:46,860
 for example, for CNN is good at certain tasks

1863
02:07:46,860 --> 02:07:48,100
 like classification,

1864
02:07:48,100 --> 02:07:51,380
 but it can only handle the current data, okay?

1865
02:07:52,940 --> 02:07:55,940
 One of the reasons is because it does not have the memory,

1866
02:07:56,059 --> 02:07:58,900
 it does not have the memory element to remember

1867
02:07:58,900 --> 02:08:02,139
 what is the data that has encountered before.

1868
02:08:02,139 --> 02:08:05,460
 So these are the shortcomings of the previous model

1869
02:08:05,460 --> 02:08:08,379
 if you want to use it on sequential data.

1870
02:08:10,139 --> 02:08:12,980
 So now, so that is the motivation,

1871
02:08:12,980 --> 02:08:17,620
 why do we want to study this recurrent neural network here?

1872
02:08:17,620 --> 02:08:20,019
 So a short form for this recurrent neural network,

1873
02:08:20,019 --> 02:08:22,500
 the simple recurrent network can be shown here.

1874
02:08:22,540 --> 02:08:25,580
 So the network actually has input,

1875
02:08:25,580 --> 02:08:27,260
 X is actually your input,

1876
02:08:27,260 --> 02:08:29,380
 H is known as a hidden state,

1877
02:08:29,380 --> 02:08:32,020
 H, I, D, D, E, N, hidden.

1878
02:08:32,020 --> 02:08:33,540
 So H is a hidden state.

1879
02:08:33,540 --> 02:08:37,300
 Hidden state is also the memory, okay?

1880
02:08:37,300 --> 02:08:40,340
 So you have this memory here, so H is the memory,

1881
02:08:40,340 --> 02:08:42,500
 and Y is the output, right?

1882
02:08:42,500 --> 02:08:44,300
 So if you look at this particular diagram here,

1883
02:08:44,300 --> 02:08:47,740
 what it means is that your memory,

1884
02:08:47,740 --> 02:08:51,300
 your current memory depends on your current input,

1885
02:08:51,300 --> 02:08:53,260
 and also this loop here means

1886
02:08:53,260 --> 02:08:56,180
 it depends on your past input.

1887
02:08:56,180 --> 02:08:59,100
 So this current memory depends on your current input

1888
02:08:59,100 --> 02:09:01,140
 as well as the past memory.

1889
02:09:01,140 --> 02:09:04,020
 So therefore it has this hidden state of memory

1890
02:09:04,020 --> 02:09:07,580
 that's able to handle sequential data.

1891
02:09:07,580 --> 02:09:10,020
 We'll see a bit more about it later.

1892
02:09:10,020 --> 02:09:12,220
 And then once you have updated this current memory,

1893
02:09:12,220 --> 02:09:15,500
 you can use it to generate some output, okay?

1894
02:09:15,500 --> 02:09:17,060
 So these types of neural network

1895
02:09:17,060 --> 02:09:19,220
 is known as a recurrent neural network.

1896
02:09:19,220 --> 02:09:23,060
 So its advantage is that you can handle sequential data

1897
02:09:23,060 --> 02:09:25,620
 like time series data or state series data

1898
02:09:25,620 --> 02:09:29,060
 such as sentences, languages, okay?

1899
02:09:29,060 --> 02:09:30,900
 You can consider the current input

1900
02:09:30,900 --> 02:09:33,500
 and also the previous received input.

1901
02:09:33,500 --> 02:09:36,020
 You can see that this hidden state,

1902
02:09:36,020 --> 02:09:37,620
 this current hidden state or memory

1903
02:09:37,620 --> 02:09:39,780
 depends on your current input,

1904
02:09:39,780 --> 02:09:43,100
 and it also depends on the previous hidden state.

1905
02:09:43,100 --> 02:09:45,500
 And the previous hidden state in turn

1906
02:09:45,500 --> 02:09:47,380
 depends on the previous input

1907
02:09:47,380 --> 02:09:50,580
 and the previous memory.

1908
02:09:50,580 --> 02:09:53,980
 So in other words, that means your current memory

1909
02:09:53,980 --> 02:09:55,940
 depends on your current input

1910
02:09:55,940 --> 02:09:59,020
 as well as all the past input.

1911
02:09:59,020 --> 02:10:02,500
 Therefore your current memory actually is a function

1912
02:10:02,500 --> 02:10:06,220
 of your current input as well as all the past input.

1913
02:10:06,220 --> 02:10:10,420
 So theoretically that means it has certain memory capability

1914
02:10:10,420 --> 02:10:15,420
 to remember all your current input

1915
02:10:15,660 --> 02:10:17,460
 as well as the past input.

1916
02:10:18,460 --> 02:10:21,900
 So therefore you can see it can memorize the previous input

1917
02:10:21,900 --> 02:10:23,740
 due to this internal memory.

1918
02:10:25,100 --> 02:10:28,060
 So what are some common applications for RNN?

1919
02:10:28,060 --> 02:10:30,900
 So it can be used in time series prediction or forecasting.

1920
02:10:30,900 --> 02:10:34,140
 For example, you want to predict the share price,

1921
02:10:34,140 --> 02:10:35,980
 the sales of a certain product.

1922
02:10:35,980 --> 02:10:38,260
 It can be used to perform speech recognition,

1923
02:10:38,260 --> 02:10:41,060
 recognizing different works that has been spoken.

1924
02:10:41,060 --> 02:10:43,380
 It can be used to perform some tasks

1925
02:10:43,380 --> 02:10:45,780
 in natural language processing or NLP,

1926
02:10:45,780 --> 02:10:47,540
 such as a language translation.

1927
02:10:47,540 --> 02:10:51,100
 Given a French, you can translate into English.

1928
02:10:51,100 --> 02:10:55,100
 It can perform sentiment, text sentiment analysis.

1929
02:10:55,100 --> 02:10:56,540
 For example, given a sentence,

1930
02:10:56,540 --> 02:10:59,220
 you tell them whether this is a positive sentiment

1931
02:10:59,220 --> 02:11:00,460
 or negative sentiment.

1932
02:11:01,820 --> 02:11:03,780
 It can also do things like image captioning.

1933
02:11:03,780 --> 02:11:06,420
 Given an image, you want to generate a sentence

1934
02:11:06,420 --> 02:11:08,900
 or a caption to describe this image.

1935
02:11:08,900 --> 02:11:11,180
 So these are all some possible applications

1936
02:11:11,220 --> 02:11:14,100
 that you can perform under RNN.

1937
02:11:15,660 --> 02:11:17,780
 Right, okay, so before we go into that,

1938
02:11:17,780 --> 02:11:20,660
 let's look at some sequence modeling

1939
02:11:21,580 --> 02:11:25,700
 for this type of a sequential data.

1940
02:11:25,700 --> 02:11:27,340
 So for the sequential data,

1941
02:11:27,340 --> 02:11:31,380
 there are quite a number of different way we can model it.

1942
02:11:31,380 --> 02:11:34,900
 So the first type is known as a one-to-many mapping.

1943
02:11:34,900 --> 02:11:36,940
 So as the name suggests, one-to-many mapping

1944
02:11:36,940 --> 02:11:41,020
 means that you have one input, but many output here.

1945
02:11:41,020 --> 02:11:44,500
 So many output can also mean sequence or output.

1946
02:11:44,500 --> 02:11:47,780
 Right, so this one-to-many mapping

1947
02:11:47,780 --> 02:11:50,020
 you can see from this diagram here.

1948
02:11:50,020 --> 02:11:53,260
 Your input, so this red color is your input.

1949
02:11:53,260 --> 02:11:55,980
 So your input is a card, one single input.

1950
02:11:57,220 --> 02:12:01,820
 Your green component here is your RNN or the memory,

1951
02:12:01,820 --> 02:12:03,460
 or the hidden state.

1952
02:12:03,460 --> 02:12:05,420
 Your blue box is actually your output.

1953
02:12:05,420 --> 02:12:08,940
 So you have one input and you have multiple outputs.

1954
02:12:08,980 --> 02:12:11,940
 So hence is one-to-many mapping.

1955
02:12:11,940 --> 02:12:14,139
 So among this one-to-many mapping,

1956
02:12:14,139 --> 02:12:18,179
 one of the application is image captioning.

1957
02:12:18,179 --> 02:12:21,299
 So for image captioning, it can be achieved using this.

1958
02:12:21,299 --> 02:12:25,339
 So this is the older model to perform image captioning.

1959
02:12:25,339 --> 02:12:28,099
 So you have a given image, right?

1960
02:12:28,099 --> 02:12:31,500
 You let it go through the CNN that we have studied early on.

1961
02:12:31,500 --> 02:12:34,980
 And then, okay, you actually extract a feature vector,

1962
02:12:34,980 --> 02:12:37,259
 one of the FC vector that you have.

1963
02:12:38,140 --> 02:12:40,540
 That is actually our feature vectors here.

1964
02:12:41,460 --> 02:12:43,300
 So that means we can get a feature vector

1965
02:12:43,300 --> 02:12:46,420
 that represents this particular image here.

1966
02:12:46,420 --> 02:12:48,300
 So we have this FC vector.

1967
02:12:48,300 --> 02:12:52,100
 So this FC vector then go through LSTM.

1968
02:12:52,100 --> 02:12:55,020
 So LSTM currently, you can just think about each of these

1969
02:12:55,020 --> 02:12:58,620
 is like a memory, this is like memory here.

1970
02:12:58,620 --> 02:13:00,980
 Okay, so at this stage, you do need to know the detail.

1971
02:13:00,980 --> 02:13:03,180
 Just have some high-level understanding.

1972
02:13:03,220 --> 02:13:07,660
 This LSTM here is just like this memory that you have here.

1973
02:13:07,660 --> 02:13:12,660
 So you get your single input of your feature vector.

1974
02:13:12,860 --> 02:13:15,180
 First, you will try to generate the first token,

1975
02:13:15,180 --> 02:13:19,260
 which is like spot, indicate the spot of the caption.

1976
02:13:19,260 --> 02:13:21,820
 So these spots here will then be used

1977
02:13:21,820 --> 02:13:24,420
 to generate your next work.

1978
02:13:24,420 --> 02:13:27,220
 For example, giraffe, this giraffe, you move it down,

1979
02:13:27,220 --> 02:13:29,820
 use to generate the next work, and so on and so forth.

1980
02:13:29,820 --> 02:13:32,020
 So the detail, you don't have to understand, right?

1981
02:13:32,060 --> 02:13:34,380
 Maybe in the later part we'll explain a bit more.

1982
02:13:34,380 --> 02:13:36,660
 But the high-level intuition I'm trying to tell you

1983
02:13:36,660 --> 02:13:40,700
 in this case is you can see your input is a single image,

1984
02:13:40,700 --> 02:13:44,500
 but your output is a sentence or a caption,

1985
02:13:44,500 --> 02:13:46,460
 which consists of many different works.

1986
02:13:46,460 --> 02:13:51,060
 So therefore it can be interpreted as a single

1987
02:13:51,060 --> 02:13:52,580
 to many mapping.

1988
02:13:53,740 --> 02:13:55,940
 So you can see input is a single image,

1989
02:13:55,940 --> 02:13:57,900
 output is a sequence of works.

1990
02:13:59,740 --> 02:14:00,940
 Right, so okay.

1991
02:14:00,980 --> 02:14:02,540
 So the next type of mapping that we have

1992
02:14:02,540 --> 02:14:04,379
 is a many to one mapping here.

1993
02:14:04,379 --> 02:14:07,620
 So many to one mapping as name suggests,

1994
02:14:07,620 --> 02:14:09,339
 input is many, right?

1995
02:14:09,339 --> 02:14:12,780
 So many input, a sequence of input, output is one.

1996
02:14:12,780 --> 02:14:15,860
 Okay, so from this visualization you can see,

1997
02:14:15,860 --> 02:14:17,620
 you have many input, right?

1998
02:14:17,620 --> 02:14:20,860
 So this is your memory or on end, okay?

1999
02:14:20,860 --> 02:14:23,259
 And then this is your output, single output.

2000
02:14:23,259 --> 02:14:27,500
 So what are some use cases that fall under this category?

2001
02:14:27,500 --> 02:14:30,299
 For example, one of them is a video classification.

2002
02:14:30,340 --> 02:14:32,420
 So for video classifications here,

2003
02:14:32,420 --> 02:14:36,740
 this can be interpreted as different frame in the video,

2004
02:14:36,740 --> 02:14:40,660
 right, and then your output is a single decision, right?

2005
02:14:40,660 --> 02:14:44,660
 Whether it's actually a comedy, a horror movie, and so on.

2006
02:14:44,660 --> 02:14:47,060
 Okay, so this is a many to one mapping.

2007
02:14:47,060 --> 02:14:49,900
 Right, it can also be sentiment classification.

2008
02:14:49,900 --> 02:14:52,580
 For example, your input can be a sentence,

2009
02:14:52,580 --> 02:14:55,700
 a sequence of work or sentence, right, different works.

2010
02:14:55,700 --> 02:14:58,060
 And your output is a single sentiment,

2011
02:14:58,100 --> 02:15:01,660
 whether it's positive, negative, or neutral, for example.

2012
02:15:01,660 --> 02:15:05,020
 So therefore it can be a many to one mapping.

2013
02:15:06,300 --> 02:15:09,420
 Right, okay, so there are also others like many to many mapping.

2014
02:15:09,420 --> 02:15:11,780
 So by now you should know many to many means

2015
02:15:11,780 --> 02:15:13,580
 that you have many input, right,

2016
02:15:13,580 --> 02:15:15,980
 and your output is also many output here.

2017
02:15:15,980 --> 02:15:20,580
 Right, okay, so, right, there's two different variants here.

2018
02:15:20,580 --> 02:15:22,700
 So one of them is this many to many mapping.

2019
02:15:22,700 --> 02:15:27,140
 So your input here, okay, this is a memory and this is output.

2020
02:15:27,140 --> 02:15:29,620
 What you notice is for this particular case here,

2021
02:15:29,620 --> 02:15:31,460
 there's a delay, right?

2022
02:15:31,460 --> 02:15:33,180
 When your input is generated here,

2023
02:15:33,180 --> 02:15:37,300
 there's a delay before your output is being generated.

2024
02:15:37,300 --> 02:15:39,540
 So this kind of use case is commonly used,

2025
02:15:39,540 --> 02:15:42,660
 and for example, in machine translation, okay?

2026
02:15:42,660 --> 02:15:46,060
 So your input is a sequence of, for example, a French work.

2027
02:15:46,060 --> 02:15:49,180
 Your output is a sequence of English works, yeah?

2028
02:15:49,180 --> 02:15:51,020
 So usually you have a delay.

2029
02:15:51,020 --> 02:15:54,540
 So why is the reason why you usually have a head for a delay?

2030
02:15:55,540 --> 02:15:57,700
 Why usually you have a delay

2031
02:15:57,700 --> 02:16:00,900
 in the context of machine translation?

2032
02:16:09,460 --> 02:16:10,740
 Now to answer this question,

2033
02:16:10,740 --> 02:16:13,019
 you just have to think of your friend.

2034
02:16:13,019 --> 02:16:15,260
 Some of your friend actually shit from the mouth

2035
02:16:15,260 --> 02:16:16,740
 before thinking right away,

2036
02:16:16,740 --> 02:16:18,300
 people say something, you just, right away,

2037
02:16:18,300 --> 02:16:20,380
 you just say something, and then later you regret,

2038
02:16:20,380 --> 02:16:21,700
 oh, I should not have said that.

2039
02:16:21,700 --> 02:16:23,580
 Right, sometimes it happened to me as well.

2040
02:16:23,580 --> 02:16:26,580
 So usually what it means is that for translation,

2041
02:16:26,580 --> 02:16:29,620
 you need to understand, you need to, first of all,

2042
02:16:29,620 --> 02:16:32,059
 take in some work first to have a good understanding

2043
02:16:32,059 --> 02:16:36,660
 of the context before you start to do the translation.

2044
02:16:36,660 --> 02:16:39,180
 So therefore you need to take in some works here

2045
02:16:39,180 --> 02:16:43,100
 to digest and understand what's the context first

2046
02:16:43,100 --> 02:16:45,379
 before you start to generate the translation.

2047
02:16:45,379 --> 02:16:46,379
 So it makes sense, right?

2048
02:16:46,379 --> 02:16:49,700
 You cannot just simply, the first work you handle is here,

2049
02:16:49,700 --> 02:16:50,860
 then right away you say something.

2050
02:16:50,860 --> 02:16:54,580
 That's what sometimes my wife say about me, right?

2051
02:16:54,580 --> 02:16:56,700
 Whatever she say, I just reflect,

2052
02:16:56,700 --> 02:17:01,700
 kind of fight back right away, so she should avoid that.

2053
02:17:01,860 --> 02:17:04,580
 Okay, anyway, so this is what you have here.

2054
02:17:04,580 --> 02:17:08,260
 For translation, you need to understand the context first

2055
02:17:08,260 --> 02:17:10,580
 before you start to do the translation.

2056
02:17:10,580 --> 02:17:14,140
 Okay, so that's why for translation,

2057
02:17:14,140 --> 02:17:15,900
 it can be modeled into this.

2058
02:17:15,900 --> 02:17:20,500
 So this one can be interpreted as a frame video classification.

2059
02:17:20,540 --> 02:17:24,020
 So each of this input here is different frame in a video.

2060
02:17:24,020 --> 02:17:26,700
 So this is actually your memory, okay?

2061
02:17:26,700 --> 02:17:28,500
 And then this could be, for example,

2062
02:17:28,500 --> 02:17:30,260
 you want to detect at each frame

2063
02:17:30,260 --> 02:17:33,540
 whether, for example, a certain object appear or not.

2064
02:17:33,540 --> 02:17:35,139
 So therefore actually you don't have to wait,

2065
02:17:35,139 --> 02:17:37,219
 you can generate the output right away.

2066
02:17:37,219 --> 02:17:39,660
 So therefore depends on cases,

2067
02:17:39,660 --> 02:17:41,860
 your modeling can be a little bit different.

2068
02:17:43,459 --> 02:17:47,840
 Right, okay, so this diagrams here shows a well-known

2069
02:17:47,840 --> 02:17:50,459
 encoder and decoder modeling here.

2070
02:17:50,459 --> 02:17:52,900
 So in this butter diagrams here,

2071
02:17:52,900 --> 02:17:57,900
 is illustrating these ideas of the language translation.

2072
02:17:58,219 --> 02:18:00,299
 Okay, so first of all, the goal is that you have,

2073
02:18:00,299 --> 02:18:03,459
 for example, in this case, an English sentence,

2074
02:18:03,459 --> 02:18:07,219
 you want to translate into, I think this is German,

2075
02:18:07,219 --> 02:18:08,580
 the Germans here.

2076
02:18:08,580 --> 02:18:12,179
 So what we do is that typically we take this particular work,

2077
02:18:12,179 --> 02:18:14,900
 we'll convert it into some vectors,

2078
02:18:14,900 --> 02:18:16,619
 because you cannot represent works,

2079
02:18:16,619 --> 02:18:19,219
 you have to convert it into some vectors here.

2080
02:18:19,260 --> 02:18:21,660
 This is known as the work embedding.

2081
02:18:21,660 --> 02:18:24,099
 Right, and then afterwards we have this particular element

2082
02:18:24,099 --> 02:18:26,059
 here, each of this is actually a memory,

2083
02:18:26,059 --> 02:18:30,059
 such as an RNN or LSTM here.

2084
02:18:30,059 --> 02:18:32,059
 Right, so what it does is that,

2085
02:18:32,059 --> 02:18:34,139
 okay, at each of the time steps here,

2086
02:18:34,139 --> 02:18:39,139
 it takes the previous memory from the previous time step,

2087
02:18:39,580 --> 02:18:41,459
 it takes its current input,

2088
02:18:41,459 --> 02:18:44,459
 and then you will try to extract some information.

2089
02:18:44,459 --> 02:18:46,619
 Right, so this information would then be passed

2090
02:18:46,619 --> 02:18:48,779
 to the next element.

2091
02:18:48,780 --> 02:18:51,740
 So it would just kind of extract information

2092
02:18:51,740 --> 02:18:54,660
 from the first work, okay, and afterwards go in,

2093
02:18:54,660 --> 02:18:55,860
 combine with the second work,

2094
02:18:55,860 --> 02:18:58,340
 combine with the third work, and so on and so forth.

2095
02:18:58,340 --> 02:19:00,180
 So at the end of this particular,

2096
02:19:00,180 --> 02:19:03,180
 so this module here is known as encoder,

2097
02:19:03,180 --> 02:19:04,820
 this is known as encoder.

2098
02:19:04,820 --> 02:19:08,540
 So the encoder, the objective of the encoder is input,

2099
02:19:09,460 --> 02:19:13,700
 okay, it can use, for example, a network such as RNN,

2100
02:19:13,700 --> 02:19:15,860
 at the end to extract a vector.

2101
02:19:15,900 --> 02:19:17,740
 So this particular vector, S here,

2102
02:19:17,740 --> 02:19:20,860
 is typically known as a context vector.

2103
02:19:20,860 --> 02:19:23,940
 So this context vector will actually contain

2104
02:19:23,940 --> 02:19:26,980
 some good information of all the works

2105
02:19:26,980 --> 02:19:29,500
 that it has encountered, okay?

2106
02:19:29,500 --> 02:19:31,300
 So this is a context vector.

2107
02:19:31,300 --> 02:19:33,620
 So once you have, so this is the encoder,

2108
02:19:33,620 --> 02:19:36,620
 you take a sentence, you encode the information

2109
02:19:36,620 --> 02:19:38,700
 into a context vector.

2110
02:19:38,700 --> 02:19:41,460
 So next, this part is known as a decoder.

2111
02:19:41,460 --> 02:19:44,700
 So the goal of the decoder is given this context.

2112
02:19:44,700 --> 02:19:49,260
 Now you want to translate into your target language

2113
02:19:49,260 --> 02:19:51,420
 such as German, right?

2114
02:19:51,420 --> 02:19:55,620
 So you start off with a start token, yeah?

2115
02:19:55,620 --> 02:19:59,460
 Okay, with the context that you have extracted previously,

2116
02:19:59,460 --> 02:20:02,980
 what you start to generate, what is the most likely works?

2117
02:20:02,980 --> 02:20:05,180
 Okay, and then based on this translated work,

2118
02:20:05,180 --> 02:20:06,660
 you bring it down, right?

2119
02:20:06,660 --> 02:20:09,500
 You then try to generate what is the most likely next work

2120
02:20:09,500 --> 02:20:12,460
 and so on and so forth, okay?

2121
02:20:12,460 --> 02:20:16,220
 Right, so that is the high level structures

2122
02:20:16,220 --> 02:20:17,419
 of encoder and decoder.

2123
02:20:17,419 --> 02:20:20,820
 Again, details you will study a bit more later on

2124
02:20:20,820 --> 02:20:24,019
 as we move on from RNN, LSTM to transformer.

2125
02:20:24,019 --> 02:20:26,220
 But these ideas of transformer,

2126
02:20:26,220 --> 02:20:29,300
 this encoder and decoder is that the objective

2127
02:20:29,300 --> 02:20:32,019
 of the encoder is you take your input

2128
02:20:32,019 --> 02:20:34,980
 and you try to extract some feature

2129
02:20:34,980 --> 02:20:37,859
 that can extract some meaningful information

2130
02:20:37,859 --> 02:20:39,140
 from your input.

2131
02:20:39,140 --> 02:20:41,259
 And then the objective of this decoder

2132
02:20:41,260 --> 02:20:45,020
 is that given this particular context information,

2133
02:20:45,020 --> 02:20:50,020
 you try to generate your target output, okay?

2134
02:20:50,420 --> 02:20:53,420
 Yeah, so one is encode, one is to decode, right?

2135
02:20:56,140 --> 02:20:58,460
 Right, okay, yeah, so we still have a bit of time.

2136
02:20:58,460 --> 02:21:00,620
 Let me just go through some of the basic

2137
02:21:00,620 --> 02:21:04,340
 about this recurrent neural network or RNN, right?

2138
02:21:04,340 --> 02:21:05,820
 So what are RNN?

2139
02:21:05,820 --> 02:21:08,140
 So RNN are actually a class of network

2140
02:21:08,140 --> 02:21:11,660
 which can handle sequential data, right?

2141
02:21:11,660 --> 02:21:14,140
 So RNN are typical AI model

2142
02:21:14,140 --> 02:21:16,660
 which is used to handle sequential data.

2143
02:21:16,660 --> 02:21:21,660
 So sequential data are data which have kind of a sequence,

2144
02:21:22,859 --> 02:21:23,699
 appear in sequence.

2145
02:21:23,699 --> 02:21:28,460
 So some very common one are including time series data,

2146
02:21:28,460 --> 02:21:29,740
 right, or state series data.

2147
02:21:29,740 --> 02:21:34,539
 So it could be, for example, share price or languages.

2148
02:21:34,539 --> 02:21:37,460
 So it's commonly used in analyzing sequential data

2149
02:21:37,460 --> 02:21:39,460
 or temporal data, right?

2150
02:21:39,460 --> 02:21:42,140
 So it can be used in applications such as NLP,

2151
02:21:42,140 --> 02:21:43,179
 natural language processing,

2152
02:21:43,179 --> 02:21:45,380
 because NLP usually is sentence

2153
02:21:45,380 --> 02:21:47,179
 and sentence have different works.

2154
02:21:47,179 --> 02:21:48,779
 So this is ideal, right?

2155
02:21:48,779 --> 02:21:51,179
 You handle using RNN, right?

2156
02:21:51,179 --> 02:21:53,619
 Other things like translation, image captioning.

2157
02:21:53,619 --> 02:21:55,339
 So these are common application

2158
02:21:55,339 --> 02:21:58,900
 that we can use RNN to solve the problem.

2159
02:21:59,900 --> 02:22:03,660
 Right, let's look at the basic structures of an RNN.

2160
02:22:03,660 --> 02:22:06,460
 So the basic structures of an RNN is like this, right?

2161
02:22:06,500 --> 02:22:09,619
 It has two forms, so this is a folded form.

2162
02:22:09,619 --> 02:22:13,500
 Okay, this is known as a folded or a roll form.

2163
02:22:13,500 --> 02:22:16,419
 This is known as unfolded or unroll form.

2164
02:22:16,419 --> 02:22:18,019
 You roll it up, okay?

2165
02:22:18,019 --> 02:22:21,019
 So if we look at this particular form here,

2166
02:22:21,019 --> 02:22:22,419
 what it means is that, okay?

2167
02:22:22,419 --> 02:22:24,539
 You'll have this X is our input,

2168
02:22:24,539 --> 02:22:27,140
 H is our hidden state of memory.

2169
02:22:27,140 --> 02:22:29,580
 This, in this case here, O is the output.

2170
02:22:29,580 --> 02:22:32,179
 So you can see your current memory

2171
02:22:32,179 --> 02:22:36,019
 depends on your current input and your past memory.

2172
02:22:36,020 --> 02:22:38,300
 And after once you've updated your memory,

2173
02:22:38,300 --> 02:22:40,460
 you can then use to generate the output.

2174
02:22:40,460 --> 02:22:45,460
 So how this particular hidden state output and memory

2175
02:22:46,300 --> 02:22:49,420
 is affected is through the matrix, okay?

2176
02:22:49,420 --> 02:22:51,380
 U, V, and W, right?

2177
02:22:51,380 --> 02:22:52,380
 So that's high level.

2178
02:22:52,380 --> 02:22:55,040
 Later on, we are going to make it more concrete

2179
02:22:55,040 --> 02:22:56,560
 and give some example.

2180
02:22:56,560 --> 02:23:00,140
 So this particular compact form,

2181
02:23:00,140 --> 02:23:02,980
 you can unroll it to become this.

2182
02:23:02,980 --> 02:23:05,540
 So if you unroll it, it's actually easier

2183
02:23:05,540 --> 02:23:06,860
 for you to see.

2184
02:23:06,860 --> 02:23:08,020
 For example, right?

2185
02:23:08,020 --> 02:23:10,860
 So this is your input at different time step,

2186
02:23:10,860 --> 02:23:13,620
 T minus one, T and T plus one.

2187
02:23:13,620 --> 02:23:15,500
 This is your hidden state of memory

2188
02:23:15,500 --> 02:23:18,660
 at time step T minus one, T and T plus one.

2189
02:23:18,660 --> 02:23:20,980
 This is your output at time step T minus one,

2190
02:23:20,980 --> 02:23:22,420
 T and T plus one.

2191
02:23:22,420 --> 02:23:24,660
 So if you look at this current memory

2192
02:23:24,660 --> 02:23:26,700
 at time step T plus one,

2193
02:23:26,700 --> 02:23:31,340
 this current memory depends on your current input

2194
02:23:31,340 --> 02:23:34,380
 and also depends on your past memory.

2195
02:23:35,259 --> 02:23:38,179
 And your past memory depends on,

2196
02:23:38,179 --> 02:23:42,179
 your past memory, HT depends on your input XT

2197
02:23:42,179 --> 02:23:45,300
 and your previous memory, HT minus one.

2198
02:23:45,300 --> 02:23:50,300
 And HT minus one depends on the input at XT minus one

2199
02:23:50,339 --> 02:23:52,019
 and so forth and so forth.

2200
02:23:52,019 --> 02:23:55,060
 So therefore you can see that this particular memory here,

2201
02:23:55,060 --> 02:24:00,060
 HT minus one, is a function of X of all the input.

2202
02:24:01,900 --> 02:24:03,460
 So in other words, if it's a function

2203
02:24:03,460 --> 02:24:05,699
 of all the input, that means it has memory.

2204
02:24:05,699 --> 02:24:10,699
 It has a memory capability of all the previous input

2205
02:24:10,699 --> 02:24:11,539
 in theory.

2206
02:24:12,900 --> 02:24:15,539
 So once you have this particular memory,

2207
02:24:15,539 --> 02:24:17,779
 then you can use it to generate the output.

2208
02:24:19,339 --> 02:24:20,179
 Right?

2209
02:24:20,179 --> 02:24:22,500
 So at high level, that's pretty much that.

2210
02:24:22,500 --> 02:24:25,740
 So we have the input, we have the output,

2211
02:24:25,740 --> 02:24:27,619
 we have the hidden state of the memory

2212
02:24:27,619 --> 02:24:29,699
 and this memory is called RNN.

2213
02:24:29,700 --> 02:24:33,500
 So this RNN have what is known as an internal state.

2214
02:24:33,500 --> 02:24:36,940
 So we can call it RNN, we can call it internal state,

2215
02:24:36,940 --> 02:24:39,700
 we can call it memory, we can call it hidden state.

2216
02:24:39,700 --> 02:24:42,620
 They're all more or less referred to the same thing.

2217
02:24:44,100 --> 02:24:46,940
 So next we are going to make it more concrete, right?

2218
02:24:46,940 --> 02:24:49,580
 And then see whether we're going to make it

2219
02:24:49,580 --> 02:24:52,060
 more concrete mathematically.

2220
02:24:52,060 --> 02:24:54,900
 So let's look at this particular modeling now.

2221
02:24:54,900 --> 02:24:58,300
 So okay, X is the input.

2222
02:24:58,300 --> 02:25:02,140
 RNN is your hidden state or internal memory

2223
02:25:03,019 --> 02:25:05,740
 or hidden state, this is your output Y here.

2224
02:25:05,740 --> 02:25:07,300
 So you can see that, right?

2225
02:25:07,300 --> 02:25:11,060
 Your current hidden state, okay?

2226
02:25:12,220 --> 02:25:15,580
 Right, your current hidden state or current memory

2227
02:25:15,580 --> 02:25:19,500
 is actually a function of your previous hidden state

2228
02:25:19,500 --> 02:25:23,179
 or previous memory and also the current input, okay?

2229
02:25:23,179 --> 02:25:25,939
 So your current memory at time step T

2230
02:25:25,940 --> 02:25:29,580
 is a function of previous memory at time step T minus one

2231
02:25:29,580 --> 02:25:32,260
 and your current input at time step T, right?

2232
02:25:32,260 --> 02:25:35,420
 It goes through some mathematical kind of mapping.

2233
02:25:35,420 --> 02:25:39,180
 So this mathematical mapping is a function of the view, right?

2234
02:25:39,180 --> 02:25:40,660
 Some weight parameter.

2235
02:25:40,660 --> 02:25:43,980
 So this is some high level understanding.

2236
02:25:43,980 --> 02:25:46,420
 Later on, as I mentioned, over time we're going to start

2237
02:25:46,420 --> 02:25:48,020
 to make it more and more concrete.

2238
02:25:49,180 --> 02:25:51,300
 So once you have updated your memory,

2239
02:25:51,300 --> 02:25:53,180
 suppose just now we updated our memory,

2240
02:25:53,180 --> 02:25:54,780
 if you want to generate the output,

2241
02:25:54,780 --> 02:25:56,380
 it's actually very simple.

2242
02:25:56,380 --> 02:25:58,780
 We take our current memory now,

2243
02:25:58,780 --> 02:26:00,820
 we let it go through a mapping

2244
02:26:00,820 --> 02:26:02,940
 to generate our current output

2245
02:26:02,940 --> 02:26:05,260
 and this particular mapping is a function

2246
02:26:05,260 --> 02:26:09,260
 of this matrix W, HY, yeah?

2247
02:26:09,260 --> 02:26:10,420
 So later on we're going to see

2248
02:26:10,420 --> 02:26:13,140
 what's this particular matrix W, HY.

2249
02:26:13,900 --> 02:26:18,900
 So next, this slide, we are going to make it more concrete now.

2250
02:26:20,060 --> 02:26:21,980
 So we can see that, okay?

2251
02:26:21,980 --> 02:26:23,900
 So early on we say that, right?

2252
02:26:23,900 --> 02:26:27,580
 Our current memory will depends on the past memory

2253
02:26:27,580 --> 02:26:30,340
 and your current input.

2254
02:26:30,340 --> 02:26:32,420
 Go through some mathematical operation

2255
02:26:32,420 --> 02:26:34,980
 to obtain your current memory.

2256
02:26:34,980 --> 02:26:36,740
 But how exactly is this?

2257
02:26:36,740 --> 02:26:38,859
 Because you can see that the current memory

2258
02:26:38,859 --> 02:26:41,380
 is a function of the current input.

2259
02:26:41,460 --> 02:26:44,220
 But how exactly is this being done here?

2260
02:26:44,220 --> 02:26:47,820
 So how is being done more concretely for the vanilla?

2261
02:26:47,820 --> 02:26:50,380
 There are many different versions of INN, yeah?

2262
02:26:50,380 --> 02:26:52,259
 So the first thing we are going to study

2263
02:26:52,259 --> 02:26:55,699
 is the vanilla INN on the most basic INN,

2264
02:26:55,699 --> 02:26:58,820
 which is also known as the Elmman INN here.

2265
02:26:58,820 --> 02:27:02,660
 Right, so for vanilla INN or Elmman INN,

2266
02:27:02,660 --> 02:27:06,179
 what we need to do is that HT, what is HT?

2267
02:27:06,179 --> 02:27:09,179
 HT is your current memory, okay?

2268
02:27:09,180 --> 02:27:11,540
 Current memory at time step T.

2269
02:27:11,540 --> 02:27:16,100
 So your current memory is equal to your previous memory,

2270
02:27:16,100 --> 02:27:18,540
 HT minus what is previous memory,

2271
02:27:18,540 --> 02:27:22,620
 you multiply with this particular update matrix,

2272
02:27:22,620 --> 02:27:24,380
 WHH.

2273
02:27:24,380 --> 02:27:27,620
 So you can see it's hidden state to hidden state, all right?

2274
02:27:27,620 --> 02:27:32,140
 So previous memory, you multiply with this WHH, right?

2275
02:27:32,140 --> 02:27:36,020
 Okay, and then afterwards you add up with your current input,

2276
02:27:36,020 --> 02:27:39,940
 which is your XT, input at time step T, right?

2277
02:27:39,940 --> 02:27:44,940
 And you multiply with this update matrix, WHHX.

2278
02:27:45,060 --> 02:27:47,860
 So from memory to hidden state, right?

2279
02:27:47,860 --> 02:27:49,140
 So afterwards you add them up,

2280
02:27:49,140 --> 02:27:52,100
 then you let it go through a nonlinear activation function,

2281
02:27:52,100 --> 02:27:53,700
 which is 10H.

2282
02:27:53,700 --> 02:27:55,620
 So once you let it go through a 10H,

2283
02:27:55,620 --> 02:27:59,300
 you will map it to a function between minus one to one.

2284
02:27:59,300 --> 02:28:01,420
 That is your new memory, okay?

2285
02:28:01,420 --> 02:28:03,380
 So therefore you can see your new memory

2286
02:28:03,380 --> 02:28:05,340
 can be obtained from your previous memory

2287
02:28:05,340 --> 02:28:08,860
 and create current input through this formula.

2288
02:28:08,860 --> 02:28:12,460
 And these are the matrix that need to be trained, okay?

2289
02:28:12,460 --> 02:28:14,820
 These are the matrix that need to be trained.

2290
02:28:14,820 --> 02:28:16,540
 When you say train and INN,

2291
02:28:16,540 --> 02:28:19,700
 these are the matrix that need to be trained, okay?

2292
02:28:19,700 --> 02:28:23,520
 And then once you obtain your memory,

2293
02:28:23,520 --> 02:28:24,780
 if you want to generate your output,

2294
02:28:24,780 --> 02:28:27,640
 you just take your updated current memory,

2295
02:28:27,640 --> 02:28:30,300
 multiply with this matrix, it will generate your output.

2296
02:28:30,300 --> 02:28:32,020
 So you can see this matrix notation

2297
02:28:32,020 --> 02:28:34,980
 is hidden state to output, why is output?

2298
02:28:35,020 --> 02:28:36,420
 So hidden state to output.

2299
02:28:36,420 --> 02:28:40,380
 So therefore you can see pretty much for vanilla INN,

2300
02:28:40,380 --> 02:28:45,500
 it can be simply reduced into this two simple equation, okay?

2301
02:28:45,500 --> 02:28:48,140
 So we can see now right from this equation,

2302
02:28:48,140 --> 02:28:51,260
 if you want to find the hidden state at time step one,

2303
02:28:51,260 --> 02:28:55,300
 you simply take your previous memory H naught,

2304
02:28:55,300 --> 02:28:57,300
 your current input X1.

2305
02:28:57,300 --> 02:28:59,600
 So this mathematical operation is defined

2306
02:28:59,600 --> 02:29:03,760
 by this equation here, you generate your next memory.

2307
02:29:03,760 --> 02:29:06,840
 So your memory at time step two

2308
02:29:06,840 --> 02:29:09,560
 is depending on previous time step H1

2309
02:29:09,560 --> 02:29:13,000
 plus the current input X2 going through this mapping here.

2310
02:29:13,000 --> 02:29:14,820
 This mapping is this equation here.

2311
02:29:14,820 --> 02:29:19,820
 Then you generate your memory at time step two

2312
02:29:20,240 --> 02:29:22,320
 and then the process just continue.

2313
02:29:22,320 --> 02:29:24,020
 So one thing to take note of is that

2314
02:29:24,020 --> 02:29:26,680
 for all this process here, you're using the,

2315
02:29:26,680 --> 02:29:27,680
 for all the time step,

2316
02:29:27,680 --> 02:29:31,640
 you're always using the same weight matrix, okay?

2317
02:29:34,360 --> 02:29:37,280
 Right, okay, so with that basic understanding,

2318
02:29:37,280 --> 02:29:42,280
 I think it's time that I can kind of share with you

2319
02:29:42,560 --> 02:29:44,960
 this particular exercise.

2320
02:29:44,960 --> 02:29:48,800
 So for this exercise here, it says that you have a vanilla

2321
02:29:48,800 --> 02:29:51,160
 INN that has a flowing starting.

2322
02:29:51,160 --> 02:29:54,120
 This is the initial hidden state, right?

2323
02:29:54,120 --> 02:29:57,040
 So the initial hidden state.

2324
02:29:57,040 --> 02:30:01,520
 So this is the hidden state weight matrix WHH, okay?

2325
02:30:01,560 --> 02:30:05,040
 So this is the input weight matrix WXH

2326
02:30:05,040 --> 02:30:09,200
 and this is the output weight matrix WHY, okay?

2327
02:30:09,200 --> 02:30:12,520
 And then assume no bias is used in the calculation.

2328
02:30:12,520 --> 02:30:15,360
 So you are told that you have a two-step input, right?

2329
02:30:15,360 --> 02:30:19,160
 So for this input, it only consists of simple two time step,

2330
02:30:19,160 --> 02:30:21,200
 right, X1 and X2.

2331
02:30:21,200 --> 02:30:24,360
 X1 is this factor and X2 is this factor.

2332
02:30:24,360 --> 02:30:27,360
 So number one, it asks you to find the hidden state H1

2333
02:30:27,360 --> 02:30:30,600
 at time step, he is equal to one, right?

2334
02:30:30,640 --> 02:30:33,920
 Okay, so I know this is the, today,

2335
02:30:33,920 --> 02:30:35,200
 if you have not studied before,

2336
02:30:35,200 --> 02:30:36,800
 this is only the first time you see

2337
02:30:36,800 --> 02:30:40,280
 and right away you're asked to solve this problem already.

2338
02:30:40,280 --> 02:30:42,840
 But this is also a good exercise for you to see

2339
02:30:42,840 --> 02:30:45,240
 whether you fully understand this equation.

2340
02:30:45,240 --> 02:30:46,680
 Because if you can solve this problem,

2341
02:30:46,680 --> 02:30:49,160
 that means you understand it, yeah?

2342
02:30:49,160 --> 02:30:51,880
 Okay, so I'll give you a few minutes to think about

2343
02:30:51,880 --> 02:30:53,720
 how to solve this part one

2344
02:30:53,720 --> 02:30:56,080
 and then we can go through the answers together.

2345
02:31:00,600 --> 02:31:01,440
 Okay.

2346
02:31:30,600 --> 02:31:31,440
 Okay.

2347
02:32:00,600 --> 02:32:01,440
 Okay.

2348
02:32:01,440 --> 02:32:02,280
 Okay.

2349
02:32:26,720 --> 02:32:30,720
 So actually it's just trying to apply this formula.

2350
02:32:30,720 --> 02:32:31,560
 Okay.

2351
02:32:32,800 --> 02:32:35,400
 And actually I already make all these decisions

2352
02:32:35,400 --> 02:32:36,400
 so straight forward,

2353
02:32:36,400 --> 02:32:37,840
 he's hasn't let the map,

2354
02:32:37,840 --> 02:32:39,320
 put it in and you will know.

2355
02:33:00,720 --> 02:33:01,560
 Okay.

2356
02:33:31,679 --> 02:33:36,679
 Okay, so yeah, in view of the time, right?

2357
02:33:36,679 --> 02:33:40,000
 Because I know that many of you once is after nine,

2358
02:33:40,000 --> 02:33:42,400
 your mind start to go somewhere else already.

2359
02:33:42,400 --> 02:33:45,039
 So let's quickly just look at this particular

2360
02:33:48,039 --> 02:33:48,880
 question seat.

2361
02:33:48,880 --> 02:33:50,439
 So essentially for this particular problem,

2362
02:33:50,439 --> 02:33:53,119
 it's nothing but applying the formula, right?

2363
02:33:53,119 --> 02:33:56,880
 So if I can memorize this, sorry, enlarge this.

2364
02:33:56,880 --> 02:33:59,119
 Right, so this is the first question.

2365
02:33:59,120 --> 02:34:00,320
 And enlarge this, right?

2366
02:34:00,320 --> 02:34:01,440
 So this is the equation.

2367
02:34:01,440 --> 02:34:05,040
 So it says that your memory at time step T

2368
02:34:05,040 --> 02:34:08,360
 is equal to your previous memory at time step T minus one,

2369
02:34:08,360 --> 02:34:10,500
 multiply with WXX,

2370
02:34:10,500 --> 02:34:15,000
 WHH matrix plus your current input at time step T.

2371
02:34:15,880 --> 02:34:19,720
 Okay, multiply with the update matrix WXH.

2372
02:34:19,720 --> 02:34:20,920
 And the whole thing you add it up,

2373
02:34:20,920 --> 02:34:23,320
 you let it go through a 10H function

2374
02:34:23,320 --> 02:34:27,560
 to obtain your current memory.

2375
02:34:27,560 --> 02:34:29,240
 And once you obtain your current memory,

2376
02:34:29,240 --> 02:34:32,160
 you can use it to generate your current output.

2377
02:34:32,160 --> 02:34:34,640
 So it's pretty much that, it's just this formula.

2378
02:34:34,640 --> 02:34:36,359
 So therefore the first thing you need to do

2379
02:34:36,359 --> 02:34:39,960
 is that when you want to find out the memory at time step one,

2380
02:34:39,960 --> 02:34:42,560
 that means we let T be equal to one.

2381
02:34:42,560 --> 02:34:46,080
 So anyway, I think let's try to maximize this.

2382
02:34:47,359 --> 02:34:49,320
 Yeah, so this formula is given

2383
02:34:49,320 --> 02:34:51,160
 from just now the lecture notes.

2384
02:34:51,160 --> 02:34:53,439
 Yeah, so at time step T is equal to one,

2385
02:34:53,440 --> 02:34:55,920
 H1, memory at time step one,

2386
02:34:55,920 --> 02:34:57,800
 is equal to memory at time step zero,

2387
02:34:57,800 --> 02:35:00,500
 multiply with WHH matrix,

2388
02:35:00,500 --> 02:35:04,760
 input at time steps one, multiply with this WHH matrix.

2389
02:35:04,760 --> 02:35:08,280
 So actually these informations are all given.

2390
02:35:08,280 --> 02:35:11,840
 So this X1 is the input at the first time step.

2391
02:35:11,840 --> 02:35:15,160
 Just plug it in, you'll get this.

2392
02:35:15,160 --> 02:35:18,880
 So this is the point you must remember, 10H function.

2393
02:35:18,880 --> 02:35:21,080
 If you do not know how to use a calculator

2394
02:35:21,080 --> 02:35:24,880
 to calculate this, please go back and thermize yourself.

2395
02:35:24,880 --> 02:35:26,920
 Almost all the calculator can calculate,

2396
02:35:26,920 --> 02:35:28,760
 but just some of you may not know.

2397
02:35:28,760 --> 02:35:31,640
 Try to thermize yourself to how to calculate

2398
02:35:31,640 --> 02:35:33,680
 this 10H function, because in the exam,

2399
02:35:33,680 --> 02:35:35,640
 if I ask this question, I will not provide you

2400
02:35:35,640 --> 02:35:37,080
 with the formula.

2401
02:35:38,000 --> 02:35:39,840
 So, okay, please try to do that.

2402
02:35:41,000 --> 02:35:44,960
 So suppose after you know how to calculate

2403
02:35:44,960 --> 02:35:47,400
 this 10H function, you see this is the output.

2404
02:35:48,240 --> 02:35:51,279
 We might as well just go through the next two parts.

2405
02:35:51,279 --> 02:35:53,520
 Actually it's not tricky, it's just,

2406
02:35:53,520 --> 02:35:56,680
 you just need to thermize yourself with the formula.

2407
02:36:02,000 --> 02:36:03,640
 So the next part is that,

2408
02:36:05,160 --> 02:36:09,640
 it asks you to find now the output at time step Y1.

2409
02:36:09,640 --> 02:36:11,600
 So the output at time step Y1 is,

2410
02:36:11,600 --> 02:36:13,160
 just now you already get X1.

2411
02:36:13,160 --> 02:36:16,119
 So X1 multiplied with W,

2412
02:36:17,160 --> 02:36:19,960
 HY will give you output already.

2413
02:36:19,960 --> 02:36:24,960
 So therefore, to calculate output at time step one,

2414
02:36:27,520 --> 02:36:29,280
 we simply use this formula.

2415
02:36:29,280 --> 02:36:30,760
 Memory at time step one,

2416
02:36:30,760 --> 02:36:33,520
 multiply with this output matrix,

2417
02:36:33,520 --> 02:36:35,640
 which is given from the fault.

2418
02:36:35,640 --> 02:36:37,400
 The question, you do some calculation,

2419
02:36:37,400 --> 02:36:41,160
 you get your output at time step one.

2420
02:36:41,160 --> 02:36:43,600
 Okay, so next part of the question,

2421
02:36:43,600 --> 02:36:48,440
 it asks you to find what is the output at time step two.

2422
02:36:48,440 --> 02:36:51,000
 So in order to find the output at time step two,

2423
02:36:51,000 --> 02:36:54,840
 you need to know the memory at time step two.

2424
02:36:54,840 --> 02:36:57,080
 In order to find the memory at time step two,

2425
02:36:57,080 --> 02:36:59,680
 you need the input from time step two,

2426
02:36:59,680 --> 02:37:01,640
 and the memory from time step one,

2427
02:37:01,640 --> 02:37:03,520
 that you have already obtained.

2428
02:37:03,520 --> 02:37:05,680
 So therefore, that means you can calculate

2429
02:37:05,680 --> 02:37:07,480
 your memory at time step two,

2430
02:37:07,480 --> 02:37:09,400
 and then afterwards, you can use to generate

2431
02:37:09,480 --> 02:37:11,320
 output at time step two.

2432
02:37:11,320 --> 02:37:12,720
 So it's just that.

2433
02:37:13,920 --> 02:37:15,359
 Okay, so let's do it.

2434
02:37:18,279 --> 02:37:20,680
 So therefore, same formula early on,

2435
02:37:20,680 --> 02:37:24,240
 if you want to find the memory at time step two,

2436
02:37:24,240 --> 02:37:26,560
 so memory at time step two is actually memory

2437
02:37:26,560 --> 02:37:29,320
 at time step one, multiply with WHH,

2438
02:37:29,320 --> 02:37:33,199
 plus input at time step two, multiply with WXH.

2439
02:37:33,199 --> 02:37:36,119
 Holding, go through 10H function.

2440
02:37:36,119 --> 02:37:39,359
 So these two are given from the question,

2441
02:37:39,360 --> 02:37:42,280
 the second time step input is also given from the question.

2442
02:37:42,280 --> 02:37:44,320
 H one is the memory that you obtain

2443
02:37:44,320 --> 02:37:47,560
 from earlier times, earlier part.

2444
02:37:47,560 --> 02:37:50,880
 Plug it in, go through 10H function, you get the result.

2445
02:37:50,880 --> 02:37:53,360
 So this is your memory at time step two.

2446
02:37:53,360 --> 02:37:55,320
 So if you want to find the output at time step two,

2447
02:37:55,320 --> 02:37:58,040
 just apply this formula to some calculation,

2448
02:37:58,040 --> 02:37:59,680
 you'll see this is the output.

2449
02:38:01,560 --> 02:38:02,440
 All right, so.

2450
02:38:03,720 --> 02:38:06,200
 Yeah, so anyway, as usual,

2451
02:38:06,360 --> 02:38:11,360
 this, the solution I will share,

2452
02:38:11,800 --> 02:38:13,600
 upload to the course slide shortly.

2453
02:38:13,600 --> 02:38:15,400
 Okay, I think this is the last slide.

2454
02:38:15,400 --> 02:38:19,000
 Okay, so for INN, what's the advantages and disadvantages?

2455
02:38:19,000 --> 02:38:21,880
 So the advantages of INN is that theoretically,

2456
02:38:21,880 --> 02:38:24,720
 you can handle sequence of any length.

2457
02:38:24,720 --> 02:38:26,560
 It can be as long as you want.

2458
02:38:26,560 --> 02:38:31,560
 But theoretically, the computation for time step T

2459
02:38:32,160 --> 02:38:34,160
 can in theory make use of information

2460
02:38:34,160 --> 02:38:35,720
 from earlier times that.

2461
02:38:35,720 --> 02:38:38,439
 This thing I've already mentioned quite a number of times.

2462
02:38:38,439 --> 02:38:40,840
 Your memory at time step T,

2463
02:38:40,840 --> 02:38:43,880
 would depends on your current input and previous memory.

2464
02:38:43,880 --> 02:38:46,599
 Previous memory depends on previous input.

2465
02:38:46,599 --> 02:38:50,000
 Previous memory, okay, so I probably said too many times.

2466
02:38:50,000 --> 02:38:51,599
 That means your current input,

2467
02:38:51,599 --> 02:38:54,279
 current memory would actually theoretically

2468
02:38:54,279 --> 02:38:59,279
 depends on many, all the inputs in the past.

2469
02:39:00,359 --> 02:39:03,519
 Okay, another thing is this model size does not change.

2470
02:39:03,520 --> 02:39:06,160
 So you can see from the formula, right?

2471
02:39:06,160 --> 02:39:09,400
 For this INN model, the only parameter you need to compute

2472
02:39:09,400 --> 02:39:13,240
 is WHH, WXH, and WHY, right?

2473
02:39:13,240 --> 02:39:17,280
 No matter how long your model is, it's only this parameter.

2474
02:39:18,760 --> 02:39:21,920
 So the model size does not change for longer sequence, right?

2475
02:39:21,920 --> 02:39:24,680
 The same weight is applied at every time step.

2476
02:39:24,680 --> 02:39:27,680
 So there's consistency and input how it's being processed.

2477
02:39:27,680 --> 02:39:30,000
 Just now from the calculation, you can see,

2478
02:39:30,000 --> 02:39:33,240
 we use the same matrix repeatedly, yeah?

2479
02:39:33,240 --> 02:39:34,720
 So what is the disadvantage?

2480
02:39:34,720 --> 02:39:37,520
 The disadvantage is recurrent computation is slow.

2481
02:39:37,520 --> 02:39:39,760
 Suppose if your sequence is very long,

2482
02:39:39,760 --> 02:39:41,560
 if you want to calculate the output

2483
02:39:41,560 --> 02:39:44,320
 after a sequence of 1,000 time steps,

2484
02:39:44,320 --> 02:39:46,000
 you need to start from the beginning,

2485
02:39:46,000 --> 02:39:50,600
 do all the calculation to find the memory at time step 1,000

2486
02:39:50,600 --> 02:39:52,039
 before you can generate output,

2487
02:39:52,039 --> 02:39:54,320
 and this obviously is very slow.

2488
02:39:54,320 --> 02:39:55,840
 So therefore it's slow.

2489
02:39:55,840 --> 02:39:58,160
 And in practice, it's difficult to leverage information

2490
02:39:58,160 --> 02:39:59,440
 from any time step back.

2491
02:39:59,440 --> 02:40:02,480
 Even though we say that current memory,

2492
02:40:02,480 --> 02:40:07,320
 it will depends on current input as well as the pass input.

2493
02:40:07,320 --> 02:40:11,199
 But in practice, those input in the pass,

2494
02:40:11,199 --> 02:40:13,480
 you can hardly leverage them, right?

2495
02:40:13,480 --> 02:40:16,439
 So therefore it's very difficult to make use of information

2496
02:40:16,439 --> 02:40:19,760
 from many times step back, right?

2497
02:40:19,760 --> 02:40:22,080
 Okay, so probably just one last slide.

2498
02:40:24,920 --> 02:40:27,480
 Don't worry, it's not a music or video,

2499
02:40:27,640 --> 02:40:29,000
 some of you are...

2500
02:40:41,640 --> 02:40:44,560
 Yeah, so some of you probably already heard about

2501
02:40:44,560 --> 02:40:48,160
 this year's Nobel Prize, if you still do not know, right?

2502
02:40:48,160 --> 02:40:50,560
 Probably I will just share this information with you.

2503
02:40:50,560 --> 02:40:54,279
 So this year's Nobel Prize for Physics

2504
02:40:54,279 --> 02:40:56,039
 is awarded to someone in AI.

2505
02:40:56,040 --> 02:41:00,440
 I think Jeff Hinton, I'm not sure whether I mentioned to him,

2506
02:41:00,440 --> 02:41:01,600
 mentioned his name.

2507
02:41:01,600 --> 02:41:04,320
 When I was a student, we studied about this Hopfield.

2508
02:41:04,320 --> 02:41:08,120
 Hopfield Network, we studied a lot when I was a student.

2509
02:41:08,120 --> 02:41:10,720
 And we do not know Jeff Hinton.

2510
02:41:10,720 --> 02:41:14,600
 Jeff Hinton is after when we studied in recent years, right?

2511
02:41:14,600 --> 02:41:17,400
 After the, just now we talked about AlexNet.

2512
02:41:17,400 --> 02:41:20,240
 So he's actually a pioneer now.

2513
02:41:20,240 --> 02:41:23,160
 Actually, he has been working for this for a long time.

2514
02:41:23,160 --> 02:41:25,600
 So anyway, both of them has been awarded

2515
02:41:25,640 --> 02:41:27,480
 the Nobel Prize in Physics.

2516
02:41:27,480 --> 02:41:30,360
 So what it means is that now the opportunity is open

2517
02:41:30,360 --> 02:41:32,400
 to you all in AI area.

2518
02:41:32,400 --> 02:41:35,400
 If you try hard, maybe you'll be the Nobel Prize winner.

2519
02:41:36,480 --> 02:41:38,560
 Okay, all right, so we'll see you.

2520
02:41:38,560 --> 02:41:42,720
 Okay, so last reminder, our next lecture is this Saturday,

2521
02:41:42,720 --> 02:41:44,760
 right, Zoom lecture.

2522
02:41:44,760 --> 02:41:47,640
 Next week, there's no lecture because I'll be away.

2523
02:41:47,640 --> 02:41:48,960
 Okay, I'll see you.

2524
02:41:50,600 --> 02:41:51,440
 Thank you.

2525
02:41:55,600 --> 02:41:57,280
 Thank you.

2526
02:42:25,600 --> 02:42:26,440
 Thank you.

2527
02:42:55,600 --> 02:42:56,440
 Thank you.

2528
02:43:25,600 --> 02:43:26,440
 Thank you.

2529
02:43:55,600 --> 02:43:56,440
 Thank you.

2530
02:44:25,600 --> 02:44:26,440
 Thank you.

2531
02:44:55,600 --> 02:44:56,440
 Thank you.

2532
02:45:25,600 --> 02:45:26,440
 Thank you.

2533
02:45:55,600 --> 02:45:56,440
 Thank you.

2534
02:46:25,600 --> 02:46:26,440
 Thank you.

2535
02:46:55,600 --> 02:46:56,440
 Thank you.

2536
02:47:25,600 --> 02:47:26,440
 Thank you.

2537
02:47:55,600 --> 02:47:56,440
 Thank you.

2538
02:48:25,600 --> 02:48:26,440
 Thank you.

2539
02:48:55,600 --> 02:48:56,440
 Thank you.

2540
02:49:25,600 --> 02:49:26,440
 Thank you.

2541
02:49:55,600 --> 02:49:56,440
 Thank you.

2542
02:50:25,600 --> 02:50:26,440
 Thank you.

2543
02:50:55,600 --> 02:50:56,440
 Thank you.

2544
02:51:25,600 --> 02:51:26,440
 Thank you.

2545
02:51:55,600 --> 02:51:56,440
 Thank you.

2546
02:52:25,600 --> 02:52:26,440
 Thank you.

2547
02:52:55,600 --> 02:52:56,440
 Thank you.

2548
02:53:25,600 --> 02:53:26,440
 Thank you.

2549
02:53:55,600 --> 02:53:56,440
 Thank you.

2550
02:54:25,600 --> 02:54:26,440
 Thank you.

2551
02:54:55,600 --> 02:54:56,440
 Thank you.

2552
02:55:25,600 --> 02:55:26,440
 Thank you.

2553
02:55:55,600 --> 02:55:56,440
 Thank you.

2554
02:56:25,600 --> 02:56:26,440
 Thank you.

2555
02:56:55,600 --> 02:56:56,440
 Thank you.

2556
02:57:25,600 --> 02:57:26,440
 Thank you.

2557
02:57:55,600 --> 02:57:56,440
 Thank you.

2558
02:58:25,600 --> 02:58:26,440
 Thank you.

2559
02:58:55,600 --> 02:58:56,440
 Thank you.

2560
02:59:25,600 --> 02:59:26,440
 Thank you.

